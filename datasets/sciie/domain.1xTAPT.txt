. Illustration of the confidence sets.
optimizing variable and ξ is the unknown parameter, distributionally robust optimization solves max x∈X [inf μ∈C E ξ∼μ u(x, ξ)], where C is an a priori known set of distributions.
We highlight our contributions by comparing with [1] . In [1] the state-wise ambiguity set is restricted to the following form:C s = {μ s |μ s (O i s ) ≥ α i s ∀ i = 1, . . . , n s }, where α i s ≤ α j s and O i s is a proper set of uncertain parameters with a "nested-set" structure, i.e., satisfying O i s ⊆ O j s , for all i < j [see Fig. 1(a) ]. This setup can effectively model distributions with a single mode (such as a Gaussian distribution), but less so when modeling multi-mode distributions such as a mixture Gaussian distribution. Moreover, other probabilistic information such as mean, variance etc. cannot be incorporated. Thus, in this technical note, we extend the distributionally robust MDP approach to handle ambiguity sets with more general structures. In particular, we consider a class of ambiguity sets, first proposed in [18] as a unifying framework for modeling and solving distributionally robust single-stage optimization problems, and embed them into the distributionally robust MDPs setup. These ambiguity sets are considerably more general: they are characterized by a class of O i s which can either be nested or disjoint [as shown in Fig. 1(b) ], and moreover, additional linear constraints are allowed to define the ambiguity set, which can be used to incorporate probabilistic information such as mean, covariance or other variation measures. We show that, under this more general class of ambiguity sets, the resulting distributionally robust MDPs remain tractable under mild technical conditions, and often outperform previous methods thanks to the fact that it can model uncertainty in a more flexible way.
Throughout the technical note, we use capital letters to denote matrices, and bold face letters to denote column vectors. We use e i (m) to denote the ith elementary vector of length m, and use R n + to denote the nonnegative orthant of R n . If C is the set of joint probability distributions of three random vectors a, b, and c, then (a,b) C denotes the set of marginal distributions of (a, b). We use ⊕ to represent mixture distribution: given two probability distributions F 1 , F 2 and a Bernoulli random variable x which takes value 1 w.p. p, xF 1 ⊕ (1 − x)F 2 is a random variable such that it follows distribution F 1 w.p. p, and follows F 2 w.p. 1 − p. We use N (m, σ 2 ) to represent a Gaussian distribution with mean m and variance σ 2 .
A (finite) Markov Decision Process (MDP) is defined as a 6-tuple T, γ, S, A, p, r . Here, T is the (possibly infinite) decision horizon; 0018-9286 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
γ ∈ (0, 1] is the discount factor; S is the state set and A s is the action set of state s ∈ S, both assumed to be finite. The parameter p and r are the transition probability and the expected reward, respectively. That is, for s ∈ S and a ∈ A s , r(s, a) is the expected reward and p(s |s, a) is the probability that the next state is s . Following [2] , we denote the set of all history-dependent randomized strategies by Π HR . We use subscript s to denote the value associated with the state s: e.g., r s denotes the vector form of the rewards associated with the state s, and π s is the (randomized) action chosen at state s for strategy π.
The elements in the vector p s are listed in the following way: the transition probabilities of the same action are arranged in the same block, and inside each block they are listed according to the order of the next state. We use s to denote the (random) state following s, and Δ(s) to denote the probability simplex on A s . We use to represent Cartesian product, e.g., p = s∈S p s . For a given strategy π ∈ Π HR , we denote the expected (discounted) total-reward under parameters pair (p, r) as u(π, p, r)
A Distributionally Ambiguous MDP (DAMDP) is defined as a tuple T, γ, S, A,C S , where the transition probability p and the expected reward r are unknown. Instead, they are assumed to obey a joint distribution μ 0 (also unknown) that belongs to a known ambiguity set
While the DAMDP framework can be very general, mostC S result in formulations that are computationally intractable (e.g., [1] , [19] ). Hence, we make the following requirement ofC S such that the parameters among different states are independent.
Assumption 1: The ambiguity setC S has the following property:
where "state-wise ambiguity set"C s is a set of distributions of parameters of state s. By the definition ofC S , the state-wise property applies to C S as well. This property is the same as the concept of "s-rectangularity" in [16] , and is essential for reducing DAMDP to robust MDP in Lemma 1. In addition, [20] showed that the robust MDP with coupled uncertainty sets is computationally challenging, which implies solving DAMDP with nonrectangular ambiguity sets is even harder.
We now discuss the admissible state-wise ambiguity set. Our formulation of the state-wise ambiguity set follows the unifying framework of [18] . In specific, given s ∈ S, the state-wise ambiguity set is representable with the following standard form:
are the lower and upper bounds of the probability that parameters belong to a confidence set. Thus, each confidence set O i s provides an estimation of the uncertain parameters pair (p s , r s ,ũ s ) subject to a different confidence level. Ambiguity setsC s contain prescribed conic representable confidence sets and mean values residing on an affine manifold, which is rich enough to encompass and extend several ambiguity sets considered in recent literature (e.g., [1] , [19] , [21] ). The set of joint distribution of (p s , r s ) is hence C s Δ = (ps ,rs)C s . Notice that a classical technique called "lifting" is used here: We introduce an auxiliary random vectorũ, so that some non-linear relationship can be modeled linearly. For example, a constraint on the variance can be modeled using this standard form (see [22, Example 2] ), which is otherwise impossible without the auxiliary variable. This lifting technique thus allows us to model a rich variety of structural information about the marginal distribution of (p, r) in a unified manner. Note when the ambiguity set only contains the support of random variables, i.e.,
where the a-priori information of unknown parameters is that they belong to an uncertainty set.
Assumptions 2 to 4 are standard requirements for the confidence sets, proposed in [18] . The first one asserts the relationship between different confidence sets.
The nesting condition is illustrated in Fig. 1(b) . Next, for any s ∈ S we require thatC s satisfies the following regularity condition.
1) The confidence set O ns s is bounded and has probability one, that is, 
s are proper cones (i.e., a closed, convex and pointed cone with nonempty interior).
This section focuses on DAMDP with a finite number of decision stages. We show that a strategy defined through backward induction, which we call S-robust strategy, is distributionally robust. We further show such a strategy is solvable in polynomial time under mild technical conditions. This generalizes results in [1] to a significantly more general class of ambiguity sets.
Similar to [10] , we assume that when a state is visited multiple times, each time it can take a different parameter realization (nonstationary model). This assumption is justified mainly because the stationary model is generally intractable and a lower-bound of it is given by the non-stationary model. Therefore, multiple visits to a state can be treated as visiting different states. By introducing dummy states as in [1, Assumption 2.2], for finite horizon DAMDP we make the following assumption without loss of generality. This will simplify our exposition.
Assumption 5: 1) Each state belongs to only one stage.
2) The terminal reward equals zero.
3) The first stage only contains one state s ini .
Using the condition 1 of Assumption 5, we partition S according to the stage each state belongs to. That is, we let S t be the set of states belong to tth stage.
For π ∈ Π HR and μ ∈ C S , we denote the expected performance of a DAMDP as w π, μ, (s ini ) Δ = E (p,r)∼μ {u(π, p, r)} = u(π, p, r)dμ(p, r).
In words, each strategy is evaluated by its expected performance under the (respective) most adversarial distribution of the uncertain parameters, and a distributionally robust strategy is the optimal strategy according to this metric. The main focus of this section is deriving approaches to solve the distributionally robust strategy. To this end, we need the following definition.
Definition 2: Given a DAMDP T, γ, S, A,C S , we define the Srobust strategy as follows
3) A strategyπ * is a Srobust strategy if ∀ s ∈ S, and every history h that ends at s, we haveπ * s , conditioned on history h, is a S-robust action.
The definition requires that the strategy must be robust w.r.t. each sub-problem, and hence the name "S-robust." The following theorem shows any S-robust strategy π * is distributionally robust, and is the main result of this technical note.
Theorem 1: Let T < ∞. Under Assumptions 1, 2, 4, and 5, if π * is a S-robust strategy, then 1) π * is a distributionally robust strategy with respect to C S . 2) There exists μ * ∈ C s such that (π * , μ * ) is a saddle point. That is
Proof: We first state a Lemma from [1, Lemma 3.2] without proof.
Lemma 1: Under Assumption 1, fix π ∈ Π HR and μ ∈ C S , denote p = E μ (p) and r = E μ (r). We have w(π, μ, (s ini )) = u(π, p, r).
Lemma 1 means for any strategy, the expected performance under an admissible distribution μ only depends on the expected value of parameters under μ. Thus, the distributionally robust MDPs reduce to robust MDPs. Next we characterize the set of expected value of the parameters.
Lemma 2: For s ∈ S and π s ∈ Δ(s), we define the set Z s = {E μs (p s , r s )|μ s ∈ C s }. Then set Z s is convex and compact.
Proof: First, we show that, for s ∈ S and π s ∈ Δ(s), the set defined asZ s = {E μs (p s , r s ,ũ s )|μ s ∈C s } is convex and compact. The convexity can be easily shown, which is omitted due to space constraints (see [22] for details). To show the compactness, notice thatC s is weakly closed (i.e., closed w.r.t. to the weak topology) since the feasible set of each of constraint is weakly closed which implies their intersection is also weakly closed. Thus,Z s is closed since it is the image ofC s under expectation (which is a continuous function). This impliesZ s is compact since O ns s is bounded and henceZ s is bounded. Finally, since Z s is the projection onto the first two coordinates of set Z s , its convexity and compactness thus follow.
Lemma 2 implies that, for s ∈ S and π s ∈ Δ(s), there exists (p * s , r * s ) ∈ Z s that satisfies inf (ps,rs)∈Zs u(π s , p s , r s ) = u(π s , p * s , r * s ). Since saddle point of the minimax objective exists for robust MDPs (e.g., [10] , [11] ), we can complete the proof of part 2) following a similar procedure as the last portion of proof for [1, Theorem 3.1]. We omit the details due to space constraint (see [22] for details). Part 1) then follows part 2) immediately.
We now investigate the computational aspect of finding the S-robust action.
Theorem 2: Under Assumption 2, 3, 4, and 5, for s ∈ S t where t < T , the S-robust action is the optimal solution of the following optimization problem (termed Srobust problem hereafter): Proof: The proof essentially follows from [18] and duality of convex optimization [23] , and can be found in the longer version [22] of this technical note.
Thus, since for s ∈ S t , Δ(s) is compact, we can solve the S-robust action in polynomial time if all K i s are "easy" cones such as linear, conic quadratic or semidefinite cones. Moreover, using Theorem 1, by backward induction, we can obtain the S-robust strategy efficiently.
By virtue of the lifting technique [18, Theorem 5], we show below several widely used ambiguity sets are indeed special cases ofC s defined in (1) . We further derive their corresponding S-robust problems. See [22] for additional examples (variance and expected Huber loss function). 
This example can also be treated via "classical" robust optimization by virtue of Lemma 1.
The finite horizon DAMDP can be easily extended to discountedreward infinite horizon setup. We can generalize the notion of S-robust strategy, which turns to be distributionally robust in both stationary and non-stationary models. This extension is similar to [1] and can be found in [22] .
In this section, we study two synthetic numerical examples: a machine replacement problem and a path planning problem. In the machine replacement problem, the reward parameters are uncertain; whereas in the path planning problem, the transition probabilities are uncertain. All results were generated on desktop with Intel Core i5-3570 CPU of 3.40 GHz clock speed and 8 GB RAM. The S-robust problems are solved in Matlab using the CVX package [24] .
We consider a machine replacement problem similar to the one in [12] . Consider the repair cost incurred by a factory that holds a large number of machines, given that each of these machines is modeled with a same underlying MDP for which rewards are subject to uncertainty.
We first consider a machine replacement problem with 50 states, 2 actions ("repair" and "not repair") for each state, deterministic transitions, a discount factor of 0.8, and uncertain rewards following Gaussian distributions independently [see Fig. 2(a) ]: For the first 48 states, the "repair" action has a cost N (130, 1) . The 49th and 50th states of the machine's life are designed to be risky: not repairing at state 50 incurs a highly uncertain cost N (100, 800), while repairing at both states is a more secure but still uncertain option with a cost N (130, 10) . The detailed implementation is as follows: We use the mean value of uncertain rewards to compute the nominal strategy. For both robust and distributionally robust strategy, we construct confidence sets usinĝ m ± 3σ for the first 49 states, andm ± 4σ for state 50 wherem andσ 2 are mean and variance estimated from samples (see [22] for details), as it is more risky and thus hard to estimate. In addition, we construct an extra confidence set (centered at the mean) with 60%-70% confidence level (i.e., α 1 50 = 0.6, α 1 50 = 0.7) for distributionally robust strategy. The optimal paths followed by three strategies are shown in Fig. 2(a) .
The performance of the strategies obtained by using the nominal, the robust and the distributionally robust approaches is presented in Fig. 3 . The corresponding average total discounted rewards and computational times are shown in Table I . The nominal strategy results in the highest average total discounted rewards. This is well expected as we are using the exact mean value of the reward as the nominal Fig. 2 . Two instances of a machine replacement problem. Fig. 2(a) shows Gaussian uncertainty in the rewards, while Fig. 2(b) shows mixed Gaussian uncertainty in the rewards. parameter. However, the nominal strategy is highly risky: it cannot prevent bad performance (e.g., −0.025) from happening, which is undesirable. While the nominal strategy, blind to any form of risk, finds no advantage in ever repairing, the robust strategy ends up following a highly conservative policy (repairing the machine at state 49 to avoid state 50). In contrast, the distributionally robust optimal strategy makes use of more distributional information and handles the risk efficiently by waiting until state 50 and then repair the machine. Therefore, this strategy beats the nominal and robust strategies in that it strikes a good tradeoff between high mean reward and low variance over 10,000 different trials. These results coincide with what one would typically expect from the three solution concepts. Fig. 4 . Illustration of the confidence sets for two distributionally robust strategies.
The second experiment has a similar setup as the previous one, except that not repairing at the 50th state has a reward which follows a mixed Gaussian distribution [see Fig. 2(b) ]. This experiment illustrates the effect of the two different nested-set structures shown in Fig. 1 . In specific, we apply the two different distributionally robust approaches (proposed in [1] and this technical note respectively), and show that our method outperforms. The detailed implementation is as follows: For the robust and two distributionally robust strategies, we construct uncertainty set corresponding to 99% probability support of the rewards for the first 49 states, and 99.9% for the 50th state that is more risky, using estimated mean and variance (see [22] for details). For the first distributionally robust strategy proposed in [1] , we construct two additional nested confidence sets O 1 50 and O 2 50 [see Fig. 4(a) ], which w.p. 40%-50% and 60%-70% respectively the uncertain rewards belong to. In contrast, for the second distributionally robust strategy proposed in this technical note, we construct two disjoint confidence sets O 1 50 and O 2 50 [see Fig. 4 (b)] with 70%-80% and 0%-10% confidence level, respectively. Specifically, we select these two intervals around the peaks of the two Gaussian elements [i.e., N (100, 10) and N (140, 2)] to better model this mixed distribution. The optimal paths followed for the three strategies are shown in Fig. 2(b) .
The performance of the three strategies obtained is presented in Fig. 5 . The corresponding average total discounted rewards and computational times are shown in Table II . As expected, the robust strategy ends up following a highly conservative policy repairing the machine at state 49 to avoid state 50. The first distributionally robust strategy, not modeling the mixture Gaussian distribution well, finds it advantageous to repair at the 50th state. In contrast, capable of capturing the distribution information in a more flexible way, the second distributionally robust strategy better models the uncertainty and finds not repairing the machine at state 50 is optimal. The performance comparison clearly shows the second distributionally robust strategy is more desirable, which highlights the distributionally robust approach with general structure of confidence sets can be beneficial in practice.
We remark that, in practice, one can obtain the modality structure of uncertain parameters in a data-driven way by applying clustering algorithms to an initial primitive data set. For example, one may check the histogram of historical observations. If the data concentrates on several distinct and disjoint bins, our multi-model DAMDP approach can be applied. Moreover, we note that networked control systems (NCSs) have recently emerged as a topic of significant interest in the control community. A typical application of NCSs is in modern [25] and [26] proposed a novel two-layer structure to solve the setpoints compensation problem for industrial processes under network-based environment.
We now consider a path planning problem, similar to the one presented in [1] : an agent wants to exit a 4 × 21 maze [shown in Fig. 6(a) ] using the least possible time. Starting from the upper-left corner, the agent can move up, down, left and right, but can only exit the grid at the lower-right corner. Here, a white box stands for a normal place where the agent needs one time unit to pass through. A shaded box represents a "shaky" place: if an agent reaches a "shaky" place, then he may risk jumping to the starting point ("reboot"). The true transition probability of the jump follows a distribution
The four approaches are implemented as follows: The nominal approach neglects this random jump. The robust approach takes a worst-case analysis, i.e., it assumes that with 30%, the whole probability support of transition, the agent will jump to the spot with the highest costto-go. The first distributionally robust approach takes into account an additional information by using two nested confidence sets: the jump probability parameter belonging to 9%-11% is of a confidence 1 − λ. The second distributionally robust approach, which is proposed in this technical note, incorporates more information. In specific, we construct an extra confidence interval disjoint with the above 9%-11% interval. It states that the chance of jumping with probability 20% is λ.
The performance of strategies of the nominal, the robust and the two distributionally robust approaches is shown in Fig. 6(b) , where the error bars show the standard error of the expected time to exit. The CPU times of computing optimal policies for four strategies are 0.461, 549, 642, and 654 seconds, respectively. The second distributionally robust approach achieves the best performance over virtually the whole spectrum of λ. This is well expected, since additional probabilistic Fig. 6(a) illustrates the maze for the path plawnning problem. Fig. 6(b) shows the performance comparisons between nominal, robust and two distributionally robust strategies over 3,000 runs of the path planning problem.
information is available to and incorporated by the second distributionally robust approach which considers ambiguity sets with more general structures.
In this technical note, we considered Markov decision problems with uncertainty. Specifically, we generalized the distributionally robust approach proposed in [1] to incorporate more general ambiguity sets proposed in [18] to model a-priori probabilistic information of the uncertain parameters. We proposed a way to compute the distributionally robust strategy through a Bellman type backward induction. We showed that the strategy, which achieves maximum expected utility under the worst admissible distributions of uncertain parameters, can be solved in polynomial time under some mild technical conditions. We believe that many important problems that are usually addressed using standard MDP models could be revisited and better resolved using the proposed models when parameter uncertainty exists, as this formulation naturally enables the decision maker to account for more general parameter uncertainty.
The ability to explore unknown spaces independently, safely and efficiently is a combined product of motor, sensory, and cognitive skills. Normal exercise of this ability directly affects an individual's quality of life. Mental mapping of spaces and of the possible paths for navigating these spaces is essential for the development of efficient orientation and mobility (O&M) skills. Most of the information required for this mental mapping is gathered through the visual channel (Lynch, 1960) . People who are blind lack this information, and in consequence they are required to use compensatory sensorial channels and alternative exploration methods (Jacobson, 1993) . This research is based on the assumption that the supply of appropriate spatial information through compensatory sensorial channels, as an alternative to the (impaired) visual channel, may help to enhance the ability of people who are blind to explore unknown environments (Mioduser, in press) .
The research on the exploration process of known and unknown spaces by people who are blind refers to the use of both low and high technologies. These technologies serve as alternative sensorial or cognitive channels to the impaired visual channel. There are two types of informationtechnology devices: (a) passive devices -providing the user with information before her/his arrival to the environment (e.g., verbal description, tactile maps and physical models) and (b) dynamic devices -providing the user with information in-situ (e.g., Sonicguide, Kaspa, Talking Signs and Personal Guidance System). Ungar, Blades and Spencer, (1996) report on differences in exploration performance of people who are blind using various technologies (e.g., verbal description, tactile maps and physical models). Warren and Strelow (1985) studied the use of the Sonic-guide device and Easton and Bentzen (1999) focused on the users' ability to navigate using the Kaspa laser-guided device. Additional examples of O&M support under study are the talking signs embedded in the environment (Crandall, Bentzen, Myers, & Mitchell, 1995) , and the global positioning system (GPS), based on satellite communication (Golledge, Klatzky, & Loomis, 1996) .
Research on mobility in known and unknown spaces by people who are blind (Golledge, Klatzky, & Loomis, 1996; Ungar, Blades, & Spencer, 1996) , indicates that support for the acquisition of spatial mapping and orientation skills should be supplied at two main levels, perceptual and conceptual. At the perceptual level, hearing, smell, and touch are powerful information suppliers about known as well as unknown spaces. The auditory channel supplies essential information about events, or the presence of other people (or machines or animals) in the environment. In indoor spaces
ORLY LAHAV DAVID MIODUSER Tel Aviv University, School of Education Exploration of unknown spaces is essential for the development of efficient orientation and mobility skills. Most of the information required for the exploration is gathered through the visual channel. People who are blind lack this crucial information, facing in consequence difficulties in mapping as well as navigating spaces. This study is based on the assumption that the supply of appropriate spatial information through compensatory sensorial channels may contribute to the spatial performance of people who are blind. The main goals of this study were (a) the development of a haptic virtual environment enabling people who are blind to explore unknown spaces and (b) the study of the exploration process of these spaces by people who are blind. Participants were 31 people who are blind: 21 in the experimental group exploring a new space using a multi-sensory virtual environment, and 10 in the control group directly exploring the real new space. The results of the study showed that the participants in the experimental group mastered the navigation of the unknown virtual space in a short time. Significant differences were found concerning the use of exploration strategies, methods, and processes by participants working with the multi-sensory virtual environment, in comparison with those working in the real space.
people who are blind can use echo feedback (i.e., by whistling, clapping hands, or talking) to estimate distances (Hill, Rieser, Hill, Hill, Halpin & Halpin, 1993) . The smell channel supplies additional information about particular situations (e.g., perfumery, bookstore, or bakery in a shopping center) or about people. Haptic information appears to be of great potential for supporting appropriate spatial performance. Fritz, Way, and Barner (1996) define haptics as encompassing touch along with kinesthetic information, or a sense of position, motion, or force. For people who are blind, haptic information is commonly supplied by the cane for lowresolution scanning of the immediate surroundings, by palms and fingers for fine recognition of objects form, texture and location, and by the feet regarding surface information.
As for the conceptual level, the focus is on supporting the development of appropriate strategies for the efficient exploration of the space and the generation of efficient navigation paths. For example, Jacobson (1993) , described indoor environment familiarization process by people who are blind as one that starts with the use of a perimeterrecognition-tactic -walking along the room's walls and exploring objects attached to the walls, followed by a gridscanning tactic, aiming to explore the room's interior.
Advanced computer technology offers new possibilities for supporting acquisition of orientation and mobility (O&M) skills by people who are blind, and the development of alternative navigation strategies, at both the perceptual and conceptual levels. Current virtual reality (VR) technology facilitates the development of rich virtual models of physical environments and objects to be manipulated, offering people who are blind the possibility to undergo learning or rehabilitation processes without the usual constraints of time, space, and a massive demand of human tutoring (Loomis, Klatzky & Golledge, 2001; Schultheis & Rizzo, 2001; Standen, Brown & Cromby, 2001) . Research on the implementation of haptic technologies within VR spatial simulation environments reports on its potential for supporting rehabilitation training with sighted people (Darken & Banker, 1998; Darken & Peterson, 2002; Waller, Hunt & Knapp, 1998; Witmer, Bailey, Knerr & Parsons 1996) , and perception of virtual textures and objects by people who are blind (Colwell, Petrie, & Kornbrot, 1998; Jansson, Fanger, Konig, & Billberger, 1998; ) .
The research reported in this paper follows the assumption that the supply (via technology) of compensatory perceptual and conceptual information may contribute to effective acquaintance with unknown environments by people who are blind. This approach differs from previous research lines and practices in several ways. First, it integrates existing knowledge from different disciplines (namely O&M, learning processes by people who are blind, virtual environments and haptic devices R&D) into a common conceptual framework for the study of O&M skills acquisition using technology. At an additional level it deals with two main drawbacks of technologies currently in use: (a) the need for prerequisite knowledge about the space to be navigated (e.g., the talking signs or GPS systems) and (b) the lack of appropriate resolution of the information supplied about the unknown space (e.g., verbal descriptions or tactile maps). The virtual tool used in this study supplies all required prerequisite knowledge, at a resolution compatible with the features of the simulated environment. To examine the above assumption we developed a multi-sensory virtual environment (MVE) and studied the exploration process of an unknown space by subjects who are blind using the MVE. Their performance was compared to that of a control group of people who are blind who explored the real environment simulated in the MVE. The main research questions of this study were:
1. What exploration strategies do people who are blind use working with the MVE, in comparison to those used by people whom are blind working directly in the real environment? 2. What characterizes the exploration processes used by people whom are blind working with the MVE, in comparison to the exploration processes used by people whom are blind working in the real environment? 3. What information collection and storage did participants use, in both the experimental and control groups?
The MVE prototype developed for this study comprised two modes of operation: (a) developer/teacher mode and (b) learning mode. The core component of the developer/teacher mode was the virtual environment editor, which included three tools: (a) a 3D environment builder, (b) a force-feedback effects editor, and (c) an audio feedback editor (see Figure 1) . By using the 3D-environment editor the developer can define the physical characteristics of the space, e.g., size and shape of the room, or type and size of the objects (i.e., doors, windows and furniture). Using the force feedback effects (FFE) editor the developer was able to attach haptic effects to all objects in the environment. Examples of FFE's were vibrations or attraction/rejection fields surrounding objects. The audio editor allowed the attachment of three kinds of auditory feedback to the objects: (a) labels (e.g., bird chirps) as representative for the windows, (b) explicit names (e.g., first door or second cube), and (c) a guiding agent reporting on features of the objects (e.g., the proximity of corners or required turns). All environments used in this study were composed by the researchers using the system editing tools.
In the learning mode, the users navigated the environment by means of the force feedback joystick (FFJ). While walking via the FFJ they interacted with the simulated space components (i.e., they perceived the form, dimensions, and relative location of objects; or identified the structural configuration of the room including presence and location of walls, doors, and windows. As part of these interactions the users got haptic feedback through the FFJ along with audio feedback. Figure 2 shows the user-interface screen. The red circles indicate the hot spots that triggered the guiding agent's intervention.
Several additional features were offered to the teachers during and after the learning session. Monitoring frames, for example, presented updated information on the user's navigation performance (e.g., position or objects already reached). Another feature allowed the recording of the user's navigation path and its replay for analysis and evaluation purposes, as shown in Figure 3 .
The study included 31 participants who were selected on the basis of the following seven criteria: (a) total blindness, (b) minimum of 12 years old, (c) not multi-handicapped, (d) received O&M training, (e) Hebrew speaker, (f) onset of blindness at least two years prior to the experimental period, and (g) comfortable with the use of computers. The participant age range was 12-70 years old. We defined two groups that were similar in gender, age and age of vision loss (i.e., congenitally blind or late blind. The experimental group included 21 participants who explored the unknown space by means of the MVE, and the control group had 10 participants who explored the real unknown space (see Table 1 ).
To evaluate the participants' initial O&M skills, all completed a questionnaire on O&M issues. The questionnaire results showed no differences in initial ability among both groups' participants.
The independent variable in this study was the type of environment (i.e., the multi-sensory virtual environment (MVE) and the real environment.
Three groups of dependent variables were defined, concerning (a) exploration strategies, (b) characteristics of the exploration process, and (c) the use of information and storage aids during the exploration.
Variables related to the exploration strategies included: 1. Exploration strategies -alternative strategies used by the subjects in their navigation: the "perimeter" strategywalking along a room's walls (see Figure 4 , Route 1); the "grid" strategy -exploring a room's interior by scanning the room (see Figure 4 , Route 2); the "object-to-object" strategywalking from one object to another (see Figure 4 , Route 3); the "points-of-references" strategy -walking in the environment and creating landmarks (see Figure 4 , Route 4), or other (new) strategies.
2. Frequency -the number of times each strategy was implemented during the exploration.
3. Distance traversed -distance traversed using each strategy.
Variables related to the characteristics of the exploration process:
1. Total duration -the total time spent accomplishing the task.
2. Total distance -the total distance traversed. 3. Strategy-switch -the frequency of strategy changes during the exploration task.
4. Sequence -the first sequence of two strategies used in the exploration (e.g., pattern strategy first then grid strategy).
5. Stops -the number of pauses made during the exploration. Two values were defined: short pauses (4-10 seconds) introduced for technical purposes (e.g., changing the hand that holds the joystick) and long pauses (more then 10 seconds) supposedly used for cognitive processing (e.g., memorization or planning).
Variables related to the use of information and storage aids included:
1. Aids -use of aids of two types: measurement aids (e.g., counting steps or using echo feedback) and informationretaining-aids (e.g., producing a verbal reconstruction of landmarks or using metaphors).
The main instruments used in the study were: 1. The unknown space -the space to be explored, both as real physical space and as virtual representation in the MVE (see Figures 5-6 ). The space was a 54 square meters room with three doors, six windows and two columns. There were seven objects in the room, five of them attached to the walls and two placed in the inner space.
2. Exploration task -each participant was asked individually to explore the room, without time limitations. The experimenters informed the participants that they would be asked to describe the room and its components at the end of their exploration.
In addition a set of three instruments was developed for the collection of quantitative and qualitative data:
1. Orientation and mobility (O&M) questionnairecomprising 46 questions concerning the participants O&M ability indoors and outdoors, in known and unknown environments. Most of the questions were taken from O&M rehabilitation evaluation instruments (e.g., Dodson-Burk & Hill, 1989; Sonn, Tornquist & Svensson, 1999) . The O&M questionnaire included four parts: (a) 19 descriptive questions (e.g., age; gender; age of vision loss); (b) 8 questions on the subject's O&M ability in known indoor environments (e.g., home; school; work; etc); (c) 12 questions about the subject's O&M ability in known outdoor environments (e.g., street crossing; using public transportation; walking in shopping centers; etc); (d) 7 questions on subject's O&M ability in unknown indoor environments (e.g., what are the O&M devices you use in unknown indoor environments?; next week you are going to move to a new office or classroom. You will be visiting the new place today. What do you need to do to ensure yourself appropriate orientation in the new space next time?). Among the questions 23 O&M-related questions were answered in a four-level ability scale: (i) I cannot do the task, (ii) I need assistance from a sighted person, (iii) I need to use an O&M device, (iv) I can do the task independently.
2. Observations were video-recorded -the participant's exploration was video-recorded during the task. The information from these recordings was combined with the computer recording.
3. Computer recording -The computer's recording data enabled the researchers to track the user's exploration in the MVE, in two ways: through a data log and through a film. This enabled the researcher to collect information about users' exploration strategies, distances, total duration, switches of strategies and stops (see Figure 3) .
Two data evaluation and coding schemas were developed, one for the participant's O&M skills and the other for his or her acquaintance process with the new space.
All participants worked and were observed individually. The study was carried out in three stages. The first stage focused on the evaluation of the participants' initial O&M skills using the O&M questionnaire. In the second stage the experimental group became acquainted with the virtual environment's components and operation modes. The series of tasks administered at this stage included (a) free navigation, (b) directed navigation, and (c) a task aimed to introduce the auditory feedback. This stage lasted about three hours (two meetings). At the end of it participants learned to work independently with the FFJ, were able to walk directly toward the objects, could say when they bumped into an object or got to one of the room's corners, and could walk around the objects and along the walls by using the FFJ and the audio feedback. The third stage, the main part of the study, focused on participants' exploration of the unknown space. The experimental group explored the space using the virtual environment, while the control group directly explored the real environment. This stage lasted about 1.5 -2.5 hours, the task was video-recorded. For the experimental group the video-recording was combined with computer-recording. The last stage consisted of the processing and analysis of the collected data.
The research results and conclusions are based and represent only the research participants' performance and achievements (n=31). The population target on this research were Israeli people who are blind, selected using the seven criteria above mentioned and that agree to take a part on this study. The actual size of the study's population did not allow a detailed examination of the effect of otherwise relevant variables (e.g., gender or age).
The results regarding the exploration strategies, methods, and processes manifested by the participants working with the MVE, in comparison with those working in the real space, are presented according to our main research questions.
Research Question 1: What exploration strategies do people who are blind use working with the MVE, in comparison to those used by people whom are blind working directly in the real environment? The participants in both groups implemented similar exploration strategies, mostly based on the ones they used in their daily navigation in real spaces. Examples of strategies implemented were: (a) perimeter (e.g., walking along the room's walls and exploring objects attached to the walls), (b) grid (e.g., exploring the room's inner-space), (c) object-toobject (e.g., walking from one object to another), and (d) points-of-references (e.g., walking in the environment and creating landmarks). However, an interesting additional finding surfaced in that several participants in the experimental group developed a few new strategies while working within the virtual environment. A constant scanning strategy was identified by which the user collected information about the room's interior while simultaneously 
collecting perimeter information (e.g., resembling the use of a long cane in real space -as shown in Figure 7 ). Those strategies could be generated only within the MVE, representing an important added value of the work with the computer system.
As already mentioned, no substantial difference between groups was observed as regards the types of strategies used, but significant difference was found concerning the frequency of use of the strategies, and distance traversed using each strategy. Data in Table 2 indicate that the strategy most frequently used by the experimental group was grid, followed by the perimeter strategy. In contrast, the control group preferred to explore the room's perimeter, and next to use the object-to-object strategy. Examining the distance traversed using each strategy, we found that both groups traversed the longest distance using the perimeter strategy.
Research Question 2: What characterizes the exploration processes used by people who are blind working with the MVE, in comparison to the exploration processes used by people whom are blind working in the real environment? Five aspects are of interest as regards to the exploration processes used in the two groups: (a) the duration of the exploration, (b) the distance traversed, (c) the number of switches among strategies, (d) the sequence of main implemented strategies, and (e) the number and kinds of stops made while examining the new space.
Concerning the duration of the exploration, it should be noted that the participants were not limited in time for accomplishing the task. Participants from the experimental group needed four times more time to explore the new environment (average time of 38 minutes) than the ones from the control group (average time of 10 minutes). This difference was significant (t (28)=7; p=.000). Significant difference was found also for the total length of the exploration path (t (29)=5.44; p=.000). Participants in experimental group traversed an average of three times more distance (M=6.3 m) than the control group subjects (M=1.9 m).
The experimental group made frequent switches of strategy during their walk in the MVE, in contrast with the control-group performance in the real space. This behavior is reflected in the total and average frequency of use of the various strategies by both groups (see Table 2 ), total frequency of 292 and mean of 14 for the experimental group, and total frequency of 64 and mean of 6.4 for the control group.
Significant difference was also found between the groups in the sequence of main strategies implemented (c2(2)=7.55; p<.05). Data in Table 3 indicate that most experimentalgroup participants (62%) used the grid strategy first and then the perimeter strategy. In contrast, most control-group participants (90%) preferred first to explore the room's perimeter and then the objects located in the inner space of the room.
Participants from both groups made many pauses during their walk, suggesting that different cognitive operations related to the task in process were activated during these intervals. In terms of duration and function, we defined two types of pauses, short and long. Short pauses (i.e., 4-10 seconds) were used for technical purposes (e.g., changing the hand that holds the force-feedback joystick) or for reflection on a recent action. Long pauses (i.e., more than 10 seconds) were used for memorizing spatial information, reflection on a recently implemented exploration strategy, or planning. As shown in Table 4 , significant difference was found between the groups (t(26)=7.65; p<.001; t(25)=2.56; p<.05 ) for both short and long pauses. The experimental group made about 3 times more short pauses, and 6 times more long pauses.
As the results indicate, significant differences were found between the experimental group and the control group concerning the characteristics of the exploration process. These differences were related to four dependent variables: (a) the total duration of the exploration, (b) the total distance traversed, (c) the sequence of main implemented strategies, and (d) the number of pauses made while exploring the unknown space. The experimental group participants, in comparison with the control group, used a more varied range of strategies to explore the room, walked a longer distance to complete the exploration, and made more pauses for technical or reflective purposes.
Research Question 3: What information collection and storage did participants use, in both the experimental and control groups? The collection and storage of relevant information is inherent to the process of exploring an unknown space. Although only a few participants in this study reported explicitly on the use of tactics and aids for performing these functions, their account on this matter is of interest. Excerpts of these participants' references to the use of such aids follow.
One important category of information-collection aids was related to measurement to support the estimation of dimensions and distances. One example is the case of T, a 25-year-old, late blind, woman who explored the room using the MVE. After 2 minutes in the system T began to walk and count out aloud steps: "The blackboard… ok, the wall, one, two, three, four, five…".
The use of echo was another useful measurement aid. For example, the control-group participants used echo for measuring their distance from the wall or from other objects, or the room size. During their exploration those participants spoke, sang or whistled to get echo information.
The participants in this study used various kinds of information-retaining means. One was verbal reconstruction of landmarks, by which the subject recalled out loud her/his exploration of the space. For example, G, a 25-year-old, late blind man who explored the room by using the MVE, after examining the room's perimeter for 13 minutes said: "I had door, a prism, a corner, when I walked I had a window at the left side and then I reached the cube, I passed it and in front of it I had a window, column, window, window, column, window…". A variant of this was to complement the verbal reconstruction with virtual drawing (i.e., accompanying the verbal description with hand movements mimicking the physical presence and distribution of spatial components). For example, G, a 12-year-old, congenitally blind girl working with the MVE, after 22 minutes of exploration discovered the second cube in the room and began to describe out loud: "second cube, this cube is in the corner of the lower wall, and the wall is here…so the cube is in this corner...of the left wall…[during this verbal reconstruction G moved with her hand on the table indicating where the surveyed objects were located] yes the left wall, yes this wall corner and that wall corner ...there is a cube...".
Another interesting aid for the reinforcement of acquired information was the use of metaphors. For example, M, a 39-year-old, congenitally blind woman, after 32 minutes of exploration said: "…now I am a tourist guide, you are standing in front of the room entrance, now you are going to follow me, we are turning to the left, ok follow me... you have reached the prism, look at the prism, it is a beautiful object. We can not walk to the left, we are stuck in… we are walking forward… and we are arriving at the wall…".
Only some of the participants explicitly reported on the use of any aid. Table 5 indicates that the participants from the experimental group who reported on the use of aids mentioned mainly retention reinforcement aids (54%). In contrast, half of the control group mentioned such aids (50%), and even more mentioned the use of measuring aids (70%).
The research reported here is part of an effort aimed to understand if and how, the work with a MVE supports the exploration of unknown environments by people who are blind . Gathering comprehensive information about new spaces is a prerequisite for the construction of effective cognitive maps of these spaces, and for supporting people's ability to navigate them. The results of this study helped uncover several issues concerning the contribution of the MVE to the exploration strategies and learning process of unknown spaces by people who are blind.
Walking in the MVE gave participants a comprehensive and thorough acquaintance with the target space. The high degree of compatibility between the components of the virtual system and of the real space on one hand and the exploring methods supported by the MVE on the other, contributed to Journal of Special Education Technology the users' relaxed and safe walking. These features also enabled participants to implement exploration patterns they commonly used in real spaces, but in a qualitatively different manner. The use of real walking strategies in virtual environments was reported in previous studies on spatial performance by sighted participants (Darken & Peterson, 2002; Witmer, Bailey, Knerr & Parsons 1996) . But this study's MVE participants applied the known strategies in novel ways. For example, they preferred to explore the inner part of the room first and only then its boundaries, in contrast with the exploration patterns described by Jacobson, 1993 . Moreover, the MVE participants created new exploration strategies, such as the one simulating walking with a long cane enabling them to walk the perimeter of the room and at the same time to explore its corresponding inner areas -a strategy only possible within the MVE.
Operation features of the MVE (e.g., the game-like physical interface, various types of feedback) contributed to participants' performance with the system while exploring the unknown space. As a result, the exploration process showed interesting qualities concerning spatial, temporal, and thinking-related aspects. Examples of spatial and temporal qualities were the range of scanning strategies implemented, the inclusion of a large number of long and short breaks, or the time spent in examining the space. In addition, the MVE users traveled as much as three times more distance than the control-group participants, allowing them to collect information about the environment at different resolution levels, and to re-evaluate the information already gathered. All these were indications of the richness and comprehensiveness of the exploration process as accomplished by the MVE participants. Although the time measures collected were similar to those reported in Darken and Banker (1998) and Waller et al. (1998) , both studies of sighted participants exploring spaces by means of VEs, it could be expected that exploration time would become shorter as participants got gradually used to working with these systems as tools for learning unknown spaces.
Concerning thinking-related aspects of the process, interesting examples were the long breaks made by the participants with the aim to reflect on the exploration steps or to memorize data concerning an explored area, or the use of virtual drawing of spatial features under examination on the table's surface as a reinforcement aid.
An important byproduct of the study is related to the definition of specifications and constraints for the appropriate design of haptic virtual learning environments for people who are blind (e.g., force-feedback in high resolution, audio feedback). We expect these virtual environments to become powerful tools for people who are blind in learning processes in which spatial information is crucial, both for understanding new concepts and phenomena, as well as for acting and performing in the real world.
Further studies should examine the participants' construction of spatial cognitive maps of spaces using the MVE and, consequently, their use of these maps for navigating in the real environments. Additional variables to be studied should relate to properties of the environment (e.g., indoor or outdoor spaces, complex public spaces, and irregular surfaces). Finally, a comparison with traditional methods used by people who are blind to learn about unknown environments (e.g., tactile maps, verbal descriptions, human guidance) may serve for comprehensive evaluation of the contribution of the virtual tools to people's spatial performance. Finally, at the implementation level the virtual tool could play a central role in training and rehabilitation processes as well. One possible application is for supporting the acquisition of O&M skills and strategies by persons who are late blind as part of their rehabilitation process. At another level, the development of more comprehensive environmentediting tools for the MVE will support the creation of a variety of models of spaces (e.g., public buildings, shopping areas) enabling pre and post-visit exploration and recall of unknown spaces by people who are blind. These implementations may also serve the research and practitioners community as models for the further development of technology-based tools for supporting learning processes and performance of people with special needs. 
name and sent by a colleague who has participated in a study of mine before, made me feel that his request was the first I should respond to.
In an ideal world, we, fellow academics, irrespective of the difference in rank, gender, or seniority, would support each other's research (and hence publications) by responding to requests for helppresumably every response, every bit of contribution to their data would count. (It should be noted that such appeals to fellow academics for help are typically made by academics in education, including language education, where they may have research interests that necessitate the participation of fellow academics across disciplines.) The literature, especially that of health research, has discussed the pros and cons of undertaking qualitative interviews with one's peers in the same profession [1, 2] . However, little reflection is found in the literature on the implications of working with fellow academics who are not necessarily in one's own field of profession or discipline. At the same time, while research methodology books give ample advice on accessing participants beyond academia, little discussion can be found on working with fellow academics as research participants. The present paper will address this gap in the literature. The observations shared here represent personal views, which have not been tested by systematic research. However, a discussion of the phenomenon is worthwhile as accessing target research participants is an important issue for many academics in social sciences.
In the following, I will begin with an example from Braine [3] to illustrate what can result from a lack of cooperation from fellow academics as potential research participants. Then, I will draw examples from the literature to sketch a picture of some studies where researchers in education have tried to involve fellow academics in U.S., European, and Chinese universities, respectively, as their research participants. I will then discuss some related issues by drawing upon my own experiences of working with Chinese academics as my research participants, as a female junior academic based at a university in Hong Kong.
We normally take precautionary measures against a potentially low response rates. For instance, we may aim to reach an extensive pool of a target population, use a "snowball strategy" when selecting participants, or personalize the invitations sent to target participants. When little can be done to change a low level of "cooperation," however, our research purposes can suffer. An example was given by George Braine [3] in an article entitled "When professors don't cooperate: A critical perspective on EAP research" (published in the journal of English for Specific Purposes, a flagship journal in a field where cooperation from colleagues across disciplines is often vital). For his doctoral research, conducted at the University of Texas (Austin) in the early 1990s, on undergraduate writing tasks in engineering and natural sciences, Braine received enthusiastic support when requesting written assignment prompts from professors across disciplines at the university; however, when trying to replicate the study at the Chinese University of Hong Kong a few years later (in mid-to late-1990s), as a faculty member in the English Department, he ran into difficulty. With the support of two research assistants, they sent requests to "223 teachers in the engineering and science faculties who were listed in the timetable as teaching in English or in Cantonese and English, inviting them to participate in the project by sending us their course syllabi and writing assignments" ( [3] , p. 297):
"Within a week, 80 had replied, citing their reasons for being unable to participate: some were too busy, others were not teaching that semester/year, and the rest did not give writing assignments in their courses. In response to follow-up requests, phone calls, and email messages, only five teachers from engineering and four from science agreed to participate in the project. Despite requests through e-mails and telephone calls, 134 teachers did not respond at all." (pp. [297] [298] In all, the research team collected 29 assignments from engineering, 22 of which were from two courses, in the form of "instructions for laboratory experiments" but were "too succinct for detailed analysis"; and none came from science (p. 298). The researchers did, instead, receive a generous donation of student reports, and they ended up trying to "retrace and reconstruct laboriously, from the students' reports, the teachers' expectations" (p. 298).
Reflecting on why colleagues in engineering and science faculties were reluctant to share their writing assignments, Braine [3] suggested that "these teachers may have received little or no instruction in writing during their secondary, undergraduate and graduate studies" (p. 299); and even having pursued graduate studies in North America usually also means a lack of training in writing for some. Hence, the professors may either take writing assignments from textbooks or give few writing assignments (p. 298 and p. 302). Another reason for the teachers' reluctance to share the materials requested, Braine suggested, might be that they did not want to let an English teacher see their "poorly written or poorly designed texts" ([4], p. 33) (cited in [3] , p. 302).
Nearly two decades after Braine's effort to reach out to fellow academics at the Chinese University of Hong Kong, there has apparently been heightened awareness for writing across disciplines in Hong Kong universities, and professors across faculties generally seem to possess relatively strong English writing skills, as, after all, a track-record of English publications, coupled with a PhD degree earned at an English-speaking country, has often been an instrumental factor in faculty recruitment. However, there is no guarantee that Braine would have received more positive responses to his request now than over a decade ago.
In Braine's [3] study, another factor seems important for the contrast of the responses he received from subject professors at the University of Texas (Austin) and later at the Chinese University of Hong Kong: there was a long-established Writing Across the Curriculum (WAC) program at the former, so that, presumably, the subject professors were accustomed to working with language teachers on a regular basis; while, at the latter university, with the absence of such a program and with a language specialist-subject specialist partnership largely an alien concept at the time of Braine's study, the subject specialists might not feel comfortable moving out of the tradition. However, other than this divergence at the two sites, one may wonder if the difference in responding to the same request might reflect some sort of "cultural" difference among the professors on the two sides? Namely, do U.S. academics tend to be more responsive to research-support requests from fellow academics than (Hong Kong) Chinese academics? Such a question will be difficult to answer. However, it may be useful to look into the literature to get a sense of the extent to which academics in different parts of the world are willing to become research participants, by responding to fellow academics' request of completing a questionnaire.
In the following, I will provide an overview of some studies, which were based on data gathered through questionnaires administered to academics across disciplines in U.S., European, and Chinese universities, respectively. I focus on questionnaire-based studies rather than interview-based ones here as the former tend to give clear indications of the response rate (i.e., the percentage of the responding fellow academics in the total target population). Most of the studies examined below were conducted by researchers in language education (which is my own disciplinary area) and a few by researchers in education more generally. The target questionnaire respondents were typically fellow academics across disciplines rather than in the researchers' own discipline (an exception being Min's [5] study, in which the participants were from the researcher's own disciplinary area, i.e., applied linguistics). It should be noted that my selection of the studies is more of a result of my having knowledge of these studies than due to any other reason. For consistency, five studies have been selected from each of the three geographical locations: the U.S., Europe, and several Chinese-speaking regions. The purpose of the overview is to offer a glimpse into some studies that involved academics responding to peers' request for participation in research. There is no intention to pick "representative" studies or to make any generalizations based on these studies. Table 1 summarizes five questionnaire-based studies, conducted by language professionals teaching in U.S. universities. These studies, with an aim of informing English for Academic Purposes (EAP) pedagogy, mostly focused on the communication or, especially, the academic writing requirements for students across disciplines. 1518 foreign language (FL) instructors working at the chosen universities, receiving an email containing a secure URL link providing access to an online survey system
The extent to which the current university FL instruction is informed by second language writing research 153 (9.92%) FL instructors completed the survey Ferris and Tagg [8] described the response rate they received from fellow academics (at 25.4%, with 234 responses to the 921 delivered surveys) as "fairly low" (p. 37). In another study, not included in Table 1 but apparently drawing data from the same questionnaire described in Ferris and Tagg [8] , the same authors [11] focused on listening/speaking tasks for ESL (English as a Second Language) students, and pointed to an (understandable) connection between fellow academics' decision over whether to respond and their perceived relevance of the topic of the study for them: "As the survey was rather long, it is likely that the respondents were primarily those who had strong interest in or concerns about their ESL students and may thus not be representative of all instructors" (p. 303).
While the surveys conducted by Casanave and Hubbard [6] , Jenkins, Jordan, and Weiland [7] , and Ferris and Tagg [8, 11] , in the late 1980s to early 1990s, were paper-based with hard copies of questionnaire mailed to target participants, over time it has increasingly become a norm to conduct surveys using the Internet, by distributing the questionnaire directly via email (as in [9] ) or sending an invitation email containing a URL link to an online survey (as in [10] ). Of the latter, as the researchers Hubert and Bonzo ([10,] p. 521) described: "The survey was administered using an online instrument to which potential respondents were provided access via a secure URL link. This link was provided to potential respondents via email." Table 2 shows five survey-based studies conducted in European universities also by language professionals. In these studies, questionnaires were administered to fellow EAL (English as an Additional Language) academics to find out about their attitudes toward the dominance of English in academia in relation to their first language, perception of difficulties in writing for publication in English, and their related practices in overcoming the potential language barrier. Table 2 . A summary of five surveys conducted by language professionals in European universities.



Duszak and Lewkowicz [12] A university in Poland
The questionnaire was emailed to "academics in medicine, psychology and language studies" (p. 111) Polish academics' perception of difficulties in writing for publication in English "99 completed questionnaires were received"; response rate is unknown as the population size is not indicated Ferguson, Pérez-Llantada, and Plo [13] University of Zaragoza, Spain A questionnaire (written in Spanish) was emailed "through a university server to all 3,000 academic and academic-related staff" (p. 47)
Scientists' perception of disadvantage in using English in academic/scientific publication "a modest though not impossibly low response rate of 10 per cent", with 300 questionnaires returned (p. 47) "online questionnaires which were posted on the university server during the period April-June 2009" (p.
433)
The use of English at undergraduate and postgraduate levels and across disciplines, and the target respondents' attitudes toward the use of English in teaching and research "highly satisfactory" response rates: 19% of the target students responded, and 668 (40%) out of 1683 staff responded; excluding administrative personnel and PhD students from the staff category, "498 teaching and research staff remained" (p. 433) Table 2 shows that from the researchers' point of view, the response rates ranged from "disappointingly low" [15] , "modest" [13] , to "highly satisfactory" [16] , but, apparently, tipping toward the modest and low side. The top reason for a colleague not to respond, as some suggested, is "pressures on academics' time" ( [13] , p. 47). In addition, Duszak and Lewkowicz [12] noted that 62% of their respondents were aged between 22 and 45, which is "likely to reflect the fact that younger academics are more willing to respond to requests for data of this nature" (p. 111). This suggestion can be interpreted as implying that younger academics may find the topic of the survey, namely perception of difficulties in writing for publication in English, particularly relevant for them and are therefore more willing to respond. A similar point was made by Ferris and Tagg [8] , as noted earlier.
Finally, Table 3 includes five questionnaire-based studies concerning academics' teaching/research activities, conducted in universities in Mainland China, Hong Kong, and Taiwan, respectively. It can be seen from Table 3 that the studies of Li et al. [17] and Min [5] achieved particularly high response rates: 89.3% (268 responses out of a target of 300) and 76% (38 responses out of 50), respectively. However, this is perhaps not surprising, given that, as indicated in the table, Min [5] identified her target respondents from applied linguists (language educators) she had met at local conferences, so that presumably the researcher and the target respondents were members of a local academic community based on professional relationships. In the case of Li et al.'s [17] study, "printed surveys were handed out" (p. 282) to the target respondents, apparently through personal connections and with facilitation of the approval of the study by "the leaders of the institution" (p. 279). The potential implication of personal connections (or guanxi) in conducting research in the Chinese context will be re-visited later in the present paper, when I reflect upon my own experience of working with Chinese academics.
Incidentally, questionnaire-based research conducted by language educators with fellow academics across disciplines in mainland Chinese universities is hard to find. This seems to echo the current general lack of exchange and collaboration in EAP instruction between language and subject specialists in the country [21, 22] . In this sense, it can be suggested that access is only likely to occur when there is a desire to access. On the other hand, potential accessibility of subject specialists and language specialists to each other in Chinese universities might, in fact, be a reminder of the existence of opportunities for collaboration between the two parties in EAP teaching and research, with such collaborations having long been advocated in the literature (e.g., [23, 24] ).
In the above, I summarized a total of 15 questionnaire-based studies conducted by researchers in education, especially language education, among fellow academics mostly working in disciplines other than their own. It can be seen that when questionnaires are distributed within a relatively closely-knit academic community, where the researcher has personal connections, either in the context of one university [16, 17] , or a disciplinary/professional community of which the researcher is a member [5] or to which the researcher has access through a listserv [9] , a high response rate is likely to achieved. We also notice that researchers may express an emotional attitude toward the response rates: while for Ferris and Tagg [8] , a response rate of 25.4% was "fairly low" (p. 37), for Ferguson et al. [13] , 10% was "modest" (p. 47), and for Olsson and Sheridan [15] , 17.5% was "disappointingly low" (p. 39); in addition, a response rate of 40% was "highly satisfactory" (p. 433) for Bolton and Kuteeva [16] . Thus, researchers do care about the response rates they receive.
Casanave and Hubbard ( [6] , p. 35), Jenkins, Jordan, and Weiland [7] , and Ferris and Tagg ( [11] , p. 303) all admitted that their surveys were long and complex. The trend over the past two decades is probably for questionnaires targeted at academics to become shorter and easier to respond to. Nevertheless, Ferguson et al. [13] suggested that curtailing the number of questions in a questionnaire, including by "overriding the customary practice of including multiple items focusing on the same target", "can sometimes impact on reliability" (p. 56). Researchers are thus caught in a dilemma: in trying to overcome low response rates, which may threaten the authenticity of the data (e.g., by not being representative of the larger population), they may feel compelled to adopt strategies that can potentially undercut the reliability of the questionnaire. When they do get a low response rate, researchers may have to make a case that the data are trustworthy and usable (presumably justified in doing so). Ferguson et al. [13] , for example, suggested that despite the "modest" response rate of 10% that their questionnaire received from their target Spanish academics respondents, the data "do appear utilizable within the inherent limitations of the methodology", "bearing in mind that the sample size is not lower than in many comparable surveys", plus their survey study was exploratory and would be followed up by interviews with a selection of academics (p. 47).
In the early 2000s, when I first embarked on an academic career, I had ingrained beliefs to dismantle first: having a scholar father specializing in historical Chinese linguistics, I grew up assuming that research should be based on books and study of texts; collecting "data" from human beings (including students) for research felt unnatural, and even unethical, to me. However, having decided to study what difficulties Chinese doctoral science students experience in trying to meet the degree conferment requirement of SCI (Science Citation Index) publication (a requirement becoming increasingly popular at Chinese universities at the time), i.e., publishing in (English-medium) international journals included in the SCI, and how they can be supported in the endeavor, I seemed to have little choice but to muster the courage to approach students to collect "data" from them. With little training in research methodology at the time, it was curiosity and a desire to learn that sent me onto a track of "empirical research"-which was a privileged term in the language education circle in China at the time (and it still is), in reaction to the much-criticized "impressionistic," "reflective" tradition in Chinese research [25] . I remember the encouragement I received from an American professor at the time: that my project of working with science students and scientists would be very "doable," as long as I could "penetrate" their "academic tribes" [26] .
Over the past decade, in a number of projects on scholarly publication, I have interviewed academics across disciplines in Hong Kong and Mainland China on their attitudes toward and practices in linguistic choice between English and Chinese in research and publication; I have conducted case studies of novice scientists (doctoral science students) in mainland universities writing for publication in English, which included an examination of how supervisors revise papers for novices, and, in particular, how supervisors perceive and tackle the issue of text-based plagiarism in novice texts. Other than working with university-based academics and students, I also accessed the orthopedics department of a Chinese hospital to investigate how medical doctors with doctoral degrees engage in research activities amidst their busy schedules of clinical practice. The challenge of gaining access to, and maintaining, contact with participants in these projects has not been small. On reflection, I would say that, on the whole, opportunities for access have come mostly from personal contacts, while constraints seem to be often related to my geographic location.
Personal contacts were formed by my having taught English at a university in a major city in east China, so that I was able to invite my students-doctoral students across disciplines-as my initial participants in my doctoral research project on Chinese novice scientists writing for international publication. The literature is rich with examples of English language professionals working with ESL students to study the latter's academic literacy pratices (e.g., [27, 28] ). In conducting such research, language professionals are able to draw upon their insider knowledge [29] while capitalizing on the advantage of access. As Casanave [30] commented on Spack's [28] three-year study of a Japanese student (named "Yuko") at a U.S. university: "Like many other qualitative studies of writing, this case study used Spack's insider knowledge, contexts, and contacts at her own university to comfortably get access to what she needed from and about Yuko."
During my doctoral years I made painstaking efforts to stay in touch with my participants, so that the number of words I wrote in emails greatly surpassed the total number of words of my doctoral dissertation. In those emails, I played the roles of friend, English teacher, and researcher. I observed the principle of reciprocity [31] , and did what I could for my student participants, above all by editing English papers for them. Over time I got to know some of the students' supervisors, who also participated in my research. Later through the introduction of colleagues and friends, I reached more academics across disciplines, who, regarding me as a previous colleague, kindly accepted my invitation to participate in interviews. Additionally, the relevance of my topic of research, namely the impact of English and English publication requirements, for my target participants may have been a facilitative factor in my access. Examples of such reciprocal relationships between EAP language specialists and EAL academics, or students in disciplinary areas who need to publish in English, can be found in the literature: the former providing instruction, training, or editorial support to the latter, and the latter becoming research participants in interviews, case studies, or questionnaire surveys in return (e.g., [32, 33] ).
Other personal relations, or family relations to be more specific, have facilitated my access to researchers beyond academia, i.e., doctors and medical students at a Chinese hospital engaging in research for publication (e.g., [34, 35] ). Evidence can be gleaned from the literature to indicate that family relations can sometimes be an important factor in shaping one's research path. For example, we learned that for Dorothy Winsor, an accomplished professional and technical writing researcher, access to engineers through family relations has been critical: "Winsor realized that with her access to engineers via her husband's profession she was in a position to learn something about engineering writing" ( [36] , p. 355). Similarly, we learned from Dressen (or "Dressen-Hammouda" in her later publications) [37] that her husband was a geologist so that she had opportunities to learn about the discipline "through osmosis" (p. 284) and built her research path over time around geology text and geologists' genre mastery.
To employ notions widely adopted in the literature, in terms of social science power, I am located in a "semi-periphery" region, but have conducted research in a "periphery" setting, and published in "center"/"semi-periphery" journals [38, 39] . ("Semi-peripheral social science power", according to Alatas ([38] , p. 606), "may be defined as a social science community that is dependent on ideas originating in the social science centres, but which themselves exert some influence on peripheral social science communities". From this perspective, Hong Kong may be regarded as a semi-peripheral social science power.) I am aware of the connection between my practice and the scenario described by Canagarajah [40] : that scholars from better-resourced and English-dominant countries/regions utilizing research data in the periphery to develop interpretations, typically through the lens of center-origined theories, and, henceforth, claiming leadership in scholarship by publicizing their work in Englishmedium journals. In contrast to this scenario, had local scholars "enjoyed similar resources", they "would have also possessed the power to orchestrate the whole research enterprise under their own leadership" and "present the discovery in a manner favoring their community's interests, knowledge, and values" ( [40] , p. 5). Canagarajah is sharp in his observation.
As a Hong Kong-based academic conducting research with Chinese mainland fellow academics, I am conscious of the way I might be perceived: unlike some of my target participants, I am not a returnee (having completed graduate studies outside Mainland China and returned to become a mainland-based academic). I suspect that this kind of background gives me both an advantage and disadvantage when I approach my target participants. There may be perceptions that I cannot change; but I do have a clear goal in my research. With the advantage that I may enjoy-e.g., resources and, perhaps, the name of my home institution-my central concern in my work with Chinese academics has been to let the outside world hear their voices. I am indebted to their support of my research; and I decided that the best thing I can do for them in return is to do good research and disseminate my findings so that there will be a more balanced view about Chinese academics on the international stage: that other than corruptive practices that seem to be wide-spread in some quarters of Chinese academia [41, 42] , the general Chinese academics' diligence and studious efforts to achieve success in research and international visibility and impact should also be made known.
I am also aware that I am just one of many education and social science academics in Hong Kong universities who conduct research in Mainland China and publish in English journals (for illustration of this phenomenon, see, for example, [43] ). The steady growth of the number of SSCI (Social Science Citation Index) journal articles co-authored by Chinese and overseas academics [44] seems to indicate a growing interest in exploring Chinese research sites. As the joint publications would indicate, a primary means for academics outside China to access Chinese research sites is to form research partnerships with their Chinese counterparts, with the latter's role possibly ranging from mere data collection to participating in the design of research and writing for publication. Over the years I have not made special efforts to forge research partnerships with mainland Chinese counterparts (of my discipline), partly due to the absence of linguistic or cultural barriers in my doing research in Mainland China-an advantage enjoyed by multilingual researchers, as having been discussed in the literature (e.g., [45] ). Of course, I have often needed the assistance of "gatekeepers" [46] in my efforts of access. Yet I have perhaps been viewed as an "outsider" sometimes. Outsiders from a reputable university in Hong Kong, a privileged semi-periphery region, are likely to be treated with some politeness; but this does not guarantee their target participants' cooperation. It has been suggested that collecting data in "emerging societies" is difficult, "as there is little tradition of independent enquiry" and "asking questions in any form is viewed with suspicion" ( [47] , p. 164). If this applies to collecting data in Chinese business organizations [47] , this should arguably be less true when working with Chinese academics. However, connections, or what is commonly known as guanxi in the Chinese culture, would still be crucial.
While having mostly worked with mainland Chinese academics, I have also reached out to academics in Hong Kong on two occasions, the first to interview some scholars in humanities and social sciences to find out how they negotiate between local engagement and international participation through publication [43] , and the second in a project on university students writing from sources, with my own university as the research site. In the first study, I approached the target interviewees one-byone by emailing them, and most agreed to be interviewed. In the second study, in an early attempt, I only got three positive responses to my email invitations sent to over three-hundred Turnitin instructors (academic staff who have a Turnitin account) on campus to recruit interview participants in order to explore the issue of plagiarism among students. All these three respondents were expatriates (i.e., English-speaking academics, two from North America and another from the UK). I surmised what has led to the poor response to my request: firstly, an interview is perceived to be more timeconsuming than answering a questionnaire online; secondly, the topic of plagiarism is perhaps not an attractive one for busy colleagues; and thirdly, the fact that three expatriate colleagues responded might indicate that they are probably a little more interested in talking about the issue of plagiarism than the general Cantonese-speaking professors, having come from the Anglo-American academic culture where a concern of the issue has been long institutionalized. As the invitation for voluntary participation did not work, later, with the help of a research assistant, I utilized personal contacts and a snowball sampling approach ( [48] , p. 89) to access target interviewees.
Reciprocity [31] can be an important principle in our research relationships with fellow academic participants, an obvious example being an English language professional editing English papers for her EAL participants, as mentioned earlier. In this scenario, the two sides are assisting each other in a high-stakes commitment, i.e., research and publication. Nevertheless, such reciprocity, in the form of the researcher giving support to the researched on the latter's work, is not always possible or required. In this situation, I have tried to pay my participants. Some may think paying academics for their participating in research is "unusual" (as a reviewer has written in commenting on the budget portion of a research proposal of mine). However, if it is appropriate to compensate research participants, such as by paying patients in health research [49] , it seems reasonable to pay fellow academics a certain token amount to express appreciation of their time and contribution of data, if it is also considered culturally acceptable to do so in a given context. Indeed, the payment can only be a symbolic compensation. An academic's hour spent on being interviewed cannot be measured by a modest honorarium. At the same time, it has to be said that the amount of such a payment is restricted, both by budget and by the consideration that it is important not to give an impression of bribing fellow academics into participation. A question here may be: how much is considered appropriate as a token fee (rather than an act of corruption) in paying a participant academic? It seems that the researcher's judicious judgment, in light of the local culture, will have to be relied upon in the decision. One may also suggest that if it is justifiable to pay a token fee to a fellow academic who participates in an interview, then one who completes a questionnaire should also be paid-yet, the latter is not usually possible, given the typical anonymity of respondents in questionnaire surveys.
It should be pointed out that, when participant fellow academics are one's own colleagues at the same university, or have been colleagues with the researcher at one time, payment of token fees may be inappropriate. It is also very likely that, despite not having been a colleague of the researcher, a fellow academic may decline to be paid. In this case, needless to say, the fellow academic's preference should be respected. To be fair, when an academic does become a fellow academic's research participant, and gets paid a token fee, the incentive should not be the honorarium, but rather, a desire to support the fellow academic. This is "academic citizenship" [50] at play.
It seems that nowadays whether we as academics are willing to sacrifice some of our time to help a fellow academic is often hinged on our schedule: the pressure of publication, coupled with the often growing, rather than reducing, load of teaching and administrative duties, means that we have become highly selective in whether or not to respond, not to say that the frequency of such requests that we receive may have led to a sense of fatigue. Of the large number of emails we receive on a daily basis, those addressed to us personally from students, department colleagues, research collaborators, and certain professional colleagues are to be filtered in; while the rest is likely to be filtered out. It does take some conscious collegiality and a desire to support fellow academics for us to respond to a request to fill in an online questionnaire, to participate in an interview, or to share our syllabi or writing assignments (the request made by Braine [3] to fellow academics). It may be fair to suggest that academics nowadays, in general, have become more pragmatic than before: more often than not, we invest time only when something is judged to be "relevant" to our personal interest or duty.
It might be suggested that if Braine's [3] study were to be conducted today, nearly two decades after it actually took place, he probably would not even get as many as 80 (36% of his total targeted 223) responding and citing reasons for not being able to participate. Olsson and Sheridan ([15] , p. 39), who reported that their survey among Swedish fellow academics got "a disappointingly low response rate of 17.5%" with 35 completed questionnaires, noted their questionnaire was partly based on Phillipson and Skutnabb-Kangas's [51] questionnaire, which was administered to Danish scholars two decades earlier and received a nearly 50% response rate with 83 completed questionnaires. Evidence is lacking to claim that the difference in the response rates between the surveys represents any general trend, but we probably have a telling pair of examples here.
Macfarlane [50] has insightfully discussed the "apparent decline of academic citizenship" (p. 296) and emphasized the essential role of "service" to "the preservation of community life" (p. 299). "Service in academic life", as Macfarlane put it, "is fundamentally about citizenship inasmuch that it demands participation as a member of a community of scholars rather than simply the individualised (and perhaps, selfish) pursuit of research and teaching interests" (p. 300). Clearly, supporting fellow academics by playing the role of being their research participants, thus indirectly helping to push forward understanding of educational issues, Macfarlane would agree, is a form of service, just as "teaching observation, mentoring, reviewing of academic papers and the organisation of conferences" are (p. 299). However, institutional forces, propelled by a performative culture which emphasizes "efficiency, effectiveness and profitability" ( [52] , p. 17), threaten to "disengage" [50] academics from their service role. Macfarlane ([50] , p. 309) called for "more explicit emphasis on the importance of the service role within reward and recognition structures" to reverse the trend of disengagement. It would be fair to suggest that, even if such structural adjustment cannot occur in the near future, intellectuals can still justifiably resist the trend of disengagement on a personal level, and, henceforth, on a community level.
Researchers may be taking many issues into consideration in order to maximize the chance of fellow academics' participation: e.g., the mode of surveying (which, nowadays, is typically sending an email invitation, embedded with a link to an online questionnaire, as noted earlier), the design of the questionnaire (user-friendly and focused), the timing of administering the questionnaire, and the mobilization of personal contacts. In addition, researchers make sure the cover letter or invitation is composed professionally, shows their credentials, and connects to the target respondents in a personalized way; they would also aim to clearly state the purpose of the study, what participation in a study involves, what sorts of questions will be asked, etc., all of which are, of course, also required in the application for ethical clearance. On the whole, they may demonstrate an understanding of the pressures in academia, and remarkable resilience and willingness to accommodate the target participants' schedule. An example of a request follows: "E-mail Request for Participants" "I know you are very busy, but I am asking for about half an hour to interview you about intellectual property issues from an administrator's point of view… … I am willing to conduct the interview at your convenience during the next two months. … … Please reply if you would be willing to help me with this, and I'll try to arrange a time that works for you." ( [53] , pp. 154-155)
After every effort has been made to incentivize fellow academics' participation, how many will give a positive response may be out of a researcher's control.
Silverman [54] reminded us that, in qualitative research in particular, we may aim to turn the negative (such as the difficulty in access) into a positive factor. He said: "Remember that the beauty of qualitative research is that it offers the potential for us to topicalize such difficulties rather than just treat them as methodological constraints. This is an issue of the creative use of troubles." (p. 153) Silverman did not illustrate how "creative use of troubles" might be achieved. However, it should not be difficult for an experienced researcher to name a couple of examples of how this might be done. Researchers, including those education researchers who need to engage fellow academics as participants, are resilient. Still, lack of cooperation from fellow academics may defeat our research purpose, as suggested earlier in the present paper.
By the time I finished writing this paper, I had responded to all three requests from fellow academics, mentioned at the beginning of this paper. I even emailed the requesting colleague in my faculty, after responding to his online questionnaire, that I would be happy to participate in an interview at a later stage if necessary, to elaborate on my responses in the questionnaire. I am glad I have added my two cents in a few fellow academics' research projects.
Traffic engineering is one of the active research areas in communication networks. The traditional form of routing and resource allocation, as the two major building blocks of traffic engineering cannot address quality of service requirements of flows while optimizing network utilization for complex communication networks. In this paper we consider ant colony algorithms to address this problem. In this approach foraging ants find the shortest path in a synergistic way. While moving back and forth between nest and food, ants mark their paths by secreting pheromone.
Step-by-step routing decisions are biased based on the local intensity of pheromone field which is the colony's collective and distributed memory. Ants will follow the most dense route in a maximum likelihood way. The actual algorithm implemented in nature by real ants is slow in convergence.
Our studies show that the ant-based routing models are sensitive to initial parameters settings. Only careful adjustments of these initial parameters results in an acceptable convergence behavior. The robust behavior of the real ant compared to the routing algorithms derived from it justifies the investigation of these algorithms in depth to find the reasons behind their shortcomings. We present results from our study of ant behavior in a quest for a robust algorithm. Most of the ant-based algorithms have been studied with limited sourcedestination traffic. In this work we have extended the algorithm to a more realistic environment in which multiple sourcedestination flows compete for the resources. We study the routing and load balancing behavior that emerges and show how the behavior relates to analytical approaches for optimal minimum delay algorithms by Gallager [3] , Mitra [6] , and others [4] , [5] . We show the results using simulations in OPNET and derive recommendations on the improvement of the ant-like algorithms to achieve load balancing.
The rest of this paper is organized as follows. In chapter 2 we highlight the problems of traffic engineering. Chapter 3 is dedicated to the ant algorithm to provide the overall view of the ant approach. In chapter 4 we briefly describe the outstanding analytical methods introduced to address the problem of dynamic routing and flow assignment. Our experiments and the results including our view of the ant algorithm is discussed in chapter 5. Finally we conclude the paper in chapter 6.
Traffic engineering is the process of mapping traffic flows onto the physical topology to meet traffic requirements, to enhance overall network utilization and create a uniform distribution of traffic throughout the network.
Traffic engineering in the traditional Internet is achieved by manipulating routing metrics, such as monetary cost, hopcount, bandwidth, reliability and delay. Since IGP (Interior Gateway Protocol) route calculation is topology driven and based on a simple additive metric such as the hop-count, it does not consider other important dynamic criteria such as bandwidth availability. As a result, traffic can be unevenly distributed across the network causing inefficient use of resources. Uneven distribution of traffic is complicated since it can be the product of the dynamic routing protocols such as OSPF and IS-IS, that select the shortest paths to forward packets. Hence a solution is required that takes into account more factors than the common path-metrics. While using shortest path conserves network resources, it may cause some other problems, such as congestion on some paths and under utilization of other paths.
These known problems with routing and the trends in networking and telecommunication provide incentive to look for another approach in routing and flow assignment as two main parts of traffic engineering.

Nature has always been an important source of inspiration for academic research. In particular there is much interest in the behavior of ants. Individual ants seem to move at random, do nothing but wander off, and yet groups of ants can accomplish complex tasks. Somehow, a collective intelligence is formed out of many simple elements which is called swarm intelligence. Each agent (ant) processes a very simple algorithm. The collective outcome realizes a much more complex algorithm. The whole system is distributed and adaptive. Ants cannot see or hear. They only sense the environment, and also the food. Ants cannot talk either, they communicate indirectly through the environment. An ant can leave a trial of pheromones which are materials with particular fragrance. Ants can smell and sense the pheromones left by other ants, moreover, ants can detect the density of the pheromones.
Ants find the shortest path to food according to this procedure: two ants start their random walk. They both eventually find the food. The one taking the shorter path finds the food first. Each ant leaves a trail of pheromones behind. Having taken the food the ants follow their pheromone trail towards the nest. The one with the shorter path returns first and arrives back to the nest first. Now a third ant wants to search for food. The ant realizes the trials left behind by its predecessors. Most likely it follows one of the existing trials rather than initiating a new trial and most likely it follows the trial with the higher density of pheromones. This results in even denser pheromone trial on the shorter path and in the long run this results in most ants using the shortest path.
When an ant starts its walk with some small probability it starts a new trial. The first ants may not necessarily have chosen the shortest path but starting new paths helps continue the quest for shorter paths until finding the shortest one and eventually the ants emerge around the shortest path. The pheromones evaporate over time. This is an essential requirement for the dynamism of the algorithm. The algorithm is adaptive because of the evaporation and the fact that ants keep starting new paths with some probability.
Ants in networks are emulated by mobile agents. Mobile agents are carried by packets. Special packets can be used as mobile agents (ants). Pheromones pass the information about the length of the path (time) to other ants. The agents can pass the same information to data packets at the nodes. Ants decide based on the density of the pheromones and some probability values. The probability values can be calculated based on the path information and listed in routing tables in the nodes. Starting with a static routing table for each node, every individual routing table stores the probabilities of using the next hops to reach all possible destinations. The sum of all the probabilities at each row should be equal to one.
Different methods have been used in the literature for implementing and updating the routing tables using the ant approach such as AntNet [1] .
In this section we will briefly review existing analytical approaches to addressing the traffic engineering issue. Analytical routing algorithms can be distributed or centralized and also static or dynamic. Static routing algorithms cannot keep themselves up-to-date with the continuing changes in networks. Centralized methods suffer from the lack of scalability and having a single point of failure. Other distributed and dynamic routing and flow assignment algorithms have stability and convergence issues.
A common characteristic of these methods is their dependence on one or more heuristic parameters that are found based on experiments. In Gallager's algorithm [3] in order to avoid loops the algorithm uses a parameter η that should be globally chosen and every router must use it to ensure appropriate behavior but this parameter depends on input traffic pattern. It is impossible to find one working value for all input traffics.
In Mitra's approach [6] also a heuristic parameter is used which is critical for the robustness of the algorithm. This factor is called "bandwidth protection" R. In [5] again a heuristic is used which is in the form of a function.
In this section we present our study of the ant colony algorithm based on simulation and discuss the results. We followed [1] for implementing ant colony algorithm, but we also examined modifications to show how the ant approach can be exploited to achieve load balancing.
Our simulations of the ant algorithm are applied to a fishlike network configuration as illustrated in figure 1 . The network consists of four routers and four hosts at the edges. Ants are generated at routers regularly and addressed to the destination hosts randomly. We use a uniform distribution to assign the destinations to the ants which are called forward ants at this stage.
The forward ants are routed to output links at each router until they find their way to their assigned destination hosts. A trace of the route traversed by the ant is also stored in the ant. At the host the ants are transformed to backward ants and sent back to the source through the same path that they took to arrive to the destination. At each router across the path the ants will then update the routing table that is used to route data packets. In the original ant colony algorithm the same table is used to route forward ants as well.
Data packets are generated by the hosts and addressed to certain destination hosts to create data traffic flows. At each router data packets will be routed to output links based on the information listed in the routing table. In an ant colony algorithm the routing table contains a probability number for every destination host though every existing output link. Packets are directed to an output link in proportion to the link probability.
The forward ants measure the travel time from each interim router to the final destination. The travel time which is a reflection of the route conditions is used to update the probability table when the backward ants come back to the router.
Except for the routing table, each node also keeps a table with records of the mean and variance of the trip time to every destination (delay). At each node, backward ants update the trip time statistics to the destination in addition to the output link probability. We derived these equations for updating the probabilities [ This equation gives an interim probability value for destination i from this router. In this equation j refers to all output links and k is the link that backward ant came from.
The probability values are filtered according to the following equation before being set into the routing table. This is to reduce the variations in the table because of temporary increases in the travel times. 
We examined three different approaches. In the first approach we used the original ant colony algorithm. In this approach forward ants are routed at the routers using the probability values in the routing tables similar to data packets.
In the second approach we used round-robin to forward the forward ants to the existing output links. The probability values are used to route only data packets.
In the third approach we modified the probability calculation algorithm while using the same round-robin approach for forward ants.
First we consider the results from the first set of experiments based on the first approach. In the majority of scenarios using various settings of the parameters, the ant algorithm reveals a strong tendency towards finding and using a major route to the destination (which is most likely the fastest one). Other routes exist but with much less share of the carried traffic.
As shown in figure 2 , even though links 3 and 4 of router 0 are exactly alike, the probability value for link 3 rapidly rises and as a result the traffic from source 1 to destination 2 eventually flows entirely through link 3.
This result seems to be in agreement with the actual behavior of ants in the real world which is basically directed in finding the best path to the food source while exploring other paths. This behavior leads to finding a robust path to the destination with enough dynamism to adapt to changes and to find new and better paths. multiple routes from source to destination in a balanced way such that overall delay and utilization of the links is optimized over the network. The ant colony implementation of ant behavior does not seem to be the best solution to this situation according to our observations of the results from the first approach.
In analytical solutions for the multi-commodity flow assignment problem, information about the links is used at the edge routers in a distributed way to calculate the best distribution of traffic into possible routes to achieve load balancing aimed at reducing delay and increasing link utilization.
Using probability tables for forwarding ants in the ant colony algorithm creates a feedback loop in favor of links that have better trip time results. This is the reason behind the convergence behavior of this method.
In the second approach we eliminate this mechanism and replace it with a round-robin selection of output links for forward ants. As expected the result as shown in figure 3 , shows a better load balancing between existing possible links to the destination represented in the form of closer probability values.
Inspired by the analytical solutions, in a third approach we used local processing of the link information gathered by the ants to achieve a better load balancing, which also results in a better delay performance and link utilization. Our results, as shown in figure 4 , show that a better and more robust load balancing (closer and more stable probability values) is achieved between link 3 and 4, compared to the previous cases.
Our experiments revealed that somewhere a friendly relationship between analytical and bio-inspired approaches has to be made. We believe the real ant in nature has a robust behavior but this process is slow and also is not necessarily promoting load balancing. While the evaporation property of ant pheromone brings some relevance to the concept of loadsharing, in fact the evaporation in ants is to provide the dynamism necessary for adaptive behavior. In other words ants have completely robust, adaptive and distributed behavior but this behavior is not aimed at load-balancing as an objective.
On the other hand there are some analytical solutions to balance the load in the whole network and to do resource allocation and resource assignment as a direct result.
Our findings show that there is opportunity for thinking differently. We can be inspired by the wonders of the nature but we need not imitate them. We are free to modify the bio-inspired approaches (ant in our discussion) to obtain new desired behavior. 
In conclusion, our suggestion regarding the use of ant algorithms is that improved behavior is possible by augmenting it with the analytical computation.
Our current work involves applying analytical solutions to ant like algorithms for routing and flow assignment to extract a robust and autonomic routing algorithm capable of tolerating predictable changes in traffic patterns and learn the unpredicted ones. 
Nowadays, the possibility to collect, store, and process very large amounts of data in combination with powerful data transformation and analysis techniques have raised privacy concerns such as when in early 2018, Cambridge Analytica had been granted access to millions of Facebook user profles for political campaigning without users being aware of it. Statutory counter measures to curb data misuse have in particular been undertaken by the European Union in May 2018 by adopting the General Data Protection Regulation (GDPR). The GDPR imposes strict rules on data processing, ownership, information obligation (including raising consent), transparency, and collection of user-related data. Since data protection laws penalize misuse of personal data reactively, they do not actively prohibit misuse on a technical level. In the literature, three major recommender system architectures have been proposed to address privacy issues arising from collecting large authoritative data pools.
Federated learning produces personalized recommendation models by communicating model building between a central server and mutually disconnected peers holding personal information [10, 22] . Thus, federated learning enacts model building centrally on data that is distributed across personal data owners.
In contrast, decentralized recommender systems feature direct interactions between distributed peers without a central server. They are commonly built on top of fle-sharing peer-to-peer networks [3, 21, 28, 31] that use gossip mechanisms [19] in order to establish a logic overlay network for fast network search and network resilience in view of peers joining or churning the network. In short, dissimilar peers are dropped from and similar peers are added to views (lists of visible peers in the network) iteratively. In so doing, the network establishes homogenous interest groups, which share recommendations explicitly among each other.
Pervasive (or ubiquitous) recommender systems [23, 27] are systems that often revolve around location-aware recommendations of for instance items in a nearby shop, restaurants, or events in proximity. The location-based approach naturally circumvents transmitting personal data to a central remote authority for recommendation by instead requesting closeby profle as well as context information.
Pervasive recommender systems are naturally confronted with limited profle data, for usually only a small subpopulation is available in proximity. Fortunately, there are indications that integrating context data into the recommendation process allows to outweigh profle data scarcity [1] .
Among these three approaches we believe that pervasive recommender systems yield the highest potential for data privacy for two reasons. First, the availability of context data as well as mobile compute power on smartphones increases rapidly rendering more and more complex recommendation algorithms feasible on smartphones holding personal data. Second, the model building process happens on-device and does not require connectivity to the network. Consequently, peers become invisible in the network when they do not interact with other peers thus adding privacy on the level of model building. In spite of their potential for recommendation, we see two main burdens. First, pervasive recommender systems are susceptible to data privacy issues since the recommendation mechanism usually builds on the exchange of raw profle data with nearby peers. Second, local scarcity of profle data renders item recommendations on location-independent items taxing. We believe that introducing gossip-based mechanisms and data sampling strategies to the feld of pervasive recommender systems can alleviate both issues. We present the following preliminary results:
• The design of Propagate and Filter, a gossip-based method that addresses the problem of data privacy and data scarcity in pervasive recommender systems. • An implementation of the propagation part in the form of an Android mobile application utilizing Google's Nearby Connections API and its evaluation.
In the present work, we propose a method for disseminating recommendations epidemically in combination with an on-device fltering process for which we proposed a mobile software architecture in [5] . The most similar work is that of Barbosa et al. [25] that propose device-to-device raw profle exchanges in an opportunistic networking scenario. Yet, it difers in that they neither address scalability nor privacy issues. Other related work subsumes: Information Dissemination: Technical works on device-to-device information exchange in proximity are key enablers for collaboratively built recommender systems on mobile devices. The Haggle API [24] implements data-centric message forwarding using Bluetooth, Ethernet, and WiFi 1 . In [26] , the authors implement a Bluetooth-based middleware to create opportunistic networks between passing users. On top of such enablers, the authors in [7] leverage real-world social phenomena such as large crowds at sports events for event-based mobile communication.
Information Filtering: Many approaches to collaboratively and decentrally build recommender systems on mobile devices in pervasive computing environments disseminate information epidemically and generate recommendations on top of user similarity. User similarities have been found to be reliably estimated by contextual information in the form of proximity at a music festival [11] 1 http://user.it.uu.se/~erikn/papers/haggle-arch.pdf or spatio-temporal information in the tourism domain [16] . Estimating user similarity on rating vectors has been performed on subcommunities [13] or afnity networks [12, 29] .
At the proposed information dissemination method's core is the social movement of people carrying mobile devices. On top of that core movement, we conceive a background data exchange between devices whenever they are geographically close to each other followed by an on-device customizable fltering process. We call this method Propagate and Filter.
Each device carries four types of data (see Figure 1 ), where the entire stack of data 2 is used for the derivation of local personalized recommendations.
• Peer Preference List: A list of items rated by the peer 3 . It can contain binary or scalar ratings. In our prototype implementation, the peer can rate movies, where each movie is identifed with a unique identifer provided by the publicly available Internet Movie Database 4 (IMDb). Peer preference lists are kept on the device. • Neighborhood Preference List: Every peer mixes 5 previously collected neighborhood preference lists received from the k most similar peers into a single list of item ratings. It is thus an aggregated preference list of an unknown subset of peers. Neighborhood preference lists get propagated to other peers. Note that every peer controls the amount of his/her own peer preference list that gets propagated to nearby peers. • Similarity Data: Any kind of data that can be used for peer similarity comparison. Similarity data gets propagated to nearby peers and therefore has to be privacy-preserving. Privacy-preserving similarity comparison can among others be performed on item vectors [4] as well as texting data [15] . • Context Data: Data that characterizes the encounter such as location, time, weather, or peer activity (running, eating, commuting) that can be sensed (for example via sensors) or retrieved (for example from the web) [6, 30] .
When two or more peers are geographically close to each other, their smartphones establish pairwise fast and secure connections and exchange their neighborhood preference lists and similarity data (propagation). At propagation time the received data is enriched with context data such as time or location characterizing the encounter. 
Data collection in the propagation step includes unfltered data from every encounter. It is necessary to flter by similarity in order to arrive at relevant information. Upon receiving data from another peer, the fltering process starts on the device. Three steps happen:
(1) Similarity Comparison: Similarity data is used in order to compare peer similarity between sender and receiver. (2) Sample Neighborhood Preference List: If the peer similarity is above the k-th highest, resample the neighborhood preference list on the basis of the peer's preference list and the neighborhood preference lists by the k most similar peers. (3) Update Personal Recommendations: Run a recommendation algorithm 6 on the locally available data (see Figure 1 ) in order to derive new recommendations or update ratings of previously generated recommendations.
Propagate and Filter's propagation step establishes wireless connections between smartphones in proximity in an ad-hoc fashion, exchanges similarity data and neighborhood preference lists, and thereafter terminates the connection. Consequently, the network topology is essentially disconnected and no information on interpeer relationships -such as is the case with Peer-to-Peer network overlays in social networks [20] , recommender systems [3] , or vehicular networks [17] -are exploitable. The entire peers' local databases form a geographically distributed and essentially disconnected database, where every peer holds only a very limited portion of data. Data query and data search are thus not possible at will. Access to other peers' data is limited to the time of contact and amount of data individually made available by nearby peers. 6 Propagate and Filter is independent of any specifc recommendation algorithm. The fltering techniques described in [2] are viable approaches leveraging contextual data.
We call this property Privacy by Disconnection. It is Propagate and Filter's contribution to data privacy.
Gossip protocols require a connected peer-to-peer network in order to converge similar peers toward each other, where network connectivity is retained by peer sampling [18] . In traditional decentralized recommender systems, peer sampling that requires network connectivity is applicable since neither items nor peers are spatial, that is both can be moved in the network at will. In the Propagate and Filter scenario, peers are spatial and cannot be moved in the network at will, though items can. Therefore, Propagate and Filter proposes to converge the recommendation list of latent interest communities in the following way: Peer sampling does not have to be administered by any protocol since it is performed by the global movement of self-organized agents carrying smartphones. Propagate and Filter unfolds its dissemination potential at locations of high degrees of peer mobility such as urban areas [9] . When a peer receives data from a similar peer, he/she resamples his/her neighborhood preference list and else does nothing. As a consequence, Propagate and Filter creates a constant fow of item recommendations that fows between similar peers and dries out between dissimilar peers. This property allows to relay recommendations between peers that have never been geographically close to each other and avoids having to transmit any information on dissimilar peers. In that sense Propagate and Filter addresses the profle data scarcity problem prone to pervasive recommender systems.
We present an Android mobile application that implements a prototype of the propagation step described in Section 3.2. We restrict to the transmission of peer preference lists for reasons of simplicity. The application is available in the Google Play Store 7 .
The prototype allows to search and rate movies locally which are registered and uniquely identifed in the IMDb. Movie ratings are scalar, ranging from 1 to 5 stars, and stored in the format (userID, movieID, scalarRating). A list of movie ratings implements the peer preference list introduced in Section 3.1. Once ratings have been specifed by the user, he/she can activate sharing.
The application's active sharing mode applies advertise -the device broadcasts its existence to other devices in proximity -and discover -the device listens on other device's broadcasting. The sharing process is handled by the Google Nearby Connections API 8 and entirely happens in the background. The Google Nearby Connections API connects devices using one of the three wireless technologies Bluetooth, Bluetooth Low Energy (BLE), or WiFi, automatically selecting the most efcient one in each scenario. When two smartphones establish a connection, ratings are exchanged immediately and stored locally. Note that ratings can be exchanged without a connection to the internet as it is commonly d the case in underground trains. As soon as the internet connection is re-established, detailed movie information such as the movie genre, cast or, trailer can be queried via movieIDs.
We evaluate the prototype implementation in view of its potential to facilitate the proposed pervasive recommender system sketched in Section 3 in particular in urban areas. We conducted six distinct experiments, where every experiment was conducted with Nexus 5 smartphones that support Bluetooth up to version 4 (including BLE) and Wif direct. We re-ran experiments 10 times by default, unless stated otherwise, with distinct experiment setups in order to have empirical evidence of the results' soundness.
(1) Share Large Amounts of Ratings: We share a bulk of 1000 ratings, accounting to roughly 100 kB uncompressed. As soon as the connection is established, all 1000 ratings get transmitted instantly and reliably without any data loss. (2) Share Data Between Multiple Devices: We share ratings between four smartphones simultaneously holding mutually disjoint ratings. Ratings get transmitted correctly and losslessly. (3) Share Data in Public Transportation: We successfully share ratings between three devices in the bus and the underground in the urban city of Berlin being exposed to many WiFi and Bluetooth disturb signals. The loss of internet connectivity does not impair the data transfer. As soon as internet connectivity is re-established, the shared placeholder recommendations get flled with movie metadata fetched from the Open Movie Database 9 (OMDb) API. (4) Efective Transmission Range: Recall that the Nearby Connections API comprises Bluetooth, BLE, and Wif direct under its hood. All three allegedly provide an efective transmission radius of 10 meters for class 2 devices such as smartphones. We tested transmission ranges between 3 and 12 meters, both outdoors and indoors, and with and without obstacles. The results are shown in Table 1 . We conclude that the efective radius of ratings propagation is between 3 and 6 meters. (5) Average Initial Connection Delay: We measure the initial connection delay for two devices, that is the elapsed time before a connection gets established. We place the devices at a distance of 1 meter in order to guarantee connectivity, where the connections were made with distinct app sessions (partner device's id not in cache). The average connection delay (pre-connection) has to be performed continuously, we would like to know whether or not advertising, discovering, and sharing information in the background for extended periods of time is feasible or not. Since we cannot experiment in a real-life scenario with at times no devices to connect to, and then multiple devices to connect to, we only measure the application's pre-connection battery drainage, which therefore presents a lower bound for battery drainage. We use two devices reset to factory settings. We track the battery levels for three distinct scenarios (a) application running in the background with sharing on, (b) with sharing of, and (c) factory settings, where in all three cases the displays were of. The results are shown in Table 2 .
The experimental fndings indicate that the Propagate and Filter's propagation step works reliably with larger amounts of ratings, in multi-device scenarios, and in areas without internet connectivity such as underground trains. Yet, the implementation has certain limitations. First of all, information can only be disseminated reliably within a radius of 6 meters, which on the one hand strengthens privacy, and on the other limits the number of potential peers to share information with. Furthermore, the average initial connection delay of 25.9 seconds is considerable and does not allow to share information between for instance passing pedestrians, yet includes scenarios such as waiting at the trafc lights, sitting next to each other in a café or restaurant, or taking public transportation. Last but not least, battery drainage is relatively high at at least 5% per hour, which thus prohibits continuous advertising and discovery. Apart from ever improving transmission standards and hardware, we believe that it is possible to limit activity and inactivity of the sharing process efciently on the logical level leveraging sensor information on smartphones.
Future work includes the launch of the mobile application for the collection of usage data. Recall that we left the sampling process and the recommendation algorithm as placeholders (see Section 3.3 (2) and (3) respectively). Once usage data is collected it will be possible to test distinct combinations of sampling and recommendation strategies. Furthermore, the evaluation of feedback on the application including the user interface and user experience is due.
Over the years, the performance of monocular 3D Human pose estimation has improved significantly by leveraging complex CNN models. [37, 18, 35] . However, these methods rely heavily on large-scale 3D pose annotated training data, which is difficult and costly to obtain, especially under in-the-wild setting for articulated poses. The two most popular 3D ground-truth annotated datasets, Human3.6M [7] and MPI-INF-3DHP [13] , have 3.6M and 1.3M annotated poses, respectively. Unfortunately, these datasets are biased towards typical indoor setting like uniform background and illumination and lack real-world environment variations [37] . However, it is relatively easier to obtain time-synchronized video streams of human poses from multiple different viewpoints. Therefore, techniques that can employ un-annotated multi-view human-pose data to learn the 3D structure and geometry could prove benefi- * -equal contribution cial for human-pose estimation with small amount of annotated data. To this end, we propose a metric learning based approach to jointly learn a 3D human pose embedding and pose regression using the embedding from synchronized videos of human motion with very limited pose annotations. Our approach doesn't require camera extrinsics or prior background extraction. Therefore, it can be easily extended to train with further un-annotated in-the-wild data. We seek motivation from a recent work in [22] , where image generation in different views via a geometry-aware latent space is used to improve pose-estimation under limited 3D supervision. This method, however, requires camera extrinsics and static background during training, which limits its application to indoor datasets. Our proposed approach is free from these constraints and, therefore, can potentially be used for in-the-wild setting. Moreover, we also show superior performance with faster inference.
We utilize our framework to improve pose estimation accuracy under limited 3D supervision. We show that weak supervision in learning the embedding ensures that our model's performance degrades gracefully when 3D supervision is progressively reduced. Additionally, we eliminate the subject-specific appearance information from our latent embedding with the help of an adversarial mechanism which leads to further improvements and outperforms the current state-of-the-art [22] . Lastly, we use smaller network architecture that affords 3X faster inference time. A simplified overview of our approach and its utilization is shown in Fig. 1 . The formulation of our loss function leads to a view-invariant embedding, and in Sec. 5, we demonstrate the richness of our learned embedding to capture human pose structure invariant to viewpoint by way of carefully designed pose retrieval experiments and establish novel benchmarks on Human3.6M and MPI-INF-3DHP to facilitate future research. A summary of our contributions is,
Figure 1: A schematic diagram explaining the motivation of our work of learning an pose embedding from multi-view images and utilizing the embedding for 3D pose estimation and view-invariant pose retrieval. The learned embedding space lies on the surface of a multi-dimensional unit hyper sphere. A detailed 2D T-SNE visualization of the embedding space in presented in the supplementary material.
• Formulating view-invariant pose retrieval benchmarks based on Human3.6M and MPI-INF-3DHP datasets.
In this section, we first review prior approaches for learning human-pose embedding followed by a discussion of previous weakly supervised methods for monocular 3D human pose estimation to bring out the differences between our approach and the previous art. Later, we discuss the usage of deep metric learning in capturing image similarity.
Historically, human-pose embedding have been employed in tracking persons [34, 10] . Estimation of 3D human pose and viewpoint from input silhouettes via learning a low dimension manifold is shown in [4] . Pose regression and retrieval in 2D by learning pose similarity embedding is shown in [9, 16] , but they require 2D annotations. In [27] , the need for 2D annotations is eliminated by using human motion videos and temporal ordering as weak supervision with a metric learning based loss. Unlike the aforementioned approaches, we learn a view-invariant 3D human pose embedding by taking advantage of semantically similar images in synchronized multi-view videos. In [29] a 3D pose embedding learnt using an over-complete autoencoder for better structure preservation, however unlike us they requires require full 3D annotations.
Majority of supervised 3D Human Pose Estimation algorithms [14, 12, 31, 21, 17] use 3D pose labels to train a model for regressing 3D joints locations from images or decouple the problem into 2D joint regression followed by 2D-to-3D lifting. In either case, they need large amount of annotated 2D and 3D training data. Another line of work [33, 26] focuses on training for 3D estimation with datasets capturing the scene in multi-view images. In [19] , approximate 3D human joint labels for supervision are generated by triangulating its corresponding 2D annotations from multiple view images. Utilizing multi-view images during training has recently been proposed in [23, 22, 20, 3] . Methods using multi-view images can further be classified into the following categories, strong 2D and limited 3D supervision -Methods mentioned in [23, 20, 3] use full 2D supervision from in-thewild datasets like MPII [1] to either estimate 3D pose from images or perform 2D to 3D pose lifting. In [3] , a latent embedding capturing 3D pose is learned by reconstructing 2D pose from the embedding in a different view. A shallow network requiring much less supervision is subsequently learned to regress 3D pose from the embedding. In [20] , back-projection of predicted 3D pose to its 2D representation and its difference with the input 2D pose is used as a weak supervision in [20] . Additionally, it uses multiple temporally adjacent frames at inference to refine predictions. A network with pre-trained weights for 2D pose estimation is used for 3D estimation in [23] . limited 3D supervision -To alleviate the need for a large amount of 2D annotations, [22] learns an unsupervised embedding and estimates pose from it with limited 3D supervision. Novel view synthesis using synchronized videos from multiple views is used to learn a geometry aware embedding capturing human pose. This method however still requires camera extrinsics and background extraction.
Our proposed method also utilizes synchronized videos from multiple views to learn a pose embedding but unlike [22] does not require camera extrinsics and background information. Moreover, due to our metric learning based approach, we do not require to perform image-reconstruction that affords smaller networks, Resnet-18 [6] vs. Resnet-50. Subject-specific appearance disentanglement from human pose embedding has been shown in [22] using appearance swap followed by an image reconstruction task. Such swapping mechanism doesn't guarantee removal of appearance from pose embedding as the network has an alternate information pathway through the pose branch. We, on the other hand, adopt the method in [11] using adversarial losses to remove the subject-specific appearance information from our pose embedding.
To learn image similarity, images are mapped to a low dimensional embedding space via a CNN and trained with a contrastive or triplet loss. In contrastive loss [25, 36, 5] , semantically similar image pairs (positive pairs) are mapped close together in the embedding space while those dissimilar content (negative pairs) are mapped far apart. In triplet loss [2, ?] , the hard constraint of contrastive loss is relaxed by ensuring a relative separation between positive and negative image pairs by a pre-determined margin. Hence, the euclidean distance between two images in the embedding space gives the measure of their similarity. For our application a pair of images are semantically same if they represent humans with the same underlying 3D pose.
The performance of models using either of the losses is highly dependant on the quality of dissimilar samples used during training [25, 36] . The current state-of-the-art image descriptor learning framework Hardnet [15] provides a good-trade-off between performance and training time by selecting the hardest negative within a batch. Inspired by its performance and simplicity in training, we adopt the hardnet to learn our pose embedding.
Our proposed approach is comprised of two modules i) learning an embedding capturing human pose information from multi-view time synchronised videos using metric learning ii) regressing 3D human pose from the embedding using minimal 3D supervision. We jointly learn the two modules as shown in Fig. 3 . Metric learning provides a weak supervision and reduces the dependency on large 3D annotations in our framework while pose regression guides the framework to learn pose specific features. The following sub-sections explains the two modules,
To learn our pose embedding via metric learning, we utilise Hardnet framework [15] due to its the state-of-the-art performance in image patch matching invariant to camera viewpoints. The datasets used for training have the following generic format. The entire data is divided into images belonging to one of S = {S 1 , S 2 , . . . S n } set of subjects. The set P ⊂ IR 16×3 is the set of all possible poses and each pose is viewed from V = {v 1 , v 2 , . . . v q } set of viewpoints.
In the hardnet training regimen, each batch consists of paired anchor( § va p ∈ X ) and positive( § v b p ∈ X ) images that share same pose p ∈ P but taken from different viewpoints v a and v b . X ⊂ IR 3×256×256 is the set of all images. It is to be noted, since we use time synchronization to choose a pair of anchor and positive, it is implied that they share the same subject. However different anchors within a batch can be from different subjects. We pass both the anchor and positive images through feature extractor (F θ F : X → Ψ; Ψ ⊂ IR 512×4×4 ) to generate features {ψ
The feature extractor network is parameterised by θ F . The features are then finally passed through an embedding generating network (G θ G : Ψ → Φ; Φ ⊂ IR dim φ ; where dim φ is dimension of our embedding). Let's assume we feed anchor and positive images to F in batches of m. Once corresponding features {φ pi respectively. Mathematically, the sampling is formulated in Eq. 1. Here, α denotes the margin.
The average triplet loss over the batch is then given by,
Similarly, the average contrastive loss is given by,
Our anchor and positives examples always share the same subject and it results in unwarranted appearance bias in the embedding. Hence, to improve pose accuracy, it is necessary to disentangle appearance information from our learned embedding. To this end, we introduce an adversarial loss on our ResNet feature extractor F θ F so as to fool an ap-
Our formulation is inspired from [11] where adversarial training is used to disentangle individual identity and other facial information from images of faces. Formally, we define our adversarial formulation with input image x i and subject label y i ∈ Y and predictionŷ i in Eq. 4,
In Eq. 4, L class tries to make the classifier M predict higher probability for the correct target while L adv tries to fool the classifier to predict uniform probability for all subjects by tuning the ResNet feature generator F. At equilibrium, F generates features Ψ which are devoid of any subject appearance information. Note that our weak supervision losses namely, L class , L adv , L cnstr / L trip do not require camera extrinsics, background extraction, pose annotations etc. and the only sources of supervision are synchronizing the videos, annotating the subject and pre-trained ImageNet [24] weights.
Most 3D human pose estimation approaches focus on regressing pose in the local camera coordinate system. In this representation, frames captured from different camera views but of the same time instant will be associated poses with different 3D co-ordinates values of the body joints. However, the frames are all mapped to the same point in our embedding space irrespective of their viewpoint by our formulation. Hence, regressing pose in this representation from our learned embedding is ambiguous as the relation to be learned by the regressor is one-many. In this regard, one can utilise pose represented in the MoCap system's coordinate system. We term this representation as global pose. In this representation, frames captured from different viewpoints belonging to a particular time instant are associated with one pose. However, frames captured at different time instants can contain poses which are rigid body transforms of one another while having same set of 2D projections. In such cases, regressing pose from our embedding is again learning a one-many relation. In Fig. 2 , an example of such ambiguity is illustrated.
To estimate 3D pose from our embedding and bene- fit from the embedding loss, we formulate a corresponding view-invariant pose representation. We term this representation as canonical pose. To ensure consistency among poses captured from different camera views and at different time instants in our canonical pose representation, we ensure that the bone connecting the pelvis to the right hip joint is always parallel to XZ plane. In Human3.6M dataset, the upward direction is +Z axis while XY plane forms the horizontal. So, we rotate the skeleton about the +Z axis until the above mentioned bone is parallel to the XZ axis. This makes the depth aligned along Y axis. We don't require any translation since the joint positions are root relative with pelvis being the root. As an added bonus, unlike pose estimated in camera coordinates, our predicted canonical pose does not change orientation with variations in camera viewpoint. A similar approach to achieve a rotation invariant pose is suggested in [32] . Note that the canonical pose is constructed directly from MoCap system's coordinates and doesn't require camera extrinsics. We learn our canonical pose from the latent embedding Φ space mentioned. To this end, we use a shallow network (H θ H : Φ → P), as shown in Fig. 3 . We regress pose using L pose = p −p 1 , with target pose p ∈ P and predicted posep.
We train our framework with all the losses simultaneously by default and optimize different network parameters according to Eq. 5. Note that the the gradient from L class does not flow through the network F. We also provide ablation on the different losses to understand their impact on pose estimation accuracy.
The metric learning loss used to learn Φ serves as a weak supervision in canonical pose estimation. The L conrst loss ensures latent embeddings φ 
We build our architecture on the ResNet framework and choose the 18 layer version in our implementation. We only use the first 4 residual blocks and initialize them with pretrained ImageNet [24] weights. In addition, we modify the batch-norm layers by turning off the affine parameters as suggested in [15] . When the input image size is 256 × 256 , the output of the ResNet network is 512 × 8 × 8. We down- p are a pair of anchor and positive images taken from different camera views. F is the ResNet based feature extractor. G maps features extracted ψ from F to our embedding φ. The Hard Negative Sampling module performs in-batch hard mining as given in Eq. 1. Network H regresses posep from our embedding φ. Classifier M is used to classify subjects from set S from features ψ. L class , L adv and L pose are discussed in Sec. 3.2. Network blocks sharing same colour also share parameters. sample the ResNet output by half using a MaxPooling layer to get Ψ. The embedding network G maps it to the output embedding of dimension dim φ , using a Fully-Connected layer and BN layer followed by L2-Normalization as done in [15, 30] . The value of dim φ is 256 for all our experiments. The classifier network M consists of Conv(512, 256, kernel=1), BN, ReLU, Conv(256, 256, kernel=4), BN, ReLU, FC(256, |S|) with |S| = 5.
For regression, similar to [12] , we normalize the dataset for each joint. The pose regression network G consists of fully-connected layer FC(256, 48), with Φ ⊂ IR 256 . We choose the margin α for L conrst to be 0.6. Adam [8] with default parameters (α = 0.9, β = 0.99) is used as the optimizer with initial learning rate 10 −3 . The model is trained for 25 epochs with the learning rate dropped by 0.1 after every 15 epochs with a batch size of 128. A schematic diagram of our network architecture is shown in Fig. 3. 
We use the popular Human3.6M [7] and MPI-INF-3DHP [13] datasets.
• Human3.6M -The dataset contains 3.6 million frames captured from an indoor MoCap setting with 4 cameras(V). It comprises of 11 subjects (actors)(S), each performing 16 actions with each action having 2 sub-actions. Following the standard protocol [28] , Protocol 2, we use subjects (S1, S5, S6, S7, S8) for training and (S9, S11) for testing. As used by several other methods, we use images cropped using subject bounding boxes provided with the dataset and temporal sub-sampling to include every 5 th and 64 th frame for training and testing phase, respectively,
• MPI-INF-3DHP -This dataset is generated from a MoCap system with 12 synchronized cameras in both indoor and outdoor settings. It contains 8 subjects(S) with diverse clothing. We use the 5 chest height cameras(V) for both training and test purposes. Since the test set doesn't contain annotated multi-view data, we use S1-S6 for training and S7-S8 for evaluation.
We perform the same quantitative experiment as presented in Rhodin et. al [22] to establish the benefits of the learned embedding in pose estimation. We evaluate using two well adopted metrics, MPJPE and Normalized MPJPE (N-MPJPE) (introduced in [23] ) which incorporates a scale normalization to make the evaluation independent of person height as our evaluation metric. We compare our proposed approach and its variants against a baseline which only uses L pose . In addition, we compare our method against the approach proposed by Rhodin et. al [22] and [23] , although it estimates human poses in the camera coordinate system. [22] . Our proposed model outperforms the current state of the art.
We also report the performance of Rhodin et. al [22] using ResNet-18 as the feature extractor instead of ResNet-50. It is to be noted [22] uses additional information at training time in form of relative camera rotation and background extraction which requires sophisticated, well calibrated setup. We acknowledge existence of more accurate methods than [22, 23] on Human3.6M when abundant 2D and 3D labels are available. However, like [22] to highlight the point of limited supervision, we omit them in our comparison. We also report performance of [3] which requires limited 3D supervision but uses full 2D supervision from MPII [1] dataset. We did not include the results of [20] as it requires multiple temporally adjacent frames at inference. We report both N-MPJPE values when our model is trained and tested on Human3.6M dataset with progressively less supervision for pose regression in Fig. 4 . The amount of supervision is reduced gradually from full supervision using all 5 subjects, to S1+S6, only S1, 50% S1, 10%S1 and finally 5% S1. Our proposed model clearly outperforms the baseline as 3D supervision is reduced with a gain of 33 mm (21.5%) N-MPJPE when only S1 is used for supervision). The performance of our model shows little degradation even on further reduction in supervision. A variant of our proposed model with appearance not disentangled also beats the baseline convincingly but performs worse compared to when appearance is disentangled. The observation validates the importance of L conrst in providing weak supervision capturing 3D pose and the need to eliminate appearance bias. Qualitative comparison of our method against the baseline is shown in Fig. 4 . In Table. 1, we compare MPJPE and N-MPJPE values of our approach against baseline and [22] . Considering N-MPJPE, our method outperforms [22] by 14 mm when fully supervised on 3D data and by 2 mm when supervision is limited to S1. When MPJPE is considered the difference is 4 mm. Interestingly as mentioned in [22] , the performance of [23] drastically falls when pre-trained weights from strong 2D pose supervision is not used (reported in Table 1 as Rhodin [23] * and Rhodin [23] ). Table 1 : Comparing N-MPJPE and MPJPE values between different approaches on Human 3.6M dataset when supervised on all 5 subjects and on only S1. Note: Pre-trained ImageNet weights are used to initialize the networks by all the methods. Methods or its variants marked with '*' are supervised with large amount of in-the-wild 2D annotations from MPII [1] dataset either during training or by means of a pre-trained 2D pose estimator. All other methods use much weaker supervision by assuming no 2D annotations and ours outperforms the state-of-the-art [22] in such settings.
Method N-MPJPE MPJPE We additionally compare performance of our learning framework when target pose is represented in MoCap's(global pose) against our canonical representation in Table. 2. The increase in N-MPJPE by 43mm for global pose validates the importance of our canonical representation to the efficacy of our approach.
An additional benefit of our proposed framework is that it uses a much smaller ResNet-18 feature extractor as compared to ResNet-50 used in Rhodin et. al [22] . This enables our model achieve an interference time of 24.8 ms in comparison to 75.3 ms obtained obtained by [22] averaged over batch size of 32 using NVIDIA 1080Ti GPU. Hence, our method performs roughly 3X faster inference while attain- Figure 5 : Qualitative results on canonical pose estimation by our proposed framework (Ours) against our Baseline on Human 3.6M test split (S9, S11). Both the models are trained with supervision from labels of subject S1. Our method produces more accurate estimates for even for challenging poses like 'sitting', 'kneeling', 'bending' ing better accuracy.
In this section, we demonstrate the quality of our learned embedding through a series of retrieval tasks and provide benchmarks against an oracle on popular human pose datasets. Given a query image of a human from a particular view, our learned embedding ensures that images from all the other views are mapped close to it in the embedding space.
To quantify the view-invariance, we formulate Hit@K which measures the percentage of queries with the exact pose among the top K poses retrieved through the embedding. For example, 85% Hit@5 indicates that out of 100 queries, 85 queries have atleast one of the images with exact pose but from a different viewpoint in the top 5 retrievals. We also define Hit@K All which registers a hit when all of the images with exact pose are present in top K retrievals.
Further, we want to ensure that the images with similar pose should be clustered together in the embedding space. In this regard, we propose Mean PA-MPJPE@K which measures the Procrustes Aligned Mean Per Joint Position Error(PA-MPJPE) of K closest neighbours from other views. The retrieved poses, although similar to the query in terms relative skeleton configuration, can have different orientations. Hence we use PA-MPJPE, which is MPJPE calculated after rigid alignment of retrieved pose with the ground truth of query pose, for a fair evaluation.
We compare our model against an oracle which uses ground truth 3D annotations. Given a query image, we ensure that the retrieval database contains images taken from viewpoints other than that of the query image. It is done to clearly bring out the view invariance property of the proposed embedding. The aforementioned two performance metrics are used to quantify the pose retrieval performance, namely Mean PA-MPJPE@K and Hit@K. First, we report the Mean PA-MPJPE@K between query pose and its K nearest neighbors in the embedding space. In Fig. 6 , we show comparison of Mean PA-MPJPE@K of retrieved poses when retrieval is done from images with: Case 1: all test subjects including that of query's. Case 2: all test subjects except that of query's, termed as cross. We report our results relative to the oracle. The horizontal plots with low errors suggest that our model picks poses similar to that of oracle irrespective of K. The error is lower for Case 1 than Case 2 due to the presence of images from different viewpoints sharing the exact pose as that of query's.
Our second metric, similar to [9] , is computing Hit@K which measures the occurrence of a correct pose among top K retrievals using nearest neighbors. A retrieved pose is considered correct if it is exactly same as the query pose but from a different viewpoint. However, unlike [9] , we do not retrieve poses having same viewpoint as that of query. We measure Hit@K under Case 1 settings previously mentioned. Under easy('E') setting, a hit is registered when a retrieved image with the correct pose differs in viewpoint with the query by less than 90
• . Under hard('H'), the viewpoint difference has to be more than 90
• . Unlike, 'E' and 'H', in 'All', a hit is considered when correct poses from every viewpoint other than that of query are present in the top K. The evaluation of this metric on our model is shown in Fig. 7 . We achieve high accuracy rates of more than 85% for easy('E') viewpoint differences on Human3.6M even for low values of K = 2, 5. For hard ('H') viewpoint differences, the performance goes down but still remains above 70%. However, the most impressive performance of our model is shown when retrieving 'All' other views with K = 5. The accuracy is close to 50%. This implies our model retrieves all the 3 corresponding views in the top 5 slots, half of the time. The performance on MPI-INF-3DHP is comparatively lower than Human3.6M, specially on low K values. We attribute this to MPI-INF-3DHP having smaller training data. We have included qualitative results of our retrieval experiments in the supplementary material.
An interesting observation from pose regression results shown in Fig. 4 , is that our proposed model performs worse than the baseline under full 3D supervision. This is also observed in [22] , and one possible explanation is that the additional weak supervision losses lead to a joint optimum which is sub-optimal for pose regression. But in case of reduced 3D supervision, all the losses work in synergy and produce a high improvement over the baseline pose accuracies. To analyse this further, in Table. 3 we show the result of adding progressive pose supervision on Mean PA-MPJPE@5 for cross subject retrieval on Human 3.6M dataset. We observe that even a limited amount of pose supervision (5% S1), reduces Mean PA-MPJPE@K by 13.23mm. We can also see that using only L pose supervision on S1 without other losses, leads to poor retrievals. Here, we note that a single embedding trained with both L pose and L conrst outperforms the respective task specific embeddings, when 3D supervision is limited. 
In this paper, we demonstrated a metric learning approach to capture 3D human structure and its effectiveness in both pose estimation and pose retrieval tasks. More specifically, the information from our embedding reduces the need for 3D supervision when regressing human pose, enabling our method to outperform contemporary weaklysupervised approaches even while using a smaller network. Further, we provided strong benchmarks for view-invariant pose retrieval on publicly available datasets.
In future, we plan to use multi-view synchronised videos captured in-the-wild and synthetically generated, consisting of images taken from a large no. of viewpoints with diverse appearances, to improve the quality of the embedding and in the wild generalisation. Also, we plan to apply our approach to recognize actions from unseen viewpoints. Figure 8 : Shows top 5 pose retrievals using our embedding on Human3.6M [7] and MPI-INF-3DHP [13] datasets. The top row marked in Red is the query image, the next five rows shows retrieved images taken from different viewpoint having similar poses.

In Fig. 8 , we provide examples of retrieved poses from viewpoints different to that of the query from the popular Human3.6M [7] and MPI-INF-3DHP [13] datasets.
In Fig. 9 , a 2D T-SNE visualisation of our leaned pose embedding space is shown. Figure 9 : The top image shows a 2D T-SNE visualisation of our learned embedding space on test split of Human3.6M [7] . In bottom left and right images, zoomed in views of green and red boxes from top are shown respectively. One can observe the clusters of similar poses formed in the zoomed in boxes. Note: The 2D visualization provided is an approximation of the original embedding space which lies on the surface of a multi-dimensional unit hyper-sphere. Hence, there are inconsistency in smoothness in the pose space at some places.
transcriptional and metabolic changes that increase production and accumulation of the compatible solute glycerol. Mounting a rapid response to increased osmolarity is essential to yeast survival [3, 4] . Accordingly, S. cerevisiae can activate the HOG pathway within one minute of experiencing an osmotic shock [5] . Yeast can also effectively respond to rapid periodic oscillations (with frequencies up to 0.0046 Hz) between low and high external osmolyte concentrations [3] .
Despite its importance during periods of increased osmolarity, unintended activation of the HOG pathway during growth in normal osmolarity conditions is severely deleterious [6, 7] . The Sln1-Ypd1-Ssk1 three-component phospho-relay is responsible for maintaining inactivation of the HOG pathway under normal conditions. This three-component phospho-relay is a variant of the twocomponent signaling systems used by many prokaryotes for osmoregulation, chemotaxis, and other key cellular processes. Sln1 is active in vivo as a membrane-bound dimer [6] . Under normal osmolarity conditions, Sln1 autophosphorylates on a histidine residue and then irreversibly transfers the phosphate to an aspartate in its response regulator (RR) domain (Figure 2A ). Aspartatephosphorylated Sln1 binds to the histidine-containing phospho-transfer (HPt) protein Ypd1 and reversibly transfers its phosphate to Ypd1 ( Figure 2B ). Finally, phospho-Ypd1 transfers its phosphate to dimeric Ssk1, preventing it from interacting with Ssk2 (or the functionally redundant Ssk22) and inhibiting HOG pathway activity [5, 8, 9] . The sequence of phosphate transfers in the three-component relay is summarized in Figure 2A . In response to osmotic shock, the phospho-relay is inactivated, and Ssk1 is rapidly dephosphorylated through an as-yet unknown mechanism. Unphosphorylated Ssk1 then activates Ssk2 and Ssk22, leading to induction of the HOG pathway [1, 2] . It is thus the essential controller of HOG pathway activity, and variations in its concentration could compromise fitness.
There is limited existing experimental evidence that the phospho-relay is able to maintain robust phosphorylation of Ssk1 and inactivation of the HOG pathway despite changes in the levels of some pathway components [7, 9, 10] . We undertook a comprehensive characterization of the sensitivity of HOG pathway activation to changes in the expression levels of the phospho-relay proteins Sln1, Ypd1, and Ssk1. We systematically under-and overexpressed the three proteins using the GEV artificial induction system, which allows for rapid and nearly gratuitous induced expression of individual yeast genes [11] . We found that the phospho-relay maintains inactivation of the HOG pathway even after moderate perturbation of Sln1, Ypd1, and Ssk1.
We developed a detailed, biochemically realistic mathematical model of the HOG pathway three-component phospho-relay to elucidate the mechanism underlying this robustness ( Figure 2C ). Our model incorporates extensive structural and mechanistic information about the phospho-relay and considers nearly all possible interactions between the three relay proteins. We used massaction kinetics and algebraic calculations to characterize the steady-state behavior of the model. Steady-state algebraic models are a useful alternative to existing computational models of the HOG pathway for understanding robust behavior [3, 7, 12] . Unlike numerical simulations [3, 7, 12] , algebraic manipulations can be done without ever assigning specific values to the parameters (i.e., the rate constants in the reaction network), many of which are difficult or impossible to measure experimentally [13] . This advantage enabled us to design and analyze a more biochemically realistic model. A steady-state approximation Phospho-transfer proceeds from Sln1 (after autophosphorylation on H576 and transfer to D1144) to Ypd1 to Ssk1, as indicated by the numbers in circles. B Crystal structure of Ypd1 (green) in complex with the response regulator domain of Sln1 (Sln1-R1, red). Drawn from data presented in [18] . C Reaction network diagram describing our model of the phospho-relay. The network includes nearly all possible interactions between the three proteins subject to the biochemical assumptions outlined in the main text. For clarity, the reaction network is color-coded to indicate the groups of reactions involved in each phosphorylation event. S denotes Sln1, Y denotes Ypd1, and K denotes Ssk1. Phosphorylated residues are denoted by p, unphosphorylated residues by o. D Directed graph describing the subnetwork involving phosphorylation and dephosphorylation of Ssk1. The graph contains four loops that are connected as a branched tree.
is appropriate because previous studies have shown that activation of the HOG pathway does not vary under normal growth conditions [3, 4, 12, 14, 15] .
Our steady-state analysis predicted that relative levels of dephosphorylated Ssk1 depend solely on Ypd1 levels and that robustness is achieved by maintaining Ypd1 in large excess. We experimentally tested this prediction by perturbing protein expression levels so as to deplete this buffering pool of Ypd1. All such perturbations compromised the ability of the phospho-relay to inhibit the HOG pathway, leading to hyperactivation in normal osmolarity conditions. The presence of a large buffering pool of an intermediate phospho-relay component is a previously underappreciated mechanism for robustness and suggests a possible advantage of a three-component relay over a two-component system.

Inappropriate HOG pathway activation during normal osmolarity growth unnecessarily alters transcription and metabolism [7, 9, 10] . To assess the robustness of HOG pathway inhibition by the three-component phosphorelay, we created strains capable of overexpressing Sln1, Ypd1, and Ssk1 in response to β-estradiol. For these overexpression experiments, we used diploid strains homozygous for the GEV artificial transcription factor [11] . GEV consists of the Gal4 DNA-binding domain, the estrogen receptor, and the VP16 activation domain. Upon treatment with the hormone β-estradiol, GEV rapidly translocates to the nucleus, where it activates transcription from promoters containing the Gal4 DNA-binding target sequence. The GEV system enables rapid induction of individual yeast genes with limited off-target effects [11] . To make a given phospho-relay gene GEVinducible, we placed it under the control of the GAL1 promoter (SLN1/P GAL1 -SLN1, YPD1/P GAL1 -YPD1, and SSK1/P GAL1 -SSK1), as described previously [11] .
Inappropriate activation of the HOG pathway is known to cause a growth defect [7, 9, 10] . We therefore measured growth of these GEV strains after induction with β-estradiol at a range of concentrations to screen for HOG pathway hyperactivity. A strain carrying an inducible allele of Pbs2 (P GAL1 -PBS2/PBS2) was used as a control because Pbs2 overexpression is known to cause severe growth defects from inappropriate activation of the HOG pathway [7] . As shown in Figure 3A , strains overexpressing Pbs2 exhibited a measurable growth defect, while strains overexpressing components of the phospho-relay (Sln1, Ypd1, and Ssk1) showed no significant change.
We also assayed for Hog1 phosphorylation following GEV induction of phospho-relay components to obtain direct evidence that moderate overexpression does not cause HOG pathway hyperactivation. We used overexpression of Pbs2 and Ssk2 (also known to cause hyperactivation of the HOG pathway [7] ) as positive controls. After 30 minutes of GEV induction, there was no detectable increase in Hog1 phosphorylation in strains overexpressing phospho-relay components ( Figure 3B ). In contrast, Pbs2 and Ssk2 overexpression caused phosphorylation of Hog1. Interestingly, overexpression of the Ssk2 homolog Ssk22 had the strongest effect on Hog1 phosphorylation.
We then constructed diploid strains with a single inducible copy of the HOG phospho-relay gene of interest and a single P STL1 -YFP reporter to assay for HOG pathway transcriptional activity in response to overexpression of relay components. Stl1 is a glycerol/H + symporter whose expression is strongly upregulated in response to osmotic shock [16] . We overexpressed all three relay components, Pbs2, and Ssk22. Here we used Ssk22 as a control instead of Ssk2 because it showed a strong effect on Hog1 phosphorylation in the previous experiment. After 120 minutes of induction with 10 μM β-estradiol, Ssk22 and Pbs2 overexpression led to HOG-dependent transcription from the STL1 promoter, as indicated by an increase in YFP fluorescence ( Figure 4 ). Over the same period of time, overexpression of the phospho-relay components (Sln1, Ypd1, and Ssk1) caused almost no transcription from the STL1 promoter. After 19 hours, overexpression of Ssk1 and Sln1 did increase expression of YFP from the P STL1 -YFP reporter. These effects on longer time scales may have been due to factors beyond Ssk1 and Sln1 overexpression, however, as there was a population expressing YFP even in the control strain at 19 hours.
Growth, phosphorylation, and transcriptional measurements of HOG pathway activity all indicated that HOG pathway activation is robust to fluctuations in the Sln1-Ypd1-Ssk1 phospho-relay components. These results prompted us to investigate the mechanistic basis of this robustness using a biochemical model. The reaction network underlying our model ( Figure 2C ) has 37 nodes and involves 13 species. In this section we discuss the biochemical justification for key assumptions in the model.
There exist high-resolution crystal structures of Ypd1 alone and in complex with the Sln1 receiver domain ( Figure 2B ) [17, 18] . Genetic and biochemical evidence suggest that Sln1 forms an obligate homodimer and that Ypd1 can interact with either half of the dimer [6, 19] , which implies that formation of a Ypd1-Sln1(dimer)-Ypd1 ternary complex is possible. Accordingly, we include Sln1 in the reaction network as a dimer with four relevant phosphorylation sites. The Sln1 dimer is referred to as S H1D1H2D2 , where H1 and D1 denote the phosphorylatable histidine and aspartate residues in one half of the dimer and H2 and D2 denote the corresponding residues in the other half. The dimer is allowed to autophosphorylate on either histidine residue. Phospho-transfer from the histidine to the aspartate in the RR domain is treated as irreversible. A reverse reaction is included with all histidine autophosphorylation steps to account for possible hydrolysis of the phosphate prior to transfer [2, 20] . Each half of the dimer is assumed to be independent from the other. Coincident phosphorylation events (e.g., S OOOO forming S POPO in one step) are therefore considered to be unlikely and are excluded from the model. Following these assumptions, the model includes nine different forms of free Sln1 that are interconverted as shown in the reaction network.
In the second leg of the phospho-relay, any Sln1 phospho-form with at least one phosphorylated aspartate is allowed to reversibly associate with unphosphorylated monomeric Ypd1 (Y ) to form a series of binary complexes (YS OOOP , YS OPOO , YS POOP , YS OPPO , and YS OPOP ). Sln1-Ypd1 phospho-transfer has been shown to be reversible [21] . As such, all reactions that produce phospho-Ypd1 (Y P ) are treated as reversible. No
A Homozygous GEV diploid strains with a single inducible copy of a phospho-relay gene were grown to saturation in different concentrations of β-estradiol. The optical density (OD 600 ) after 13 hours of growth is plotted. Overexpression of Pbs2 caused a growth defect at higher concentrations of β-estradiol, while no significant growth defects were observed following overexpression of relay components (beyond the defect in the wild-type due to overexpression of Gal4 target genes) . Each point represents the mean and standard deviation over four replicates. B We assayed for Hog1 phosphorylation using an antibody specific to doubly phosphorylated Hog1 to confirm that moderate overexpression of phospho-relay components does not lead to activation of the HOG pathway. Overexpression of phospho-relay components using a saturating dose (10 μM) of β-estradiol did not lead to Hog1 phosphorylation. Overexpression of the positive controls Ssk22, Ssk2, and Pbs2, however, led to clear upregulation of Hog1 phosphorylation after 30 minutes of induction. Total Hog1 is shown as a loading control.
assumption is made about which half Ypd1 binds to in the YS OPOP binary complex because the halves of the Sln1 dimer are considered to be indistinguishable. Accordingly, the YS OPOP complex is allowed to form from either Y P + S OOOP or Y P + S OPOO . Additionally, YS OPOP can bind to a second Y molecule to form a ternary complex (YS OPOP Y ) that produces Y P using either of the Sln1 phospho-aspartate residues.
Phospho-Ypd1 then binds to and phosphorylates Ssk1, which is modeled as a dimer with two phosphorylation sites (K OO ) [9] . The two monophosphorylated forms of Ssk1, which are known to be fully inactive [9] , are assumed to be identical (i.e., K PO = K OP ). Y P can form a complex with K OO , leading to the production of K OP , and in turn Y P can bind to K OP and transfer a phosphate to produce K PP . The network includes one further interaction between Ypd1 and Ssk1 deduced from kinetic data. The half-life of phospho-Ssk1 in vitro has been measured to be dramatically different with and without the presence of Ypd1 (over 40 hours vs. 13 minutes, respectively), suggesting that Ypd1 binds to K OP and K PP to prevent hydrolysis of the phosphate [20, 22] . Accordingly, the reaction network includes the reversible formation of dead-end complexes between Ypd1 and phospho-Ssk1. Finally, unstabilized K PP and K OP are allowed to lose phosphates via spontaneous hydrolysis. Inclusion of these hydrolysis reactions ensures that there is a complete cycle for Ssk1 modification/demodification and that the system can reach a stable steady state. We emphasize, however, that spontaneous hydrolysis of complexed Ssk1 is likely not the mechanism for rapid dephosphorylation of large quantities of Ssk1 in response to osmotic shock. The mechanism for this rapid activation remains unknown but is irrelevant for our model, which is restricted to yeast growing in steady-state normal osmolarity conditions.
Robust inactivation of the HOG pathway requires that only a small fraction of total Ssk1 (K T ) be in the active (K OO ) modification form at steady state. The goal of this section is to derive a simple steady-state expression (an invariant) for the ratio of active to total Ssk1. We find an invariant of the form
where the coefficients are combinations of the rate constants. In this section we derive Eq. 1, and in the following section we discuss experimental tests of its predictions. The subnetwork involving Ssk1 contains four loops, which are linked in a branched tree ( Figure 2D ). It is a general feature of such networks that, at steady state, each individual loop is at steady state, irrespective of any other loops in which the components participate [23, 24] .
If each individual loop is at steady state, then the forward flux through each loop must be balanced by the backward flux, which yields the following four equations:
Because the intermediate complexes Y P K OO and Y P K OP are also at steady state, we can write
from which we deduce that
Substituting Eq. 3 into Eq. 2, we obtain expressions for K OP , K PP , YK OP , and YK PP in terms of K OO , Y , and Y P . From Eq. 3 we already have expressions for Y P K OO and Y P K OP in terms of K OO and Y P . As such, we are able to calculate the total amount of Ssk1 (K T ) in terms of just K OO , Y , and Y P . We have
Substituting for the individual terms, we obtain
The relative concentration of K OO is thus given by the invariant in Eq. 1.
The tree of loops structure of the Ssk1 network has an important consequence. It implies that the steady-state ratio of active to total Ssk1 is independent of the upstream biochemistry (i.e., the mechanistic details of the various Sln1 and Ypd1 reactions) as long as some process exists to generate positive levels of Y and Y P . In that case, the ratio will always be given by Eq. 1, although the numerical value will of course differ depending on steady-state concentrations of Y and Y P . The implications of this result, including its suggestion that robustness in the HOG pathway is independent of putative Sln1 bifunctionality, are considered in the conclusion.
The invariant derived from our mathematical model (Eq. 1) suggests that Ypd1 levels are critical to robustness. The denominator of the invariant is quadratic in the concentration of free Y P and linear in the concentration of free Y . Provided that the upstream network favors production of Y P over Y and that there is substantially more Ypd1 than Ssk1, the denominator of Eq. 1 will be large, and the relative concentration of K OO will be maintained at a low level. This situation allows for considerable underor overexpression of pathway components without spurious activation of the HOG pathway, in agreement with our experimental findings.
The invariant predicts that massive overexpression of Ypd1 should not cause phosphorylation of Hog1. In fact, additional Ypd1 would drive the [K OO ] K T ratio even closer to zero, lowering the amount of unphophorylated Ssk1 required for HOG pathway activation. In contrast, Ypd1 underexpression should increase the ratio, potentially compromising fitness due to inappropriate activation of the HOG pathway. Similarly, massive overexpression of Ssk1 should deplete free Y and Y P due to increased levels of the four intermediate complexes (Y P K OO , Y P K OP , YK OP , and YK PP ). Under the assumption of tight binding between Ypd1 and Ssk1 in each of these complexes, which is well-supported by existing kinetic data [20, 22] , very little free Ypd1 will be present at steady state if there is much more Ssk1 than Ypd1. As such, Eq. 1 predicts that the ratio will be higher following massive overexpression of Ssk1 than under wild-type conditions.
We experimentally validated these three predictions. The GEV system can achieve at most a 10-fold increase in protein expression from a single inducible allele [11] . We created haploid GEV yeast strains carrying high-copy 2μ plasmids with a GEV-inducible allele (P GAL1 -GENE) of a gene of interest, which allowed us to test the model prediction that massive overexpression of Ssk1, but not of Ypd1, should lead to inappropriate HOG pathway activation. These high-copy yeast plasmids are estimated to be present at 15-50 copies per cell [25] [26] [27] [28] .
We measured the growth of these strains in different concentrations of β-estradiol to assay for growth defects that might be due to HOG pathway hyperactivation. Massive overexpression of both Sln1 and Ssk1 caused a growth defect over a range of β-estradiol concentrations, but the strain with Ypd1 overexpressed grew as well as a wild-type strain carrying only the empty vector (P GAL1 2μ scURA3) ( Figure 5A ). Examination of growth over a finer range of β-estradiol concentrations indicated that overexpression of Ssk1 caused a more severe growth defect than overexpression of Sln1 ( Figure 5B ). These defects were also visible on solid media ( Figure 5C ). As such, extreme overexpression of Ssk1 compromises fitness.
We again assayed phospho-Hog1 levels to check if overexpression of Sln1 and Ssk1 causes activation of the HOG pathway in normal osmolarity conditions ( Figure 6 ). We measured Hog1 phosphorylation levels after GEVinduction of relay components and of the positive controls Pbs2 and Ssk22 from a multi-copy plasmid. Overexpression of Pbs2, Ssk22, and Ssk1 caused a significant change in Hog1 phosphorylation after 30 minutes ( Figure 6B ). Although some Hog1 phosphorylation was observed after overexpression of Sln1, the increase was insignificant. Interestingly, overexpression of Ypd1 did not cause an increase in the level of phosphorylated Hog1. In fact, levels of Hog1 phosphorylation were reduced in the Ypd1 overexpression strain (p = 0.0246, two-way ANOVA).
We could not perform underexpression experiments in a ypd1 background because deletion of Ypd1 is lethal. To underexpress Ypd1, we instead sporulated the diploid strain (P GAL1 -YPD1/YPD1) containing a wild-type copy of YPD1 and a single copy under the control of P GAL1 onto media containing 10 nM β-estradiol. We reasoned that 10 nM β-estradiol would give sufficient expression of Ypd1 for cell growth, which was confirmed by observation of four viable spores. We then grew these spores on media containing a range of β-estradiol concentrations (Figure 7) . Ypd1 underexpression caused a clear growth defect compared to the wild-type control on 0 nM and 5 nM β-estradiol, indicating that Ypd1 underexpression is toxic. In contrast, underexpression of Sln1 and Ssk1 did not cause a growth defect (Additional file 1: Figure S1 ).
As discussed above, we observed a slight but nonsignificant increase in Hog1 phosphorylation following massive overexpression of Sln1 ( Figure 6 ). We therefore investigated whether the growth defect in response to Sln1 overexpression is only partially due to HOG pathway activation by creating yeast strains null for SSK1. Activation of the HOG cascade through the Sln1 branch requires Ssk1, so in ssk1 strains it is not possible for Sln1 overexpression to activate the HOG pathway.
Overexpression of Sln1 from a 2μ plasmid using GEV was still detrimental to growth in the ssk1 strain (Figure 8) , indicating that the growth defect due to Sln1 overexpression is only partially due to HOG pathway activation. This result held both for growth in liquid cultures ( Figure 8A ) and on solid media ( Figure 8B ). It is consistent with Sln1 overexpression causing a smaller effect on Hog1 phosphorylation levels ( Figure 6 ).
Robustness of the Sln1-Ypd1-Ssk1 phospho-relay is essential to prevent spurious activation of the HOG pathway, which severely compromises yeast fitness. We established that the phospho-relay is robust to perturbations in the concentrations of the three relay components. A theoretical analysis suggested that a large pool of the intermediate component Ypd1 can buffer fluctuations in other pathway components to maintain robustness. This suggestion was consistent with earlier published measurements indicating that Ypd1 is at least 5 times more abundant than Ssk1 at normal expression levels [7, 29] . Although Ypd1 may also bind to the protein Skn7, combined levels of Ssk1 and Skn7 have been measured to be below total Ypd1 levels [29] . Our subsequent experiments confirmed that depletion of this buffering pool of Ypd1 leads to inappropriate activation of the HOG pathway.
The differential expression of Ypd1 and Ssk1 enables phosphorylation of excess Ssk1 and stabilization of the new phospho-Ssk1, buffering HOG pathway activation to fluctuations in Ssk1 levels. This novel mechanism of robustness suggests an advantage of a three-component Figure 5 Massive overexpression of phospho-relay components leads to growth defects. A Haploid GEV strains carrying a high-copy plasmid with an inducible HOG pathway gene were grown in different concentrations of β-estradiol. The OD 600 after 36 hours of growth is plotted as a function of β-estradiol concentration. Each point represents the mean and standard deviation of four replicates. Overexpression of Sln1 and Ssk1 (but not Ypd1) caused a growth defect. B The same strains were grown over a finer titration of β-estradiol concentrations. The OD 600 after 36 hours is plotted. At this resolution, it is clear that the growth defect from Ssk1 overexpression is more severe than the growth defect from Sln1 overexpression at low β-estradiol concentrations. C The same strains were frogged onto plates containing different concentrations of β-estradiol. Massive overexpression of Sln1 and Ssk1 again caused a growth defect comparable to that from overexpression of Pbs2. In all experiments, the parent strain carrying the empty vector plasmid [2μ P GAL1 scURA3] was used as a negative control.
architecture over a two-component one. In particular, the implementation of an analogous buffering strategy in a two-component system would be difficult because it would require expressing the sensor histidine kinase at very high levels. This situation might lead to imprecise sensing and various other off-target effects. In contrast, Figure 6 Growth defects following massive overexpression of phospho-relay components are due to activation of the HOG pathway. A We assayed for Hog1 phosphorylation after overexpression of relay components (Sln1, Ypd1, Ssk1) and positive controls (Pbs2, Ssk22). The parental strain carrying the empty plasmid vector was used as a negative control. B We quantified the amount of phosphorylated Hog1 (relative to Hog1) in five biological replicates of this experiment. Error bars represent the standard error. Pbs2, Ssk22, and Ssk1 caused a significant ( * ) change in Hog1 phosphorylation levels after overexpression for 30 minutes (p = 0.0395, 0.0096, and 0.0224, respectively; paired t-test). Hog1 phosphorylation levels were also significantly lower in the Ypd1 overexpression strain (p = 0.0246, two-way ANOVA).
the use of an intermediate transfer protein enables robust buffering with both the sensor and response regulator expressed at comparable levels. Our work has thus identified a potential mechanism for circumventing a tradeoff between efficient sensing and robust control. There are other possible advantages for a three-component architecture, including combinatorial control of response regulators by sensor proteins through a common phosphotransfer protein or segregation of sensing and activation functions between the nucleus and cytoplasm. Intriguingly, deletion of YPD1 has recently been shown to cause constitutive activation of the HOG pathway in Candida albicans, suggesting that its buffering capacity might also be important in this organism [30] .
Robustness in real biological systems is necessarily approximate and apt to be compromised at extreme expression levels of cellular components. In many systems, however, it has proven difficult to characterize where robustness breaks down and to reconcile such results with mathematical models, which often predict exact robustness [31] . Our combined theoretical and experimental results specify a single condition (Ypd1 in large excess) for robust regulation of the HOG pathway.
The link between bifunctionality and robustness is well-established [31] [32] [33] [34] [35] , and it is known that bifunctionality of EnvZ is essential to robustness in Escherichia coli osmoregulation [36, 37] . As such, it is intriguing that our model suggests that robustness in S. cerevisiae osmoregulation is not dependent on bifunctionality of Sln1. It is important to emphasize, however, that bifunctionality would not compromise robustness. Rather, the model indicates that any upstream process that produces non-zero levels of Y and Y P should enable the same fundamental behavior predicted by Eq. 1. The possibility that Sln1 exhibits phosphatase activity warrants further experimental investigation.

All yeast strains used in this study are listed in Additional file 2: Table S1 . The homozygous GEV diploid strain, which served as the wild-type background strain for all diploid overexpression experiments, was created by mating haploid GEV strains yMM598 and yMM1101 [11] and picking zygotes to create yMM1104. As described previously [11] , diploid yeast strains capable of overexpressing the desired HOG pathway protein from a single locus (yMM1263, yMM1272, yMM1259) were created by transforming [38] the homozygous GEV diploid strain yMM1104 with the KanMX-P GAL1 cassette amplified from yMM1100 genomic DNA using appropriate oligonucleotide pairs. Transformants were verified by colony PCR and sequencing.
Yeast strains containing 2μ plasmids for massive overexpression of pathway components (yMM1313-yMM1318) were constructed using recombination-mediated plasmid construction [39, 40] to generate the overexpression plasmids pMM330-pMM334 in vivo (as described below). Positive transformants were selected for and maintained on SC-Ura media [38] .
Yeast strains containing the P STL1 -YFP reporter of HOG pathway activity and one estradiol-inducible allele of a HOG pathway gene (yMM1296, yMM1298, yMM1300, yMM1301, yMM1304, yMM1305) were constructed by transforming the heterozygous diploid GEV yeast strains (yMM1104, yMM1272, yMM1264, yMM1259 yMM1286, yMM1287) already containing an inducible allele with the product of PCRing yECitrine-HphMX off plasmid pMM280 using appropriate oligonucleotides. The oligonucleotides contained homology such that the STL1 ORF was replaced with the yECitrine-HphMX cassette. Transformants were verified by colony PCR, and expression of YFP in 1M sorbitol was assayed.
Yeast strains null for the SSK1 gene (ssk1 ) were created by deleting the SSK1 ORF using appropriate oligonucleotide pairs to amplify KanMX from pMM131 and transforming it into yMM630. Transformants were selected for drug resistance and verified by colony PCR and sequencing.
Standard yeast media was used as noted. Low fluorescence yeast media was prepared as described previously [41] .
All plasmids used in this study are listed in Additional file 3: Table S2 . Plasmid pMM329 (P GAL1 scURA3 2μ) was constructed by PCR of the native GAL1 promoter from genomic DNA prepared from yMM1100 using appropriate primers. This promoter was ligated in pMM12 between the restriction sites KpnI and XhoI (scURA3 2μ) [42] . The resulting plasmid served as a template to create a series of overexpression plasmids with different HOG pathway genes under the control of the GAL1 promoter using yeast recombination-mediated plasmid construction [39, 40] . Appropriate primer pairs were used to amplify Pbs2, Ssk22, Sln1, Ypd1, and Ssk1, respectively, from yMM1100 genomic DNA. These PCR products were then co-transformed with pMM329 linearized Figure 8 Growth defects following Sln1 overexpression are not completely due to HOG pathway activation. A We assayed growth in wild-type and ssk1 strains overexpressing Sln1 in response to β-estradiol or carrying an empty P GAL1 vector control. Deletion of Ssk1, which prevented HOG pathway activation by Sln1, partially alleviated the growth defect due to Sln1 overexpression. B Growth of cells on plates containing 10 μM β-estradiol indicated that ssk1 reduced the toxicity of Sln1 overexpression.
with XhoI and SalI. The primer pairs used to amplify the HOG pathway genes contained homology with the pMM329 backbone such that the gene of interest was integrated after P GAL1 to create a P GAL1 -HOGGENE plasmid (pMM330-pMM334). Positive transformants in which the plasmid had been repaired were selected for on SC-Ura media. Plasmids were purified from these transformants and verified by sequencing.
Yeast strains were grown overnight to saturation in appropriate media (YPD or SC-Ura media to maintain plasmids). These saturated cultures were serially diluted in 10-fold increments and frogged onto YPD or SC-URA plates containing 0 nM, 10 nM, 100 nM, 1 μM, or 10 μM β-estradiol (Tocris Biosciences). These plates were incubated at 30°C for two days before imaging.
Yeast strains were grown overnight to saturation in YPD or SC-Ura media. In the morning, each strain was diluted 1:2000 into 200 μL of the same media containing 0 nM, 100 nM, 1 μM, or 10 μM of β-estradiol in a well of a 96-well flat-bottom plate (Costar). Each strain/estradiol combination was run in four replicates on the same plate. Growth curves were generated using a Synergy H1 microplate reader (BioTek). Cells were grown at 30°C with continuous, double-orbital (555 cpm) shaking, and OD 600 was measured every 20 minutes. Growth rates were calculated from the growth curves using spline-fits determined with the R package grofit [43] . OD was plotted at the time points indicated in the figure legends.
Diploid GEV strains yMM1104 (control), yMM1259 (SLN1/KanMX-P GAL1 -SLN1), yMM1263 (SSK1/KanMX-P GAL1 -SSK1), yMM1264 (SSK1/KanMX-P GAL1 -SSK1), and yMM1272 (YPD1/KanMXrev-P GAL1 -YPD1) were sporulated in 1% potassium acetate for 3 days and dissected onto YPD plates containing 10 nM β-estradiol. Two spores from each tetrad contained the wild-type HOG pathway gene (Sln1, Ypd1, or Ssk1), while the other contained the same gene under the control of the GAL1 promoter (KanMX-P GAL1 -SSK1, KanMX-P GAL1 -SLN1, KanMX-P GAL1 -YPD1). After all spores had grown to a sufficient size, they were diluted into YPD and frogged onto YPD plates containing 0 nM, 5 nM, 10 nM, 100 nM, 1 μM, or 10 μM β-estradiol. Spores were allowed to grow at 30°C for 2 days prior to imaging.
We created diploid GEV strains that carried both an inducible HOG gene under the control of P GAL1 promoter and a HOG pathway transcriptional reporter (P STL1 -yEVenus) to assay for downstream transcriptional activation in response to overexpression of various HOG pathway proteins. Strains were grown with agitation in low fluorescence media at 30°C to mid-log (Klett 80), at which point 200 μl of cell culture was sampled for flow cytometry by adding it to 800 μl of cold PBS + 0.1% Tween 20 stored at 4°C. Each culture was induced by adding β-estradiol to a final concentration of 10 μM. Cultures were sampled for flow cytometry after induction with β-estradiol at T = 2 hours and T = 19 hours. Fluorescence was analyzed by flow cytometry on a BD LSRII Multi-Laser Analyzer with HTS (BD Biosciences).
We measured levels of phosphorylated Hog1 following both moderate and massive overexpression of pathway components. For the moderate overexpression experiments, diploid GEV yeast strains yMM1104, yMM1263, yMM1259, yMM1272, and yMM1287, which each contained one estradiol-inducible copy of a HOG pathway gene, were grown to mid-log (Klett 80) in YPD at 30°C with shaking. To assess the effect of massive overexpression of HOG pathway proteins, yeast strains containing the P GAL1 -HOGGENE scURA3 2μ overexpression plasmids (yMM1313-yMM1318) were grown in SC-Ura media to mid-log (Klett 80) at 30°C with shaking. For all strains, at T = 0 expression of the gene of interest was induced by addition of 10 μM β-estradiol (final concentration). At indicated timepoints, 1.5 ml of culture was sampled.
Protein was prepped from samples immediately after each time point. Each sample was centrifuged (1320 RPM) and the supernatant aspirated. The resulting cell pellet was resuspended in 100 μl of 1X sample buffer (Invitrogen) with β-mercaptoethanol (final concentration of 10%), protease inhibitor (Roche), and phosphatase inhibitor (Fisher Scientific). Samples were heated at 95°C for 5 minutes, vortexed for 2 minutes, and then rapidly frozen in liquid nitrogen and stored at -20°C.
Prior to western blotting, samples were thawed and centrifuged at 1320 RPM for 5 minutes. They were run on 4-10% Bis-Tris gels (Invitrogen) and transferred to PVDF membranes (Invitrogen) by electrophoresis at 13 V for 4 hours. Membranes were blocked for 1 hour at room temperature with agitation in 1X TBS, 0.1% Tween-20, and 5% milk. Membranes were then probed with primary antibody overnight at 4°C.
The following antibodies were used to detect phosphorylated Hog1, total Hog1, and actin, respectively: antiphospho-p38 MAPK rabbit monoclonal antibodies (Cell Signaling Technology #9215), anti-c-myc goat polyclonal antibodies (Santa Cruz Biotechnology sc-6815), and antiβ-actin antibody (Abcam ab8224). All primary antibodies were diluted 1:1000 in 1X TBS, 0.1% Tween-20, and 5% milk. Following incubation with the primary antibody, membranes were washed (4 × 5 minutes) with TBST (1X TBS, 1% Tween-20) and then incubated for 1 hour at room temperature with the appropriate secondary antibody conjugated to HRP (anti-rabbit IgG (Cell Signaling #7074, 1:5000 dilution), rabbit anti-goat IgG (Santa Cruz sc-2768, 1:5000 dilution), and anti-mouse IgG (Abcam ab97023, 1:20000 dilution), respectively). All secondary antibodies were diluted in 5% milk, 1X TBS, and 1% Tween-20. After incubation with the secondary antibody, membranes were washed 4 × 5 minutes with 5% milk, 1X TBS, and 1% Tween-20.
Western blots were quantified using chemiluminescence. Membranes were developed using the Pierce Supersignal Femto kit following the manufacturer's protocol. Chemiluminescence was quantified using HyBlot CL Autoradiography film. Developed film was scanned on a Epson Perfection 4490 Photo Scanner in transmission mode. Protein levels were quantified by densitometry using the Gel Analysis plug-in in ImageJ [44] .
Each membrane was probed for phospho-Hog1, betaactin, and total Hog1 (in that order). After the chemiluminescence assay but before re-blocking, the membrane was stripped by washing in stripping buffer (2 × 10 minutes), phosphate-buffered saline (2 × 10 minutes) and 1X TBS + 1% Tween-20 (2 × 5 minutes). Stripping buffer contained 15 g glycine, 1 g SDS, and 10 mL Tween 20 in 1 L ultrapure water, with the pH adjusted to 2.2 using concentrated HCl.
Scientific workflow has become increasingly popular in modern scientific computation as more and more scientists and researchers are relying on workflow systems to conduct their daily science analysis and discovery. With technology advances in both scientific instrumentation and simulation, the amount of scientific datasets is growing exponentially each year, such large data size combined with growing complexity of data analysis procedures and algorithms have rendered traditional manual processing and exploration unfavorable as compared with modern in silico processes automated by scientific workflow systems (SWFS). While the term workflow speaks of different things in different context, we find in general SWFS are engaged and applied to the following aspects of scientific computations: 1) describing complex scientific procedures, 2) automating data derivation processes, 3) high performance computing (HPC) to improve throughput and performance, and 4) provenance management and query.
Workflows are not a new concept and have been around for decades. There were a number of coordination languages and systems developed in the 80s and 90s [1, 7] , which share many common characteristic with workflow systems (i.e. they describe individual computation components and their ports and channels, and the data and event flow between them). They also coordinate the execution of the components, often on parallel computing resources. Furthermore, business process management systems have been developed and invested in for years; there are many mature commercial products and industry standards such as BPEL [2] . In the scientific community there are also many emerging systems for scientific programming and computation [5, 22] . Before we jump on developing yet another workflow system, a fundamental question to ask is whether we can use existing technologies, or we should invent new languages and systems in order to achieve the four aspects mentioned earlier that are essential to scientific workflow systems. This paper identifies the challenges to workflow development in the context of scientific computation; we present an overview of some of the existing technologies and emerging systems, and discuss opportunities in addressing these challenges.
Software development has been on a free ride for performance gain as chipmakers continue to follow Moore's Law in doubling up transistors in minuscule space. Little consideration has been given to code parallelization since it has not been essential for the average computer user until recently, when single CPU core performance growth stagnated and multi-core processors emerged on the market in 2005.
Due to the limitations to effectively increasing processor clock frequency, hardware manufactures started to physically reorganize chips into what we call the multi-core architecture [10] , involving linking several microprocessor cores together on the same semiconductor. Various manufactures from Intel, AMD, IBM, Sun, have released dual-core, quad-core, eight-core, and 64-threaded processors in the past few years [13, 21] . Given that 128-threaded SMP systems are a reality today [21] , it is reasonable to assume that 1024 CPU cores/threads or more per SMP system will be available in the next decade.
The new multi-core architecture will force radical changes in software design and development. We are already seeing significant increase of research interests in concurrency and parallelism, and multi-core software development. The number of multiprocessor research papers has increased sharply since year 2001, surpassing the peak point in all the past years [10] . Concurrency is one of the next big challenges in how we write software simply because our industry has been driven by requirements to write ever larger systems that solve ever more complicated problems and exploit the ever greater computing and storage resources that are available [18] .
Within the science domain, the data that needs to be processed generally grows faster than computational resources and their speed. The scientific community is facing an imminent flood of data expected from the next generation of experiments, simulations, sensors and satellites. Scientists are now attempting calculations requiring orders of magnitude more computing and communication than was possible only a few years ago. Moreover, in many currently planned and future experiments, they are also planning to generate several orders of magnitude more data than has been collected in the entire human history [9] .
For instance, in the astronomy domain the Sloan Digital Sky Survey (http://www.sdss.org) has datasets that exceed 10 terabytes in size. They can reach up to 100 terabytes or even petabytes if we consider multiple surveys and the time dimension. In physics, the CMS detector being built to run at CERN's Large Hadron Collider (http://lhc.web.cern.ch/lhc) is expected to generate over a petabyte of data per year. In the bioinformatics domain, the rate of growth of DNA databases such as GenBank (http://www.psc.edu/general/software/packages/genbank/) and EMBL (European Molecular Biology Laboratory, http://www.embl.org) has been following an exponential trend, with a doubling time estimated to be 9-12 months.
To enable the storage and analysis of large quantities of data and to achieve rapid turnaround, data needs to be distributed over thousands to tens of thousands of compute nodes. In such circumstances, data locality is crucial to the successful and efficient use of large scale distributed systems for data-intensive applications [19] . Scientific workflows are generally executed on a shared infrastructure such as TeraGrid (http://www.teragrid.org), Open Science Grid (http://www.opensciencegrid.org), and dedicated clusters, where data movement relies on shared file systems that are known bottlenecks for data intensive operations. If data analysis workloads have locality of reference, then it is feasible to cache and replicate data at each individual compute node, as high initial data movement costs can be offset by many subsequent data operations performed on cached data [15] .
Modern scientific workflow systems need to set large scale data management as one of its primary objectives, and to ensure data movement is minimized by intelligent data-aware scheduling both among distributed computing sites (assuming that each site has a local area network shared storage infrastructure), and among compute nodes (assuming that data can be stored on compute nodes' local disk and/or memory).
Supercomputers had their golden age back in the 80s when there were virtually no other choices in dealing with compute-intensive tasks. They were applied mostly to scientific modeling and simulation in various disciplines such as high energy physics, earth science, biology, mechanical engineering etc. Some typical applications included weather forecasting, missile trajectory simulation, airplane wind tunnel simulation, genomics etc. However, supercomputers are expensive and scarce resources where only national laboratories, government agencies and some universities have access to them; and the parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed, such as special-purposed FORTRAN compilers, PVM, MPI and OpenMP [9] .
Over the last decade, we have observed processor speeds, storage capacity per drive, and network bandwidth increase 100~1000 times. As a consequence, cluster computing and Grid computing environments that leverage the cheaper commodity computing and storage hardware have been actively adopted for scientific computations. Cluster computing usually involves homogeneous machines interconnected by high speed network with locally accessible storage in one administrative domain, where Grid computing focuses on distributed resource sharing and coordination across multiple "virtual organizations" that may span many geographically distributed administrative domains. Grids can also be categorized into Computational Grids and Data Grids, where the former mostly tackle computation intensive tasks, and the latter target data-intensive sciences.
With the introduction of multi-core architectures, the separation between Grid Computing and Supercomputing is becoming less clear. Many supercomputers are being built on multi-core chips with high speed interconnection. The Cray XT5 system (http://www.cray.com/products/xt5/index.html) uses thousands commodity Quad-Core AMD Opteron™ processors and has a unified Linux environment. The latest IBM BlueGene/P Supercomputer (BG/P, http://www.research.ibm.com/bluegene/) has quad core processors with a total of 160K-cores, and has support for a lightweight Linux kernel on the compute nodes, making it significantly more accessible to new applications [17] . Finally, a smaller system named SiCortex (http://www.sicortex.com/) is also worth mentioning; it boasts 6-core processors for a total of 5832-cores, and runs a standard Linux environment.
Supercomputers (e.g. IBM BlueGene) have traditionally been designed and used for tightly coupled massively parallel applications, typically implemented in MPI. They have not been an ideal preferred platform for executing loosely coupled applications that are typical in many scientific workflows. Grids have seen success in the execution of tightly coupled parallel applications, but they has been the platform of choice for loosely coupled applications mostly due to the flexibility and granularity of the resource management and the execution of single processor jobs with ease. Work is underway within both the Falkon [14] and Condor [20] projects to enable the latest BG/P to efficiently support loosely coupled serial jobs without any modifications to the respective applications, and hence enabling an entirely new class of applications that were never candidates as possible use cases for the BlueGene/P supercomputer.
Scalability and performance are top priorities for SWFS. To this end, it is necessary to leverage supercomputing resources as well as Grid computing infrastructures for large scale parallel computations.
DAGMan (http://www.cs.wisc.edu/condor/dagman) and Pegasus [6] are two systems that are commonly referred to as workflow systems and have been widely applied in Grid environments. DAGMan provides a workflow engine that manages Condor jobs organized as directed acyclic graphs (DAGs) in which each edge corresponds to an explicit task precedence. Both systems focus on the scheduling and execution of long running jobs.
Taverna [12] is an open source workflow system particularly focused on bioinformatics applications and services, and it is based on the XScufl (XML Simple Conceptual Unified Flow) language. Kepler [11] is a scientific workflow system that builds on the Ptolemy-II system (http://ptolemy.eecs.berkeley.edu/ptolemyII/), which is a visual modeling tool written in Java. Triana [4] is a GUI-based workflow system for coordinating and executing a collection of services. All these systems have some visual interfaces (also referred to as workbenches) that allow the graphical composition of workflows.
While all of the existing SWFS possess great features and address many aspects of workflow specification, execution and management problems, it is unrealistic to expect one system to cover all the bases. The Workflow Bus project [23] instead tries to leverage multiple existing workflow systems to compliment each other in implementing aggregated functions and services.
Finally, the evolutions of workflows themselves (explorations) are vital in scientific analysis. VisTrails [3] captures the notion of an evolving dataflow, and implements a history management mechanism to maintain versions of a dataflow, thus allowing a scientist to return to previous steps, apply a dataflow instance to different input data, explore the parameter space of the dataflow, and (while performing these steps) compare the associated visualization results.
In response to the pressing demand of scientific applications, and the hunger for computing power, there have been a few emerging languages and systems that try to tackle the problems taking unconventional approaches.
MapReduce [5] is regarded as a power-leveler that solves complicated computation problems using brutalforce computation power. It provides a very simple programming model and powerful runtime system for the processing of large datasets. The programming model is based on just two key functions: "map" and "reduce," borrowed from functional languages. The MapReduce runtime system automatically partitions input data and schedules the execution of programs in a large cluster of commodity machines. The system is made fault tolerant by checking worker nodes periodically and reassigning failed jobs to other worker nodes. MapReduce has been mostly applied to document processing problems, such as distributed indexing, sorting, and clustering.
The Fortress language (http://fortress.sunsource.net) recently released by Sun Microsystems is a new programming language designed for HPC, and aims to improve programmability and productivity in scientific computation. The language has been designed from ground up, supporting mathematical notation (in Unicode) and physical units and dimensions, static type checking of multidimensional arrays and matrices, and rich functionality in libraries. It supports transactions, specification of locality, and implicit parallel computation (e.g. parallel for loops). Although Fortress in a strict sense is not a workflow language, and its adoption remains to be seen, it provides the higher level abstractions and functionalities for building a parallel workflow language.
Microsoft Windows Workflow Foundation (WWF) [16] provides a generic framework for workflow development and execution. It is focused on integrating diverse components within an application, allowing a workflow to be deployed and managed as a native part of the application. The fundamental idea behind WWF is that each activity is modeled as a resumable program statement, and the invocation of an activity is asynchronously organized, thus a program can be compared to a bookmark, which can be frozen in action, serialized into persistent storage, and resumed after arbitrarily long time later. However, WWF is not a full-fledged workflow management system in that it lacks administration, monitoring, retry mechanism, load balancing, etc. for a production environment.
Star-P (http://www.interactivesupercomputing.com) approaches the integration of scientific applications and HPC via language extension -allowing scientists to work in their familiar programming environments such as MATLAB, Python, and R, with some parallel directives. Internally the system can schedule the execution of parallel tasks to a computation cluster preconfigured with scientific calculation libraries. The system has been applied to a wide variety of computation problems, but the performance improvement is mostly intra-application parallelization, instead of inter-component coordination and management.
Swift [22] is an emerging system that bridges scientific workflows with parallel computing. It is a parallel programming tool for rapid and reliable specification, execution, and management of largescale science and engineering workflows. Swift takes a structured approach to workflow specification, scheduling and execution. It consists of a simple scripting language called SwiftScript for concise specifications of complex parallel computations based on dataset typing and iterations, and dynamic dataset mappings for accessing large scale datasets represented in diverse data formats. The runtime system relies on the CoG Karajan workflow engine for efficient scheduling and load balancing, and it integrates the Falkon [14] light-weight task execution service for optimized task throughput and resource efficiency delivered by a streamlined dispatcher, a dynamic resource provisioner, and the data diffusion mechanism to cache datasets in local disk or memory and dispatch tasks according to data locality.
Existing technologies and systems already address many of the fundamental issues in scientific workflow specification and management, and many of them have been successful applied to various scientific applications across multiple science disciplines. However, modern multi-core architectures and parallel and distributed computing technologies, and the exponentially growing scientific data are bound to change the landscape and evolution of scientific workflow systems. As already being manifested by the few emerging systems, the science community is demanding both specialized, domain-specific languages to improve productivity and efficiency in writing concurrent programs and coordination tools, and generic platforms and infrastructures for the execution and management of large scale scientific applications, where scalability and performance are major concerns. High performance computing support has become a indispensable piece of such workflow languages and systems, as there is no other viable way to get around the large storage and computing problems emerging in every discipline of 21 st century e-science, although what may be the best approach to enabling scientists to leverage HPC technologies as transparent and efficient as possible remains unanswered.
In the science domain, there is an increasing need for programming languages to expose parallelism, whether it's done explicitly or implicitly, to specify the concurrency within a component, or across multiple independent components. There is a need for new parallel or workflow languages that adopt implicit parallelism where data dependencies can be discovered by its compiler, and independent tasks in the orders of hundreds of thousands can be scheduled to run in clustered or Grid environments. Such systems could achieve improvements in both manageability and productivity.
Scientific workflow systems aim to provide a simple concise notation that allows easy parallelization and supports the composition of large numbers of parallel computations, therefore they may not need all the constructs and features in a full-fledged conventional language, and implicit parallelism is preferred to explicit parallelism specification, as the latter requires expertise and attention to the details of parallel programming, which may be difficult for end users. But in the mean time sometimes scientists do need more control in specifying how to distribute their applications and datasets.
We are also in need of common generic infrastructures and platforms in the science domain for workflow administration, scheduling, execution, monitoring, provenance tracking etc. While business process management has industry agreed upon standards and steering committees, we don't have these in the science domain, where often time people reinvent the wheel in developing their in-house yet another SWFS, and there is no easy way in integrating various workflow systems and specifications. We also argue that in order to address all the important issues such as scalability, reliability, scheduling and monitoring, data management, collaboration, workflow provenance, and workflow evolution, one system cannot fit all needs. A structured infrastructure that separates the concerns of workflow specification, scheduling, execution etc, yet is organized on top of components that specialize on one or more of the areas would be more appropriate.
Francesco Bonchi is a senior research scientist at Yahoo! Research in Barcelona, Spain, where he is part of the Web Mining Group. His recent research interests include mining query-logs, social networks, and social media, as well as the privacy issues related to mining these kinds of sensible data. In the past he has been interested in data mining query languages, constrained pattern mining, mining spatiotemporal and mobility data, and privacy preserving data mining. He is member of the ECML/PKDD Steering Committee 
Orthogonal frequency division multiplexing (OFDM) has been widely applied in wireless communication systems, because it transmits at a high rate, achieves high bandwidth efficiency, and is robust to multipath fading and delay [1] . OFDM applications can be found in digital television and audio broadcasting, wireless networking, and broadband internet access. Current OFDM based WLAN standards (such as IEEE802.11a/g) use variations of QAM schemes for subcarrier modulations which require a coherent detection at the OFDM receiver and consequently requires an accurate (or near accurate) estimation of Channel State Information (CSI). The structure of OFDM signal makes it difficult to balance complexity and performance in channel estimation. The design principles for channel estimators are to reduce the computational complexity and bandwidth overhead while maintaining sufficient estimation accuracy.
Some channel estimation schemes proposed in literature are based on pilots, which form the reference signal used by both the transmitter and the receiver. This approach has two main challenges: (i) the design of pilots; and (ii) the design of an efficient estimation algorithm (i.e., the estimator).
There is a tradeoff between the spectrum efficiency and the channel estimation accuracy. Most of the existing pilotassisted OFDM channel estimation schemes rely on the use of a large number of pilots to increase to estimation accuracy; the spectral efficiency is therefore reduced. For example, there are approaches based on time-multiplexed pilot, frequencymultiplexed pilot, and scattered pilot [2] , all achieving higher estimation accuracy at the price of using more pilots. There have been attempts to reduce the number of pilots, i.e. J. Byun et al. in [3] . The solutions generally require extra "test signal" for channel pre-estimation. By sending out "test signal", they try to find out how many pilots are needed by firstly inserting a relatively small number of pilots and then, based on the results of the "test", the number of pilots are decided. Therefore, there is no guaranteed overall reduction of pilots insertion.
As a sensing problem, OFDM channel estimation can benefit from the emerging technique of compressive sensing (CS), which acquires and reconstructs a signal from fewer samples than what is dictated by the Nyquist-Shannon sampling theorem, mainly by utilizing the signal's sparse or compressible property. The field has exploded since the pioneering work by Donoho [4] and Candes, Romberg and Tao [5] . The main idea is to encode a sparse signal by taking its "incoherent" linear projections and recover the signal through algorithms such as 1 minimization. To maximize the benefits of CS for OFDM channel estimation, one shall skillfully perform the CS encoding and decoding steps, which are precisely the two focuses of this paper: the designs of pilots and estimator, respectively.
Contributions: CS has been applied to channel estimation in [13] [14] [15] [16] , which are reviewed in subsection III-E below. For OFDM channel estimation, there are papers [6] [7] [8] [9] , to which our work differs in various ways as follows. We skillfully design CS encoding and decoding strategies for OFDM channel estimation. Compared to existing work, we are able to obtain channel response in much higher resolutions and from much fewer pilots (thus taking much shorter times). This is achieved by designing pilots with uniform random phases and using a novel estimator. The pilot design preserves the information of high-resolution channel response during aggressive uniform down-sampling, which means that receiver ADC can run at a much lower speed. The estimator is tailored for OFDM channel response; in particular, instead of the generic 1 minimization, iterative support detection (ISD) [17] and limited-support least-squares are adopted in order to take advantage of the characteristics of channel response. The resulting algorithm is very simple and performs better.
The rest of this paper is organized as follows: Section II reviews the general OFDM system model and sets up the channel estimation formulation. Section III relates channel estimation to CS and present the proposed pilot design. In Section IV, the estimator based on iterative support detection and limited-support least-squares are introduced. Section V give the simulation results. Finally, Section VI concludes this work.
II. OFDM SYSTEM MODEL A baseband OFDM system is shown in Figure 1 . In this system, the modulated signal in the frequency domain, represented by X(k), k ∈ [1, N] , is inserted with pilot signal and guard band, and then an N -point IDFT transforms the signal into the time domain, denoted by x(n), n ∈ [1, N] , where a cyclic extension of time length T G is added to avoid inter-symbol and inter-subcarrier interferences. The resulting time series data is converted by a digital-to-analog converter (DAC) with a clock speed of 1/T S Hz into an analog signal for transmission. We assume that the channel response comprises P propagation paths, which can be modeled by a time-domain complex-baseband vector with P taps:
where α p is a complex multipath component and τ p is the multipath delay (0 
where ⊗ denotes convolution and ξ(n), n ∈ 
, where the guard band and pilot signal will be removed. For pilot assisted OFDM channel estimation, we shall design the pilots X (and thus x) and recover h from the measurements Y (or, equivalently y).

CS, which will be reviewed in the next subsection, allows sparse signals to be recovered from very few measurements, which translates to slower sampling rates and shorter sensing times. Because the channel impulse response h is very sparse, we are motivated to apply CS to recover h by using a reduced number of pilots so that the estimation becomes much quicker. Furthermore, in sharp contrast to conventional OFDM channel estimation in which ADC and DAC run at the same sampling rate, we can obtain a higher-resolution h by increasing the sampling rate of only the transmitter DAC, or we can reduce the receiver ADC speed which often defines the system cost. In other words, we have N > M. The rest of this section reviews CS and introduces our proposed approach for OFDM channel estimation.
CS theories [4] , [10] , [11] state that a S-sparse signal 1 h can be stably recovered from linear measurements y = Φh + ξ, where Φ is a certain matrix with M rows and N columns, M < N, by minimizing the 1 -norm of h. Classical CS often assumes that Φ, after scaling, satisfies the restricted isometry property (RIP)
for all S-sparse h, where δ > 0 is the RIP parameter. The RIP is satisfied with a high probability by a large class of random matrices, e.g., those with entries independently sampled from a subgaussian distribution. By minimizing the 1 -norm, one can stably recover h as long as M ≥ O(S log N ).
The classical random sensing matrices are not admissible in OFDM channel estimation because the channel response h is not directly multiplied by a random matrix; instead, as describe in Section II, h is first convoluted with x, the noise is added, and then the received signal z is uniformly down-sampled to y. Because convolution is a circulant linear operator, we can present this process by y =↓ Ω z =↓ Ω (Ch + ξ), where the sensing matrix C is the full circulant (convolution) matrix determined by x, and ↓ Ω denotes the uniform down sampling at points in
As is widely perceived, CS favors fully random matrices, which admit stable recovery from fewest measurements (in terms of order of magnitude), but C is structured and thus much less "random". This factor seemingly suggests that C would be not favored by CS. Nevertheless, carefully designed circulant matrices can deliver the same optimal CS performance.
To design the sensing matrix C, we propose to generate pilots X in either one of the following two ways: (i) the real and imaginary parts of X(k) are sampled independently from a Gaussian distribution, k = 1, . . . , N; (ii) (same as [15] ) X(k), k = 1, . . . , N, have independent random phases but a uniform amplitude. Note that X(k) of type (i) also have independent random phases. Let F denote the discrete Fourier transform. Following from the convolution theorem
F h, so the measurements y can be written as
Let us explain intuitively why (3) is an effective encoding scheme for a sparse vector h. First, it is commonly known that F h is non-sparse and its mass is somewhat evenly spread over all its components. The random phases of X by design are of critical importance. They "scramble" F h component wisely and break the delicate relationships among F h's components; as a result, in contrast to the sparse
is not sparse at all. Furthermore, X has a random-spreading effect. Due to a phenomenon called concentration of measures [12] , the mass of Ch spreads over its components in a way that, with a high probability, the information of h is preserved by down sampling of a size essentially linear in P -the sparsity of h (whether or not the down-sampling is equally spaced, i.e., uniform). Up to a log factor, the down sampled measurements permit stable 1 recovery. Both types (i) and (ii) of X have similar encoding strength, but X of type (ii) gives an orthogonal C, i.e., C * C = I, so x ⊗ h transforms h into a random orthobasis. Such orthogonality results in multiple benefits such as faster convergence of our recovery algorithm. Due to the page limitation, we omit rigorous mathematical analysis of (3) and its guaranteed recovery.
Note that the proposed sampling ↓ Ω (F −1 diag(X)F ) is very different from partial Fourier sampling ↓ Ω F . The latter requires a random Ω to avoid the aliasing artifacts in the recovery but the former, with random-phased X, permits both random and uniform Ω. Below we numerically demonstrate its encoding efficiency.
CS performance is measured by the number of measurements required for stable recovery. To compare the proposed sensing schemes with the well-established Gaussian random sensing, we conduct numerical simulations and show its results in Figure 2 . We compare three types of CS encoding matrices: the i.i.d. Gaussian random complex matrix, and the circulant random complex matrices corresponding to X of types (i) and (ii) above, respectively. In addition, 1 minimization is compared to our proposed algorithm CS-OFDM, which is detailed in the next section. The simulations results show that the random convolutions of both types perform as well as the Gaussian random sensing matrix under 1 minimization, and our algorithm CS-OFDM further improves the performance by half of a magnitude.

Random convolution has been used and proved to be an effective way of taking CS measurements that allow the signal to be recovered using 1 minimization. In [13] , Toeplitz 2 measurement matrices are constructed with i.i.d random row 1 (the same as type (i)) but with only ±1 or {−1, 0, 1}; their down sampling effectively takes the first M rows; and the number of measurements needed for stable 1 
[14] uses a "partial" Toeplits matrix, with i.i.d. Bernoulli or Gaussian row 1, for sparse channel estimation where the down sampling effectively also takes the first M rows. Their scheme requires M ≥ O(S 2 ·log N ) for stable 1 recovery. In [15] , random convolution of type (ii) above with either random downsampling or random demodulation is proposed and studied. It is shown that the resulting measurement matrix is incoherent with any given sparse basis with a high probability and 1 recovery is stable given M ≥ O(S · log N + log 3 N ). Our proposed type (ii) is motivated by [15] . On the other hand, no existing work proposes uniform down-sampling or shows its recovery guarantees. In addition, most existing analysis is limited to real-valued matrices and signals.
Our work is closely related to [14] and [16] . In [14] , i.i.d. Bernoulli or Gaussian vector is used as training sequence, and downsample is carried out by taking only the first M rows. While channel estimation is obtained as a solution to the Dantzig selector. In [16] , MIMO channels are estimated by activating all sources simultaneously. The receivers measure the cumulative response, which consists of random convolutions between multiple pairs of source signals and channel responses. Their goal is to reduce the channel estimation time. 1 minimization is used to recover channel response.
Our current work is limited to estimating a signal h-vector. While our work is based on similar random convolution techniques, we have proposed to use a pair of high-speed source and low-speed receiver for the novel goal of high resolution channel estimation. Furthermore, we apply a novel algorithm for the channel response recovery based on iterative support detection and limited-support least-squares, which is described in details in Section IV below.

As a result of rapid decaying of wireless channels, P -the number of significant multipaths -is small, so the channel response h is a highly sparse signal. Recall that the nonzero components of h only appear in the firstÑ components. We shall recover a sparse high-resolution signal h with a constraint from the measurements y at a lower resolution of M . We define operation | · | as the amplitude of a complex number, h 0 as the total number of nonzeros of |h| and
where φ denotes ↓ Ω C in (3), the submatrix of C formed by its rows corresponding to the down-sampling points in Ω.
Generally speaking, problem (4) is NP-hard and is impossible to solve even for moderate N . A common alternative is its 1 relaxation model with the same constraints.
which is convex and has polynomial time algorithms. If y has no noise, both (4) and (5) can recover h exactly given enough measurements, but (5) requires more measurements than (4).
Instead of using a generic algorithm for (5), we design an algorithm to exploit the OFDM system features, including the special structure of h and noisy measurements y. At the same time, we maintain its simplicity to achieve low complexity and match with easy hardware implementation.
First of all, we can simply collaborate two constraints into one by letting the variables beh = [h 1 , h 2 , . . . , hÑ ] and dropping the rest components of h. Letφ be the matrix formed by firstÑ columns of φ. Hence, the only constraints arẽ φh = y, which reduces the size of our problem.
We also develop our algorithm CS-OFDM for the purpose of handling noisy measurements. The iterative support detection (ISD) scheme proposed in [17] has a very good performance for solving (5) even with noisy measurements. Our algorithm uses the ISD, as well as a final denoising step. In the main iterative loop, it estimates a support set I from the current reconstruction and reconstructs a new candidate solution by solving the minimization problem min{ i∈I c |h i | :φh = y}, and it iterates these two steps for a small number of iterations. The idea of iteratively updating the index set I helps catch missing spikes and erase fake spikes. This is an 1 -based method but outperforms 1 . Analysis and numerical performance of ISD can be found in [18] . Because the measurements have noise, so reconstruction is never exact. Our algorithm 
Returnh uses a final denoising step, which solves least-squares over the final support T , to eliminate tiny spikes likely due to noise. Our pseudocode is listed in Algorithm 1. In Algorithm 1, at each iteration j, (6) solves a weighted 1 problem, and the solution h j is used for support detection to generate a new I j+1 . After the main loop is done, a support T is estimated above a threshold, which is selected based on empirical experiences. If the support detection is executed successfully, T would be the set of all channel multipath delay. Finally,h is constructed by solving a small least-squares problem, andh i , ∀i ∈ T fall to zero.
This algorithm is efficient since every step is simple and the total number of iterations needed is small. The subproblem is a standard weighted 1 minimization problem, which can be solved by various 1 solvers. Since φ is a convolution operator, we choose YALL1 [19] since (i) it allows us to customize the operators involvingφ and its adjoint to take advantages of DFTs, making it easier to implement the algorithm on hardware, (ii) YALL1 is asymptotically geometrically convergent and efficient even when the measurements are noisy. With our customization, all YALL1 operations are either an DFT/IDFT or one dimensional vector operations, so the overall complexity is O (N log N ) . Moreover, for support detection, we run YALL1 with a more forgiving stopping tolerance and always restart it from the last step solution. Furthermore, YALL1 converges faster as the index I j gets closer to the true support. The total number of YALL1 calls is also small since the detect support threshold decays exponentially and bounded below by a positive number. Numerical experience shows that the total number of YALL1 calls never exceeds P . The computational cost of the final least-squares step is negligible because the associated matrixφ T has its number of columns approximately equal to the number of spikes in h, which is far less than its rows. For example, if the system has P multipaths, the associated matrix for least-squares has size M × P . Generally speaking, the complexity for this leastsquares is O(MP + P 3 ). Since P and M are much smaller than N , the complexity of the entire algorithm is dominated by that of YALL1, which O(N log N ).
In this section, we perform numerical simulations to illustrate the performance of the proposed CS-OFDM algorithm for high resolution OFDM channel estimation. We focus on the mean square error (MSE) of channel estimation as well as the multipath delay detection when channel profile and signal to noise ratio (SNR) changes.
We consider an OFDM system with 1k-point IDFT (N = 1024) at the transmitter and 64-point DFT (M = 64) at the receiver, where we have a compression ratio of 16. The number of silent sub-carrier which acts as guard band is 256 among 1024 sub-carriers. The channel is estimated based on 768 pilot tones with uniformly random phases and a unit amplitude, with the Gaussian noise level ranging from 10 dB to 30 dB. We assume the usage of cyclic prefix and the impulse response of the channel is shorter than cyclic prefix which means there is no inter-symbol interference. For all simulations, we test the total numbers of multipath from 5 to 15. Moreover, we use only one OFDM symbol, i.e. use all non-silent subcarriers only once to carry pilot signals. All reported performances will substantially improve if more pilots are inserted. Figure 3 is a snapshot of one channel estimation simulation. It suggests that the proposed pilot arrangement and CS-OFDM successfully detect an OFDM channel with 7 multipaths when the signal to noise ratio is 30 dB. Our method not only exactly estimates the multipath delays, but also correctly estimates the values of the corresponding multipath components. Figure 4 depicts the MSE performance on OFDM channels with the number of multipaths ranging from 5 to 15 and noise level ranging from 10 dB to 30 dB. When there are only a moderate number of multipaths on the OFDM channel, e.g. 10 multipaths, even when SNR is 20 dB, MSE is as low as −17 dB. Figure 5 shows the reconstructed SNR vs. the number of multipaths when the input SNR changes. We can see that CS-OFDM achieves a gain in SNR. For example, when the input SNR is 10 dB, we obtain a reconstructed SNR higher than 20 dB when there are 5 multipaths. As the number of multipaths increases, the SNR gain from the reconstructed signal to the input signal decreases. However, even when the number of multipaths is 10, we still have a 5 dB gain, e.g. reconstructed SNR is 15 dB when the input signal SNR is 10 dB. The similar SNR gain appears for input SNR= 20 dB and SNR= 30 dB cases. From the entire input SNR and the number of multipath range we have tested, there is an average gain of 6 dB from the input SNR to the recovered SNR. Figure 6 and Figure 7 depict the probability of correct detection (POD) of the multipath delay and the false detection rate (FAR) while we change the SNR and the number of multipaths. When the SNR is above 10 dB, simulation shows almost 100% POD when the number of multipaths changing from 5 to 12. When there is a relatively large number of multipaths, e.g. 15, the probability of correct multipath delay detection is higher than 95% as SNR≥ 10 dB. Even when SNR is low, as long as the number of multipaths does not exceed 10, we still have a POD of greater than 95%. The FAR performance shows the similar results, as the SNR decreases and the number of multipaths increases, performance decreases. But, in a large range, e.g. SNR≥ 10 dB, the number of multipath≤ 10, we have almost zero FAR.

VI. CONCLUSIONS Efficient OFDM channel estimation will drive OFDM to carry the future of wireless networking. A great opportunity for high-efficiency OFDM channel estimation is lent by the sparse nature of channel response. Riding on the recent development of CS, we propose a design of probing pilots with random phases, which preserves the information of channel response during the convolution and down-sampling processes, and a sparse recovery algorithm, which returns the channel response in high SNR. These benefits translate to the high resolution of channel estimation, the lower speed of the receiver ADC, as well as shorter probing times. In this paper, the presentation is limited to an idealized OFDM model, intuitive explanations, and simulated experiments. In the future, we will formalize the work with rigorous theorems and fuse it into more realistic OFDM frameworks. The results presented here hint a high efficiency improvement for OFMD in practice.
Multi-resolution data representations are becoming increasingly popular in image processing applications. Pyramid data structures, in particular, play an important role in coding, and are ideally suited for progressive image transmission [13, 15] . In these data structures, the image is represented hierarchically with each level corresponding to a reduced-resolution approximation. An example of such a coding scheme is the Laplacian pyramid proposed by Burt and Adelson in which the difference between successive levels of a Gaussian pyramid is transmitted [3] . This approach compares favorably with earlier techniques, such as transform or predictive image coding, especially when large compression ratios are desired [7] . Recent developments in pyramid image compression also include subband coding techniques [ 18, 20] , orthogonal pyramid structures [1, 12] and wavelet transforms [9] , which are all based on the concept of quadrature mirror filters (QMF) [4] .
The Laplacian pyramid coding technique described by Burt and Adelson relies on the use of two complementary functions: REDUCE and EXPAND. REDUCE computes a lower resolution level of the Gaussian pyramid by decreasing the resolution by a factor of two. EXPAND performs the reverse operation by mapping the coarser level onto a finer sampling grid. These two functions, as defined initially, were sub-optimal in two respects. First, the basic EXPAND function induces some image blurring, tending to increase the energy of the residual image. Second, the initial REDUCE function fails to minimize the loss of information (in the least squares sense) from one level to the next one. It will be shown here that these limitations can be corrected through the appropriate Signal Processing insertion of additional post-and pre-filtering modules. These operators have an infinite impulse response (IIR) and yet can be implemented very efficiently using simple forward and backward recursions, as discussed in Appendix A.
The presentation is organized as follows. Following a series of definitions, a brief review of the Laplacian pyramid coding concept is given in Section 2. A modified EXPAND function that guarantees an exact image interpolation is described in Section 3. The least squares Laplacian pyramid is introduced in Section 4 and the corresponding REDUCE function is derived. The performance improvement of this new approach is illustrated both qualitatively and quantitatively with some experimental results in Section 5. Finally, the present approach is reinterpreted in terms of quadrature mirror filters in order to bring out the relationship with recent subband (or wavelet transform) coding techniques.

The techniques described in this paper are intended for the processing of digital images. However, to simplify the presentation, we have chosen to concentrate on the pyramidal representation of a one-dimensional signal: {f(k)}k~. All subsequent results carry over directly to higher dimensions if one makes use of separable filtering kernels. In practice, for digital images, this means that a pyramid representation can be obtained from the successive application of one-dimensional operators along the rows and columns.
There are two operations that are particularly useful for our purpose: the up-sampling of a signal by an integer multiple m (in particular, m=2), which is defined as (2.6) and the successive coarser resolution levels are constructed iteratively using the REDUCE operator
This operation requires some form of lowpass filtering and decimation by a factor of 2. Two examples of pyramid representations are shown in Fig. 1 . Burt and Adelson [3] use a 5-point quasiGaussian pre-filter and their REDUCE function can be described as
We will rely heavily on the z-transform representation of a signal, which, as a reminder, is defined as
In particular, Burt's generating kernel [3] is central to the construction of the Gaussian or Laplacian pyramid and is conveniently represented as 8) where the generating kernel w2 is defined by (2.4). The complement of REDUCE is the EXPAND function, which performs a signal extrapolation to a finer resolution level,
This operation involves an up-sampling by a factor of two and some form of interpolation. Burt and Adelson use the following operator [3] :
This operator is symmetric and has a sum equal to two, independent of a. The decimated version of this kernel is 5) and has a sum equal to one.
These two procedures are summarized in Figs. 2(a) and 2(b).
The Laplacian pyramid captures the loss of information resulting from an application of the REDUCE function and is the difference between two successive levels of the Gaussian pyramid:
(2.11)
The Gaussian pyramid is a multi-resolution representation of a signal. It is characterized by a sequence of signals f0,fl ..... fn, with the number of samples reduced by a factor of two in each of the principal directions from one level to the next. The finer or zero level of the pyramid is given by The key idea in the Laplacian coding scheme is to transmit the sequence of difference images Afl ..... Af, with sample values less extensively correlated than are the initial image pixels. The original image is then recovered by progressively expanding and summing the levels of the Laplacian pyramid, starting at the coarsest level. The main advantage of this approach is that the entropy of GAUSSIAN PYRAMID : the difference images is usually smaller than that of the initial image. Thus, the amount of transmitted information can be reduced by source coding. If one is willing to accept some image degradation, a substantial improvement of performance can be further achieved through quantization. Burt and Adelson have shown that the degradation can be made almost imperceptible through a proper choice of the number of quantization levels. The scheme they propose uses more quantization steps Signal Processing for coarser levels of the pyramid. The sample values at coarser spatial resolutions have to be coded more carefully because their contribution affects a larger number of pixels in the final reconstructed image.
Although this approach achieves excellent image coding performance, we have evidence that it can be further improved. The reason for this is that Burt's construction of the Laplacian pyramid is sub-optimal by several criteria: 
The following sections will show how these criteria can be taken into account.
It is straightforward to verify that (3.2) (3.4) where g is the inverse filter given by
The poles of this filter are -2a+ 4x/~l Zl,2 -(3.6) 1 -2a
The major limitation of the method proposed by Burt et al. is that the EXPAND function defined by (2.10) does not produce a valid image interpolation in the sense that the pixel values at the nodes are not preserved when a coarser level is used to approximate the next finer level. In fact,~,~+ l(k) is a smoothed extrapolation off+ 6k) and the energy of the difference signal is therefore unnecessarily large. We have defined a modified EXPAND function that guarantees strict signal interpolation in the sense defined above. This constraint is formally expressed as
This condition can be satisfied by applying the previous EXPAND operation to an auxiliary sequence {p,÷t (k) } :
l=--o¢ chosen to satisfy the constraint
and form a reciprocal pair. For a >~ ~, these poles are real and the system can be decomposed as a cascade or a sum of causal and anti-causal simple exponential filters. For a = I, G(z) = 1 and an exact interpolation can be achieved with no filtering at all. Otherwise, this operator can be implemented recursively with as few as two adds and three multiplies per sample point, as shown in Appendix A (see Table A ). As the signals encountered in practice are of finite extent (e.g. { f(k) Lk = 1 ..... K}), we have chosen to implement both finite and infinite impulse response filters using the following boundary conditions:
This type of signal extrapolation using mirror symmetry is commonly used in image processing applications and has the advantage of suppressing border artifacts.
Our modified EXPAND operator is represented schematically in Fig. 2 (e) and is described formally
It differs from (2.10) only by the adjunction of a pre-filter (g).
A further refinement is to choose a compression scheme that minimizes the energy of the Laplacian. For this purpose, it is convenient to use the auxiliary coeffÉcient sequence {pi(k)} defined earlier and to express the Laplacian as
We now seek the series of coefficients {p~(k)} that minimizes the error criterion,
As demonstrated in Appendix B, the optimal sequence of coefficients pt(k) satisfies the following equation:
The solution is determined by first convolvingf_ l with w2, performing a decimation by a factor two, and finally filtering the resulting sequence with the operator h that implements the inverse of [w22]lz:
By determining [w22]~2(k) explicitly, the corresponding IIR filter is characterized in the z-transform domain,
The poles of this operator are simple and real for ~< a ~< ½. As shown in Appendix A, h can be implemented recursively with as few as five multiplications and four additions per sample point. The relevant filter parameters for different values of a are given in Table A . By substituting (4.4) in (4.1), we find that the least squares Laplacian is given by
Similarly, the corresponding REDUCE function is obtained by substitution of (4.4) in (3.3),
and differs from (2.8) by the inclusion of two additional levels of post-filtering provided by wl and h. By recalling that g, wffk)= ~(k), we note that (4.6) is fully compatible with both (2.11) and the modified EXPAND function defined by (3.8).
However, a direct evaluation of the least squares Laplacian through (4.6) is preferable for most practical purposes. It is more economical and also reduces the propagation of roundoff errors. These results are summarized in Fig. 2 which provides a block diagram representation of the EXPAND, REDUCE and LAPLACIAN functions and a comparison of the conventional and least squares Laplacian pyramids.

The experiments were performed with a =3, unless indicated otherwise. In these comparisons, the three following procedures were considered: (i) the initial Laplacian pyramid (LP) based on (2.8) and (2.10), (ii) the Laplacian pyramid with interpolation (LPI) based on (2.8) and (3.8) , and (iii) the least squares Laplacian pyramid (LSLP) based on (4.6), (4.7) and (3.8).
The Gaussian and least squares pyramidal representations for two test images are shown in Figs. 1 and 3, respectively. In both cases, the sharpness of the least squares pyramid is preserved at all resolution levels, while the corresponding images in the Gaussian pyramid seem increasingly blurred by comparison. The distinction between the two methods is even more striking if one looks at the Laplacian images displayed in Figs. 4 and 5. The same intensity scaling factors were applied to all images to facilitate the comparison. For the initial LP, the amount of information at each level is quite significant and the initial subject is still recognizable. In the case of the LSLP, the energy of the Laplacian is reduced drastically and only very high frequency details are visible in this representation. In a first stage, the performance of the decomposition can be assessed in terms of simple statistics These measures are given in Tables 1 and 2 for test images (a) and (b), respectively. For a = 2, the LPI is superior to the basic LP in all respects (e.g., reduced range, smaller standard deviation and entropy, and better signal approximation). As expected, the LSLP provides an even better signal approximation. In fact, the SNR values obtained for an LSLP extrapolation at a given level i are comparable to those obtained for an LP extrapolation at level i-1 with four times more sample values. The improvement of the LSLP is particularly striking at the finer resolution level at which the residual RMS error is approximately reduced by a factor of 2. Note, however, that this effect is reversed for the coarser levels and that the LSLP has the tendency to pack the energy into the top of the pyramid. In terms of image coding, this means that while fewer bits are required for representing the finer levels of the LSLP, more bits will be necessary for coding the coarser levels, a result consistent with the bit allocation strategy used by Burt and Adelson. For lossless image coding, the number of bits per pixel (bit-rate) necessary to transmit the top of the pyramid up to level i is approximately 
The rate-distortion curves for our test images are given in Fig. 6 . For both images, the LSLP achieves the best performance at all resolution levels. The LP is the worst and the LPI is in between. An aspect that must also be taken into account in this comparison is that the performance of the pyramid decomposition depends on the value of the parameter a. In principle, our modified scheme should result in some improvement for any value of this parameter, although this effect may not always be as dramatic as in the examples discussed above. A case of special interest occurs when a = 0.5 in which case the LP is equivalent to the LPI (i.e. g(k) = identity). The corresponding error statistics for the MRI image are given in Table 3 . The performances of the LPI are slightly superior to those obtained with a = 3. The LSLP performs best but the improvement is not as dramatic as in Table  2 . For comparison, we have also included the results for the LP with a---0.6, the parameter value that resulted in the greatest reduction in entropy and variance in the series of experiments reported by Burt and Adelson [3] . The improvement over 3 the LP with a=~ is substantial, emphasizing the importance of the optimization of this parameter. Despite these excellent results, the optimized LP is still less performant than the LSLP which provides its best results for a = 3.
The Laplacian pyramid coding scheme proposed by Burt and Adelson is especially suited for lossy image transmission [3] . The quantization scheme that they propose uses fewer bins for the higher resolution levels of the pyramid, which takes into account the fact that human contrast sensitivity decreases with high spatial frequencies. We have conducted some preliminary experiments to compare the efficiency of the different pyramid representations for this type of image coding. The experimental procedure is similar to the one used in [3] with some minor differences. The important features of the present compression algorithm are as follows:
(i) The coding and the decoding are performed in parallel starting at the coarsest level of the pyramid. In the present case, the pyramid has three levels and the coarsest (~) is coded precisely using all eight bits per node (256 gray level values). The corresponding contribution to the total bit-rate is only 8=0.125bits/ pixel. (ii) A Laplacian image is computed from the difference between a particular level of the Gaussian pyramid and the expanded version of the encoded image one level coarser. This technique takes into account quantization errors introduced at coarser resolution levels. (iii) The number of levels for each Laplacian image is fixed and should be determined using psychophysical information. The values of these levels are determined using a discrete form of the Max minimum error quantization algorithm [ 11 ] applied to the histogram of the Table 3 Comparison of performance measures at successive pyramid levels for the 'MRI' image with a=0.5 and a=0. images. The corresponding quantization levels are selected to minimize the approximation error and are not necessarily equidistant as was the case in the approach chosen by Burt and Adelson. In this series of experiments, the Laplacian images 2 and 3 were represented by 5 and 15 levels, respectively. The finest level of the pyramid was either not transmitted at all to achieve bit-rates lower than 1 bit/pixel or represented by 3 levels. (iv) The effective bit-rates are estimated from the entropies of the quantized images using (5.2). These estimates are somewhat optimistic as they ignore the transmission of the code book information. A practical approach to this problem is to summarize this information in terms of the coefficients of a parametric model of the Laplacian histogram (for example, the two parameters a and fl of a generalized exponential model p(Af) = Co e-~lAJ-I/~)~). These parameters can then be used to determine uniquely the optimal quantization levels in the Lloyd Max scheme and their corresponding code words in a variable length Huffman code [6] . Some examples of image coding with bit-rates as low as 0.7 bits/pixel are shown in Fig. 7 . The same number of quantization levels were used in all cases with the exception of Fig. 7(d) . This latter image is an improvement of Fig. 7 (c) obtained by adding a finer level of the LSLP quantized with three levels; it is visually indistinguishable from the original. The image obtained using LP (Fig. 7(a) ) appears to be out of focus and is of lesser quality (both qualitatively and quantitatively) than the results obtained with the LPI and LSLP. The LSLP scheme is clearly superior and appears to preserve most of the image details. The same qualitative behavior has also been observed for different compression ratios and test images. For the test image in Fig. 7 , we have also observed that the quality of the LP reconstruction is noticeably degraded for bit-rates lower than 1.5 bits/pixel, while for the LSLP greater compression ratios still produce acceptable results, as illustrated by Fig. 7(c) . In 197 these preliminary experiments, the performance of LSLP appears to be consistently superior.
Our experimental results show that both the LPI and LSLP should be superior to the standard LP proposed by Burt et al. Two types of improvements have been considered and both seem to be equally 1 helpful, at least for a < ~. The first is the requirement that an image extrapolation be a true interpolation of a lower level approximation. A simple way to enforce this constraint is to add a pre- filter  (g(k) ) to the basic EXPAND operation. The effect of this operator is less significant when a is close to ~, in which case the LP and LPI are essentially equivalent. The second is to minimize the amount of transmitted information. The only adjunction here is a post-filter following the basic REDUCE operation. The LSLP incorporates both of these mechanisms and has surprisingly good compression properties. This approach provides an attractive alternative to the standard LP and should allow greater efficiency in image coding. Since multi-resolution techniques are being used increasingly in image processing, there are many other potential applications including image segmentation [2, 16] , edge detection [10] , feature extraction and a variety of multi-grid algorithms for computer vision [14] .
The experimental results presented in Section 5.1 indicate a performance improvement in a lossless progressive data transmission scheme (cf. Fig. 6 ). The reduction of the RMS error also suggests that the LSLP should result in some improvement for lossy image coding as confirmed by our preliminary experiments (cf. Fig. 7 ). These results, however, are still preliminary and require further investigation. For instance, it seems important to determine an optimal bit allocation strategy for a given compression ratio and to compare the coding results for a variety of test images using objective psychovisual criteria. A detailed evaluation of the dependence of the relative performance of the algorithms on the parameter a may also be appropriate. As described in Appendix A, the additional preand post-filters can be implemented very efficiently and the increase in computation is negligible. For instance, the CPU times (standard 16 MHz Apple Macintosh Ilcx) required to compute the first level of the Laplacian of a 256 x 256 image using Burt's LP, the LSI and LSLP are 18 s, 25 s and 27 s, respectively. The complexity of the LSI and LSLP are comparable because the use of the interpolation pre-filter can be avoided in the second scheme (cf. Fig. 2(f) ).
The value a--83---0.375 was used for most of our experiments. It is close to the value 0.36 recommended by Burt for the greatest reduction of the side lobes of the transfer function [2] . Note that a =3 corresponds to an implicit choice of a Signal Processing quadratic B-spline interpolator [17] . In terms of performance, this value of a seems to be preferable over others (cf . Tables 2 and 3) , largely because of the smoothness and Gaussian-like shape of the corresponding interpolation kernel, which appears to be most appropriate for a large class of images. The unmodified LP, on the other hand, seems to perform best for a=0.6 [3] . An explanation for this observation is that the corresponding correction filters in our modified scheme have a very fast decay (i.e., g(k)= O(z~ ~1) and h(k)= O(plkl), where Zl = 0.084 and p = 0.074), and can be relatively well approximated by an identity filter. Another value 1 of interest is a = ~. This value leads to a triangular interpolation function and corresponds to image reconstruction by piecewise linear (or bilinear) interpolation. This scheme is equivalent to a firstorder spline interpolation.
Clearly, the theory presented here is not restricted to the particular form of interpolation function given by (2.4). It is straightforward to adapt these results to any given kernel w(k) ~ W(z). The only constraint is the stability of the approximation and interpolation filters, which, in the general case, are given by
There is another advantage for the use of the least squares pyramid. In a standard complete pyramidal representation the number of nodes is 1 increased by 5 when compared to the initial number of pixels. In the LSLP, the total number I of nodes can be reduced by a (e.g., made equal to the initial number of pixels) because the residual error at each step is orthogonal to the reduced resolution signal approximation. In other words, the LS REDUCE function is a projection operator with the property that
For a bi-dimensional image with M grid points, (5.6) provides us with a set of 1 gM linear constraints. The true number of degrees of freedom of the LS Laplacian is therefore 3M and not M as may be thought initially. In fact, we will show in the last section that the quadrature mirror filter (QMF) concept offers a simple solution for dealing with this redundancy. We will thereby also establish the relationship between the present approach and recent work in orthogonal pyramid structures [1, 12] , wavelet transforms [9] and subband coding techniques [ 18, 20] .
Quadrature mirror filters, introduced by Croisier et al. in 1976 [4] , provide an attractive method for splitting a signal into critically sampled filtered components. Such filter banks can be applied iteratively to produce a subband decomposition of the spectrum into octave bandwidth pieces [18] . The two attractive features of this technique are (i) the reversibility of the process (error free reconstruction) and (ii) the fact that the resulting signal decomposition uses no more samples than the initial representation. Recently, several authors have applied this concept to pyramid image compression and have reported substantial improvements in performance [1, 12, 18, 20] . QMF banks also provide an efficient way of computing wavelet transforms, as has been shown recently by Mallat and Daubechies [5, 8, 9] .
The block diagram of a QMF bank is represented in Fig. 8 . In the basic QMF design [12, 19] , the transfer functions of the filters are chosen such that
where F(z) is a lowpass filter prototype satisfying the perfect reconstruction property
(5.8)
To establish its relationship to the present approach, we will construct a QMF bank such that its lower branch (lowpass) precisely computes the least squares signal estimates derived in Section 4. We derive this result by manipulating the block diagram in Fig. 9(a) , which performs successively the REDUCE and EXPAND functions described in Sections 3 and 4. The first step is to note that Fig. 9 . QMF interpretation of the least squares approximation procedure: equivalent block diagrams.
the two central filters (W~ (z) and G(z)) cancel each other. Second, the filter H(z) is factored into a product of square-root components ( Fig. 9(b) ). Finally, the filters are moved on each side of the sampling modules by upsampling their impulse response by a factor of two (this is achieved by replacing z by z 2 in their z-transform) ( Fig. 9(c) ).
At the end of this process, we have an equivalent system (i.e., same input and output) for which the pre-filters and post-filters are identical and given by
Using (5.4), it is then easy to verify by substitution that this operator satisfies the perfect reconstruction property (5.8) . Since the final output of the QMF bank is equal to its input, it follows that the corresponding highpass branch precisely codes for the residual signal displayed in the least squares Laplacian pyramid. This approach is easily extended to higher dimensions by iterating the subband decomposition along the rows and columns according to the procedure initially described by Vetterli [ 18] . The main advantage of such a QMF decomposition is that the residual signal is now represented without redundancy (i.e., the sum of the number of lowpass and highpass samples is equal to the initial number of samples).
In order to obtain a decomposition closer to our initial design, we choose an alternative, but globally equivalent, factorization with
for which it can be verified that the filters Fo(z) and Go(z) are precisely those required for the REDUCE and EXPAND function described in Sections 3 and 4. The advantage of this latter decomposition is that it can be implemented recursively using the fast algorithms described in Sections 3 and 4 (see Figs. 2(d) and 2(e)) and Appendix A. The highpass components can be evaluated using the same procedure, provided that the FIR smoothing kernel (Wz(z)) (which is used as a pre-and post-filter) is replaced by its modulated and shifted counterparts: z W2(-z) and W2(-z)/z, respectively. We note that this particular choice of filters corresponds to a linear algebraic transform that is non-orthogonal, in contrast to a standard QMF bank as defined by (5.7) (5.8), which can be interpreted as an orthogonal transformation [12] . These results also suggest that a QMF implementation of the present least squares image pyramid could provide a further I improvement by z over the coding procedure used in the experimental part of this paper.
Two methods for improving the Laplacian pyramid proposed by Burt and Adelson for image coding have been described: (i) The EXPAND function has been redefined to ensure that the expansion of a coarser level onto a finer grid is an exact interpolation. (ii) An improved REDUCE function has been derived in order to minimize the loss of information occurring during resolution conversion.
It is easy to modify the initial scheme to incorporate these new functions. This is achieved by adding a pre-filter and a post-filter in the expansion and reduction modules, respectively. These filters can be coded very efficiently and the resulting increase of computations is moderate.
For lossless progressive data transmission, the performance improvement that can be achieved in this way is significant. The least squares scheme performs best according to the quantitative criteria used in this paper. Preliminary results suggest that this approach allows improved image coding according to the lossy scheme developed by Burt and Adelson. The least squares pyramid also stands as an interesting alternative to the widely used Gaussian pyramid and should be useful in a variety of multi-resolution image processing algorithms. It has also been shown that the present approach can be linked to the family of QMF image pyramids (e.g., orthogonal pyramids, wavelet transforms, subband coders). 
The implementation of these elementary units is based on the decomposition of H(z; z,% into a sum of simple causal and anti-causal first order systems, as given by the right-hand side of (A.3). The corresponding recursive filter equations are also more economical to combine the individual scaling factors in (A.1) and (A.5) or (A.7) into a single multiplication at the end of the process. The relevant filter parameters for implementing some of the operators described in Sections 3 and 4 using this strategy are given in Table A . This approach is also applicable in higher dimensions through the successive use of the same onedimensional filter along the various dimensions of the data. For digital images there is no need for floating point data storage other than the onedimensional array(s) required by the basic onedimensional filtering module.
We note that the second equation is borrowed from the sum decomposition and is required to initialize the backward recursion correctly.
All operations in (A.7) (respectively (A.5)) are real, and it is necessary to use one (respectively two) one-dimensional real array(s) for storing the filtered sequences with sufficient precision to avoid a recursive propagation of errors. It is relatively straightforward to write a general subroutine that implements (A. 1) from a succession of simple convolutions of the form (A.5) or (A.7); no additional intermediate storage is necessary for this task. It is
The error criterion (4.2) is decomposed as 
(B.1)
The partial derivative of (B. 1) with respect to p~(k)
is given by The optimal sequence of coefficients is obtained by setting this expression equal to zero, which results in (4.3). Q.E.D.
Albert Einstein discovered noise accidentally in 1905, when he observed that atoms move according to the Brownian molecular motion [1] . Following his discovery, numerous descriptions of physical and biological systems have made incidental reference to noise, without recognizing its essential contribution. Noise is often regarded as an unwanted component or disturbance to a system, even though it has a tremendous impact on many aspects of science and technology [1] , including medicine and biology. A typical example for such a statement is a field of engineering called signal processing. On one hand, many signal processing algorithms have been designed to remove noise from a system, since greater noise levels are associated with degraded performance of algorithms.
On the other hand, noise has been shown to enhance system performance in many areas of signal * Ervin Sejdić is with the Department of Electrical and Computer Engineering, Swanson School of Enginering, University of Pittsburgh, Pittsburgh, PA, 15261, USA. E-mail: esejdic@ieee.org. Ervin Sejdić is the corresponding author.
† Lewis A. Lipsitz is with Harvard Medical School, Beth Israel Deaconess Medical Center and Hebrew Senior Life, processing including stochastic optimization techniques, genetic algorithms, dithering, just to name a few. Similarly, another concept called stochastic resonance (SR), first proposed in 1981 (e.g., [2] , [3] ), describes a positive impact of noise in nonlinear systems. SR refers to the fact that at an optimal level of input noise, signal detection is enhanced [4] , [5] . SR is observed in both man-made and naturally occurring nonlinear systems [6] . For example, paddlefish were shown to use SR to locate and capture prey, implicating this phenomenon in animal behavior [7] . Also, small noisy input can influence the firing patterns of squid axons [8] , enhance breathing stability in pre-term infants [9] , improve postural control in human aging, stroke or peripheral neuropathy [10] , [11] , and stabilize gait in elderly people with recurrent falls [12] .
The intent of this manuscript is to inform researchers from multiple scientific disciplines that noise (i.e., stochastic processes) is a critical component of many biological and physiological systems that may be exploited in the future to develop interventions for the prevention and treatment of diseases. In other words, this manuscript is a crossover between a review paper and a position paper and as such is meant to initiate further discussions about the role of stochastic processes in modeling of physiological systems.
To gather previous contributions cited in this manuscript, we utilized PubMed and Google Scholar to find manuscript published in English using a variety of search terms (e.g., "noise physiology," "noise medicine," "noise brain," "noise aging"). These search terms yielded thousands of manuscripts and we focused only on representative publications from several fields. Extensive coverage of all topics is beyond the scope of this paper, since excellent extensive reviews of each have been previously published (e.g., [6] , [13] ).
The paper is organized as follows: Section 2 introduces various stochastic processes considered in biomedical systems, while also describing the physiological meaning of these processes. Section 3 discusses the important role of noise in fundamental biomedical systems. In Section 4, we discuss several translational applications of noise to treat diseases, while in Section 5, we provide concluding remarks along with an outline of possible future directions.
2 Noise and variability in physiological systems
By definition, noise is a stochastic process with specific spectral characteristics. While many different stochastic processes exist, we consider here the most common types discussed in the literature.
White noise is a stochastic process characterized by equal energy over all frequencies. In mathematical terms, its power spectral density is equal to:
where C w is a constant. The name "white" stems from the fact its power spectral density is the same at all frequencies in an analogy to the frequency spectrum of white light. A time-domain realization of the white noise is depicted in Fig. 1 (a), while its power spectral density is depicted in Fig. 1 
Pink noise (also called fractal or 1/f noise) is a stochastic process suitable for modeling evolutionary or developmental systems characterized by equal energy per octave as depicted in Fig 1(d) [14]. The power spectral density of pink noise is roughly inversely proportional to frequency [14] :
where C f is a constant and 0 < α < 2. 1/f noise is a stochastic process between white noise
(1/f 0 ) and red (Brownian) noise (1/f 2 ); hence, the name pink noise. Pink or fractal noise is found in numerous biological and physiological processes, including the organization of neural networks, Purkinje fibers in the heart, the vascular tree, bronchial tree, and bone trabeculae, as well as electroencephalographic rhythms, heart rate variability, and respiratory intervals [15] , [16] .The omnipresence of pink noise in many diverse applications has led researchers to speculate that there exists some profound law of nature that applies to all nonequilibrium systems and results in such noise [14] .
Because pink or fractal noise arises from the interaction of multiple physiologic or biologic control systems operating over different scales in time or space, it may confer system resiliency, adaptability, and structural integrity. For example, the structural (e.g. bone trabeculae) or functional (e.g., heart rate control) networks that generate such noise retain their integrity or functional ability if individual components are lost or interrupted. This fractal network organization also enables a system to adapt to stress by drawing on specific components and fine tuning its response to overcome a given perturbation [16] . Brownian or red noise is a stochastic process whose power spectral density, as depicted in Fig. 1(f), is defined as:
where C b is a constant.Mathematically, the Brownian noise can be defined as the integral of the white noise.
There are other types of noise specific to certain applications (e.g., blue noise, diotic noise, and dichotic noise). However, the extensive coverage of these topics is beyond the scope of the current manuscript.
Physiology teaches us that healthy systems are self-regulated to reduce variability and maintain physiologic constancy [17] . However, that is not the case in reality. Small amounts of noise, as depicted in Fig. 2(a) , can have a very beneficial role in physiological systems (e.g., [9] , [10] , [15] , [16] ). Also, the non-linear interactions of multiple regulatory systems and environmental influences operating over different time scales produce highly variable "noisy" behaviours in physiological processes that are far from constant [18] . For example, the normal human heartbeat fluctuates in a complex stochastic manner [17] , and can be modeled as a 1/f process (e.g., [19] ).
On the contrary, the stochastic properties of the heartbeat time series degrade in subjects at high risk of sudden death (e.g., congestive heart failure patients) becoming more characteristic of white noise. This situation is depicted in Fig. 2(b) , where a deviation from 1/f noise can result in reduced functional capacity and the onset of disease. Similar counterintuitive results have been obtained in other fields. For example, gene expression can be thought of as a stochastic process [18] , [20] . Stochastic gene variations can have both beneficial and harmful roles. Different patterns of gene expression can influence the stress response, metabolism, development, the cell cycle, circadian rhythms, and aging [18] . Therefore, elucidating the stochastic mechanisms involved in physiologic control systems and complex signaling networks is emerging as a major challenge in the postgenomic era [17] .
A number of studies elucidating the fundamental mechanisms of biological systems suggest that noise is an "essential ingredient" in these systems, without which they cannot function. For example, noise plays an important role in molecular transitions or interactions that control cellular behavior (e.g., how cells acquire fate) [21] . Furthermore, several mathematical models used to describe biological processes require a noise term to adequately model the behaviour of these processes.
Cellular processes, such as transcription and translation, chromatin remodeling and pathwayspecific regulation, are sources of stochastic events leading to cell-to-cell variability [18] , [22] . In fact, cellular behaviour varies in clonal cell populations despite their development in identical environments [23] .
Stochastic processes can have a dual role in these systems. One point of view is that the stochasticity obstructs the efficient functioning of cellular processes [24] . The accuracy of cellular processes, such as the circadian oscillator, is limited by noise in gene expression [20] . Noise can interfere with the operation of engineered genetic circuits [25] and cell-to-cell variability can be reduced by engineering a circuit with negative feedback [26] . There is also evidence that aging is associated with increased randomness in gene expression [18] . For example, cell type-specific gene expressions in individual murine cardiac myocytes [27] , murine muscle tissues [28] and C. elegans [29] become increasingly stochastic as the organism ages. While the mechanisms underlying these stochastic phenomena are still unclear, the process of aging may be dependent on the effects of stochastic gene expression [18] . Another interpretation of these observations is that changes in gene expression with aging are associated with a shift from the more adaptive 1/f or fractal-like noise, to more random or white noise-like behavior that cannot adapt to the metabolic demands of the aged cell. This notion will need experimental validation.
The second point of view is that noise might have beneficial properties [30] . For example, living cells usually acquire their fate deterministically by virtue of their lineage or their proximity to an inductive signal from another cell. However, a cell can choose to differentiate stochastically without apparent regard to environment or history [31] . This random behavior can arise from significant stochastic fluctuations (i.e., noise) in cellular components and biochemical reactions [30] , [32] . Additionally, differences in the micro-environments inhabited by individual cells and preexisting heterogeneity propagated to subsequent cell generations can be sources of such cell-to-cell variations [33] . Cell-to-cell variability is thus a complex function of regulation of gene expression and the regulatory and biochemical networks in which the gene products are embedded [30] .
Neuronal networks are known to have noisy, heterogeneous and compact structures [34] , [35] . There are two points of view regarding the role of noise in these networks. One point of view argues that noise lowers the signal-to-noise ratio causing the performance of these networks to degrade. The second point of view states that noise reduces spike-timing precision and therefore, the information rate is lowered. However, noise can play important and constructive roles for the amplification of information transfer in neuronal networks [36] , [37] .
Stochastic variations are an essential part of the nervous system [37] , and the effects of these variations in dynamical neurobiological systems have been studied extensively for both single neurons and neural networks. Pioneering works by Derksen and Verveen in 1966 [38] and by Katz and Miledi in 1970 [39] were the first to establish the probabilistic behavior of neurons in the central nervous system [34] . Derksen and Verveen investigated the role of membrane noise in the probabilistic behavior of neurons in the central nervous system [38] , while Katz and Miledi studied signal fluctuations associated with ACh receptor-mediated muscle depolarization [39] . The foundations set by these two groups were later applied to experimental data to gain an insight into the nature of transmembrane-conductance changes and information processing in the brain [34] . Subsequent publications showed that intracellular recordings of cortical neurons consistently display highly complex and irregular activity due to an intense and sustained discharge of presynaptic neurons in the cortical network [36] .
In addition to synaptic noise, the stochastic activity of ion channels is another significant source of noise in the nervous system. For example, thermal agitation causes voltage-gated ion channels in neuronal membranes to fluctuate randomly between conducting and nonconducting states inducing noisy membrane currents and subthreshold voltage fluctuations [40] , [41] . It is now understood that channel noise affects spike-timing reliability, action potential dynamics, signal detection, the tuning properties of the cell and overall has important effects on neuronal information processing capabilities [36] , [40] , [41] . Lastly, while noise often leads to increased responsiveness in the nervous system, empirical data and neuronal models demonstrate that noise can also subdue or turn off repetitive neuronal activity [8] , [42] .
Noise generated in the brain may influence brain behavior. Noise is generated in the brain by random spike firing times of neurons [13] . By influencing the variability of the firing of neurons, noise may influence decision-making, memory, and the stability of short-term memory and attention [43] . Furthermore, cognitive operations are also affected by stochasticity in N-methyl-D-aspartate activated receptors, which affect the stability of short-term memory and attention, and in alterations of gamma-amino-butyric acid receptor activated synaptic ion channel conductances which are predicted to influence how likely the system is to jump incorrectly into a pathological state of high activity [13] . Similarly, in motor learning, the brain uses movement errors to adjust planning of future movements. This physiologically plausible strategy is optimally tuned to the properties of motor noise, and likely underlies learning in many motor tasks [44] .
Overall, noise in the brain promotes decision-making, creativity and the shifting of attention to new tasks [13] . The presence of stochastic brain variations (e.g., due to stochastic variations in spiking of neurons and in synaptic transmissions) is helping investigators and clinicians to understand pathological brain stability states, such as schizophrenia and obsessive-compulsive disorder.
One notion is that there is a range of stability states in different individuals. Instability (e.g., due
to random firing of neurons) contributes to the symptoms of schizophrenia [45] , while too much stability contributes to the symptoms of obsessive-compulsive disorder [45] . Of potentially great importance is that by having a model that is based on the ion channel conductances affected by different neurotransmitters, it is becoming possible to make predictions about what could be favorable combinations of treatments for particular disorders [45] .
Noise is omnipresent in sensory systems, ranging from the emission of neurotransmitters from the presynaptic membrane to the behavioral results in visual and auditory experiments (e.g., [6] , [46] , [47] , [48] , [49] , [50] , [51] ). For example, a recent study suggested that the addition of an appropriate amount of external noise can improve the perception of an "uncertain" visual signal that is difficult to detect [52] , [53] . Figure 3 depicts how adding an appropriate amount of noise improves image contrast and then degrades as we add too much noise [54] . Noise of a particular magnitude (i.e., the SR effect) also tends to enhance visually evoked responses in electroencephalography (e.g., [55] ) and magnetoencephalography studies (e.g., [56] ). Similarly, in [57] , the authors showed that a certain amount of noise reduced the pedestal effect, i.e., the improved detectability of a grating in the presence of a low-contrast masking grating. Their results supported the idea that a single mechanism underlies the pedestal effect and stochastic resonance in contrast perception [57] . 1/f noise is also effective in driving hallucinatory pattern formation as shown in [58] , where the authors explored the relationship between ordinary stimulus-controlled pattern perception and the autonomous hallucinatory geometrical pattern formation that occurs for unstructured visual stimulation (e.g., empty-field flicker). Similar results were observed in human hearing experiments [4] . 
Noise is a necessary component even in modeling of certain biomedical systems. In some cases, noise has a specific physiological meaning, while in others, limited knowledge about the systems under investigation yielded creation of a noise category to capture variability observed in experimental data. The next few subsections briefly cover some of the most well-known models requiring a noise term in order to adequately describe a function of a biomedical system. Although there are many more mathematical models that require a noise term in order to accurately model the phenomenon under consideration, it is beyond the scope of this manuscript to review all these different models.
The Hodgkin-Huxley model, one of the most important models in biomedicine, describes membrane potential, activation of Na and K currents, and inactivation of Na current [59] . Specifically, the Hodgkin-Huxley model describes the spiking behavior and refractory properties of neurons and serves as a paradigm for spiking neurons based on the nonlinear conductance of ion channels [60] .
The model is given by four nonlinear coupled equations, one for the membrane potential V , and three for gating variables m, n, and h:
where m ∞ , h ∞ , n ∞ , τ m , τ h , τ n represent the saturation values and the relaxation times of the gating variables. The membrane potential is driven by three types of currents: ionic current I ion , external stimulus current I ext , and synaptic current I syn . I ion is related to the gating variables of m, n, h and describes the ionic transport through the membrane:
where V N a , V K , V l are the corresponding reversal potentials and the constants g N a , g K , and g l are the maximal conductances for ion and leakage channels. I ext is the external stimulus usually serving as a bifurcation parameter of the system. I syn is the sum of the current inputs from all synapses connected to the other neurons and can be modeled as:
where ξ(t) is Gaussian white noise, and σ and τ d are the intensity and the correlation time of the synaptic noise, respectively [60] . Fig. 4 examines the effects of varying σ on the membrane potential, V . As σ decreases, the potentials become highly regular as depicted in Fig. 4(d) . This shows that without noise in the organism, the human body would be a highly deterministic system, and would not be able to account for any changes in the environment.
The Hodgkin-Huxley-type models are important not only because their parameters are biophysically meaningful and measurable, but also because they allow us to investigate questions related to synaptic integration, dendritic cable filtering, effects of dendritic morphology, the interplay between ionic currents, and other issues related to single cell dynamics [59] and there are extensions to various other fields such as cardiology (e.g., [61] ) in the literature.
The Fitz Hugh-Nagumo model is a simple but representative example of excitable systems that occur in application ranging from kinetics of chemical reactions and solid-state physics to biomedical processes [62] . Originally it was suggested for the description of nerve pulses [62] , but it found its applications in other fields as well. The equations are:
where ε << 1 is a small parameter allowing one to separate all fast and slow motions; the parameter α governs the character of solutions; and the parameter σ governs the amplitude of the noisy external force ξ assumed to be additive white Gaussian noise with zero mean [62] .
Similarly as the Hodgkin-Huxley model, the Fitz Hugh-Nagumo model is sensitive to the magnitude of σ as depicted in Fig. 5 . The presence of noise is necessary in order for the model to accurately represent a biomedical process. 
Cancer is stimulated by successive somatic mutations [63] . Here, we briefly review a stochastic model for the computation of cancer risks based on the hypothesis of two successive mutations [63] .
The model assumes that cells likely to mutate will divide over the lifetime of the tissue. Next, the number of type 1 mutation cells produced over the lifetime of the tissue is distributed according to the Poisson distribution with mean µ. The branching process begins with the appearance of the first type 1 cell. This type 1 cell may die with probability 1 − p 1 . The second option is that the type 1 cell divides in two type 1 cells with probability p 1 . At each division of a type 1 cell, there is a probability p 2 for each daughter cell to be a type 2 cell. The probability that a branching process started by a single type 1 cell eventually gives birth to at least one type 2 cell may be computed exactly:
The number of type 1 branching processes that eventually produce at least one type 2 cell is given by the Poisson distribution with mean µP b (p 1 , p 2 ). Hence, the probability that cancer will occur in a particular tissue is given by
The parameter µ is crucial in this model as shown in Figure 6 , as its value will dictate the shape of the probability density function of cancer. For small values of µ, the probability of cancer is almost negligible, even when the probability of type 1 cell branching increases past 50%. However, as we increase the mean number of first mutations to µ = 100, the probability of cancer becomes 100%, as the probability of type 1 cell branching increases past 50%.
Here, we present several applications of noise to enhance health. Also, we briefly discuss how noise (i.e., stochastic processes) can be used to model the effects of aging or social networks.
Noise-based bioengineering techniques and medical devices can play an important role for treating diseases and enhancing health overall. From a clinical standpoint, noise-based techniques and devices have been used to enhance signal detection in patients with significant sensory deficits, such as older adults [64] , [65] , patients with diabetic neuropathy [66] , patients with stroke [67] , or profoundly deaf people receiving speech cues by direct electrical stimulation of the cochlear nerve [68] . Nose-based devices have been used to increase tactile sensations [69] , [70] to help post-partum women achieve higher pelvic floor muscle activation [71] , or to alleviate postural instability due to ankle sprains [72] , [73] and lower back pain [74] . Noise-based solutions can even be used for the enhancement of brain-to-computer interfaces [75] .
Noise-based devices, such as randomly vibrating shoe insoles [10] that apply noise during specific activities or throughout the day, may enable people to overcome functional difficulties due to age-related sensory loss [11] , [64] , [76] , [77] , [78] , [79] . Furthermore, noise-based mechanical ventilators can improve gas exchange and could have a significant effect on morbidity by breaking the chain of injury propagation in acute lung injury [80] . These devices could potentially reduce the morbidity associated with various health issues, such as sensory loss and postural instability in elderly and disabled people or to help stroke patients and individuals with muscle and joint injuries in rehabilitation activities [64] . Noise-based techniques could potentially accelerate a patient's rehabilitation. In this regard, the ultimate realization of a noise-based device may be one that provides durable benefit that lasts long after the device is removed [76] .
Noise has detrimental effects on cognitive performance due to the competition for attentional resources between the distracting and the target stimuli. This has been observed for a wide variety of tasks and stimuli as well as in different participant populations [81] , [82] . However, recent empirical evidence suggests that noise can also improve central processing and cognitive performance. For example, auditory noise enhanced the speed of arithmetic computations [83] and recall on visual memory tasks [84] . Thus, adding a moderate level of noise to the input of the information processing system can increase its signal-to-noise output. On the other hand, adding too little or too much noise attenuates performance [82] . This is consistent with the phenomenon of SR. Noise exerted a positive effect on cognitive performance for patients with the attention deficit hyperactivity disorder, indicating that these subjects need more noise than controls for optimal cognitive performance [82] . The Moderate Brain Arousal model suggests that noise in the environment introduces internal noise into the neural system through the perceptual system. This noise induces SR in the neurotransmitter systems and makes noise beneficial for cognitive performance [81] . Similarly, a recent experiment showed that background noise had opposing effects on inattentive and attentive children. While it enhanced performance for the former group, background noise deteriorated performance for latter group. Background noise also reduced episodic memory differences between these two groups of school children. This suggests that cognitive performance can be moderated by external background white noise stimulation in a non-clinical group of inattentive participants [82] . However, one should be aware that these stochastic resonance effects were not always present [50] .
As described above, many healthy physiologic processes exhibit stochastic variations due to multiple regulatory influences operating over different time scales. These influences include biochemical pathways, opening and closing of ion chambers, feed-forward and feed-back loops, temperature fluctuations, circadian rhythms and environmental changes. Together, they produce pink noise in the output signal as is evident in healthy heart rate, blood pressure, respiratory rate, electroencephalographic potentials, or center-of-pressure time series [15] , [16] . These noise signals lose their 1/f characteristics with aging and disease due to the degradation of various control mechanisms and their interactions, becoming more white or Brownian. As a result, the organism loses resiliency or adaptive capacity [15] , [16] .
This has been demonstrated in the postural control system by examining the body's centerof-pressure (COP) excursions while standing on a force plate [85] . Under normal circumstances, the COP time series exhibits 1/f behaviour, characteristic of pink noise. However, with the loss of vision, sensation in the feet, or both, there is a progressive loss of complexity and long-range correlations in the data. As a result, the individual has more difficulty adapting to a superimposed cognitive task (e.g., counting backwards while standing) and postural sway increases [85] .
Similarly, anatomic structures lose fractal-like architecture with aging, leading to a loss of functionality. This is evident in degeneration and loss of connectivity of the bone trabecular network leading to osteoporosis and fractures; the breakdown of fractal-like alveoli in the lungs leading to emphysema; and the disruption of the collagen matrix in the dermis leading to skin fragility and hemorrhage. In addition, age-related diseases such as the Alzheimer's (e.g., [86] ) or Parkinson's (e.g., [87] ) diseases affect the stochastic variations of physiological variables. For example, the insole forces during the freezing of gait in patients with the Parkinson's disease have been shown to have stochastic behaviour similar to a Brownian process [87] . Furthermore, Parkinson's patients lose the noisy, fractal-like physiologic tremor of the normal motor control system and develop a highly periodic tremor, which is characteristic of their disease. Fortunately, there is evidence that noise can be restored in at least the postural control system by exploiting the phenomenon of SR. When subsensory vibratory white noise was applied to the soles of the feet in healthy elderly subjects while standing on a force plate, the fractal-like multiscale complexity of COP displacements increased to values similar to those seen in young subjects [65] . This intriguing finding supports the notion that noise is an important component of a healthy, and highly functional, postural control system.
Noise has also been shown to play role in the social sciences. For example, the psychic structure long known as the "self" is best conceptualized as a dynamical stochastic system [88] . Among the various topics addressed in this field, it has been found that models for opinion formation in a society exhibit a rich variety of nonlinear behavior, such as phase transitions and critical phenomena, stochastic resonance, chaos, and bistability [89] . In fact, the existence of SR in a model of opinion formation yields the appealing implication that there is an optimal noise level for a population to respond to an external "fashion" modulation. Lower noise intensities lead to the dominance of the majority's opinion, irrespective of external influences, while sufficiently stronger random fluctuations prevent the formation of a definite collective opinion [90] .
Since the recognition of noise at the beginning of the twentieth century, the prevalent view in most fields is that noise degrades system performance and most real-life events do not exhibit noise-like behavior. In this manuscript, we reviewed several biomedical fields where noise plays a constructive role and in some cases is necessary for a biomedical system to function properly. Such a constructive behavior is particularly obvious in systems that depend on the complex interactions of many different components operating on different time scales (i.e., nonlinear systems). Therefore, most of the research efforts have been geared towards:
• understanding the sources of stochastic fluctuations in biomedical systems and possible advantages and/or adverse consequences of these fluctuations on the systems;
• understanding why and how these systems have become robust in their noisy environments; and • how we can use noise to develop treatments and enhance human health.
Further development of noise-based devices or treatments in biomedicine depends on available computational and experimental tools that will answer questions about the the origins of noise in physiological systems and the mechanisms by which noise affects their function. On the computational side, we need to develop more sophisticated algorithms that are capable of simultaneously extracting important stochastic and deterministic variations from the system and handle huge amounts of data. Also, the software applications needed for the understanding of noise in biomedical/physiological systems are almost non-existent. Most computational investigations are carried out using custom-made functions or toolboxes via commercially available packages such as MATLAB (MathWorks, Natick, MA, USA) or SAS (SAS Institute, Cary, NC, USA). On the experimental side, we need to develop experiments and tools that can characterize the noise behavior in systems. The ultimate goal for these advances is to achieve full stochastic resolution over different scales and systems. However, a plan for wide dissemination of data acquired in these experiments should be embedded in these projects to accelerate advances in noise physiology. We anticipate that a limited number of laboratories will have necessary monetary, equipment and staff resources needed to carry some of these sophisticated experiments.
Noise is potentially a very powerful tool in physiology and medicine. We hope this paper will catalyze further research and applications of noise to improve human health and ameliorate diseases.
In this paper, we introduce Swift protocol. Swift was originally designed to be a replacement for the BitTorrent protocol and inherits some of the characteristics that have made BitTorrent successful, but was not intentionally designed according to the principles of information-centric networking (ICN) paradigm. It is, thus, until now a product of (unintentional) evolution towards ICN that we now seek to direct and accelerate, while retaining all the properties that make it work well on top of the existing network.
We find the ICN concept to be increasingly reflected in both the way Internet is being used and in how Internetbased services are being implemented today. In many cases, we find that the problems we struggle with in the current * Work by Victor Grishchenko was carried out while at the Technical University of Delft incarnation of the Internet are those that ICN design proposals seek to address.
For example, over 90% of today's Internet bandwidth [4] is effectively devoted to disseminating static multimedia content. In order to do so to an increasingly large and geographically diverse audience, various approaches are used. For example, for web-based content, Content Delivery Networks (CDNs) like Akamai use modified DNS servers that generate responses based on the topological/geographical location of the requester. These "tricks" are there to achieve on the current Internet the features that are at the core of ICN.
During the past years, a chain of new network architectures based on the information-centric paradigm (also content-centric, name-oriented, named-data) have been proposed, including CCN [17] , DONA [20] , NetInf [11] , secure naming by Wong et al [23] , and content-centric router by Arianfar et al [6] to address the limitations of IP, namely the inability to decouple data from storage, inefficient data dissemination, lack of support for middleboxes, ubiquitous availability of data, and security.
The historical conversation-centric end-to-end model, embodied in the TCP/IP stack, is based on message exchange between pairs of peers, typically servers and clients. On the other hand, ICN is a paradigm in which focus shifts away from the mechanics of moving bits between peers (endhosts). Instead, the focus in on the information itself, and the underlying network only a conduit for the information. In a sense, named-data network breaks with the end-to-end abstraction, as there are no ends and the entire network is considered a cloud, which both stores and serves data.
Similarly, we can see in the evolution of peer-to-peer (P2P) file-sharing technologies how they have adopted ways of managing content that more and more look like ICN. While BitTorrent [10] has always used a SHA-1 hash of the content data to identify that unique content item, it used to be that you also needed a location identifier (the address of the tracker through which the peers hosting the content can be located). However, the current incarnation of BitTorrent instead uses a shared global Distributed Hash Table (DHT) to locate peers using the aforementioned content hash as the key.
The historical Usenet discussion system [5] had all the key information-centric features: logical namespace and unique message identifiers, flood message propagation and caching. The git [3] revision control system represents version history of a project as a directed acyclic graph of revisions, where every revision is identified with SHA-1 hash of its contents; repositories push and pull content, thus forming a network of arbitrary topology.
This tendency towards information-centricity implies a strong demand for a generic named-data substrate that is not reflected yet in the de jure network architecture. Given this dissonance, some have proposed a networking revolution to dethrone the Internet Protocol (IP) in favor of a cleanslate redesign of the networking infrastructure.
While intellectually attractive, we do not consider such an approach realistic. Not only because it would require the expensive and disruptive wholesale replacement of the existing infrastructure, but more importantly because such a migration is unlikely to happen before the wholesale conversion of applications to information-centric analogues of the current application ecosystems, and that conversion is unlikely to happen until the required infrastructure is in place.
Having made this observation, it seems clear that evolution, not revolution, is the best way towards ICN, and by recognizing and helping this along, we can both make ICN happen sooner and ensure that the ICN approach will be one tested in both lab and real-world settings, and hence "the fittest".
We start with the necessary basic properties of any information centric architecture and determine which of them Swift already supports. Further, we determine which information centric primitives Swift does not provide and address them by leveraging existing technologies, such as DHTs to find peers and standard IP to route packets.
In this paper, we argue that our modular design addresses the gap between Internet usage and the underlying network, without requiring clean-slate redesigning of the architecture. In particular, Swift protocol supports most properties proposed in information-centric architectures like CCN [17] , DONA [20] , and NetInf [11] . First, Swift uses names -flat identifiers -to request content instead of end-point addresses; in addition, it segments named objects in uniquely identified chunks. Second, Swift employs perpacket integrity check, enabling any peer in the network to cache and relay content and verify the integrity of each piece. Third, Swift avoids transmitting additional metadata and is suitable for live/mutable data, by employing Merkle hashes [21] .
Moreover, Swift is a receiver-driven chunk-level transport protocol; the receiver may send concurrent requests for chunks to multiple peers in the network in order to enhance its content retrieval rate. To efficiently exploit available bandwidth, Swift employs a delay-based congestion control algorithm named LEDBAT [22] , and to address the issue of middleboxes Swift employs a NAT hole punching mechanism. For peer discovery, Swift can use centralized trackers or DHTs; in Section 7 we explain how Mainline DHT (MDHT) can be used to find peers offering the given data object in sub-second time periods.
The paper proceeds as follows. In Section 2 we introduce ICN related work and summarize system description. Section 3 discusses design properties and the resulting separation of transport and internetworking layers. Section 4 describes our variation of the Merkle hashing scheme and its extensions. In Section 5 we introduce a vocabulary of messages that constitutes our protocol. Section 6 describes our UDP-based implementation. Section 7 discusses the implications for peer discovery and packet routing. Section 8 concludes.
Most proposals on ICN architectures -evolutionary and clean-slate designs -aim to define the main building blocks of an information-centric network. In the CCN [17] design, content names have a hierarchical structure and are constructed according to the standard URI form. Content is requested using an interest packet which contains the name of the content. Every content router receiving the interest packet checks if the given packet is in its local cache and thus returns a corresponding data packet along the reverse path, otherwise, it forwards the interest to the correct interface using longest prefix matching. A similar approach for content retrieval is reflected in the PSIRP architecture [2] , albeit it uses flat instead of hierarchical names to address content.
Some recent ICN projects adopt flat, self-certifying, labels to name content. Initially employed in DONA's design [20] , flat names are used by the route-by-name protocol (devised on top of the IP layer) to request content. Similarly, efforts by Dannewitz et al [11] and Wong et al [23] explore secure naming schemes to ensure the data is persistent and not accessed by unauthorized users.
Self-certifying (flat) names have been criticized for their lack of scalability (cannot be aggregated), flexibility, and lack of security during the translation of flat names to humanreadable names. However, recent work by Ghodsi et al [13] argues that self-certifying names exhibit better security properties than human-readable names because 1) they can handle better denial-of-service attacks -the network knows the binding between the name and the key thus it can verify that a given object is associated with a given name, and 2) may scale better through explicit aggregation -using concatenations of the form A.B.C, where each letter is a name itself.
Despite differences in the naming scheme, the necessary mainstay of any name-oriented network architecture is to employ either cryptographic hashes or signatures in order to enable indiscriminate caching of data in the network and the possibility of its retrieval from any available peer. In Swift, we employ hashes -Merkle hashes -and argue that they are sufficient to perform any transport function. More specifically, Merkle hash trees [21] allow to identify and verify data, thus enabling any peer in the network to request, relay and store data. Furthermore, our variant of hash trees needs no supplementary transfer metadata, rendering transport into a thinner layer than usual.
Swift must be implemented by all inter-operating network peers, and its functioning involves cross-layer relaying of the data, known as internetworking (see Figure 1) . Hence, the required functionality needs to be as simple and formalized as possible. It follows naturally that any rich semantic data names or transfer metadata is unnecessary and should therefore not be part of transport.
We define a natural separation of Swift from the upper naming part which deals with problems inherently semantic and the lower internetworking layer. Leveraging existing deployed routing infrastructure and a simple hash-based naming mechanism, Swift retrieves pieces of content requested by the receiver from peers in the network -functionality that researchers propose to incorporate in any transport protocol for information-centric networks [9, 7] . The naming layer is out of scope, thus we make no specific assumptions.
In our design, content is identified by a single cryptographic hash that is the root hash in a Merkle hash tree, calculated recursively from the content (see details in the Merkle hash extension document [8] ). The ability to verify data against its name allows for storage in the network and retrieval of data from an arbitrary location. Second, as a (packet) network may need to check data integrity piece by piece, possible options boil down to either per-packet signatures, as in CCN, or Merkle hash trees. Differently from signatures, Merkle hash trees provide strict permanent identifiers of static data pieces, so we chose them as the foundation, later extending the approach to dynamic data (see Section 4).
The hashing scheme enables the entire informationcentric stack, illustrated in Figure 1 , in two ways. First, it allows for a perfect application-to-transport handover. Semantically-rich and application-dependent queries are eventually converted into requests to the transport layer for particular data pieces, precisely identified with hashes. Second, hashing enables information-centric internetworking, i.e. identification and relay of data pieces, data verification, and storage in the network.
As depicted in Figure 1 , Swift embeds a layer separation scheme very much reminiscent of TCP/IP. Namely, there is a relay internetworking layer that only deals with separate datagrams. On top of it, there is a somewhat more intelligent transport layer that deals with entire data streams, performing verification, caching, and storage.
Any peer running Swift may cache content. Technically, there is no difference between a peer and a cache -they run the same protocol; the conceptual difference lies in the intention: a cache "stores" content to further disseminate it but is not particularly interested in the given content. The caches may be regular peers or peers put in place by ISPs -who are interested in replicating "popular" content within their administrative domains and thus avoid transit traffic and costs to external domains. If operated by ISPs, such caches (interchangeably, peers) may manage the content they offer according to some basic rules, such as LRU or demand.
Discovering peers or caches may be done centrally through trackers or ISP-based trackers, or in a decentralized fashion through PEX or DHTs. We explain how discovering new
In Swift, data storage and data verification are highly interdependent and important in terms of security. For example, if a caching peer does not verify data integrity, it makes cache poisoning possible. While a final recipient does not accept (drops) incorrect data, an erroneous cache may form a clot in the network, preventing the correct data from passing through. Similarly, data verification requires storage in peers to some degree, as Merkle hash trees need accompanying uncle hash chains to be available in order to verify data pieces.
In a sense, hashes replace IP addresses as end-point identifiers. A receiver uses a root hash to "open" the connection to the network and retrieve the data. The receiver requests specific pieces of data using a novel method called bin numbers (see details in the RFC document [14] ) which allows the addressing of a binary interval of data using a single integer. This numbering mechanism reduces the amount of state that needs to be stored in each peer and minimizes the space required to denote intervals on the wire. Because the receiver directly addresses the data instead of a single end-point at a particular IP location, it has no control over which peer (replica) will respond; the receiver controls the reception of pieces based on local parameters.
We modified Merkle hash trees and focused on smooth operation of both vertical (application to transport) and horizontal (internetworking) handovers to ensure that no peer requires third parties to verify bindings between keys and names (as in CCN [17] ) or to retrieve additional metadata to perform their function. We ensure their operation is as simple and formalized, as possible. In Sec. 4.3, we extend our basic technique to the cases of live data streams and versioned data.
We developed a variant of the Merkle hash tree scheme [21] to satisfy three key requirements: (a) per-packet data integrity checks, (b) no additional metadata and (c) suitability for live/mutable data. The general concept is to start with the root hash only, then incrementally acquire data and hashes, while verifying every single step.
First, content is divided into 1KiB chunks named packets, except for the tail packet, which may have less than 1KiB of data. A cryptographic hash, such as SHA1, is then calculated on every packet. Second, a hash tree is defined over the complete [0, 2 63 ) byte range, which we consider to be a good approximation of infinity in relation to content size. The tree consists of aligned binary intervals called bins, i.e. [i2
Bins are nested, forming a strict binary tree (see Figure 2) . Each tree contains 2 64 bins of different sizes, including one void and one root bin; the base -the lowest level -of the tree is composed of 2 10 byte long bins. The base of the tree (the leaves) accommodates all the data chunks, starting from the left-most leaf. Normally, the base of the tree is wider than the number of chunks, thus the remaining empty leaves in the tree are assigned hash values of zero. In higher levels of the tree (above base), bins contain hashes which are calculated as a SHA1 hash of a concatenation of two -left and right -child (lower-level) hashes. This hashing process iterates until a hash value for the root bin is calculated, known as the root hash. Figure 2 illustrates an example where the file size is less than 8KiB long. Its [8192, 12288) empty bin has zero hash by definition, as do the rest of empty bins outside the [0, 8192) range. The root hash covers the entire [0, 2 63 ) range; this approach gives us a fixed point of reference when growing the hash tree down from the root.
The concept of peak hashes enables two cornerstone features: file size proving and unified processing of static data and live streams. In addition, they help avoid the usage of additional transmission metadata. Formally, peak hashes are hashes defined over filled bins, whose parent hashes are defined over incomplete (not filled) bins. A filled bin is a bin which does not extend past the end of the file, or, more precisely, contains no empty packets.
Practically, we use peaks to cover the data range with a logarithmic number of hashes, so each hash is defined over a "round" aligned 2 k interval. As an example, suppose a file is l = 7162 bytes long (see Figure 2) . That fits into seven packets ( 7162 1024 < 7), the tail packet being 1018 bytes long. For this particular file we will have three peaks, covering [0, 4096), [4096, 6144) and [6144, 7162) ranges (triangles depicted with double lines). The last range might also be written as [6144, 8192) because we round-up to 1KiB packet size.
The number of peak hashes can not exceed log 2 l 1024 . Practically, peak hashes provide us with more convenient "reference roots", as compared to the root hash which is 53 levels higher than the packets. More importantly, peak hashes allow a sender to quickly prove the file size to a recipient who only knows the root hash; otherwise, file size would have to be supplied as a separate metadata piece and thus separately verified, showing up in the protocol and in the interfaces.
In the case of live data streams, the root hash is undefined or, more precisely, transient, as long as new data keeps coming, filling new packets to the right. Hence a transfer has to be identified with a public key instead of a root hash. Keys are more difficult to deal with than hashes, as they have more degrees of freedom. For example, once a key is compromised, any party may rewrite a pre-existing stream.
Also, while a hash might be derived directly from the data, a signature can only be verified once known.
Because of such issues, we try to minimize key/signature usage by using the same peak hashes scheme as in the case of static data. Indeed, once a peak hash is defined, it never changes. Thus, we only need a logarithmic number of signatures to sign peak hashes. After that, we may deal with the same Merkle hash tree as before.
Signing the peak hashes only requires the sender to issue the newly formed peak hashes with their signatures attached. On the receiver side, the recipient will only have to check the signature of a new peak hash and whether it matches its child hashes. Such a calculation is incremental and local. Otherwise, if the root hash were to be signed instead, this would require constant re-verification of all the encompassing peak hashes.
Until this point, we assumed that the sender emits data in "round" 1KiB long packets. What if smaller portions of data need to be committed to the network? We do not equal, but we strongly associate our "packets" with linklayer "frames". Thus, once data is worth sending, before it fills a packet, then it also needs a hash and a signature.
In this section we describe the set of messages that constitute Swift protocol. In this section, we refer to it as a vocabulary which is instantiated as a transport protocol (as in Figure 1 ). No particular serialization, encapsulation schemes, or message exchange patterns are specified, beyond the very basic requirements.
A DAT A message simply carries pieces of data. A DAT A message must carry a bin of data. Specifically, each DAT A message contains the bin number of the piece and the piece itself. This way, uniform pieces or multiples of pieces can be processed, making it easier to check the data hash tree at once. A HASH message carries the necessary hashes that the receiver needs in order to verify the integrity of a piece. We employ the principle of atomic datagrams, which means that every piece of data must be verified once received and accepted, otherwise dropped. At this point, the sender must make sure the receiver has every hash needed to verify the incoming data immediately. Finally, it is possible to supply the recipient with parts of the hash tree incrementally, to allow for an amortized and local verification of data.
As we allow for the possibility of data retrieval from multiple peers in parallel, the vocabulary employs HIN T and HAV E messages. A HIN T (request) message indicates which pieces of data a receiver wants to retrieve, while a HAV E message conveys what pieces of data a sender has available. On incoming data, a receiver uses ACK messages to acknowledge the received pieces; acknowledgements follow the logic of hash trees, which means that data must be acknowledged in bins as well.
We define channels as a means to identify ongoing transfers, where each transfer is identified by either a hash or a public key. Channel identifiers are conveyed through the datagram headers.
As previously stated, Swift [1] protocol is implemented over UDP; the detailed design is described in an IETF draft [14] and an overview is outlined in a technical report [15] . The protocol is a direct implementation of the vocabulary (see Section 5), with some additions and extensions.
Messages are serialized as fixed-width fields starting with a single-byte message type field, followed by fixed-width payload fields, such as bin numbers, data, hashes and such. Messages are packed into UDP datagrams. Datagram processing is event-driven, fully implementing the atomic datagram concept. This means that every datagram is either immediately committed to storage or immediately dropped; there are no buffer-re-assembly mechanics.
The UDP implementation employs LEDBAT [22] congestion control algorithm, which allows streams to run virtually lossless under normal conditions. LEDBAT is a delaybased congestion control algorithm which increases/decreases the congestion window based on the estimated queuing delay. It uses an increased queuing delay as indicator of congestion and thus immediately reacts by backing off (decreasing the rate).
Queuing delay in LEDBAT is known as the one-way delay (label owd in Figure 3 ) and it is calculated as the difference of timestamped packets between the sender and the receiver. The receiver also maintains a minimum over all one-way delays -base delay -which indicates the amount of delay due to queuing. LEDBAT compares this estimated queuing delay against a fixed target delay value (line target in Figure 3) ; the difference determines if the congestion window should be increased or decreased. Figure 3 depicts how LEDBAT predicts congestion and avoids data losses for a given exchange between two peers (the figure only illustrates a preliminary test performed in a controlled environment with several peers spread across continents). In the testing scenario, one peer acts as a content provider -has the whole content -and other peers (requesters) are interested in the given content. The requesters retrieve the pieces, initially from the only content provider, and later on, they continue retrieving pieces from the participating requesters -who already obtained some pieces of the desired content.
Furthermore, Swift implements a unified mechanism of PEX and NAT hole punching functionality [12] . It uses two types of P EX messages -P EX REQ and P EX ADDto retrieve/exchange addresses among the peers, in a gossip fashion. However, P EX messages are transmitted in such a way that they facilitate the communication between peers that are located behind middleboxes: once a peer A introduces peer B to C, it should -within a period of 2 seconds -introduce peer C to B. This mechanism makes Swift agnostic to middleboxes.
To guarantee that a receiver can verify every packet, the sender has to prepend it with the missing hashes. In a network with no data loss, the receiver builds the hash tree incrementally, thus every packet of data needs one hash on average. More precisely, every even packet needs a hash for its sibling, every fourth also needs a hash for its uncle, every eighth also needs a hash for its parent's uncle, and so forth, thus the average is 1. In practice, some packets are lost, so a prudent sender over-provisions hashes to compensate for possible loss. Thus, the actual traffic overhead of hashes is somewhat above the perfect value of 2% (assuming 20 byte hashes for a 1024 byte packet).
The protocol needs to keep more state on the transfer progress, as data might arrive out of sequence -mostly because data is delivered from different peers in parallel. The state must also be communicated over the wire, using unreliable datagrams. We adopted a generic compressed-bitmap data structure named binmaps [16] , a hybrid of bitmap and a binary tree, which allows to track data at an arbitrary scale, starting from a single packet. Data is requested and acknowledged in bins; this provides the necessary compression and redundancy as continuous data pieces are acknowledged with a logarithmic number of messages.
In order to retrieve data associated to a root hash, Swift needs to discover peers. This peer discovery process is performed by requesting peers from a tracker.
Trackers are used in peer-to-peer systems to keep track of peers sharing a given piece of content. The tracker's interface is simple. Peers can request a list of peers for a given content identifier (a root hash in Swift, an info hash in BitTorrent). Peers also register itself in the tracker to be discovered by others.
Swift can use any tracking mechanism regardless of its particular implementation. Tracker mechanisms used in BitTorrent are prime candidates to be used due to their proven merits on large-scale deployments, but other implementations offering equivalent functionality may be used [24] .
BitTorrent's trackers can be centralized or DHT-based. In the first case, the URI of the tracker tracking a given piece of content is necessary. The DHT-based option, on the other hand, forms a global tracking system where all content is tracked, thus no tracker URI is needed.
We favor the DHT-based tracker mechanism due to the scalability of the DHT and the minimization of metadata for Swift (no tracker URI is needed, just a root hash) to retrieve the data. Scalability is well illustrated by Mainline DHT, the BitTorrent's largest DHT-based tracker on the Internet. Mainline DHT is supported by most of the popular BitTorrent clients, forming a DHT overlay of between 6 and 11 million nodes [19] 1 . It is difficult to estimate how many pieces of content Mainline DHT tracks at a given time, but given the size of the BitTorrent ecosystem, even conservative estimations would yield six-digit numbers.
Furthermore, recent measurements [18] have shown that Mainline DHT's response time is consistently low, which makes it suitable for latency-sensitive applications such as on-demand video streaming.
In this paper, we presented a peer-to-peer based transport protocol for content dissemination named Swift and argued that the protocol exhibits ICN properties that help close the gap between the way Internet applications are used today and the underlying infrastructure supporting such applications. Further, we explored ways Swift may embed additional ICN properties in its behavior by leveraging existing technologies and infrastructure, such as decentralized peer discovery mechanisms and standard IP routing.
We would like to thank Arno Bakker and Pehr Söderman for providing us with valuable feedback. The research leading to these results has received funding from the Seventh Framework Programme (FP7/2007-2013) under grant agreement No. 216217 (P2P-Next).
Analyzing an input-output relationship from samples is one of the central challenges in machine learning. The most common approach is regression, which estimates the conditional mean of output y given input x. However, just analyzing the conditional mean is not informative enough, when the conditional density p(y|x) possesses multimodality, asymmetry, and heteroskedasticity (i.e., input-dependent variance) as a function of output y. In such cases, it would be more appropriate to estimate the conditional density itself (see Figure 2 ). The most naive approach to conditional density estimation (CDE) would be -neighbor kernel density estimation ( -KDE) , which performs standard KDE along y only with nearby samples in the input domain. However, -KDE does not work well in high-dimensional problems because the number of nearby samples is too few. To avoid the small sample problem, KDE may be applied twice to estimate p(x, y) and p(x) separately and the estimated densities may be plugged into the decomposed form p(y|x) = p(x, y)/p(x) to estimate the conditional density. However, taking the ratio of two estimated densities significantly magnifies the estimation error and thus is not reliable. To overcome this problem, an approach to directly estimating the density ratio p(x, y)/p(x) without separate estimation of densities p(x, y) and p(x) has been explored (Sugiyama et al., 2010) . This method, called least-squares CDE (LSCDE), was proved to possess the optimal nonparametric learning rate in the mini-max sense, and its solution can be efficiently and analytically computed. Nevertheless, estimating conditional densities in high-dimensional problems is still challenging.
A natural idea to cope with the high dimensionality is to perform dimensionality reduction (DR) before CDE. Sufficient DR (Li, 1991; Cook & Ni, 2005 ) is a framework of supervised DR aimed at finding the subspace of input x that contains all information on output y, and a method based on conditional-covariance operators in reproducing kernel Hilbert spaces has been proposed (Fukumizu, Bach, & Jordan, 2009) . Although this method possesses superior theoretical properties, it is not easy to use in practice because no systematic model selection method is available for kernel parameters. To overcome this problem, an alternative sufficient DR method based on squared-loss mutual information (SMI) has been proposed recently (Suzuki & Sugiyama, 2013) . This method involves nonparametric estimation of SMI that is theoretically guaranteed to achieve the optimal estimation rate, and all tuning parameters can be systematically chosen in practice by cross-validation with respect to the SMI approximation error.
Given such state-of-the-art DR methods, performing DR before LSCDE would be a promising approach to improving the accuracy of CDE in highdimensional problems. However, such a two-step approach is not preferable because DR in the first step is performed without regard to CDE in the second step, and thus small errors incurred in the DR step can be significantly magnified in the CDE step.
In this letter, we propose a single-shot method that integrates DR and CDE. Our key idea is to formulate the sufficient DR problem in terms of the squared-loss conditional entropy (SCE), which includes the conditional density in its definition, and LSCDE is executed when DR is performed. Therefore, when DR is completed, the final conditional density estimator has already been obtained without an additional CDE step (see Figure 1 ). We demonstrate the usefulness of the proposed method, named least-squares conditional entropy (LSCE), through experiments on benchmark data sets, humanoid robot control simulations, and computer art. 
In this section, we describe our proposed method for conditional density estimation with dimensionality reduction.
be the input and output domains with dimensionality d x and d y , respectively, and let p(x, y) be a joint probability density on D x × D y . Assume that we are given n independent and identically distributed (i.i.d.) training samples from the joint density:
The goal is to estimate the conditional density p(y|x) from the samples. Our implicit assumption is that the input dimensionality d x is large, but its intrinsic dimensionality, denoted by d z , is rather small. More specifically, let W and
is an orthogonal matrix. Then we assume that x can be decomposed into the component z = W x and its perpendicular component z ⊥ = W ⊥ x so that y and x are conditionally independent given z: y ⊥ x|z.
(2.1)
This means that z is the relevant part of x, and the rest z ⊥ does not contain any information on y. The problem of finding W is called sufficient dimensionality reduction (Li, 1991; Cook & Ni, 2005) .
Let us consider a squared-loss variant of conditional entropy, squared-loss CE (SCE):
By expanding the squared term in equation 2.2, we obtain
Then we have the following theorem (its proof is given in appendix A), which forms the basis of our proposed method:
This theorem shows SCE(Y |Z) ≥ SCE(Y |X ), and the equality holds if and only if
This is equivalent to the conditional independence, equation 2.1, and therefore sufficient dimensionality reduction can be performed by minimizing SCE(Y |Z) with respect to W :
(2.5)
(R) denotes the Grassmann manifold, which is a set of orthogonal matrices without overlaps,
where I denotes the identity matrix and ∼ represents the equivalence relation: W and W are written as W ∼ W if their rows span the same subspace.
Since p(y|z) = p(z, y)/p(z), SCE(Y |Z) is equivalent to the negative Pearson divergence (Pearson, 1900) from p(z, y) to p(z), which is a member of the f-divergence class (Ali & Silvey, 1966; Csiszár, 1967) with the squaredloss function. Ordinary conditional entropy (CE), defined by
is the negative Kullback-Leibler divergence (Kullback & Leibler, 1951) from p(z, y) to p(z). Since the Kullback-Leibler divergence is also a member of the f-divergence class (with the log-loss function), CE and SCE have similar properties. Indeed, theorem 1 also holds for ordinary CE. However, the Pearson divergence is shown to be more robust against outliers (Basu, Harris, Hjort, & Jones, 1998; Sugiyama, Suzuki, & Kanamori, 2012) , since the log function, is very sharp near zero, is not included. Furthermore, as we show, SCE can be approximated analytically, and thus its derivative can also be easily computed. This is a critical property for developing a dimensionality-reduction method because we want to minimize SCE with respect to W , where the gradient is highly useful in devising an optimization algorithm. For this reason, we adopt SCE instead of CE below.
Since SCE(Y |Z) in equation 2.5 is unknown in practice, we approximate it using samples
(2.6)
If we set a = p(y|z), we have
If we multiply both sides of the above inequality with −p(z) and integrate over z and y, we have
where minimization with respect to b is now performed as a function of z and y. (For more general discussions on divergence bounding, see Keziou, 2003, and Nguyen, Wainwright, & Jordan, 2010) . Let us consider a linear-in-parameter model for b:
where α is a parameter vector and ϕ(z, y) is a vector of basis functions. If the expectations over densities p(z) and p(z, y) are approximated by sample averages and the 2 -regularizer λα α/2 (λ ≥ 0) is included, the above minimization problem yields
The solution α is analytically given by
which yields b(z, y) = α ϕ(z, y). Then, from equation 2.7, we obtain an approximator of SCE(Y |Z) analytically as
We call this method least-squares conditional entropy (LSCE).
The SCE approximator depends on the choice of models-i.e., the basis function ϕ(z, y) and the regularization parameter λ. Such a model can be objectively selected by cross-validation as follows:
ii. Evaluate the upper bound of SCE obtained by b (M, j) using the hold-out data S j :
where |S j | denotes the cardinality of S j . b. The average score is computed as
3. The model that minimizes the average score is chosen:
4. For the chosen model M, the LSCE solution b is computed from all samples S, and the approximator SCE(Y |Z) is computed.
In the experiments, we use K = 5.
Reduction with SCE. Now we solve the following optimization problem by gradient descent:
(2.9)
As shown in appendix B, the gradient of SCE(Y |Z = W X ) is given by
In the Euclidean space, the above gradient gives the steepest direction. However, on a manifold, the natural gradient (Amari, 1998) gives the steepest direction.
The natural gradient ∇ SCE(W ) at W is the projection of the ordinary
is equipped with the canonical metric W , W = 1 2 tr(W W ), the natural gradient is given as follows (Edelman, Arias, & Smith, 1998) :
Then the geodesic from W to the direction of the natural gradient ∇ SCE
where "exp" for a matrix denotes the matrix exponential and O d,d denotes the d × d zero matrix. Note that the derivative ∂ t W t at t = 0 coincides with the natural gradient ∇ SCE (see Edelman et al., 1998, for details) . Thus, line search along the geodesic in the natural gradient direction is equivalent to finding the minimizer from {W t |t ≥ 0}. Once W is updated, SCE is reestimated with the new W , and gradient descent is performed again. This entire procedure is repeated until W converges. When SCE is reestimated, performing cross-validation in every step is computationally expensive. In our implementation, we perform cross-validation only once every five gradient updates. Furthermore, to find a better local optimal solution, this gradient descent procedure is executed 20 times with randomly chosen initial solutions; the one achieving the smallest value of SCE is chosen.
Since the maximum of equation 2.6 is attained at b = a and a = p(y|z) in the current derivation, the optimal b(z, y) is actually the conditional density p(y|z) itself. Therefore, α ϕ(z, y) obtained by LSCE is a conditional density estimator. This implies that the upper-bound minimization procedure described in section 2.3 is equivalent to least-squares conditional density estimation (LSCDE) (Sugiyama et al., 2010) , which minimizes the squared error:
Then, in the same way as the original LSCDE, we may postprocess the solution α to make the conditional density estimator nonnegative and normalized as
where α l = max α l , 0 . Note that even if the solution is postprocessed as equation 2.10, the optimal estimation rate of the LSCDE solution is still maintained (Sugiyama et al., 2010) .
In practice, we use the following gaussian function as the kth basis:
where (u k , v k ) denotes the kth gaussian center located at (z k , y k ). When the sample size n is too large, we may use only a subset of samples as gaussian centers. σ denotes the gaussian bandwidth, which is chosen by cross-validation, as explained in section 2.4. We may use different bandwidths for z and y, but this will increase the computation time for model selection. In our implementation, we normalize each element of z and y to have the unit variance in advance and then use the common bandwidth for z and y.
A notable advantage of using the gaussian function is that the integral over y appeared in¯ (z) (see equation 2.8) can be computed analytically as
Similarly, the normalization term in equation 2.10 can also be computed analytically as
2.8 Discussion. We have proposed minimizing SCE for dimensionality reduction:
In previous work Suzuki and Sugiyama (2013) , squared-loss mutual information (SMI) was maximized for dimensionality reduction:
This shows that the essential difference is whether p(y) is included in the denominator of the density ratio. Thus, if p(y) is uniform, the proposed dimensionality-reduction method using SCE is reduced to the existing method using SMI. However, if p(y) is not uniform, the density ratio function p(z,y) p(z)p(y) included in SMI may be more fluctuated than p(z,y) p(z) included in SCE. Since a smoother function can be more accurately estimated from a small number of samples in general, the proposed method using SCE is expected to work better than the existing method using SMI. We will experimentally demonstrate this effect in section 3.
Sufficient dimension reduction based on the conditional density p(y|z) has also been studied in the statistics literature. The density-minimum average variance estimation (dMAVE) method (Xia, 2007) finds a dimensionreduction subspace using local linear regression for the conditional density in a semi-parametric manner. A similar approach has also been taken in the sliced regression for dimension reduction method (Wang & Xia, 2008) , where the cumulative conditional density is used instead of the conditional density. A Bayesian approach to sufficient dimension reduction called the Bayesian dimension reduction (BDR) method (Reich, Bondell, & Li, 2011) has been proposed recently. This method models the conditional density as a gaussian mixture model and obtains a dimension-reduction subspace through sampling from the learned prior distribution of low-dimensional input. These methods have been shown to work well for dimension reduction in real-world data sets, although they are applicable only to univariate output data where d y = 1.
In regression, learning with the squared loss is not robust against outliers (Huber, 1981) . However, density estimation (Basu et al., 1998) and density ratio estimation under the Pearson divergence are known to be robust against outliers. Thus, in the same sense, the proposed LSCE estimator would also be robust against outliers. We experimentally investigate the robustness in section 3.
In this section, we experimentally investigate the practical usefulness of the proposed method. We consider the following dimensionality-reduction schemes:
None: No dimensionality reduction is performed. dMAVE: The density-minimum average variance estimation method where dimension reduction is performed through local linear regression for the conditional density (Xia, 2007) . 1 BDR: The Bayesian dimension-reduction method where the conditional density is modeled by a gaussian mixture model and dimension reduction is performed by sampling from the prior distribution of low-dimensional input (Reich et al., 2011) . 2 LSMI: Dimension reduction is performed by maximizing an SMI approximator called least-squares MI (LSMI) using natural gradients over the Grassmann manifold (Suzuki & Sugiyama, 2013) . LSCE (proposed): Dimension reduction is performed by minimizing the proposed LSCE using natural gradients over the Grassmann manifold. True (reference): The "true" subspace is used (only for artificial data).
After dimension reduction, we execute the following conditional density estimators:
-KDE: -neighbor kernel density estimation, where is chosen by leastsquares cross-validation. LSCDE: Least-squares conditional density estimation (Sugiyama et al., 2010) .
Note that the proposed method, which is the combination of LSCE and LSCDE, does not explicitly require the post-LSCDE step because LSCDE is executed inside LSCE. Since the dMAVE and BDR methods are applicable only to univariate output, they are not included in experiments with multivariate output data.
3.1 Illustration. First, we illustrate the behavior of the plain LSCDE (None/LSCDE) and the proposed method (LSCE/LSCDE). The data sets illustrated in Figure 2 have d x = 5, d y = 1, and d z = 1. The first dimension of input x and output y of the samples is plotted in the graphs, and the other four dimensions of x are just standard normal noise. The results show that the plain LSCDE does not perform well due to the irrelevant noise dimensions of x, while the proposed method gives much better estimates.
Next, we compare the proposed method with the existing dimensionality-reduction methods on conditional density estimation by LSCDE in artificial data sets.
For d x = 5, d y = 1, x ∼ N (x|0, I 5 ), and ∼ N ( |0, 0.25 2 ), where N (·|μ, ) denotes the normal distribution with mean μ and covariance matrix , we consider the following artificial data sets: a. d z = 2 and y = (x (1) ) 2 + (x (2) ) 2 + . b. d z = 1 and y = x (2) + (x (2) ) 2 + (x (2) ) 3 + . c. d z = 1 and y = (x (1) ) 2 + with 0.85 probability, 2 − 4 with 0.15 probability. The first row of Figure 3 shows the dimensionality-reduction error between true W * and its estimate W for different sample size n, measured by
where · Frobenius denotes the Frobenius norm. All methods perform similarly for data set a, and the dMAVE and BDR methods outperform LSCE and LSMI when n = 50. In data set b, LSMI does not work well compared to other methods especially when n ≥ 250. To explain this behavior, we plot the histograms of {y} 400 i=1 in the left column of Figure 4 . They show that the profile of the histogram (a sample approximation of p(y)) in data set b is much sharper than that in data set a. As discussed in section 2.8, the density ratio (y) . For data set c we consider the situation where {y i } n i=1 contain outliers that are not related to x. The data profile of data set c in the right column of Figure 4 illustrates such a situation. The result on data set c shows that the proposed LSCE method is robust against outliers and gives the best subspace estimation accuracy, while the BDR method performs unreliably with large standard errors.
The right column of Figure 3 plots the conditional density estimation error between true p(y|x) and its estimate p(y|x), evaluated by the squared loss:
is a set of test samples that have not been used for training. We set n = 1000. For data sets a and c, all methods with dimension reduction perform equally well, which is much better than no dimension reduction (None/LSCDE) and is comparable to the method with the true subspace (True/LSCDE). For data set b, all methods except LSMI/LSCDE perform well overall and are comparable to the methods with the true subspace.
Next, we use the UCI benchmark data sets (Bache & Lichman, 2013) . We randomly select n samples from each data set for training, and the rest are used to measure the conditional density estimation error in the test phase. Since the dimensionality of the subspace d z is unknown, we chose it by cross-validation. More specifically, five-fold cross-validation is performed for each combination of the dimensionalityreduction and conditional-density estimation methods to choose subspace dimensionalities d z such that the conditional-density estimation error is minimized. Note that tuning parameters λ and σ are also chosen based on cross-validation for each method. Since the conditional-density estimation error is equivalent to SCE, choosing the subspace dimensionalities by the conditional-density estimation error in LSCE is equivalent to choosing subspace dimensionalities that give the minimum SCE value.
The results of univariate output benchmark data sets averaged over 10 runs are summarized in the subspace dimensionalities chosen by cross-validation averaged over 10 runs. It shows that all dimensionality-reduction methods reduce the input dimension significantly, especially for Yacht, Red Wine, and White Wine, where the best method always chooses d z = 1 in all runs.
The results of multivariate output Stock and Energy benchmark data sets are summarized in Table 3 , showing that the proposed LSCE/LSCDE method also works well for multivariate output data sets and significantly outperforms methods without dimensionality reduction. Table 4 describes the subspace dimensionalities selected by cross-validation, showing that LSMI/LSCDE tends to more aggressively reduce the dimensionality than LSCE/LSCDE.
We evaluate the performance of the proposed method on humanoid robot transition estimation. We use a simulator of the upper-body part of the humanoid robot CB-i (Cheng et al., 2007;  see Figure 5 ). The robot has nine controllable joints: shoulder pitch, shoulder roll, elbow pitch of the right arm, shoulder pitch, shoulder roll, elbow pitch of the left arm, waist yaw, torso roll, and torso pitch joints.
The posture of the robot is described by 18-dimensional real-valued state vector s, which corresponds to the angle and angular velocity of each joint in radians and radians per seconds, respectively. We can control the robot by sending the action command a to the system. The action command a is a nine-dimensional real-valued vector that corresponds to the target angle of each joint. When the robot is at state s and receives action a, the physical control system of the simulator calculates the amount of torque to be applied to each joint. These torques are calculated by the proportional-derivative (PD) controller as
where s i ,ṡ i , and a i denote the current angle, the current angular velocity, and the received target angle of the ith joint, respectively. K p i and K d i denote the position and velocity gains for the ith joint, respectively. We set K p i = 2000 and K d i = 100 for all joints except K p i = 200 and K d i = 10 for the elbow pitch joints. After the torques are applied to the joints, the physical control system updates the state of the robot to s .
In the experiment, we randomly choose the action vector a and simulate a noisy control system by adding a bimodal gaussian noise vector. More specifically, the action a i of the ith joint is first drawn from uniform distribution on [s i − 0.087, s i + 0.087]. The drawn action is then contaminated by gaussian noise with mean 0 and standard deviation 0.034 with probability 0.6 and gaussian noise with mean −0.087 and standard deviation 0.034 with probability 0.4. By repeatedly controlling the robot n times, we obtain the transition samples {(s j , a j , s j )} n j=1 . Our goal is to learn the (Sutton & Barto, 1998) . We consider three scenarios: using only two joints (right shoulder pitch and right elbow pitch), only four joints (in addition, right shoulder roll and waist yaw), and all nine joints. Thus, d x = 6 and d y = 4 for the two-joint case, d x = 12 and d y = 8 for the four-joint case, and d x = 27 and d y = 18 for the nine-joint case. We generate 500, 1000, and 1500 transition samples for the two-joint, four-joint, and nine-joint cases. We then randomly choose n = 100, 200, and 500 samples for training, and use the rest for evaluating the test error. The results are summarized also in Table 3 , showing that the proposed method performs well for all three cases. Table 4 describes the Figure 6 : Three actions of the brush, which is modeled as the footprint on a paper canvas. dimensionalities selected by cross-validation, showing that the humanoid robot's transition is highly redundant.
Finally, we consider the transition estimation problem in sumi-e style brush drawings for nonphotorealistic rendering (Xie, Hachiya, & Sugiyama, 2012) . Our aim is to learn the brush dynamics as state transition probability p(s |s, a) from the real artists' stroke-drawing samples.
From a video of real brushstrokes, we extract footprints and identify corresponding three-dimensional actions (see Figure 6 ). The state vector consists of six measurements: the angle of the velocity vector and the heading direction of the footprint relative to the medial axis of the drawing shape, the ratio of the offset distance from the center of the footprint to the nearest point on the medial axis over the radius of the footprint, the relative curvatures of the nearest current point and the next point on the medial axis, and the binary signal of the reverse driving or not. Thus, the state transition probability p(s |s, a) has nine-dimensional input and six-dimensional output. We collect 722 transition samples. We randomly choose n = 200, 250, and 300 for training and use the rest for testing.
The estimation results are summarized at the bottom of Tables 3 and 4 . These tables show that there exists a low-dimensional sufficient subspace and the proposed method can find it.
We proposed a new method for conditional-density estimation in highdimension problems. The key idea of the proposed method is to perform sufficient dimensionality reduction by minimizing the square-loss conditional entropy (SCE), which can be estimated by least-squares conditional-density estimation. Thus, dimensionality-reduction and conditional-density estimation are carried out simultaneously in an integrated manner.
We have shown that SCE and the squared-loss mutual information (SMI) are similar but different in that the output density is included in the denominator of the density ratio in SMI. This means that estimation of SMI is hard when the output density is fluctuated, while the proposed method using SCE does not suffer from this problem. The proposed method is also robust against outliers since minimization of the Pearson divergence automatically weighs down the effects of outlier points. Moreover, the proposed method is applicable to multivariate output data, which is not straightforward to handle in other dimensionality-reduction methods based on conditional probability density. The effectiveness of the proposed method was demonstrated through extensive experiments, including humanoid robot transition and computer art. Using ∂X −1 ∂ h k ∂W l,l = − 1 σ 2 n n i=1 ϕ k (z i , y i ) ((z (l) i − u (l) k )(x (l ) i −ũ (l ) k )).
Aviation meteorology is the most important aspect of supporting the safety and security of air traffic from extreme weather [1] . Weather conditions can cause or contribute to the aviation accidents included wind, visibility or ceiling, high-density altitude, turbulence, carburetor icing, updrafts or downdrafts, precipitation, icing, thunderstorms, wind shear, thermal lift, temperature (T) extremes, and lightning [2] . A weather forecaster is an actor who guarantees the efficiency and effectiveness of airport operational without affected by weather, so they have to observe all the weather parameters such as air temperature, winds, weather condition, and visibility. Furthermore, these parameters are analyzed in an isobar chart, streamline chart, and upper air chart to get an accurate weather forecast.
Based on Figure 1 , accidents by flight phase as a percentage of all accidents from 1998 to 2017 have dominantly occurred with approach (up to 20%) and landing (up to 50%). Next, it was followed by parking and taxi as a non-fatal hull loss, but it was also essential. Building and cargo in aerodrome may be received damage from the weather such as floods or strong surface winds. To minimize the negative risks, World Meteorological Organization (WMO) has arranged the rules with the use of Aerodrome Warning in Technical Regulations [4] , Volume II, Part I, 7.3. In Indonesia, The Agency for MeteorologyClimatology and Geophysics (BMKG) also compiled the detailed of Aerodrome Warning in PERKA BMKG No. 13 Tahun 2015 [5] . Aerodrome Warning is concise information about meteorological conditions that can affect aircraft and airport service facilities on land such as runway. Aerodrome Warning (AW) consists of weather conditions, wind direction and wind speed, and visibility with observing time and validity time of forecast. Weather conditions as though rainfall, tropical cyclones, thunderstorm, squall, hail, fog, volcanic ash, tsunami, smoke, and toxic chemistry gases should be reported if it occurred or will occur in AW format. Delay avoiding, cargo activities will be fluent and aircraft parking will exists in safety cone and remains sterile if AW is disseminated well [6] .
WIII AD WRNG 02 VALID 170420/170530 HVY TSRA WIND 28015KT MAX 25KT OBSAT 170400 NC= (1)
The code form above explains about Aerodrome Warning in Soekarno-Hatta Meteorological Station (WIII) number 2 with time validity on date 17 from 04:20 UTC until 05:30 UTC will occur heavy rain with thunderstorm with the average wind direction is from 280⁰ (South-West) and wind speed is 15 knot, and maximum wind speed up to 25 knot. The observation of AW was 04:00 UTC on date 17, with no change of phenomena intensity. Improvement in aerodrome warnings nowcasts need better predictions, thus a verification becomes its measurement. Weather forecast verification provides benefits, such as knowing the mistake which causes false prediction [7] . In cases, all stakeholders in the airport can prepare and have plans to mitigate undesirable activity disturbance.
Aerodrome warning archives from January to April 2019 are needed as the basic materials. A computer, an especially calculator, are used to calculate all formula in statistic verification. Automatic Weather Observation System (AWOS) data from January to April 2019 are collected in one folder including rainfall events, thunderstorm events, the peak of wind speed, and minimum visibility. Aerodrome warning and AWOS data are from Soekarno-Hatta Meteorological Station (07L) and Tanjungpinang Meteorological Station. 
Aerodrome warning as a nowcasting forecast and AWOS data as observation references are verified into 5 statistic parameters, such as Probability of Detection (POD), Bias, False Alarm Ratio (FAR), Threat Score (TS), and Heidke Skill Score (HSS). Value of Hits, False Alarms, Misses, and Correct Negatives be required for calculating those 5 parameters. Hits means the prediction and the observation occurred simultaneously, besides Correct Negatives means both of them do not occur. False Alarms shows that the forecast said "Yes", but in observation, it does not occur. Misses means that there is no prediction, but there happened extreme weather that passes the threshold.

(2)
Hits +False Alarms 
Hits +False Alarms +Misses (5) Probability events that can be detected by the Probability of detection (POD), is part of the incident what was observed occurred ("yes") and predicted. The value between 0 and 1, the best value when POD equals 1. The number of events predicted will occur ("yes"), but it does not occur as indicated by False Alarm Ratio (FAR). The value is between 0 and 1which FAR equals 0 is the best value.
Bias is a comparison of the average forecast towards the average observation and shows the frequency of a forecast event compared to observed events. Bias value can show how is the relationship between predictions occurring "yes" with "yes" observations, can be obtained by equation (1) . The value is between 0 and ∞, with bias equals 1, is the best value. Threat score (TS) can show a comparison predictions of the occurrence of "yes" with observations of events "yes", TS values range from 0 and 1, 0 indicates predictions without skill and 1 for the best predictions. HSS = Hits +Correct Negatives −(Expected Correct ) random N − (Expected Correct ) random
Verification will also be used Hiedke Skill Score (HSS) which can provide relative accuracy of forecasts against the chance of random. The HSS value interval is -∞ to 1. Value 1 shows the perfect forecasts.
Mean Absolute Error (MAE) measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. MAE is a linear score which means that all the individual differences are weighted equally in the average. Root Mean Squared Error (RMSE) measures the average magnitude of the error. The difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means RMSE is the most useful when large errors are particularly undesirable. MAE and RMSE can be used together to diagnose the variation in the errors in a set of forecasts. RMSE will always be larger or equal to MAE which there isthe difference between them especially for thevariance in the individual errors in the sample. If RMSE is similar to MAE, then all the errors are of the same magnitude. Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores which are lower values are better.
Rainfall variability in the Tanjungpinang was influenced by many factors. Weather patterns in Tanjungpinang were affected by its geographical location which was surrounded by the ocean so that the convection that occurs was more influenced by local factors [8] .
Climatologically, it will go through wet conditions which were the precipitation that occurs quite frequently from November to January and went through dry conditions with little precipitation in February. The Agency for Meteorology Climatology and Geophysics (BMKG) was intensively socialize the making of aerodrome warnings as a basis for early warning for extreme weather events such as heavy rain, low visibility, and strong winds which may occur around the runway or airport. BMKGled Tanjungpinang Meteorological Station to make an aerodrome warning for extreme weather conditions at Raja Haji Fisabilillah International Airport. Precipitation forecasting was one of the difficult parts to predict and was still being studied [9] . It could be done subjectively based on the forecaster point of view and objective by using a statistical or numerical methods. Figure 2 interpreted the aerodrome frequency warning chart produced bythe forecaster of Tanjungpinang Meteorological Station for extreme weather that occurred at the runway of Raja Haji Fisabilillah International Airport. It showed that aerodrome warning issued not routinely given when there was the potential for extreme weatherespecially when rain has occurred, which could affect the safety of airplane operation. Aerodrome warning for January and February was not made by forecasters of Tanjungpinang Meteorological Station during rain event that observed by AWOS or manual observation. Precipitation intensity that occurred in JanuaryandFebruary was mostly in the light intensity where some rainfall events were measured in rain gauge or AWOS as a trace of rain or no measurable accumulation. Rainfall frequency observed by AWOS in March and April also did not record actual rainfall events or measure it as a trace of rain (about 8.33%) a couple of times, but the rainfall 
Ad Warning Rainfall in AWOS Rainfall Observation Result AW frequency was smaller compared for January and February(about 53.84%). The frequency of aerodrome warnings in March and April ranged about 41.67% from the occurrence of rain events. It happened because of the rain that occurred is only in light category so the aerodrome warning was not made and disseminated to related parties. Aerodrome warning was verified by using the accuracy of statistical value predictions. The parameter values used to verify aerodrome warning issued by the Tanjungpinang Meteorological Station were presented in Table 3 . Aerodrome warning issued from March to April2019 was verified to find out the accuracy of the actual forecast towards extreme weather events recorded by AWOS. Verification was assessed only when aerodrome warning was issued and extreme weather conditions occurred, especially for the rainfall with light to heavy intensity followed by the potential for strong wind and thunderstorm. As for the dates which became the focus of verification in March 2019 that is 28 and 30 as well as in April 2019 that is on 10, 11, 12, 14, 20, 26, 29, and 30. The results of verification were presented in Table 4 .
Soekarno-Hatta Meteorological Station has been as a unit that disseminated Aerodrome Warning in Soekarno-Hatta International Airportincluded for 2 runways (07R/25L and 07L/25R) and has added a challenge to cover new runway (06/24). Figure 3 showed that in January and February, there were quite differences counted which recorded in Automatic Weather Observing System (AWOS) and Hellmann rain gauge paper as observation results. It was caused by rainfall pattern that enters to the aerodrome in other points of the runway. 
Ad Warning Rainfall in AWOS Rainfall Observation Result AW Furthermore, wind direction and speed was as the main factor of those conditions. 07L point in North runway would be touched if the wind moved from West. Figure 3also interpreted the highest number of aerodrome warning production was in January. Table 5 above indicated that the best score of POD was in Marchand also became the highest Bias value. FAR and TS amount among 4 months were not in big difference and they showed a good prediction. For HSS, it was still around fifty percent that meant the forecast was still in a good based on forecast relative accuracy. However, Soekarno-Hatta Meteorological Station in 4 months above was in critical season that was the rainy season. By analyzing all the values in Table 5 , weather forecasters in Soekarno-Hatta Meteorological Station were capable to provide Aerodrome Warning. 
Correct Negative January and February as the peak of the rainy season were needed to minimize False Alarm prediction. Figure 4 illustrated that most of the Hits and Correct Negative were dominating in all data. The bad result of prediction for misses in January was two times higher than February. False Alarm in February was still bigger.To more detail, March exists as the biggest amount of False Alarm ( Figure 5 ). But, Hits number was also in the biggest one. It meant that a weather forecaster wouldbe difficult to predict the time of rainfall occurrence in a transition season. Figure 6showed the number of Aerodrome Warning (AW) issued for wind parameters and visibility. The warning given was in the form of information on increasing wind speed significantly and visibility reduction which could affect aircraft operation. March was a month with the highest number of AW issued for both wind and visibility parameters, while February was a month with the lowest number of AW issued for wind parameters and visibility. Table 6showed the AW verification value issued by comparing AWOS data. Verification for wind and visibility parameters was done by statistical methods, namely MAE and RMSE. Verification was carried out in the AW issued from January to April2019. The verification value would be a good value if MAE and RMSE values were close to zero, so AW was able to Table 6 showed that AW could predict theextreme events specifically wind and visibility parameters. However, AW for wind parameter that issued in February 2019 was not able to predict correctly and showed a large verification value. 
Quantum physics allows us a perfect randomness, so most of all quantum information-theoretic primitives try to offer an unconditional security under the randomness. For examples, quantum key distribution protocols such as BB84 [1] and B92 [2] highly depend on a random measurements for given classified non-orthogonal quantum states.
Instead of the random measurement on non-orthogonal states, we can consider a direct randomization of quantum states through a quantum channel. This randomizing procedures are efficiently accomplished via the private quantum channels (PQC) or quantum one-time pads [3] . In the paper we are interest to some schemes for approximate encryptions (no perfect) and we make an attempt to reducing some classical communication resources. We would like to call the randomizing procedures or maps as random unitary channels (RUC) in terms of quantum channels. There are several methods for the approximate randomizing quantum states, for examples, [4, 5, 8] : We here adapt the procedure of Hayden et al. [4] .
Many applications of RUC in quantum protocols (See e.g., [4, 6, 7] .) are started from the approximate version of PQC. Here we will propose new approximate quantum state sharing (AQSS) scheme, which uses two approximate PQCs (APQC) and reduces the classical pre-shared secrets about one-half as compared with a perfect protocol. Actually our protocol could be including the (well-known) quantum secret sharing protocols [9, 10] , because a quantum state itself is able to operate special quantum tasks, though those are impossible in the classical power. Imagine that if there is a quantum computer only activated under a bipartite quantum state (or quantum key), then our AQSS protocol will give a efficient and secure solution for the quantum key. These approximate quantum state sharing protocols may offer us more opportunities as compared with the quantum secret sharing.
Let's take account of the pre-shared secrets for the approximate quantum state sharing protocols under RUC-based PQC roughly. Assume that a sender Charlie prepares a quantum state ϕ AB (two-qudit) and transmits the state through two independent RUCs, then two distant agents Alice and Bob will receive some output state of including high entropy. For the state ϕ AB the perfect randomization protocol will require exactly the amount of 4 log d-unitary matrices (Pauli matrices). On the other hand, the construction of Hayden et al. [4] for our AQSS scheme implies that only 2 log d + o(log d)-unitaries sufficient. In other words, the perfect quantum state sharing protocol needs to 2l bits of pre-shared secret information, while the AQSS protocol demands about l bits of information. Note that the works in [5, 8] will give a similar result for l bits bound.
We will prove the information-theoretic security of the AQSS scheme in two kinds of eavesdropping: an interior and exterior attackers. The proof of having higher entropy condition for the exterior attacks is not easy fact, so we split the input state ϕ AB to separable and entangled cases. As a result, the von Neumann entropy in both cases can be chosen sufficiently larger, and a leakage information will be arbitrarily small. Finally the authors show that our bipartite AQSS scheme naturally can be generalized to an one-sender and multiparty-receivers schemes.
In section II we introduce the definition of random unitary channels, and briefly mention about special property known as the destruction of quantum states on a product random unitary channel. We present our AQSS protocol based on two approximate PQCs in section III, and investigate the security of AQSS of considering two attacks: an exterior and interior strategies. we finally conclude our results in section IV. 
Now let us define the random unitary channel, and then construct an approximate private quantum channels. For all density matrices ϕ ∈ B(C d ), a completely positive trace-preserving map N :
where the trace norm is defined by
This definition directly induces the notion of random unitary channels. That is, for every ϕ, a quantum channel N :
is ε-randomizing, where the unitary operators U i ∈ U(d), and the probability p i 's are all positives with i p i = 1.
(The notation B(C d ) denotes the set of bounded linear operators from C d to itself and
Note that the parameter n is the number of Kraus operation elements for RUC, so it corresponds to the dimension of arbitrary environment.
For the approximate constructions of RUC, it was known that for all ε > 0 there exist random unitary channels in sufficiently larger dimension d, such that n can be taken to be O(d log d/ε 2 ) in [4] and O(d/ε 2 ) in [12] where U i 's are chosen randomly according to the Haar measure. We here fix the number n of having exactly n = 150d ε 2 , the Theorem 1 in [12] .
As mentioned in the Introduction, most intuitive application of the random unitary channel is the approximate private quantum channel [4] , which is a modification of the perfect private quantum channel [3] via RUC. The RUCbased APQC is the main tool of constructing the proposed AQSS protocol.
The security of PQC is preserved by the argument of the accessible information in which the leakage information is less than ε. Although small information is leaked to exterior attackers, Bob's decoding state is almost equal to Alice's original state ϕ. The FIG. 1 describes the total procedure of APQC.
In the next section we use two one-way independent PQCs between a sender Charlie and a receiver Alice, and the sender Charlie and another receiver Bob. Let's define two RUCs, from the definition of (Eq. (2)), such that
where we fix the probability as an equally weighted probabilities p i = 1 nA and p j = 1 nB for all i, j, and assume that the number of n A is equal to n B , i.e., n A = n B = 150d/ε 2 . For an approximate state sharing of any bipartite quantum state, above two channels play an important role in the approximate quantum state sharing scheme. 
where a security parameter ε be a positive less than 1. The relation above asserts that all encoding states are information-theoretically secure. Unfortunately, for any entangled states proving the bound is not a simple task. Note that the argument for the (efficient) randomization is related to a destruction of correlations in quantum states [4, 11] . The following section gives the AQSS protocol and the security of the protocol. The last of the section, we briefly describe a multiparty AQSS scheme.
Let us assume that Charlie-Alice and Charlie-Bob have independent two APQCs, and Charlie wants to sharing a bipartite quantum state ϕ AB securely between Alice and Bob.
The protocol for a bipartite quantum state sharing is simple ( See FIG. 2 ):
(i) The sender Charlie selects a quantum state ϕ AB and transmits the state through the channel N A ⊗ N B to the receivers Alice and Bob.
(ii) Distant two parties Alice and Bob just hold the state N A ⊗ N B (ϕ AB ) they received.
(iii) When Alice and Bob want to reveal the original state ϕ AB , they must cooperate in a single location. They perform the inverse unitary operations under the locally shared keys.
The security of the AQSS protocol is divided two cases of an exterior and interior attacks. Actually the security is based on information-theoretic assumption, which means that the intercepted states must have the higher von Neumann entropy. Thus any attackers cannot obtain sufficient information for the original states.
First, let us consider an attack accomplished by an exterior Eve. Assume that Eve intercepts the state N A ⊗ N B (ϕ AB ). We here claim that
as d goes to infinity. We don't know the accurate description for the state N A ⊗ N B (ϕ AB ) for all inputs, so we will divide the state ϕ AB into the separable and entangled one and investigate the behavior each other. If product state is given, it is possible to infer the inequality Eq. (4) easily. By using the triangle inequality with respect to the trace norm for the two RUCs, if
, a separable state is given, then
where the inequalities Eq. (6) and Eq. (7) come from the norm convexity and the triangle inequality, respectively [4] . Thus any separable inputs for the product channel are very close to the maximally mixed state
For the separable input cases, there is another bound that depends on the dimension parameter d and n: We can prove that the expectation value for the difference between the channel output and the maximally mixed state (with respect to the trace norm) is very close, that is,
where E {Ui,j } denotes the total expectation value of
and {U j } nB j=1 for the independent RUCs N A and N B , respectively. The Appendix in this paper states that the inequality Eq. (8) is non-trivial and obtained precisely by exploiting the relation between the trace norm and the Hilbert-Schmidt norm. As mentioned above, let's take
This implies that Eve's attack is impossible in principle. What can we do for an entangled input state? Though a direct proof could be impossible, there is an evidence for the statement, the Eq. (5). The Theorem III.3 in [4] states that, for a positive operator-valued measure (POVM) {L i } which is implemented using local operation and classical communication (LOCC), i p i − q i 1 ≤ ε, where Bob cannot obtain any information for ϕ A without Charlie-Alice's key information. Symmetrically Alice's attack is useless. In other words, the Charlie's aim of sharing a quantum state ϕ AB between Alice and Bob will be securely accomplished.
At least above-mentioned two attacks (exterior and interior eavesdropping) cannot break the security of the proposed AQSS protocol. so the cooperation between Alice and Bob always restores the original state approximately.
In the proposed scenarios, the perfect protocol for quantum state sharing requires exactly d 4 unitary operators, while our protocol only needs to total 22500d 2 /ε 4 unitaries for sufficiently larger d. This fact directly means that some pre-shared key bits are reduced by factor 2, since the AQSS is needed 2 log d − 4 log ε + O(1) secret bits, but the perfect QSS is required 4 log d bits. For any state ϕ AB ∈ B(C d 2 ), and for any channel N AB (for an ε > 0 is arbitrary), let's consider a relation like that
Then, it is sufficient to construct the perfect QSS (ε = 0) with d 4 Pauli operators for the channel N AB in the sense of PQC [4, 8] . In the case of our approximate QSS, the product channel of two RUCs (N AB = N A ⊗ N B ) just consume of half secret bits, so we say that it is efficient in weak sense (though small information is always leaking).
Without loss of generality, a direct extension of the bipartite quantum state sharing protocol (Eq. (8)) gives the security of a multiparty approximate quantum state sharing (MAQSS). Assume that a sender Charlie (C) prepares an m-qudit ϕ A1A2···Am . If they initially have shared PQCs between C-A 1 , C-A 2 and so on, then, for any ε > 0,
The above Eq. (12) implies that any exterior attacks will be failed. Furthermore all interior attacks (including group conspiracy) will be frustrated to obtain the whole state without others secrets, it has similar reason to the two receivers protocol. Let's look at the cost of secret bits for the MAQSS scheme. Roughly speaking, the perfect scheme requires 2m log d secret bits, but MAQSS only m log d + o(log d)-bits sufficient.
We studied that the approximate quantum state sharing schemes are efficient from the classical information cost of view and those are robust to the two kinds of attacks. The proposed AQSS protocol basically depends on an approximate private quantum channel, which is constructed via two independent random unitary channels. Although the protocol leaks small information corresponding to the security parameter ε, the scheme preserves its informationtheoretic security, and so the AQSS and MAQSS schemes can be interpreted as some high-efficiency state sharing protocols for any bipartite and multipartite quantum states.
The Direct Simulation Monte Carlo (DSMC) method is a computational tool for simulating flows in which effects at the molecular scale become significant [1] . The Boltzmann equation, which is appropriate for modeling these rarefied flows, is extremely difficult to solve numerically due to its high dimensionality and the complexity of the collision term. DSMC provides a particle based alternative for obtaining realistic numerical solutions. In DSMC the movement and collision behavior of a large number of representative "simulation particles" within the flow field are decoupled over a time step which is a small fraction of the local mean collision time. The computational domain itself is divided into either a structured or unstructured grid of cells which are then used to select particles for collisions on a probabilistic basis and also are used for sampling the macroscopic flow properties. The method has been shown to provide a solution to the Boltzmann equation when the number of simulated particles is large enough [2] . The sizes of DSMC cells have to be much smaller than the local mean free path for a meaningful simulation in general.
Since its introduction, the Direct Simulation Monte Carlo (DSMC) [1] has become the standard method for simulating rarefied gas dynamics. It is generally very computationally intensive, especially in the near-continuum (collision-dominated) regime. The general wisdom for accelerating the DMSC computation is to parallelize the code using the MPI protocol running on clusters with large numbers of processors such as PDSC by Wu et al. [3] . Such implementations rely upon the Multiple Instructions on Multiple Data (MIMD) parallelization philosophy and parallel efficiency over massive numbers of nodes is not optimal. Recently, Graphics Processing Units (GPUs) have become an alternative platform for parallelization, employing a Single Instruction on Multiple Data sets (SIMD) FIGURE 1. A flowchart describing the application of DSMC to GPU-accelerated computation.
parallelization philosophy. The resulting parallelization is much more efficient at the cost of flexibility -as a result, the computational time of several scientific computations, especially those which are optimally applied to vectorized computation strategies, have been demonstrated to reduce significantly. The application of GPU computation also has significant advantages in lower power consumption and significantly reduced equipment costs.
Experience showed that the higher the locality of the numerical scheme/algorithm, the higher the speedup is. However, there seems no successful previous study applying GPUs to accelerate the DSMC computation, which employs a (generally) highly local algorithm. The DSMC method, which is a particle-based method developed by Bird [1] , is described in Figure 1 . Following initialization, the DSMC method generally involves:
 Moving all the simulated particles, which includes treatment of boundary conditions,  Indexing all simulation particles (sorting particles into cells),  Performing collisions between particles -outcomes of collisions are stochastic in nature, hence the use of the term "Monte Carlo" in the DSMC name,  sampling the molecules within cells to obtain the macroscopic quantities.
In this study, an all-device (GPU) computational approach is adopted, which includes particle moving, indexing, colliding between particles and sampling. This required some changes of the original DSMC method in order to allow efficient all-device computation. Figure 1 shows the flowchart of DSMC computation using a single-GPU. During the initialization stage, input data is loaded into memory and initial states are determined on the host (CPU). This information (including particle and computational cell information) is transferred to the GPU device global memory. Following this, the unsteady phase of the DSMC simulation is performed -particle moving, indexing, particle selection and collisions and sampling are executed on GPU. During particle movement, each particle is tracked by a thread, with each thread tracking N p /N thread +1 particles, where N p is total simulated particles and N thread is number of threads employed by the GPU device. Each thread reads/writes particle data to/from the global memory of the GPU device [4] . The particle indexing phase of the computation is similar to Bird's DSMC implementation [1] . We use a function contained within the Software Development Kit (SDK) of CUDA, scanLargeArray, to scan through data elements of large arrays contained within global memory. This function is used to efficiently perform particle indexing. During the collision phase, a different parallelization philosophy employed -all particle collisions within a cell are handled by a single thread, allowing efficient recollection of data since all data is coalesced. During the sampling phase, shared memory of GPU [4] is used to store properties of all particles in each cell. After all particles within the same cell are sampled, we copy the sampled data from the (faster) shared to (larger) global memory. When our simulation is nearing completion (i.e. the flow has reached steady state and the sampled data is sufficient to remove undesired statistical scatter) we move the sampled data from device (GPU) global memory to CPU memory. Finally, calculation of the macroscopic properties is performed by the host and the data is written to file for further analysis.
In this study, we verify our DSMC implementation and demonstrate significant speedup of DSMC computations using a single GPU device through the simulation of several two dimensional benchmark problems: namely, (i) supersonic flow over a (fixed temperature) horizontal flat plate, and (ii) a supersonic lid-driven cavity problem. Approximately 50 particles per cell for all benchmark test cases are maintained throughout the simulations. The DSMC computation employs a VHS collision model [1] for each case. All benchmark simulations are performed on the latest high end computation equipment: single CPU computations employ an Intel Xeon X5472 CPU (3.0 GHz, 12 MB Cache) while GPU computations employ an Nvidia Tesla C1060 (240 microprocessors @ 1.4 GHz, 4 GB DDR3 global memory) hosted by the same Intel Xeon CPU employed for the single CPU test cases.
This benchmark involves the two dimensional supersonic flow over a flat plate. Ideal argon (γ=5/3) with temperature 300 K is initially assumed to be moving with Mach Number M = 4.4 over a diffusely reflecting flat plate of fixed temperature 500 K. The length of the flat plate is L = 0.9m, with the initial flow-field density based on the Knudsen number (computed with characteristic length based on the plate length). The initial simulation conditions are summarized in Table 1 .
Following the initialization, the simulation is allowed to progress in an unsteady fashion until a steady solution is reached, after which samples are taken to eliminate statistical scatter. Figure 2a shows the resulting contours of temperature for Case III. The results show that the GPU code can reproduce the data simulated by the serial (CPU) code with allowances for statistical scatter. Figure 2b shows the speedup obtained using GPU as compared to using a single core of the Intel Xeon X5472 CPU for each case. We demonstrate a decrease in computational time of 3~10 times when the GPU device specified is employed for the simulation. Table 2 summarizes the computational time and speedup of each component of all cases using both CPU and GPU. We observe that the speedup using GPU computing increases with reduced rarefaction of the flow. This is also justified since (i) application of a GPU device requires significant overhead due to data transfer and device initialization, and (ii) the general ratio of memory bound communications to device computations is reduced for larger flow problems, resulting in more time spent in computation than in communication. In the current implementation, the sampling phase of the simulation performs best due to the ideal parallelization using the device shared memory. 
The second test case is a two-dimensional supersonic lid driven cavity problem. Here the simulation domain is a square cavity (1x1m) with diffusely reflecting walls of fixed temperature (300K). All walls are stationary except the upper wall which is moving (with positive velocity) at Mach Number M = 2. The gas (ideal argon, γ=5/3) is initially at rest with a temperature of 300 K and various density depending on the governing Knudsen number, computed with a characteristic length equal to the box width. These initial conditions are summarized in Table 3 .
Following initialization, the simulation is progressed in time until the flow is steady and samples are taken to reduce the statistical scatter. Figure 3a show the temperature contour of Case II. It is worth noting that the region in the central region of the lid-driven cavity is rarefied, with a low density and relatively high temperature (as also shown in Figure 3 ). The GPU results are almost identical to the equivalent CPU results which again validate the CUDA GPU implementation. Differences between the results are (probably) able to be explained by differences between the random numbers employed by the GPU and CPU solvers and general levels of statistical scatter. The random numbers employed by the GPU DSMC implementation are drawn from pre-computed (fixed-length) arrays to improve the efficiency of random number use on the GPU device. Figure 3b shows the speedup using GPU is about 3~9 times as compared to that using a single core of the CPU described above. All computational timings are summarized in the Table 4 for reference. The ratio of communication to computation, a critical factor in GPU efficiency, is minimal for larger DSMC problems. Hence, the speedup using GPU computing is shown to increase with reducing rarefaction of the flow (i.e. increasing collision dominance). The sampling phase of the DSMC simulation is shown to be the most efficient phase of the computation when applied to GPU computation due to the low number of communications required. 
Presented here is the application of DSMC to a novel (entire-device based) GPU acceleration. In the implementation discussed, the CPU is only employed during the initialization and concluding phases of the simulation -the main processes employed during the DSMC simulation are performed on the GPU device while the CPU remains idle. The resulting computations demonstrate a speedup of 3~10 times, depending on problem size, resulting from the lack of communication-bound processes. Efficient use of various memory (caches) on the GPU device for different phases of the DSMC simulation also allow high levels of parallelization. The GPU-DSMC code has been verified through comparison against a conventional serial DSMC computation, with results showing excellent comparison in a fraction of the time. The GPU device used for the benchmark problems (Nvidia Tesla C1060) is commonly available for a fraction of the price associated with a conventional computer cluster required to match its performance.

Light detection and ranging (LIDAR) is an application of lasers used to discover information about a distant object, usually some form of distance information. LIDAR data are usually collected via airplane, with the airplane traversing a subject area of interest and collecting data about this area through the use of laser pulses. These laser pulses are emitted from the airplane fuselage from a laser scanner device, and the round trip travel time from this emission and the returning light is measured. This time delay is then used to determine the elevation of the ground in this particular area. LIDAR equipment is usually paired with a global positioning system (GPS) in order to accurately pinpoint the x and y coordinates of a specific data measurement.
The way in which LIDAR data is collected is a very detailed one that needs to be explained more thoroughly in order to appreciate it fully. This information as released by the National Oceanic and Atmospheric Administration (NOAA) details in depth about how this data is so intricately extracted. Already discussed is the use of laser pulses, which scan the area of interest up to 5000 pulses per second.
The laser type used is generally the neodymium-doped yttrium aluminum garnet (Nd:YAG) laser with a wavelength around 1064 nanometers, placing the laser in the infrared range of light. This pulsed laser is emitted when the population inversion in the resonator has reached its maximum, achieved by what is known as Q-switching, which places a switch inside the resonator which releases the stored pulse when the maximum amount of atoms are in an excited state.
Of major interest is the mechanism which allows this technology to perform its task of collecting LIDAR data. Two mirrors, one a 45-degree angled folding mirror and the other a moving mirror, combine to direct the laser pulses towards the ground. The laser pulses encounter the folding mirror first, with the reflection directed towards the moving mirror below. The reflection from the moving mirror directs the laser pulses towards the ground, with an overall coverage angle of 30 degrees. Since these aircraft fly at heights of around 700 meters, the 30 degree angle allows an overall circular coverage area with a diameter of approximately 350 meters. Therefore, in order for the aircraft to cover a broad area of land, multiple paths are usually flown, resulting in some overlap between coverage areas, as illustrated in Figure 1 
With the LIDAR data collected, the real question becomes apparent: how is LIDAR data processed? LIDAR data in combination with a GPS allow for each measured elevation to be paired with its location, effectively creating a three-dimensional coordinate for each measured elevation. The x and y coordinates are usually latitudes and longitudes derived from the GPS system, while the z coordinate is the measured elevation using the LIDAR collection device, measured usually in either meters or feet. An ASCII file with each line containing a measurement point (x,y,z) having each coordinate separated by either a tab or comma is common when working with LIDAR data. The precision on this height measurement is usually in the range of 10-25 centimeters, allowing for a very accurate measurement of ground elevation.
LIDAR data is currently being used in a diverse array of applications, ranging from the study of seismology to traffic control analysis. LIDAR is used in seismology to detect faults, with one such example relating to a particular fault in Seattle. LIDAR allowed the detection of the Seattle fault from an earthquake that occurred over 1000 years ago, effectively painting a picture of the surface in the area by penetrating through tree canopies, allowing for a view of what is now known as the Seattle fault. LIDAR data is also used in traffic law enforcement, replacing radar as a speed detector in police laser systems. Other applications of LIDAR relate to its use in adaptive cruise control systems in cars and its use as a height measuring tool in the forestry industry.
One of the main uses of LIDAR data and an important idea in the development and identification of building footprints pertains to its ability to create topographic maps of areas. LIDAR data is able to be manipulated and processed in such a way to allow a visual representation of an area based on the time delay measurements translated into heights along with the paired GPS coordinates. The final result of such processing techniques on this data is known as a digital elevation model (DEM).
While the creation of DEMs is not directly considered here, it is important to note its use as it is the main scientific and image processing based application of LIDAR data. In the figure below, a DEM of Mount Saint Helens is shown as an example of this particular LIDAR application performed by LIDAR mapping company EarthData [9]. Colors gravitating towards the red end of the spectrum represent areas of higher elevation, whereas colors present on the blue end of the spectrum represent areas of lower elevation. This particular DEM is shown in three dimensions in order to enhance the view and appearance of the elevation data generated using LIDAR. LIDAR intensity images are also an important visualization of LIDAR data, allowing irregularly spaced LIDAR data to be represented as a digital grayscale image of varying intensities, with higher intensities representing higher elevations, and lower intensities representing lower intensities. This particular creation of grayscale intensity images will prove extremely important in the classification of particular areas of interest, in this case, the classification of buildings.
LIDAR intensity images can be examined using image processing techniques. These techniques allow the visual representation of LIDAR data to be identified, classified, and separated into areas of interest to be analyzed. One such classification involves the identification of buildings, with this identification proving important in various real life applications, including the study of urban population and the development of ground plans. The identification of buildings can also be extended into the development of three-dimensional building models.
Previous work has successfully identified building footprints through the use of filtering, region growing, and dominant direction estimation [1] . However, the use of dominant direction estimation as a means of cleaning building footprints assumes a particular relationship between perpendicular/parallel building edges and oblique building edges. Also, a visual comparison to aerial imagery was used to detect omission and commission errors. With a refined definition of what constitutes a building boundary along with a new method for confirming building boundaries through the use of a satellite image region growing algorithm, this work provides a new view of building footprint detection.
One of the major applications of LIDAR technology has been the ability to use the data for feature extraction and identification. Building footprint extraction is just one of the many feature extraction opportunities provided by airborne LIDAR data. The goal is to determine the location of buildings in the area of interest, effectively separating buildings by determining their boundary. There are many ways to accomplish this task, with commonalities along the way in each. One such method of extracting building footprints will be described in great detail here.
Before any analysis can be done on LIDAR data, the data must become useful in a way that allows understanding and interaction. Once the area of interest has been determined, the LIDAR XYZ files need to be interpreted in a way to allow such an understanding to be possible. A visual depiction of LIDAR data becomes a necessary means for understanding and future analysis, and the use of image processing provides such a means to the researcher. The LIDAR data will be visualized in a digital image, with the x and y values representing pixel coordinates of the image, and the elevation value z representing the intensity value of the grayscale image. This pre-processing stage sets up the ability for any application of LIDAR data and will help to extract building footprints.
First, the LIDAR data XYZ files are read into a three-dimensional array, with each coordinate (x,y,z) being read into a row of the array. The goal is to create an image using the LIDAR data points that maps each coordinate to a pixel or group of pixels. LIDAR data by its very nature has a randomness to its data points, which prohibits a one-to-one mapping of each LIDAR point to a pixel, as some areas have such a dense coverage but others are sparsely sampled. Upon reading the data into the array, the number of points read is determined and stored in order to find the minimum x and y values of the LIDAR data. The horizontal size of the image is determined by the user, and the vertical size is derived based on this horizontal size by manipulation of the maximum and minimum coordinates, as indicated in the following equation.
Basically, the user controls how large of an image the LIDAR data will be processed into, but only to an extent. The LIDAR data itself and its range determines the height to prohibit too small or too large an overall image size chosen. However, this can result in some information loss because of the lack of a one-to-one mapping as stated above. Also, each pixel could contain more than one LIDAR coordinate point, in which case the smallest elevation value is chosen as the pixel value at that location. This would seem to grid the LIDAR data into an image reasonably, but not all pixels have a corresponding LIDAR data point and elevation value. In this case, a value is used as a place holder if there is no corresponding LIDAR point for that pixel.
Once the above step is performed, the image is processed further by interpolation of pixel values. The image is interpolated in order to replace the place holder points described above with a minimum elevation value in a window. A window size w is specified for this interpolation with a value of 3. Each pixel value is scanned in a search for the place holder value, indicating that there is no data for this specific location in the area of interest. When a pixel of this value is found, the pixel values in the window are scanned. If less than 25% of the pixel values in the window contain the place holder value, the minimum pixel value in the window is determined and assigned to the pixel in question. However, if more than 25% of the pixel values in this window also contain the place holder value, then the pixel value in question remains the place holder value, but the window size is increased by two. This process continues up to a maximum window size specified by the user, in this case the maximum window size being 15. This progressive window operation allows stray place holder pixels to be replaced by surrounding values in order to have a more complete image, but place holder pixels that do not get replaced through this interpolation remain as such and are represented in the resulting pre-processed grayscale image as the highest intensity possible, resulting in a white color. The resulting image after interpolation is shown in Figure 3 below. This image represents a subset of Lenoir County, North Carolina. The pre-processed image above is of size 1201x401, obtained through the interpolation methods discussed above. Upon examination of this final pre-processed image, a few traits are immediately noticeable. First, the portions of the image that had no LIDAR data points corresponding to the actual pixel coordinate have a white intensity. Also, the overall intensity differences, ranging from light grey to a grayscale intensity that is close to black in color, indicate elevation differences of the LIDAR data coverage area. The lower intensities (dark grey / black) represent very low-lying areas relative to the rest of the coverage area, indicating that these points are most likely ground points. The higher intensities (lighter grey) indicate that the elevation is higher in those locations and could therefore correspond to buildings, but further detail will be necessary to make such a determination. This pre-processed image represents the starting point for all further analysis, as various image processing techniques will be employed in order to accomplish the task of identifying building footprints.
With the pre-processed image obtained, and the overall goal being to determine the locations of buildings and their boundaries, a method must be determined to perform such a separation of the building pixels from the rest of the image. However, one cannot simply separate the building objects from the rest of the image immediately. First, the pixels representing ground elevations must be separated from the pixels representing non-ground elevations. Non-ground elevations refer to any pixel whose intensity has a value greater than a predefined threshold for height, regardless of whether or not the point corresponds to a building, vegetation, or other object that is higher off the ground than this predefined threshold.
There are many methods that have been employed in order to accomplish this task of separating non-ground points from ground points. One such way was formulated by Vosselman, who identified non-ground points and separated them from ground points by analyzing the slope between a given pixel and the eight neighbors of the pixel. Vosselman determined the slope from a given pixel to each of its neighbors, and then found the overall maximum slope. If the maximum slope was less than a predefined threshold, then the point was labeled a ground point; otherwise, the point was considered a non-ground point.
The major pitfall with separating non-ground points from ground points arises in the derivation of a best-fit filtering threshold. Zhang [2] has identified the following two general problems with separating ground points from non-ground points: commission errors and omission errors. Commission errors refer to mistakenly identifying non-ground points as ground points, while omission errors refer to mistakenly including ground points as non-ground points. If one selects the best-fit threshold for the filtering method of choice, then these errors will be minimized, and the resulting separation will be as accurate as possible. Zhang proposed what is known as the progressive morphological filter to mathematically remove non-ground points from a processed grayscale image like the one in Figure 3 above.
A morphological filter is a filter that utilizes a combination of dilations and erosions in order to dilate and/or erode features of an image. When dealing with a grayscale image as in the preprocessed image of Figure 3 , these dilation and erosion operations actually become maximum and minimum operations. The progressive morphological filter combines the use of maximum filters and minimum filters in order to detect the non-ground elevations by effectively replacing all of these values with ground pixel values, removing the non-ground pixels from the image.
In order to implement progressive morphological filter, an initial window size w is chosen (3 in this case). A minimum filter of size w x w (3x3) was performed on the pre-processed image of Figure 3 . This minimum filter operates on the original image by sweeping the window over the entire image, and for each given pixel, replacing the pixel's value with the minimum pixel value included in the window neighborhood. This effectively removes any non-ground object smaller than the window size because it changes the appropriate pixel value to the minimum value in the region. This minimum filter is immediately followed by a maximum filter of the same window size, effectively repairing any damage done to non-ground objects that are larger than the window size by replacing the pixel value with the maximum value in the window area. This combination of a minimum filter following by a maximum filter is known as an opening operation of the morphological filter.
Once this initial opening operation is performed, the difference between the original image and the new image filtered using the opening operation is obtained. This image should therefore have pixel intensity values of zero for all ground pixels since the ground pixels were not removed in the opening operation performed. However, this image should have pixel values relating to the non-ground values in the original image, as the difference image was obtained by subtracting each intensity value of the filtered image from the original image. This difference image now becomes extremely important in the implementation of the progressive morphological filter as it relates directly the best-fit threshold described above. The height threshold was predefined to be 0.1 for this morphological operation. However, the best-fit threshold was determined to be dependent on the window size in the following way:
Therefore, the actual filtering operation occurs by comparing each difference image pixel value with this best-fit threshold value. If the given difference image pixel value is greater than this best-fit threshold value, then the corresponding original pixel value is replaced with the pixel value of the image obtained after the opening operation. If the given difference image pixel value is less than the best-fit threshold value, then the corresponding original pixel value remains the same. This process repeats every pixel of the image is traversed.
Upon initial viewing, one would think that the operation performed in equation (2) would simply set the best-fit threshold to a value of 0.3 since the window size above was chosen to be three. However, this is not the case, as this morphological operation is progressive in nature. The progressive nature of the morphological filter refers to the fact that the window size progresses to a higher value after each performance of this algorithm. A maximum window size is set by the user initially. A window step size is also set initially by the user, which indicates how much the window will increase after each traversal of the algorithm. In this particular case, a maximum window size of 40 and a window step size of 10 were chosen. The following table shows the progressive increasing nature of the window size for the morphological filtering operation. This table above shows that the initial window size was set by the user at a value of 3. The opening operation was performed with a window size of 3x3, and then the difference image values were compared to the best-fit threshold as described above. The window step size was then added to the initial window size to achieve a new window size of 13x13 for the opening operation. This process is repeated until the maximum window size of 40 was achieved. The best-fit threshold also progressively changes with respect to the window size according to equation (2) above. The grayscale image obtained after removal of all non-ground objects through the implementation of the progressive morphological filter is given in Figure 4 below. In comparison to the original pre-processed image in Figure 3 , one can see that the areas of higher intensity are now completely removed in Figure 4 . This proves that the progressive morphological filter performed well in separating ground pixel values from non-ground pixel values.
With the non-ground pixel values successfully separated from the ground pixel values and identified, the algorithm to identify building areas and their boundaries becomes the focus. The algorithm proposed to accomplish the task of separating building areas from other non-building non-ground objects is the region growing algorithm. The region growing process will iteratively separate the non-ground pixels into distinct regions of interest which will allow a successful building footprint extraction.
Region growing is an image processing concept that allows an image to be separated into areas (regions) that have a predefined characteristic. This process starts from a single pixel that satisfies a given condition, effectively becoming the first pixel of the region. Then, the eight neighbors of the pixel are examined against the same condition that included the first pixel in the region. If any of these pixels also satisfy the criteria given for acceptance, these pixels are also considered part of the region and indicated as such. Then, the eight neighbors of each of these added pixels are examined, with the process continuing in this way until no new pixels are added to the region. Figure 5 below shows the beginning of the region growing process, the choice of a seed point that meets the specific condition under testing. The seed pixel is the first pixel in the scanned image containing a "true" output to the testing condition. The region will then grow outward from this pixel by testing each of the pixels eight neighbors individually, adding said pixels to the region if the condition is satisfied. Figure 6 below illustrates the region growing process after multiple iterations of the condition check. As can easily be seen, the highlighted area in the above figure indicates that these particular pixels satisfied the condition for inclusion and therefore are now members of the region. This process continues until no new neighboring pixels can be added to the region, creating an effective boundary for the region in question.
With the basics of the region growing process now explained, the detailed region-growing algorithm applied to the specific Lenoir County area of North Carolina is performed. The progressive morphological filter has effectively separated the ground and non-ground pixels by creating an output image that has removed all non-ground pixels and replaced them with average ground pixel values.
With this initial separation complete, the non-ground pixels must be further segmented into consolidated regions of interest, as each non-ground pixel should belong to an area of nonground pixels, representing anything from a car to a building to an area of vegetation. Zhang [2] referred to two different types of non-ground pixels as inside points and boundary points. An inside point is a non-ground pixel whose eight neighbors are also non-ground pixels. A boundary point is defined as a non-ground pixel with at least one of its eight neighbors being a ground pixel. Therefore, all non-ground pixels can be segmented further into two distinct subsets of pixels, classified based on whether the pixel is an inside point or a boundary point. This revelation allows the first region growing segmentation to take place. The desired outcome is a set of regions of inside points, along with a set of regions of boundary points corresponding to these inside point regions.
The region growing process for segmenting regions of inside points is the first task after the progressive morphological filtering operation is performed. First, a testing condition must be established which will allow for a new region to begin from an initial seed point with properties that satisfy the condition. A height (pixel intensity value) threshold is chosen as this initial condition, given a value of 10, and this threshold will be the first testing condition upon which a region is initiated. This threshold is variable, allowing the algorithm to run with any threshold with value greater than zero desired. The image to be operated on is the difference image, which now contains intensity values near zero for ground pixels and intensity values equal to the height of non-ground objects for all pixels that are not considered part of the ground area. The difference image is scanned, starting from the upper left corner, until a pixel with intensity value greater than the threshold is found. This initial conditioning does not identify the point as an inside point, so the pixel cannot yet be added to a region.
Once a pixel satisfying the initial threshold condition is found, further conditioning is performed to determine whether or not this pixel is an inside point. Of note is the fact that each pixel will be identified as "belonging to a region" or "not belonging to a region" by a labeling scheme within the algorithm itself. Initially, all pixels will contain a label of "not belonging to a region."
The second condition tests whether the pixel belongs to a region already by scanning the pixel's label for this information, represented in the algorithm as a Boolean '1' (belongs to a region) or '0' (does not belong to any region). If the pixel does not belong to any region as of yet, the second condition is satisfied and further conditioning is performed. The third and final condition does not test the pixel in question itself, but the pixel's eight neighbors. The third condition checks whether all of the eight neighbors satisfy the initial condition of having an intensity value greater than the threshold. This is accomplished by performing an AND operation of each neighbor condition, resulting in a satisfied condition only when all pixels are greater than the threshold. If all of the eight neighboring pixels have a value greater than this threshold, then the pixel in question is indeed an inside point. Therefore, the pixel is added to the region as the seed point and labeled as described above as "belonging to a region" to avoid placing the same pixel in multiple regions or in the same region more than once. The pixel is added to the region in the algorithm itself by utilizing linked lists, with each region being identified by its own linked list. This allows for each region to have varying size along with the possibility of adding pixels at a random rate based on condition testing. If the above process fails to produce a pixel that satisfies the conditions of an inside point, the difference image scan continues until the first inside point is found, making it the initial seed point for the region.
With the seed point for the inside point region determined, the region must then be grown. Since a seed inside point was found, the scanning of the difference image stops. The location of the scan is saved so that when the region is completely grown, the scan can begin again from the same location, looking for another region to grow. As shown in Figure 5 above, the region growing process grows outward from the seed point, so the conditioning moves on to the eight neighbors of the seed point.
First, the upper-left neighbor of the seed point is identified for testing. Since the seed point itself was labeled as an inside point, by definition this already indicates that this upper-left neighbor passes the initial threshold test. Therefore, the upper-left neighbor is tested based on whether or not it already belongs to a region. If it already belongs to a region, then the testing of this pixel fails, and the upper neighbor of the seed point is identified for testing. However, if the upper-left neighbor is not already included in a region, then further testing is performed to determine whether this neighbor is also an inside point of the region in question. This further testing involves the eight neighbors of this upper-left neighbor. If all of the eight neighbors of the upper-left neighbor have intensity values greater than the threshold, then the upper-left neighbor is considered an inside point, labeled, and added to the region. This process continues, with each neighbor of the original seed point being checked for its possibility of belonging to the region.
Once all of the eight neighbors of the original seed point are tested, the linked list which contains the pixels in the region will now have a maximum of nine pixel members. The region growing implementation does not stop here, however, as some regions may have more than nine total pixels in the region. Assuming all of the eight neighbors of the original seed point have been added to the region, the region contains nine pixel members, and the next available pixels to undergo testing for inclusion in the region are the neighbors of these eight neighbors. Therefore, the linked list's link is incremented in order to get to the second pixel that was added to the region. Then, the algorithm tests all of its eight neighbors in the same way as the original seed point was tested, and if any of its neighbors meet the conditions for inclusion, these pixels are added to the list. This process continues until the end of the linked list is reached, successfully exhausting all possible pixel values to be included in the region, effectively creating a region of labeled pixels with a defined boundary.
With the end of the region determined, the difference image scan continues from the same pixel in which it left off, checking for the next seed point for a new region to begin. The image is scanned until the next pixel with intensity greater than the threshold and with property of not already belonging to a region is found. If a pixel value is found to successfully meet this condition, another region is grown as described above. This region growing process for inside points continues until the entire difference image is scanned, effectively segmenting the nonground pixels into regions of inside points with well defined boundaries.
With the inside points completely separated into regions, the determination of boundary points becomes the important priority. These inside point regions are incomplete in the sense that along their boundaries, there are still non-ground pixels that are not members of the region. This occurs due to the definition of an inside point, which states that all eight of its neighbors must be non-ground points as well. With this in mind, what if one or more of these eight neighbors already defined as non-ground points have neighbors that are ground points? These pixels are also part of the non-ground region of interest, just not part of the specific inside point regions. These pixels are the boundary points for the inside point regions, and they need to be separated as such in order to identify every non-ground point as either an inside point or a boundary point.
In order to determine these boundary point regions, a new scan of the difference image is performed after each inside point region is determined. The initial condition for inclusion in a boundary point region is threefold: the pixel must not already be included in an inside point region, the pixel must have an intensity value greater than the threshold, and the pixel must have at least one of its neighbors be an inside point. Once the seed point for the boundary point region is found, further conditioning is performed similar to the conditioning described above for determining inclusion in the inside point regions. If at least one of the eight neighbors of this new seed point is a ground point, and at least one of the eight neighbors of this new seed point is labeled as belonging to an inside point region, then the pixel is labeled as belonging to a boundary point region and added to the region as the initial seed point. The key element to this conditioning allowing the boundary point regions to be separated according to their relevance to other inside point regions is the fact that at least one of the eight neighbors already belongs to a ground point region. Otherwise, without this condition there would be no way to separate each boundary point region, resulting in one large region of boundary points. However, as will be described in the post-processing stages of the building footprint algorithm, the boundary point regions must be separated here as such.
The following table shows the pseudo code for the region growing separation algorithm, resulting in distinct regions of inside points and boundary points. Once this algorithm is performed, the non-ground pixels are successfully separated into regions of inside points and regions of boundary points. Figure 7 below shows the inside point regions, identified by the pixels that are shaded white in the image. In a direct comparison with Figure 3 , Figure 7 illustrates the algorithms ability to identify nonground pixels effectively. Moreover, it also illustrates the ability of the algorithm to identify regions of interest based upon a defined threshold that can be preset by the user. This allows for a height threshold specification for inclusion, allowing for the elimination of other non-ground objects that are not necessarily buildings, objects such as cars, shrubs, and other vegetation. The result of the figure above is an illustration of the automated determination of building ground plans and outlines found using the region growing algorithm for the segmentation of boundary points. The figure clearly defines non-ground regions, including those of smaller building areas. It can be noticed in this image as well that the effects of small distinct nonground pixels are not included in the region identification, having successfully been removed as not belonging to a building region. Further comparison of these two figures with the original pre-processed image will be performed after a discussion of a more advanced region growing algorithm implementation next.
The above algorithm successfully resolved the non-ground pixels into two distinct types, boundary points and inside points. The region growing process allowed these two distinct types of pixels to be represented as grown region areas with clearly defined boundaries. However, in order to reduce inclusion errors present in any region growing process, further segmentation of the inside point regions is necessary. The process involved in this further segmentation employs the use of a plane-fitting technique to separate inside point regions into smaller regions representing building surfaces. The overall goal is to segment each inside point region into smaller regions based on the formation of best-fit planes of pixels and their neighbors, resulting in a representation of planar surfaces within each inside point region.
The plane-fitting process used is a least-squares method of determining a best-fit plane from a given number of three-dimensional points. The algorithm determines a plane that best fits nine distinct three-dimensional points, in this case an inside point pixel and its eight neighboring pixels. The solution provides the least squares solution to the following equation:
with I(x,y) representing the intensity of the pixel located at coordinate (x,y) and A,B,C representing parameters of the best-fit plane determined from the nine pixels in question. The goal of this least squares solution is to minimize the sum of the squared errors between the pixel intensity values and the resulting intensity value when using the plane equation (3).
Given a set of pixels
under consideration, a function is defined as follows: . This condition results in the following system of three simultaneous equations which can be solved for the bestfit plane of the given nine pixels:
Equation (5) The above matrix equation is solved for the parameters A, B, and C respectively which defines the best-fit plane of the nine pixels in question. With these parameters determined, a plane-fitted intensity value can be found for any of the pixels in question. This plane-fitted intensity value can then be used in comparison with the actual intensity value observed at the pixel in question in order to create a condition for a region growing procedure that will allow segmentation into surface regions.
In order for this condition to be used in the region growing process, an initial seed point must be determined. Therefore, an additional detail is added to the region growing processes above -the calculation of the ABC parameters for each inside point pixel determined in the inside point segmentation process. Therefore, when a pixel meets all criteria for inclusion in an inside point region, a best-fit plane for the pixel and its eight neighbors is calculated. Along with these ABC parameters for the plane equation, the minimum sum of squared errors (SSE) is also calculated for each inside point according to the following equation:
with x i and y i representing the coordinate of the pixel in question and I actual (x i ,y i ) representing the actual intensity value at the pixel in question. This equation represents the sum of squared error calculation, determined by the square of the difference between the calculated best-fit intensity value and the actual pixel intensity value at the specific coordinate being analyzed.
This SSE calculation allows an initial seed point to be determined for the surface growing algorithm. Once an inside point region is completely grown, all best-fit plane parameters and the corresponding SSE for any inside point and its eight neighbors has been determined using the matrix equation (6). The linked list that houses the inside point region pixels and this best-fit plane data is now sorted in ascending order according to the SSE, effectively determining the minimum SSE value for the inside point region in question. The inside point with the minimum SSE determined through the plane-fitting algorithm is added to the surface region. The reason for this minimum SSE being the condition for initial inclusion in the surface algorithm is that the inside point with the smallest SSE value has the most accurate best-fit plane associated with it and its eight neighbors. This allows a fairly accurate plane representation, and therefore the best chance for a smooth surface to be determined from pixels in a particular area, for example a face on a roof of a building. The goal is to grow surfaces in this particular algorithm, so the condition must allow for the best possible surfaces to be grown, which is accomplished by selecting the inside point with this minimum SSE value.
With the initial seed point selected, the conditional testing proceeds to the eight neighbors of this seed point. The testing first checks if the neighbor is an inside point, using the same conditions described above in the general region growing algorithm. If the neighbor is an inside point, then additional surface testing is performed. Using the ABC parameters of the initial seed point and the specific x and y coordinates of the neighbor in question, a calculated intensity value is determined using the plane equation (3). This calculated intensity value is then compared to the actual intensity value at this point, and the absolute value of the difference between the two values is calculated. If this difference is greater than a specific Δh threshold (usually 15-30 cm according to Zhang [1] ), then the pixel in question has an intensity that is not within the desired range and therefore is not a close enough fit to be included in the surface. However, if the calculated height difference is less than this Δh threshold, then the pixel is a close enough fit to the growing surface and is included in the surface region.
This testing continues until all eight neighbors have been tested for inclusion in this particular surface region. Once the eight neighbors have been completely checked, the pointer to the linked list containing the surface region values is incremented, and further testing is performed on the eight neighbors of this point. This process continues until the surface region has been fully traversed and no further growing can be performed, effectively ending the surface in question.
However, testing does not stop here. Every point within the inside point region must be added to a surface, and thus far, only the initial surface determined from the minimum SSE inside point has been accounted for. Therefore, the linked list pointer to the inside point region is incremented, finding the second smallest SSE value. If this point does not already belong to a surface, then this inside point becomes the new initial seed point, and surface growing begins again in the same fashion. This process continues until the end of the inside point region is reached, completing the traversal of the entire inside point region.
The fact that every inside point must belong to a surface results in surfaces of various sizes. If, for instance, an inside point on the outer edge of an inside point region becomes the seed point, the SSE may be the largest without regard to the plane-fitting algorithm's calculations on the "smoothness" of a group of pixels, and the surface grown would be extremely small because the surrounding pixels may not lie within the defined height threshold, possibly resulting in a surface of only one pixel if no other points fit the plane for that seed point. The process is rather complicated in this sense but works as desired for breaking up the larger inside point regions into sub-regions with conditions that define well defined surfaces. This further segmentation results in many more surfaces than inside point regions, separated by their relative fit to the region in general.
For the test area in Lenoir County, North Carolina shown in figure 3 , there were 218 inside point regions found and 1655 surfaces determined from these points. This much greater number of surfaces shows that the conditioning performed in this surface growing algorithm vigorously separate the inside point regions, resulting in a much finer conditioning and therefore datasets that share very similar properties which can be used to refine the building footprint algorithm drastically.
With the non-ground points now completely segmented into surfaces, the next step is simply an adjustment phase, hoping to help eliminate any errors that may have become prevalent in the previous steps. Because the segmentation above relies on thresholds in order to separate pixels and determine building footprints from these separated pixels, the quality of the building footprints and the amount of errors present in them are all determined by how closely the thresholds chosen are to the optimum threshold. After much testing, the values for the thresholds were chosen as indicated above, with a summary of the threshold values given in Table 3 below. All of the above threshold values have been explained in the sections above relative to their use in the algorithms they are associated with. However, there is another threshold mentioned here that has not been discussed yet, which relates to the merging of building surfaces which allows the final building footprints to be determined. In order to eliminate small patches of vegetation, a minimum building area must be determined. This minimum area threshold is set to 60 square meters, encompassing an area of three pixels on average, so any inside point regions having less than three pixels are neglected in the building separation process.
The building footprints for the particular area of Lenoir County, North Carolina considered thus far have been determined using the region growing algorithm. Inside points and boundary points were identified, and the resulting images have been displayed. However, the building footprints that have been identified are noisy since LIDAR data is inherently irregularly spaced. Therefore, some simplification of these noisy footprints must be performed. In the boundary point figure above, the edges of identified buildings have a zigzag quality and need to be simplified into a line that represents the true building boundary. Many simplification algorithms exist, but the Douglas-Peucker algorithm was implemented to simplify the noisy building footprints.
One such technique to develop a cleaner building footprint is the Douglas-Peucker algorithm, an algorithm developed by cartographers D.H. Douglas and T.K. Peucker in [7] . The DouglasPeucker algorithm allows for a simplification of any polyline by a recursive threshold technique.
The Douglas-Peucker algorithm operates on the boundary point regions identified by the conditioning described above. An initial simplification guess begins the process. For each boundary point region, the initial guess contains the first boundary pixel in the region and the last boundary pixel in the region connected together to form a line segment. The algorithm then determines whether the initial guess of a line segment is a good enough simplification for the polyline in question. This determination is performed based on a user-defined distance threshold.
Assuming the boundary point regions represent buildings, a line segment should never be a good enough simplification in this case. Therefore, further processing is necessary. The entire boundary point region is scanned, and the distance from each boundary pixel to the initial guess line segment is calculated. If the boundary point in question is within the outer limits of the initial guess line segment, a perpendicular distance is calculated. If the boundary point in question is on either side of the outer limits of the initial guess line segment, then the distance from either the left endpoint of the initial guess (the boundary point is on the left side of the initial guess line segment) or the right endpoint of the initial guess (the boundary point is on the right side of the initial guess line segment) is calculated. This distance calculation is measured in pixel units, with each side of a pixel considered to have unit length.
Once all boundary point pixels in the region have had distances from the initial guess line segment calculated, the boundary point pixel at which the maximum distance from the initial guess line segment is identified. This maximum distance value is compared to the distance threshold provided by the user, and if the maximum distance is less than this threshold value, then the initial guess line segment was an accurate enough simplification, and the new boundary point region contains only two points, the endpoints of the initial guess line segment. However, if the maximum distance is greater than the threshold value, then the initial guess line segment was not accurate enough, and further processing must be performed.
Assuming the initial guess line segment was not accurate enough, a new guess is determined. The pixel at which the distance threshold was exceeded becomes one of the endpoints of two new line segments representing the new guess. The first point of the boundary point region is connected to this maximum distance pixel to form a line segment, and this maximum distance pixel is connected to the last of the boundary point region to form another line segment. Each line segment is now assumed to be a new initial guess, and the process continues recursively until all boundary point calculated distances are less than the user-defined threshold. The figure below illustrates the process described above. The boundary points that remained after the Douglas-Peucker algorithm was performed are colored white above. The building footprint simplifications are obtained by connecting each consecutive Douglas boundary point together as line segments. While most buildings are identified fairly well, some of the smaller buildings were unsuccessfully simplified. This problem is due to the order that the boundary point regions were identified in. This problem is considered and eliminated in the next chapter containing improvements to the above algorithms.
With the algorithms completed and the building footprints successfully extracted, ways to possibly improve results were considered. It was noted that the possible improvements could exist when the boundary point regions were initially identified. The resulting footprints were extremely noisy, and therefore simplification seemed to be a difficult process. Therefore, a new way of defining boundary points was considered, and a new method of growing the boundary point regions was developed.
Boundary points were initially identified as any pixel in the difference image which had a value greater than a height threshold that also had at least one neighboring pixel identified as a ground pixel. This definition is somewhat incomplete, considering that any boundary point should also have a neighboring pixel identified as an inside point. Therefore, the definition of what constitutes a boundary point was made more strict, requiring the intensity of the difference image to be greater than the given height threshold, at least one neighboring pixel identified as a ground pixel, and at least one neighboring pixel identified as an inside point. It was believed that this extra condition would obviously result in fewer pixels to pass the boundary pixel test, and therefore result in less overall boundary pixels and a better overall simplification of the building footprints and a less noisy one as well. The figure below shows the identification of boundary footprints with this new definition. The above image with boundary pixels colored white shows a definite improvement in the reduction of noisy edges from the original boundary point figure identified in Chapter 3. A visual comparison between specific buildings in the two images will be given in the next chapter.
While there is a noticeable improvement in the identification of boundary points via the new definition, the footprints are still noisy enough to require further processing improvements. The Douglas-Peucker algorithm result illustrated above was not an acceptable simplification in all cases. It was determined that the Douglas-Peucker algorithm is extremely order intensive, in the sense that the order that the boundary points were placed into regions was extremely important in the determining a successful resulting simplification.
Boundary points were initially sorted through by scanning the difference image from the left portion of the image to the right part of the image in a top-down manner. The entire difference image was scanned for each boundary point region, a fairly inefficient process. Also, because of the left-to-right, top-down identification of the boundary point regions, the order of the boundary points could become scattered and not representative of buildings. Initially, this did not seem to be causing a problem, but once the Douglas-Peucker algorithm was implemented on these unordered boundary point regions, the result was clear. Because the Douglas-Peucker algorithm only checks distances of points to the initial guess on points that exist between the endpoints of the initial guess, ordering is extremely important. The Douglas-Peucker algorithm approximates the polyline obtained by connecting every boundary point in the order that they are added to the region.
Therefore, the goal was set to develop a sorting algorithm that would order boundary points in a way that, if each boundary point in the region were connected to the adjacent boundary points in the region, the result would be exactly as shown in the boundary point region figure above, that is they would represent the buildings that are to be simplified. The sorting was performed by using a variation on the region growing algorithm. First, the definition condition of a boundary point must be satisfied. Once a pixel satisfied this condition, the pixel was labeled as a boundary point and added to the region.
The next step is key to the development of a successful sort. The region growing algorithm described above would check every neighbor and add any pixels that satisfied the condition to the region. However, this could in some cases add more than neighbor to the region if more than one neighbor satisfied the condition. This would cause a problem in the ordering, as it is desired for the sort to represent a point-to-point connection of the boundaries of buildings. If more than one neighbor were added to the boundary point region for satisfying the boundary condition, then the corners of buildings would not contain a point-to-point connection. Therefore, once a seed point for the boundary point region was determined, the region growing algorithm begins. However, if two or more neighbors satisfy the boundary condition, only the first point is added to the list, and the region growing process continues in whatever direction this point was from the seed point. All boundary points obtained via the new definition by the original left-to-right, topdown scan are contained in this new region growing scan.
Once the sorting was complete and all boundary points were identified, the Douglas-Peucker algorithm was run on this new boundary point data. The image below shows the result of the Douglas-Peucker algorithm with this new modified region growing algorithm sort. The resulting image shows that the Douglas-Peucker algorithm with this modified sorting mechanism improves upon the previous top-down sorting run. A comparison of particular problem areas will be shown in the next chapter.
The resulting images above show that the algorithms for identifying building footprints successfully identify buildings. However, it was desired that there be data available to use in the future for possible comparison purposes, that is the ability to compare the obtained LIDAR building footprints with some other form of data to determine how accurate the LIDAR building footprints were.
In [1] Zhang performs a visual comparison to aerial photos by overlaying the obtained footprints over the aerial image to determine how accurate the LIDAR building footprints were. Contrary to Zhang, this particular satellite comparison algorithm attempts to provide a tool to compare the obtained LIDAR footprints to satellite imagery automatically.
The following image shown in Figure 13 was obtained by performing a screen capture of a satellite image of Lenoir County using Yahoo Maps. The image was resized to match the LIDAR data image as closely as possible (the LIDAR image has size 1201x401 and the satellite image has size 1203x420). The goal is to perform a region growing algorithm on the above satellite image. The condition to include a point in a particular region is the Euclidean distance between the associated RGB color intensities. This color region growing algorithm will provide regions that have similar color within a user-defined color threshold. The area of each region along with the average values of the x and y coordinates of the pixels in each region are calculated. The areas of the inside point and boundary point regions along with their average x and y coordinates were calculated with the region growing approach above. The claim is that with this data, buildings can be separated from vegetation based on color characteristics, which could prove helpful in removing false building identifications that could occur in dense forest areas. Also, if some areas and averages obtained from the inside/boundary region growing algorithm are similar to the areas and averages obtained from this color region growing algorithm, this algorithm could provide confirmation that the LIDAR building footprints were obtained accurately without visual inspection.
Because all points must be grown into corresponding color regions, there is no initial condition for selection of a seed point. The seed point is selected on the condition that it has not already been placed in a region, and it is then added to a region, and its color is identified according to its red, green, and blue components. Then, a condition is placed on its eight neighbors according to the following equation: 
A user-defined color threshold is provided. If the above calculated distance is less than the color threshold, then the pixel is considered close enough in color to the seed point and added to the region. This process continues until all 8 neighbors of the original seed point have been tested. Once this process is complete, the neighbors of the neighbors that were just added to the color region are tested based on their distance away from the seed point color. This process continues until all neighbors fail the color threshold test, effectively ending the region. Then, another region is started by selecting a point that has not yet been included in a region, and the process continues until every point has been assigned to a region.
The threshold provided to the algorithm for testing was based upon the maximum value of D color . The satellite image was normalized so that the minimum possible color value was 0 and the maximum possible color value was 1. Therefore, the maximum color difference would result in a D color value of approximately 1.7. The user-defined threshold therefore is limited to a value between 0 and 1.7 for this particular algorithm. The algorithm successfully provides the area of each region along with x coordinate and y coordinate averages for each region. This data is available for comparison purposes to the LIDAR obtained building regions.

In Chapter 4, the results of changing the original approach to the region growing algorithm were presented, and it was shown that there was visual confirmation of improvement in the identification of building footprints. However, a side-by-side comparison will further show the effect the above changes have had on the building footprint results.
The figure below shows an enlarged section of the Lenoir County study area, comparing the old boundary point definition to the new boundary point definition. This building footprint comparison clearly shows that redefining what a boundary point really is results in a less noisy and simplified footprint that more accurately represents the actual building boundaries. The extreme jagged nature of the footprint on the left is replaced by a fairly smooth representation on the right, considering that LIDAR data is irregularly spaced and therefore noisy to begin with.
Yet another comparison figure below shows a problematic area in the original Douglas-Peucker algorithm and its improved result. 
The goal of extracting building footprints from a LIDAR dataset was completed successfully.
Redefining what constitutes a building boundary point helped to improve the Douglas-Peucker algorithm's identification of building corners and improved the simplification process. The additional ability to grow regions based on the RGB color attributes in a satellite image allows for confirmation or disconfirmation of building footprints. This ability to automatically prove or disprove the validity of the extraction of building footprints improves upon simple visual inspection of aerial or satellite imagery for such purposes.
Even though the algorithms above successfully identify building footprints and provide a means for comparison with satellite imagery, future work is available. Actual comparisons could be performed by sorting the color regions and inside/boundary regions according to areas and attempting to find similarities between them, confirming building locations and possibly removing dense forestry areas misidentified as buildings. Also, with building footprints successfully identified, a next step would be to perform some three-dimensional modeling using the building footprints as a guide to the type of building model and the locations of these models.
Thanks to a combination of experimental assays and computational studies, knowledge about protein function has been steadily accumulating in public databases, where it is commonly described through the Gene Ontology 1 (GO). On the one hand, hypothesis-driven research has traditionally led to the thorough characterization of one or few proteins at a time. On the other hand, high-throughput technologies have opened the way to very large-scale exploratory surveys to study biological processes, identify binding partners, or establish subcellular locations. Meanwhile, some homology-based approaches for annotation transfers have developed enough to produce fairly confident results. The GO consortium, for instance, makes wide use of a semi-automated tool for phylogenetic analysis and functional inference 2 , and of mappings between protein domain families to GO terms that are valid for all their members 3 . Despite these multi-pronged efforts, however, a substantial fraction of deposited sequences still have no functional annotation at all, and the remaining ones usually lack assignments for at least one GO domain. When available, this information may not be at the finest level of detail possible, not only because of the way some electronically inferred annotations are generated, but also because of the varying levels of resolution characterizing experimental results 4, 5 . Finally, nature can still spring surprises: protein moonlighting demonstrates that novel functions can still await discovery even for well-researched proteins 6 . One way to fill in some of these gaps employs machine learning to examine diverse biological data types separately or in combination, and to provide functional hypotheses that complement homology-based annotation transfers [7] [8] [9] . In particular, over the years several supervised methods have been devised for function prediction from amino acid sequences, which are easier to collect than structural data or genome-wide measurements of gene expression or protein-protein interactions. GOStruct 10 and FANN-GO 11 , for instance, make GO term assignments by analysing the patterns of BLAST 12 E-values to experimentally characterized proteins using structured Support Vector Machines (SVM) and multioutput neural networks, respectively. Given the computational complexity of training classifiers with multiple correlated outputs, it is difficult to learn the relationship between the input features and the whole GO; the proponents have therefore adopted workarounds such as reducing the number of output terms and ensemble modelling. Rather than tackling this complex structured learning problem, other researchers have tested with success the possibility of converting it into a set of simpler binary classification tasks. This approach has recently allowed our group to train GO term-specific neural networks from features describing the results of profile-profile comparisons 13 . Alignment-derived features, such as similarity scores, sequence coverage and E-values, can help learn which sequence similarity patterns correlate with the conservation of individual annotations, thus allowing more effective control on homology-based annotation transfers. Complementary efforts have investigated the usefulness of biophysical attributes to make homology-free inferences, under the assumption that proteins with similar functions would have similar biological features despite the lack of significant sequence similarities. For example, the occurrence of signal peptides gives useful hints about protein subcellular location, and also limits the number of their molecular functions and of the biological processes they partake. The idea was first implemented in ProtFun, which is based on neural networks trained for the functional classification of protein sequences from similarities in amino acid composition, and content of signal peptides, trans-membrane helices, post-translationally modified residues as well as other biological features 14, 15 . The observation that the length and position of intrinsically disordered protein regions strongly correlates with some molecular activities and biological processes led to an expanded set of sequence-derived features, which FFPred scans through a library of GO term-specific SVMs to annotate protein chains 16, 17 . A more recent study has confirmed the effectiveness of this feature-based approach with the use of random forests for supervised learning 18 . In this paper, we describe the latest FFPred release, which updates the previous one with an extended vocabulary spanning all three GO domains, reflecting the increasing attention in cellular component annotations, as evidenced from recent experiments in the Critical Assessment of Functional Annotation initiative. We evaluate FFPred 3 prediction accuracy using two complementary approaches and describe its improvements over the previous version. Finally, we show how its predictions can help get a glimpse into the effects of alternative splicing on human protein function. The results show patterns of functional conservation and variation consistent with the presence or absence of particular biophysical attributes and with general biological knowledge.
Summary of tool updates. Thanks to the continued growth of annotation databases, the latest FFPred release features a GO term vocabulary, which spans all three GO domains for the first time and is almost twice the size of that in the previous update. Supplementary Data file 1 lists the 868 GO terms, for which a dedicated SVM is available along with the classification accuracy estimated from the validation experiments following the training procedures. The new release makes still use of SVMs, which are known to successfully handle imbalanced classification tasks-typical in computational biology-where it is extremely important to allow for error control and avoid overfitting to known observations. Subcellular localization prediction has been the focus of many previous studies, which mostly focused on the well-known compartments of eukaryotic cells-such as nucleus, cytosol, endoplasmic reticulum, Golgi apparatus, mitochondrion and other organelles. The newly added cellular component terms in FFPred 3 also include some of the numerous macromolecular complexes found in them. The extensions to the other two sub-ontologies provide more specific descriptions for functional categories previously covered, and they reflect the increasing body of knowledge in areas such as organelle localization, immune system and reproductive processes, response to stimuli and chromosome segregation. A small fraction of molecular function and biological process terms have been removed (Fig. 1a,b) , because they no longer occur in curated databases-mostly after the GO consortium made them obsolete. The majority of functional categories that have been retained can be predicted with negligible changes in expected accuracy-though some exceptions exist. As a consequence of the extended knowledge about human protein function since the last update, the patterns of biophysical attrbutes linked to terms such as sulfur compound metabolic process (GO:0006790), neurotrophin TRK receptor signaling pathway (GO:0048011), growth factor activity (GO:0008083) and protein kinase binding (GO:0019901) can be more easily identified and modelled. For other functions, such as calcium ion transport (GO:0006816), single organismal cell-cell adhesion (GO:0016337), ATPase activity (GO:0016887), and nuclease activity (GO:0004518), SVM performance has dropped, suggesting that their relationships to sequence-derived features are more complex than previously appreciated (Fig. 1c,d) .
The tool is designed with a focus on the function of human proteins, and so annotations curated for other organisms are never used for training. To learn effectively the relationship between biophysical attributes and GO terms, sufficiently large numbers of positive instances are needed, thus limiting the specificity of the functional categories that can be currently predicted. While this feature may not be desirable for all applications, its benefits to overcome some well-known limitations of homology-based annotation transfers have already been reported 15, 17 . Interestingly, previous work showed that the tool can also help annotate protein function for other eukaryotic organisms. The updated tool is publicly available on the web at http://bioinf.cs.ucl.ac.uk/ffpred.
Performance evaluation. The accuracy estimates in Supplementary Data file 1 are GO term-specific and point out the usefulness of FFPred 3 to prioritize human genes for downstream experimental screening when homology offers little or no help. To complement this analysis and gauge how well protein function as a whole can be predicted for such difficult cases, a timed experiment similar to the Critical Assessment of Functional Annotation challenge was conducted, by training a separate SVM library using the public databases released in November 2013. The resulting 597 classifiers were then used to assign GO terms to human proteins with no experimentally verified biological roles at that time, and their accuracy was finally measured against the UniProtKB-GOA data as of March 2016. For comparison purposes under difficult working conditions with limited or completely missing homology information, additional predictions were generated by a baseline method (Naïve), which ranks GO terms by prevalence in UniProtKB-GOA, and by a sequence similarity-based approach (BLAST), which can transfer annotations only from distantly related and experimentally characterized proteins as detailed in Methods. Other machine-learning based tools for GO term prediction from patterns of biological features could not be included in the study: ProtFun 15 has not been updated in a very long time and only covers a handful of currently valid GO terms, whereas ProFET 18 requires training from scratch classifiers for all GO categories of interest.
The precision-recall plots in Fig. 2 and the data in Table 1 provide graphical and numerical reports on the evaluation results for the three separate GO domains, according to standard practice in the field. At high levels of recall (i.e. above roughly 40% for molecular function and 20% for the other two sub-ontologies), FFPred 3 predictions achieve higher precision values than the baseline approaches do, and the maximum F-scores in Table 1 clearly back up this observation. However, the highest scoring predictions made by BLAST for subcellular locations and by Naïve for all sub-ontologies attain higher precision than the corresponding ones by FFPred 3. This result surprisingly suggests that these less sophisticated approaches are more useful than FFPred 3, when only a handful of assays can be run on each protein. Or are they?
It is widely accepted that an obvious pitfall of precision-recall analysis is the total disregard of how informative predictions are. The most confident GO term assignments made by Naïve for each test protein-GO:0043226 (binding), GO:0005488 (organelle) and GO:0009987 (cellular process)-are far from useful in cutting down the options for the design of experiments, indeed. Nonetheless, their very shallow nature guarantees that they will be eventually confirmed for most, if not all, proteins. Furthermore, comparing the precision values achieved by different methods and plotted against the same level of recall could be more ambiguous than it looks at first sight. If the recall is less than 1.0, the predictors are evaluated on non-identical sets of target proteins, which can even be disjoint. Another confounding aspect is the number of GO term predictions above a given decision threshold made for individual proteins: predictors based on high-throughput functional data aim at high recall and generally produce longer lists of assignments than those generated by methods based on homology transfers, which tend to achieve higher precision. Finally, correctly assigning the term t to distinct proteins p and q can pose prediction challenges of diverse nature, depending on how many proteins are annotated with t, and on how closely p and q follow the patterns of features used to build the classifiers-e.g. sequence similarity, domain architecture, biological attributes, gene expression and so on. Therefore, it is useful to look at method performance from a different angle, by considering both the accuracy and the informativeness of equal numbers of high scoring predictions for each target and sub-ontology-thus reducing the above biases and yielding results that can be interpreted more clearly and more easily by non-specialists, too.
The top row panels in Fig. 3 summarize prediction quality in terms of F 1 measure and the underlying precision and recall values are plotted in Figure S1 . It is quite clear that FFPred 3 is superior to both Naïve and BLAST across all three GO domains, because it achieves higher recall than the other predictors do, in combination with intermediate values of precision. The data also clearly confirm the expectation that Naïve predictions Table 1 . Performance comparison between FFPred 3 and the baseline prediction methods. For each method, the table reports the total numbers of true positives (TP), false positives (FP) and false negatives (FN) each method achieves at the decision threshold that maximises the F 1 score for each GO domain. NP is the number of proteins with at least one prediction with a confidence score greater than or equal to the corresponding threshold value, which is used to calculate the average precision of each method according to equation (4) in the main text. The average recall is calculated using equation (5) using the number of proteins with annotations in the GO domain under consideration, which can be found in the section "Methods". The latter two values are used to locate the full triangles in the precision-recall space shown in Fig. 2 . generally are highly precise, but not deep enough in the GO graph to outperform the other approaches in terms of recall. The results for the CC sub-ontology are an interesting exception: the low numbers of false negatives most likely arise from the relatively shorter distances between nodes associated with experimental annotations and nodes associated with the most frequent terms in UniProtKB-GOA. The plots also clearly illustrate the limits of homology-based transfers in such challenging situations. When the evolutionary distances from previously annotated proteins are large, only the most general functional aspects are retained (e.g. catalytic or transporter activity), while the finer details diverge (e.g. the nature of the substrates and the chemistry of the reactions), thus resulting in high numbers of both false positives and false negatives, and ultimately affecting negatively precision, recall and F-measure values.
As mentioned above, the design and implementation of FFPred 3 produced a list of GO terms with varying levels of detail, so it could be questioned how informative its predictions are and how helpful they can be to experimenters. In Fig. 3 , the plots in the bottom row show the average amount of useful information the highest scoring predictions would actually provide. For this purpose, the analysis only considers true positive predictions, which are not regarded as equally valuable as in the standard precision-recall analysis, however. They are rather weighted according to their information content, which estimates their specificity and informativeness from their occurrence in the UniProtKB/SwissProt database -so that more frequent functional categories are down-weighted, and vice versa. The plots undoubtedly prove that FFPred 3 correct predictions are consistently more specific than those generated by BLAST, which in turn are more specific than those made by Naïve. Therefore, despite the relatively low levels of term specificity, FFPred 3 can give useful hints to drive the experimental characterization of proteins, when routes alternative to homology transfers are needed. Table S1 gives some clear examples of how well FFPred 3 top-ranked predictions compare with the validated GO term assignments, which some proteins with no prior experimental functional data have recently acquired.
Insights into the functional consequences of alternative splicing in humans. Experimentally supported functional information for individual splice variants is generally scarce-only a handful of isoform-level GO term annotations have been reviewed and included in public databases. Even when some isoforms encoded by the same gene have been assayed, the data are still largely incomplete, because the experiments are usually focussed on a particular functional aspect. Within this active area of research, FFPred 3 and similar methods for protein function prediction have the opportunity to help investigate the functional ramifications of alternative splicing. Indeed, very often comparative sequence analysis can only suggest that the relatively small sequence changes between splice isoforms cause more or less pronounced structural and functional differences. In other words, this approach is typically unable to put forward more detailed testable hypotheses. This opens up the possibility that alternative splicing products may not encode biochemically active molecules, but rather constitute a reservoir for natural selection [19] [20] [21] -a conjecture that is also hard to verify. Notwithstanding, experimental evidence shows that the functional divergence between alternative splice variants can vary from subtle modulations of biochemical activities to completely antagonistic regulatory roles 22 . It is therefore interesting to investigate: i) which functional aspects tend to be more robust to splicing, and consequently conserved across splice variants of the same gene; and ii) whether canonical isoforms tend to be enriched in functions that are different from those over-represented in their alternative variants-see Methods for further details on the conservation and primarity scores.
To examine these patterns, a large-scale survey was carried out on 9,214 human proteins and their recorded splice variants using FFPred 3, under the assumption that eventually they all fulfil a physiological role in the cell. The analysis was restricted to the GO term predictions compatible with the manually curated assignments existing in UniProtKB/SwissProt, as to reduce the effects of spurious results on the biological interpretation. The summary data in Supplementary Data file 2 indicate that the GO terms used in this study display varying levels of conservation across sets of alternatively spliced transcripts, even though it is difficult to assess the statistical significance of the observed differences. Only five predicted (and admittedly broad) functions appear to be consistently assigned to all the variants of a gene, and very few of them are highly conserved, when the focus is on the most reliably predicted GO terms-i.e. the SVM Matthews correlation coefficient value is in the top 50% of the distribution recorded for the corresponding sub-ontology. For instance, only six of such terms annotate all isoforms of a gene in 90% or more of the cases examined. Therefore, despite the use of a consolidated set of predictions, the findings support the expectation that alternative splicing plays a role in diversifying the cellular functional repertoire. Support for this theory is strengthened by the differential associations of individual biological roles with canonical or alternative splice isoforms -as gauged by the GO term primarity scores. The Supplementary Data file 3 indicate that there are many more GO categories preferentially associated with principal variants than with alternative ones, partly because these analyses are restricted to predicted functions in line with available annotations in UniProtKB/SwissProt. Nevertheless, the GO terms with high primarity scores tend to represent more constitutive cellular functions, and those with negative scores appear to be mostly associated with larger sets of alternatively spliced genes or to be induced by changes in the environment or in the cellular conditions. As mentioned above, it is difficult to draw statistically sound conclusions from this initial study: identifying the canonical isoform of each gene is still an open question, and here a rather simple and pragmatic approach was taken just like in previous studies.
To emphasize the unique advantages that analyzing biological features can offer, Fig. 4 gives some insight into their relationship with some of the most conserved functions in each GO domain-see Methods for more details. The heatmap allows to link the over-and under-representation of specific biophysical attributes with the conservation of particular functional aspects. Similarly, Figs 5 and 6 show the extent of positive or negative correlation between sequence-derived feature groups and the GO terms that are preferentially associated with principal or alternative splice variants, respectively. The results generally reflect well-established trends between functional categories and the occurrence or lack of intrinsically disordered residues, transmembrane helices and signal peptides, and these interpretable patterns of association also apply to extended lists of GO terms, which are either expected to be predicted with lower confidence or to be less conserved ( Figures S2, S3 and S4 ).
The figures above provide a general overview across the whole human isoform proteome; however, the online server allows to study how alternative splicing is likely to preserve or abolish individual functions, by providing a detailed graphical view of the biological features detected in the input sequences. The following showcases how functional conservation and variation are consistent with the presence or absence of particular biophysical attributes and, most importantly, with independent biological knowledge.
Protein intrinsic disorder has long been linked to binding activities and regulatory processes in the light of both experimental and computational investigations [23] [24] [25] , and its enrichment in DNA binding proteins has a two-fold explanation. Basic leucine zipper (bZIP) and AT hook domains-both well known examples of disordered regions-are frequently found in many transcription factors and regulators, and some are conserved in their splice isoforms, too. The proto-oncogene c-Fos (UniProt accession P01100) and the high mobility group protein HMGI-C (UniProt accession P52926) include one bZIP and three AT hook motifs, respectively, which are all conserved across their known splice isoforms. Most often, however, DNA binding proteins usually include additional disordered segments that are not directly involved in DNA binding, but rather in the establishment of transient and highly specific protein-protein interactions for transactivation purposes. These regions are either maintained upon splicing-like the C-terminal domain of c-Fos-or swapped with other disordered segments to rewire cellular and signaling networks 26 . Signal peptides and transmembrane helices provide useful hints about protein subcellular localization and transmembrane transporter activities. They are unsurprisingly over and under-represented accordingly in those splice isoforms that need to retain the corresponding roles. The main and alternative isoforms of both the calcium-transporting ATPase type 2C member 1 (UniProt accession P98194) and of the 5-hydroxytryptamine receptor 3E (UniProt accession A5X5Y0) clearly illustrate this point. Alternative splicing hardly affects the transmembrane segments of these channels-only the isoform P98194-2 loses one helix-therefore they still localize in the membrane, and likely act as transporters of possibly different molecules.
Some associations-such as those between beta strands and several functional categories-may not look blatantly obvious, but brief scrutiny reveals their consistency with known biological facts. Nucleotides such as FAD, NAD and NADP are commonly bound by β α β super-secondary structure motifs, which usually occur in tandem in the Rossman fold where they can form relatively large beta sheets. Mitochondrial glutathione reductase (UniProt accession P00390) has five known isoforms that all preserve the nucleotide binding site, for instance, thus suggesting that the sequence differences do not impact this functional aspect, but something else. It is known that the isoform P00390-1 is indeed found in the mithocondrion, while isoform P00390-2 is cytoplasmatic, for instance. The enrichment of residues in beta strands in isoforms at the cell periphery is also easily explained by the abundance of immunoglobulin-like (Ig-like) domains, which fold into a beta sandwich structure and are involved in a wide range of functions such as cell surface recognition, immune response and muscle structural organization. Both the mucosal addressin cell adhesion molecule 1(UniProt accession Q13477) and the leukocyte Ig-like receptor subfamily A member 5 (UniProt accession A6NI73) exemplify well this over-representation. Both proteins include a signal peptide followed by two Ig-like domains, one transmembrane helix and a C-terminal cytosolic region. All recorded splicing events cause the removal or replacement of sequence regions outside the signal peptide and the core of the Ig-like domains, thus proving that the alternative variants are still secreted. Based on these examples, we would expect that this updated version of FFPred 3 will assist experimentalists narrow down the number of assays to functionally characterize individual variants of their own interest. In turn, those efforts will definitely stimulate further bio-curation work to interpret this information and make it available in machine-readable format. Initial computational studies have been carried out to advance this area of functional genomics using gene expression profile data 27, 28 ; their integration with other complementary sources of biological information that are tissue and condition-specific will undoubtedly be the focus of many more investigations in the near future. , and the following is a brief overview of the procedure, which is also graphically summarised in Supplementary Figure S5 . Candidate functional classes were identified based on the availability of sufficiently large and confident positive and negative instances, which were split into training (70%) and validation (30%) data. The training subset was then encoded through 258 sequence-derived features covering a range of 14 different functional and structural aspects; the resulting vectors were fed into SVM-Light 32 to perform feature selection and parameter optimization. Based on the number of training instances available for each function, the number of folds k ranges between 3 and 5, within the constraint that the partitions are equally sized. Feature selection was performed using a backward elimination approach, which involves first using all feature groups to estimate classification accuracy, and then iteratively testing if the removal of each feature group improves it. At each step, a grid search of the SVM hyper-parameter space was conducted with k-fold cross-validation to estimate SVM performance using the highest average Matthews correlation coefficient (MCC)
where TP is the number of proteins correctly labelled as positives (true positives); TN is the number of proteins correctly labelled as negatives (true negatives); FP is the number of misclassified negative cases (false positives); and FN is the number of misclassified positive instances (false negatives). These parameters were used to build a binary classifier from all training examples, the performance of which was tested against the proteins in the unseen validation set. Only GO terms corresponding to predictors achieving MCC ≥ 0.05 were retained, and for them FFPred 3 makes predictions with SVMs trained on the joint training and validation sets to make the most of available annotations. Platt scaling 33 is applied to estimate the posterior probability that the input protein performs the function associated with a SVM given the raw output score.
Datasets and procedures for performance evaluation. Only for the purpose of estimating prediction accuracy, an intermediate version of the SVM library was trained using the GO OBO flat file released on 2013-11-05, the UniProt-GOA gene association file for human submitted to the GO Consortium on 2013-10-28, UniProtKB and UniRef90 release 2013_10. The training procedures outlined above produced a vocabulary consisting of 400 terms in the biological process (BP) domain, 108 in the molecular function (MF) domain, and 89 in the cellular component (CC) domain, which allowed to make predictions for all human protein sequences released as targets of the second Critical Assessment of Functional Annotation challenge 34 . The benchmark set was collated from the UniProt-GOA gene association file, by selecting those human proteins that received GO term assignments supported by evidence code EXP, IDA, IMP, IGI, IEP, TAS or IC between 2014-01-20 (end of the CAFA2 prediction stage) and 2016-03-14 (the database release date). Annotations to the term "protein binding" (GO:0005515) were discarded because they convey limited functional information unless the context is quoted (e.g. where and when the activity takes place and the requirement or absence of other molecules), and because these qualifiers are neglected by current function prediction evaluation protocols. This resulted in 3,881 annotations for 1,365 proteins in total-602 MF annotations for 454 proteins, 1,802 BP annotations for 661 proteins, and 1,477 CC annotations for 991 proteins.
Prediction accuracy was measured separately for each GO domain by precision-recall analysis as in similar studies following the lead of the CAFA experiments 34, 35 . For each protein x in the benchmark set and decision threshold v, the set of predicted terms P x,v was built by collecting all terms with confidence scores greater than or equal to v and their ancestors in GO linked by "is a" relationships and different from the root; the set of reference terms R x was generated in a similar way by up-propagating the validated annotations for x. These sets were used to calculate the number of true positives tp x,v , false positives fp x,v and false negatives fn x,v respectively as the sizes of the intersection P x,v ∩ R x , of the set difference P x,v \R x and of the set difference R x \P x,v . These data were combined into precision
Scientific RepoRts | 6:31865 | DOI: 10.1038/srep31865
and then averaged across the test set using the formulas
where m is the number of target proteins in the GO domain at hand and n is the number of those with at least one prediction scoring at least v. Finally, the average F-measure for the threshold v was calculated as
that is by taking the harmonic mean of p v and r v .
A complementary evaluation of function prediction quality was carried out on the top-ranked predictions for each target t and GO domain d. To this end, after ranking based on confidence scores, the initial predictions were trimmed to the same length l ∈ {1, 2, 3, 4, 5, n t,d }, where n t,d is the number of experimental annotations for t in the sub-ontology d. To handle ties in confidence scores, first 1,000 prediction lists of the desired length l were randomly sampled without replacement for each protein. Then, the average values of precision, recall, F-measure were calculated for each list of top l predictions; finally the average of such statistics over all replicates were analysed.
Along with the above statistics, the average sum of true positive information content was also calculated from all replicates. The information content of a GO term t was estimated in a Bayesian framework as proposed by Clark and Radivojac 36 using the equation
where P(t) represents the set of parent nodes of t, and the function N(·) returns for any set of GO terms the number of human proteins annotated in UniProtKB-SwissProt with evidence code EXP, IDA, IMP, IGI, IEP, TAS or IC. The Supplementary Data file 4 includes the complete sets of reference annotations and of predictions used in these performance comparison experiments.
Baseline function prediction methods. Naïve predictions were generated based on the frequency of the GO term annotations for human sequences recorded in UniProt-GOA as of 2013-10-28. To this end, initial counts were obtained for all GO terms except "protein binding" (GO:0005515) supported by the evidence codes EXP, IDA, IPI, IMP, IGI, IEP, IC and TAS. The data were then propagated following "is a" links in the GO released on 2013-11-05, and finally scaled between 0 and 1 for each domain separately, by dividing the final counts by the number of occurrences of the root node and rounding the result to three decimals like FFPred does. The resulting 6,504 pairs of GO terms (469 for CC, 1,268 for MF and 4,767 for BP) and scores were used to annotate all proteins in the benchmark set.
BLAST predictions were obtained by first collecting all BLAST 12 hits in the UniRef90 31 sequence database released in October 2013 with an E-value greater than 1e-03. Then the annotations in UniProtKB release 2013_10 supported by evidence code EXP, IDA, IPI, IMP, IGI, IEP, IC and TAS were transferred to the target sequences. GO term confidence scores were calculated by dividing the local alignment sequence identity by 100. When multiple BLAST hits were annotated with the same function, the highest score was retained.
Annotation and functional analysis of human splice variants. The sequences of the human isoform proteome and the classification between main and alternative splice variants were obtained from the release 2015_03 of UniProtKB/SwissProt and the accompanying "varsplic" file. Individual isoforms were discarded if a) their amino acid sequence is unknown; or b) it is shorter than 15 amino acids; or c) it is longer than 1500 amino acids, or d) it includes non-standard amino acid symbols; or e) it is recorded in a separate database entry due to substantial differences from the canonical sequence. When these filters led to the exclusion of main variants, associated alternative sequences were removed from the dataset as well. This initial screening yielded 28,310 splice variants for 9,267 UniProtKB/SwissProt entries.
FFPred 3 was run to make isoform-specific GO term predictions, which were then screened for consistency with the UniProtKB/SwissProt data. Only functional classes that were either explicitly assigned by the curators or implied by the GO data released on 2015-02-27 were retained. Removal of principal isoforms at this stage also led to the elimination of all related alternative variants, hence producing a final dataset P as consisting of 28,142 sequences for 9,214 UniProtKB/SwissProt entries.
Patterns of conservation and variation were analysed for all GO terms predicted to the splice isoforms of at least 20 distinct UniProtKB/SwissProt entries. For each functional class G, the survey aimed at quantifying its tendency to be conserved upon splicing, as well as its preference for principal rather than alternative splice variants. The average conservation of G across splice variants of the same gene was measured as the ratio between the number of UniProtKB/SwissProt entries where G was assigned to all isoforms, and the number of database records where it was predicted for at least one isoform. The primarity of G-that is its enrichment among main isoforms rather than alternative variants-was taken as where m G and a G are respectively the numbers of main and alternative isoforms annotated with G, while n = 9214 is the number of genes in the dataset, and m = 28142 is the total number of splice variants. Therefore, δ G > 0 if G is preferentially found among canonical isoform predictions; δ G < 0 if G is assigned more often to alternative variants than to main ones; and δ G = 0 if G is equally associated with the two sets of protein products.
To investigate further and interpret the conservation of each GO term g in the light of current biological knowledge, the biological attributes associated with the set of canonical and splicing variants annotated with g (V g ) were compared with those previously observed in the positive training set (T g ) of the corresponding SVM. In particular, for each sequence-derived feature f, the median value m g,f,T observed during the training process was compared to m g,f,V -the median value in V g -by first mapping the latter to the lowest percentile p g f T , , seen in T g and then by calculating
Therefore, E g,f = 0 if the two median values are identical, E g,f > 0 if on average f takes higher values in V g than in T g , while E g,f < 0 if f typically has lower values in V g than T g . Similarly, the association between a feature f and a functional class g that is over-represented in either set of canonical or alternative protein isoforms was estimated using Pearson's correlation coefficient between the values f takes on V g and the correspondin g SVM output scores.
In science policy it was assumed into the 1990s that society can benefit most from a science which pursues research at a high level. Correspondingly, indicators were (and are) used in scientometrics, such as citation counts, which measure the impact of research on science itself. Since the 1990s a trend can be observed in science policy no longer to assume that society benefits from a science pursued at a high level (Bornmann, 2012 (Bornmann, , 2013 . It is now expected that the benefit for society be demonstrated. Thus, for example, organizations which support research (such as, for example, the US National Science Foundation) now expect that supported projects lead to an outcome which is of interest not solely to science. For these organizations the consequence for the peer review procedure is that not only the possible scientific yield of the project has to be assessed, but also the returns for other sections of society.
These days, scientific work is not assessed solely on the basis of the peer review procedure, but also with indicators. A good example of these quantitative assessments is university ranking (Hazelkorn, 2011) . The most important indicators in this connection (not only with university ranking) are bibliometric indicators based on publications and their citations (Vinkler, 2010) . The impact of research is generally measured with citations. Since the impact of one publication on another publication is measured here, citations measure the impact of research on research itself. Citations allow a determination as to whether research (for example in institutions or countries) is being pursued at the highest level on average or not. But citations cannot be used to measure the impact of research on other sections of society. This is why scientometrics has taken up the wish in science policy to measure the impact of research beyond the confines of science, and is seeking new possibilities for impact measurement (Bornmann, 2014) . With societal impact assessments the (1) social, (2) cultural, (3) environmental and (4) economic returns (impact and effects) from results (research output) or products (research outcome) of publicly funded research are measured (Bornmann, 2013) . Currently the most favored procedure for measuring societal impact involves case studies, which, however, are seen as too time-consuming and therefore less practicable.
An attractive possibility for measuring societal impact is seen in altmetrics (short for alternative metrics) (Mohammadi & Thelwall, 2014) . "Altmetrics refers to data sources, tools, and metrics (other than citations) that provide potentially relevant information on the impact of scientific outputs (e.g., the number of times a publication has been tweeted, shared on Facebook, or read in Mendeley). Altmetrics opens the door to a broader interpretation of the concept of impact and to more diverse forms of impact analysis" (Waltman & Costas, 2014, p. 433 ). An overview of various altmetrics may be obtained from Priem and Hemminger (2010) . Twitter (www.twitter.com), for example, is the best known microblogging application. This application allows the user to post short messages (tweets) of up to 140
characters. "These tweets can be categorized, shared, sent directly to other users and linked to websites or scientific papers … Currently there are more than 200 million active Twitter users who post over 400 million tweets per day" (Darling, Shiffman, Côté, & Drew, 2013) . Priem and Costello (2010) define tweets as Twitter citations if they contain a direct or indirect link to a peer-reviewed scholarly article. These Twitter citations can be counted and assessed as an alternative metric for papers.
There are already a number of studies concerning altmetrics. An overview of these studies can be found in Bar-Ilan, Shema, and Thelwall (2014) , Haustein (2014), and Priem (2014) . Many of these studies have measured the correlation between citations and altmetrics.
Since the correlations were often at a moderate level, the results are difficult to interpret: Both metrics seem to measure something similar but not identical. The studies published so far cannot yet provide a satisfactory answer to the question whether altmetrics is appropriate for the measurement of societal impact or not. That is the reason for this investigation of the question.
In January 2002, a new type of peer-review system has been launched, in which about 5000 Faculty members are asked "to identify, evaluate and comment on the most interesting papers they read for themselves each month -regardless of the journal in which they appear" (Wets, Weedon, & Velterop, 2003, p. 251) . What is known as the Faculty of 1000 (F1000) peer review system is accordingly not an ex-ante assessment of manuscripts provided for publication in a journal, but an ex-post assessment of papers which have already been published in journals. The Faculty members also attach tags to the papers indicating their relevance for science (e.g. "new finding"), but which can also serve other purposes. One example of the tags which the members can attach is "good for teaching". Papers can be marked in this way if they represent a key paper in a field, are well written, provide a good overview of a topic, and/or are well suited as literature for students. Papers marked with this tag can be expected to have an impact beyond science itself (that means societal impact), unlike papers without this tag. If altmetrics indicate a greater impact for papers with this tag than those without, this would suggest that altmetrics measure societal impact.
This study is essentially based on a dataset with papers (and their evaluations and tags from Faculty members) extracted from F1000 (see also Mohammadi & Thelwall, 2013) . This dataset was extended with further data -bibliometric (e.g. citation counts) and altmetric (e.g.
). There follows in the next sections a comparison of altmetric counts with citation counts, to investigate the differences between the two metrics in relation to tags and recommendations.

F1000 is a post-publication peer review system of the biomedical literature (papers from medical and biological journals). This service is part of the Science Navigation Group, a group of independent companies that publish and develop information services for the professional biomedical community and the consumer market. F1000 Biology was launched in 2002 and F1000 Medicine in 2006. The two services were merged in 2009 today constitute the F1000 database. Papers for F1000 are selected by a peer-nominated global "Faculty" of leading scientists and clinicians who then rate them and explain their importance (F1000, 2012) . This means that only a restricted set of papers from the medical and biological journals covered is reviewed, and most of the papers are actually not (Kreiman & Maunsell, 2011; Wouters & Costas, 2012) .
The Faculty nowadays numbers more than 5,000 experts worldwide, assisted by 5,000
associates, which are organized into more than 40 subjects (which are further subdivided into over 300 sections). On average, 1,500 new recommendations are contributed by the Faculty each month (F1000, 2012) . Faculty members can choose and evaluate any paper that interests them; however, "the great majority pick papers published within the past month, including advance online papers, meaning that users can be made aware of important papers rapidly" (Wets, et al., 2003, p. 254) . Although many papers published in popular and high-profile journals (e.g. Nature, New England Journal of Medicine, Science) are evaluated, 85% of the papers selected come from specialized or less well-known journals (Wouters & Costas, 2012) .
"Less than 18 months since Faculty of 1000 was launched, the reaction from scientists has been such that two-thirds of top institutions worldwide already subscribe, and it was the recipient of the Association of Learned and Professional Society Publishers (ALPSP) award for Publishing Innovation in 2002 (http://www.alpsp.org/about.htm)" (Wets, et al., 2003, p. 249 ). The F1000 data base is regarded as a significant aid for scientists seeking the most relevant papers in their subject area: "The aim of Faculty of 1000 is not to provide an evaluation for all papers, as this would simply exacerbate the 'noise', but to take advantage of electronic developments to create the optimal human filter for effectively reducing the noise," (Wets, et al., 2003, p. 253) .
The papers selected for F1000 are rated by the members as "Good," "Very good" or "Exceptional" which is equivalent to scores of 1, 2, or 3, respectively. In many cases a paper is assessed not just by one member but by several. The FFa (F1000 Article Factor), given as a total score in the F1000 database, is calculated from the different recommendations for a publication. Besides making recommendations, Faculty members also tag publications with classifications, as for example (see http://f1000.com/prime/my/about/evaluating):
investigates the effects of an intervention (but neither randomized nor controlled) in human subjects. The classifications, recommendations and bibliographic information for publications form the fully searchable F1000 database containing more than 100,000 records (end of 2013). Overall, the F1000 database is regarded simply as an aid for scientists to receive pointers to the most relevant papers in their subject area, but also as an important tool for research evaluation purposes. So, for example, Wouters and Costas (2012) write that "the data and indicators provided by F1000 are without doubt rich and valuable, and the tool has a strong potential for research evaluation, being in fact a good complement to alternative metrics for research assessments at different levels (papers, individuals, journals, etc.)" (p.
14).
In January 2014, F1000 provided me with data on all recommendations (and classifications) made and the bibliographic information for the corresponding papers in their system (n=149,227 records (Bornmann, in press ). In addition, Altmetric can only reliably provide data for papers published after 2011. For this reason the dataset is reduced in what follows -where altmetric data is statistically evaluated -to papers from the period after 2011.
Since Altmetric could not add altmetric data for all the papers, but only for 69%, the question arises how the remaining papers should be treated in the statistical evaluation. One could perhaps argue that these papers should be set to zero counts for all altmetrics.
Apparently, not even a mention is available for these papers in any of the social media platforms. On the other hand, the dataset from Altmetric should have at least a mention for all papers under F1000 since the data to which Altmetric attaches altmetric data originate from F1000, and F1000 recommendations are also evaluated by Altmetric. But since this is not the case in the current dataset, the 31% of the papers for which Altmetric was not able to supply any altmetric data were recorded as missing and excluded from the statistical analysis. As the following analyses shows, only a few papers published after 2011 are affected by this problem -that is those papers which were used in the current study for the analysis of alternative metrics. the fraction of papers with 0 counts is also provided. As the results show, the counts in the altmetrics are generally low. For example, the papers in the dataset have an average of 0.46 blogs mentioning a paper with a minimum of 0 and a maximum of 111. An important reason for the generally low averages in the altmetrics is the large fraction with 0 counts: For almost all altmetrics in the table, (significantly) more than two thirds of the papers show zero counts.
Since the total altmetric counts and the unique tweeters mentioning papers (Twitter counts) are the only altmetrics with significantly higher average counts and significantly lower share of 0 counts than with the other altmetrics, these are the only ones included in the following statistical analysis.
On the one hand, the analysis of Twitter counts in this study has the further advantage that the data evaluated originates only from one service (which represents the standard in the area of microblogging). This facilitates the collection of data for Altmetric and ensures the reliability of the counts. Blogs, for example, do not have this advantage: "While most other Web 2.0 applications are closely identified with a few 'name-brand' services (for instance, Twitter for microblogging and delicious for social bookmarking), blogging is not" (Priem & Hemminger, 2010) . Blogs are distributed over the whole Web, and there is no standard service aggregating these blogs. On the other hand, Twitter is particularly in use by people who operate outside the area of science: Although Twitter is one of the most often used social media platforms, it is generally assumed that only few scientists actually tweet (Darling, et al., 2013; Mahrt, Weller, & Peters, 2012) .
The statistical software package Stata 13.1 (http://www.stata.com/) is used for this study; in particular, the Stata commands nbreg, margins, and coefplot are used.
A series of regression models has been estimated. The outcome variables (number of citations, number of tweeds, number of total altmetric counts) in the models are count variables. They indicate "how many times something has happened" (Long & Freese, 2006, p. 350) . The Poisson distribution is often used to model information on counts. However, this distribution rarely fits in the statistical analysis of bibliometric and altmetric data, due to overdispersion. "That is, the [Poisson] model underfits the amount of dispersion in the outcome" (Long & Freese, 2006, p. 372) . Since the standard model to account for overdispersion is the negative binomial (Hausman, Hall, & Griliches, 1984) , negative binomial regression models are calculated in the present study (Hilbe, 2007) .
The violation of the assumption of independent observations by including several different items of information about the same paper (such as several F1000 recommendation scores or several subject categories associated with a paper) is considered by using the cluster option in Stata (StataCorp., 2013) . This option specifies that the information items are independent across papers but are not necessarily independent within the same paper (Hosmer & Lemeshow, 2000, section 8.3 ).
The publication years of the papers were included in the models predicting different counts (e.g. citations) as exposure time (Long & Freese, 2006, pp. 370-372) . The exposure option provided in Stata takes into account the time that a paper is available for citations or other mentions (e.g. in Twitter).
In this study, adjusted predictions are used to make the results easy to understand and interpret. Such predictions are referred to as margins, predictive margins, or adjusted predictions (Bornmann & Williams, 2013; Williams, 2012; Williams & Bornmann, in preparation) . The predictions allow a determination of the meaning of the empirical results which goes beyond the statistical significance test. Whereas the regression models illustrate which effects are statistically significant and what the direction of the effects is, adjusted predictions can provide us a practical feel for the substantive significance of the findings. Table 2 shows the distribution of the tags over the records in the dataset (in which papers appear more than once) and total tag mentions ("total" line). It is very clear that the tags are applied very differently: Whereas, for example, "new finding" makes up about half of the tag mentions, for "review" it is only about 2%. In order to be able to make a reliable statement about the validity of the altmetrics, the following statistical analysis does not include all tags, but only those with more than 5% of mentions or allocated to more than 10% of records. What expectations are there in the current study in relation to the connection between altmetrics counts or citation counts and the categorization of papers with the five selected tags (which are described in further detail in section 2.1)? In connection with "new finding", "confirmation" and "interesting hypothesis", it is expected that the citation counts for such papers would be higher for those where a Faculty member has used this tag than for those where this did not happen. Since these tags particularly relate to aspects which are relevant in a scientific context, it would not be expected that the altmetric tags show this difference between tagged and untagged papers. In contrast to this, we could expect that papers tagged with "good for teaching" would (also) be interesting for a group of people outside science or research. These are papers which are well written, provide an overview of a topic and are well suited for teaching. Therefore, a higher altmetrics count would be expected for papers with this tag than for papers without it. The "technical advance" tag is used on papers that present a new technique or tool (whether that's a lab technique/tool or a clinical one) that make an advance on an existing technique. The tag can be used both for research papers and outside,

i.e. clinical or fieldwork. Thus, a similar effect of this tag on altmetric or citation counts would be expected in the statistical analysis.
In order to ascertain how total altmetric counts, Twitter counts, and citation counts differ with differently tagged papers, three regression models were calculated with the three counts as dependent variables and the tags as independent variables (see Table 3 ). Each model includes the individual recommendation scores of the Faculty members alongside the tags.
This enables us to ascertain the influence of the tags on the different counts, controlling for the effect of the recommendations. Since the recommendations reflect the quality of the papers, the results of the tags are adjusted for the quality of the papers. In other words: the different results for the tags can hardly be traced back to the differing quality of the papers. As Table 3 shows, the three models involve papers from different years: The models with altmetrics as dependent variables can only take into account papers published after 2011 (see above). The model with citation counts as dependent variable only involves papers published before 2011. Since the citation window for the papers extends from the publication year to the end of 2012 in this study, the citation window for papers published after 2011 is too narrow to measure the citation impact reliably (Wang, 2013) . The inclusion of papers from before 2011 leads, however, to a shortage of records tagged with "good for teaching" (0.1%, n=181) (see Table 3 ). The "good for teaching" tag is relatively new for F1000Prime; it was introduced only in 2011. Therefore, it cannot be included in the analysis of the citations. The results of the regression models are shown in Table 4 . These are the test statistics depend on the models with all independent variables, they are calculated for the different tags under control of the recommendation scores (and adjusted for quality). In all the models in Table 4 , a statistically significant result is seen for the recommendation scores of the Faculty members. Since the coefficients have a positive sign, higher total altmetric counts, Twitter counts, and citation counts are to be expected with better scores. Thus the quality of the papers does not only play an important role for the citation impact, but also for the altmetric counts. The relation between the different recommendation scores and the predicted numbers of counts is presented in Figure 1 : It is very clear that citation counts in particular separate the differently evaluated papers.
In the two models for the altmetrics (models 1 and 2), the coefficient for "good for teaching" is statistically significant. Correspondingly, Figure 2 and Figure 3 show higher predicted numbers of counts for papers where this tag is set, than for those papers where this was not the case. For example, we can expect a paper with this tag to have around seven
Twitter citations more than one without -if the paper is rated as "very good" by Faculty members and has no other tags. These results for "good for teaching" correspond to the expectations (see above) and indicate that altmetric data (and especially tweets) can indicate papers which are of interest outside of science.
Unfortunately, the "good for teaching" tag could not be included in the model for the citation counts (see above). Therefore, there is a lack of results which could be included in a comparison. Model 3 for the citation counts provides two statistically significant results (see Table 4 ): Citations are particularly to be expected if a paper presents original data, models or hypotheses (tag: "new finding") or introduces a new practical/ theoretical technique (tag:
"technical advance"). Whereas the results for "new finding" correspond with the expectations (see above), the results for "technical advance" can clarify the unspecific expectations formulated above: Papers tagged with "technical advance" seem to involve techniques with relevance for research rather than for areas outside research. For both tags, Figure 4 shows a clear citation impact advantage for papers with this tag than for those without.
The results in Figure 4 also show that confirmatory results (tag: "confirmation") and interesting hypotheses (tag: "hypothesis") can hardly be associated with higher or lower citation counts (against the expectation). 
If altmetric data is to be used for the measurement of societal impact in the evaluation of research, the question arises of its normalization (Torres-Salinas, Cabezas-Clavijo, & Jimenez-Contreras, 2013). With citation counts, there is a consensus in the bibliometric community that the impact of papers should be normalized in relation to the subject category (the field) and the publication year (the time) (Bornmann, Leydesdorff, & Wang, 2013) . Is this also necessary for the Twitter counts and total altmetric counts investigated here? The following statistical analysis will focus on the question of taking into account the subject categories, since, for the papers in this study, only the publication year is available and not the publication month or day. Unlike citations which arise only a long time after the appearance of a paper, altmetric data generally appears relatively quickly (Priem, Taraborelli, Groth, & Neylon, 2010; Rodgers & Barbrow, 2013) . The temporal aspect in the normalization of altmetric data can therefore only be clarified with data on the month or day level. Other empirical studies have already indicated subject area differences with altmetric data. Thus, for example Loach (2014) shows from Twitter counts in the Altmetric database "that medical articles receive a disproportionate amount of online attention. In fact, 60% of tracked tweets from the last week pointed to articles from journals publishing Medical and Health Science research. Interestingly, 63% of these were directed to articles from journals tagged as relating to Clinical Medicine or Public Health specifically." An important disadvantage of the studies which have so far appeared on subject area difference is that the quality of the papers is not controlled in the analyses of the subject area differences. Therefore, it is not known whether the differences between the subject areas depend on aspects specific to the subject or quality differences between the papers. Thus, medical papers could receive more online attention just because these papers are generally of a higher quality than papers from other subject categories. The quality of the papers should therefore be controlled in the analysis of subject area differences.
In the current study, WoS subject categories are used to determine subject area differences in the counts. Most bibliometric studies use these categories, which, however, are not applied on the level of individual papers, but on the level of journals: A set of journals is combined in a subject category by Thomson Reuters. Table 5 shows the distribution of the papers over the subject categories published after 2011 in the dataset. Since the evaluation of the altmetric data could only include papers after 2011, the table refers to this part of the data.
As the table shows, around 14% of the category classifications relate to "multidisciplinary sciences" -that corresponds to around 20% of the papers. This journal set includes the two multi-disciplinary journals Nature and Science. Around 13% of the papers in the dataset were published in a journal belonging to the category "cell biology". The subject categories in Table 5 are included as independent variables in two negative binomial regression models, where one includes the total altmetrics counts and the other the Twitter counts as dependent variable. With the help of this model, the predicted numbers of counts could be determined for the individual subject categories, where the quality of the papers is controlled for by the individual recommendation scores (which are included as mean scores per paper in the model alongside subject categories). The model also takes into account that the papers appeared in different publication years and have different numbers of subject categories. The results of the regression models will not be presented in
table form in what follows, since the tables are very extensive given the large number of different subject categories. But the predicted numbers of counts with 95% confidence intervals for the individual subject categories are presented as the results of the models (see Figure 5 and Figure 6 ). Figure 5 . Predicted numbers of total altmetric counts with 95% confidence intervals. The graphic is based on 13,278 papers published after 2011 with 18,254 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, in which the quality of the papers is controlled by the individual recommendation scores of the Faculty members. Predicted number of count Figure 6 . Predicted numbers of Twitter counts with 95% confidence intervals. The graphic is based on 13,278 papers published after 2011 with 18,254 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, where the quality of the papers is controlled for with the individual recommendation scores of the Faculty members.
In order to determine whether the predicted numbers of counts for the subject categories with the altmetric data follows a similar (or different) pattern to that with the citation counts, Figure 7 shows the predicted numbers of citation counts with 95% confidence intervals. As with the evaluations described in section 3.2, these predicted numbers arise from a negative binomial regression model based on papers from the time period before (and not after) 2011. Even if the analysis underlying Figure 7 only took into account subject categories with more than 100 instances in the dataset (similarly to Figure 5 and Figure 6 ), the subject Predicted number of count categories in Figure 7 do not coincide with those shown in Figure 5 and Figure 6 . The reason for the discrepancies lies in the different publication years involved. Figure 7 . Predicted numbers of citation counts with 95% confidence intervals. The graphic is based on 42,858 papers published before 2011 with 60,468 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, where the quality of the papers is controlled for with the individual recommendation scores of the Faculty members.
As the results in the three figures show, the predicted numbers of counts for the altmetric data on the one hand is very different from that for the bibliometric data, on the other. With the altmetric data (total altmetric counts and Twitter counts), the predicted Predicted number of count number of counts is relatively low for almost all subject categories. Only for "biology", "ecology", "evolutionary biology", "multidisciplinary sciences" and especially for "medicine, general & internal" are they higher. Particularly in the journals of the subject category "medicine, general & internal" an especially large number of contributions seem to be published which are not only of scientific interest.
The predicted numbers of citation counts shown in Figure 7 , shows a different pattern from the predicted numbers of altmetrics data. In Figure 7 there are quite a few subject categories which stand out with relatively high counts (and many categories with hardly any), but the individual subject categories are distributed over a large bandwidth of different predicted numbers of counts. This difference between the altmetric data and citations in the distribution over the predicted numbers of counts is visualized in Figure 8 . Box plots are used to represent the distribution of the counts which are visualized in Figure 5 , Figure 6 and Figure 7 . In Figure 8 it can clearly be seen that the predicted numbers of citation counts are distributed over a greater area than the predicted numbers of total altmetric counts and Twitter counts. Correspondingly, the citation counts show a significantly greater standard deviation than the altmetric data (see Figure 8 ). Twitter count (n=45) Citation count (n=58) Figure 8 . Distribution of the predicted number of total altmetrics counts, Twitter counts and citation counts. Whereas the standard deviations for the total altmetrics counts and Twitter counts are std=11.3 and std=9.5, for the citation counts it is std=23.5.
The results for the differences in the distribution of the predicted numbers of counts between the altmetric and the bibliometric data indicate that the subject categories have a different meaning in this area. Whereas the evaluation of the bibliometric data indicates different citation practices in the fields (which should be taken into account with a normalization), the evaluation of the altmetric data gives the impression that only papers from a few specific subject areas receive a larger number of mentions. With the altmetric data, it does not therefore appear a matter of different habits in the mentioning of papers between the fields, but of a particularly large (or small) interest among people outside science for papers from a few specific areas (or for the bulk of scientific papers). Therefore, a normalization of the counts on the level of subject categories (journal sets) is not regarded as reasonable.
Can altmetric data be validly used for the measurement of societal impact? The current study has sought to answer this question with a comprehensive dataset from very disparate sources (F1000, Altmetric, and an in-house database based on WoS). In the F1000 peer review system, experts attach particular tags to papers which indicate whether a paper could be of interest for science or rather for other segments of society. In this study, these tags were
used in an attempt to analyze the validity of altmetric data. A "good for teaching" tag indicates that a paper could be of interest outside the science. If papers with this tag receive more altmetric counts than those without, this would be an indication of the validity of measuring societal impact with altmetric data. Conversely, papers with tags for specifically scientific aspects (such as "new finding" or "hypothesis") should show no effect on the altmetric counts. For contrast with the altmetric data results, this study analyzed citation counts.
First of all, the results of the regression model in relation to all counts (bibliometric and altmetric) show a correlation with the quality of the papers: With better recommendation scores of the Faculty members, the higher the counts are. For example, for recommendation scores "good", "very good", and "exceptional" the corresponding predicted probabilities of citations are 69, 113, and 178. The effect of the recommendation scores occurs -as expected -more strongly with the citation counts than with the altmetric data, and is in agreement with the results of Bornmann (in press). In the study of Bornmann (in press), it is shown that the recommendations of the Faculty members are correlated with field-und time-normalized citation impact scores. The further results of the regression models show substantial differences between altmetrics counts and citation counts (see Bar-Ilan, 2012 ). With regard to a possible societal impact measurement with altmetrics, the results of the present study indicate that with altmetric data impact measurement beyond the science seems possible:
Papers with the tag "good for teaching" do really achieve higher altmetric counts than papers without this tag -if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented ("new finding"). Although the tag "good for teaching" could not be included in the model with the citation counts (so the contrasting comparison was absent), no (statistically) significant effect was demonstrated for the tag "new finding" in the models with the altmetric data.
The results of this study possibly indicate that papers tailored for a readership outside the area of research or science lead to societal impact. This result is in agreement with the proposal of Bornmann and Marx (2014) . To produce societal impact, the authors suggest that scientists write assessment reports summarizing the status of the research on a certain subject and representing knowledge which is available for society to access. An assessment report should be couched in generally understandable terms so that readers who are not familiar with the subject area or the scientific discipline can make sense of it. In the view of Bornmann and Marx (2014) , these reports could be seen as part of the secondary literature of science, which has up to now drawn on review journals, monographs, handbooks and textbooks (primary literature is made up of the publications of the original research literature). With the help of these assessment reports it should be possible to reach people from other segments of society (besides science) and to achieve a correspondingly high impact that would then have an effect on the altmetric data.
If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. Bibliometric data -citations -are normalized for subject area and time. This study has therefore taken a second analytic step involving a possible subject area normalization of altmetric data. In this analysis too, additional citation data was considered to be able to determine common factors and differences in the results. In contrast to the predicted numbers of citation counts, where the subject categories each showed very different clustering, the predicted numbers of altmetric counts (total Altmetric counts and Twitter counts) showed very few subject categories producing high levels of clustering (and the great bulk of the categories low clustering): "biology", "ecology", "evolutionary biology", "multidisciplinary sciences" and especially "medicine, general & internal".
In comparison with the other subject categories (which obtained relatively low counts), these categories are of the sort which appeal to a wider audience public. This wider audience is generally especially interested in topics like ecology and evolution, as well as research results from (internal) medicine (or particular diseases). In addition, there is a special interest in contributions from the best-known scientific journals Nature, Science, and
Proceedings of the National Academy of Sciences (PNAS), which publish research from all disciplines. There are obviously -as the results of this study show -particular topics in the biomedical area which are of especially great interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters' journal sets, a normalization of altmetric data (especially Twitter) should not be based on the level of subject categories, but on the level of topics: Thus, for example, Twitter's homepage includes a current list of trending topics as a main feature. "These terms reflect the topics that are being discussed most at that moment on the site's fast-flowing stream of tweets. In order to avoid topics that are popular regularly (e.g., good morning or good night on certain times of the day), Twitter focuses on topics that are being discussed much more than usual, that is, topics that recently experienced an increase of use, therefore trending" (Zubiaga, Spina, Martínez, & Fresno, 2014) . The comparison of Twitter citations of papers published on a particular topic would then show a greater or lesser interest in papers on this topic. A normalization of Twitter citations could then be performed on the level of papers on a topic.
In relation to the measurement of societal impact, the results of this study are promising: Altmetric data (Twitter counts) seem able to indicate papers which produce societal impact. However, it is not clear which kind of impact is measured: Does it measure social, cultural, environmental and/ or economic impact? With evaluating citations in university text books (impact on education), patents (impact on industry) and clinical guidelines (impact on clinical praxis), there are already some approved instruments available for the reliable societal impact measurements which could be complemented by altmetrics.
In a bid to measure the influence of research on industry, Narin, Hamilton, and Olivastro (1997) studied the frequency with which scientific publications were cited in US patents. They evaluated 400,000 US patents issued between 1987 and 1994. Their results
show that the knowledge flow from US science to US industry tripled in these years. Grant (1999) and Lewison and Sullivan (2008) pursued a similar objective to Narin, et al. (1997) with their evaluation of clinical guidelines: how does knowledge flow from clinical research to clinical practice? The pilot study by Grant (1999) examined three guidelines and was able to ascertain that they contained citations of a total of 284 publications (which can be categorised by author, research institution, country, etc.). For Grant (1999) , the study results demonstrate the usefulness of his approach to tracing the flow of knowledge from research funding into clinical practice.
As most of the former empirical studies on altmetrics have pointed out, we need further studies (including a broad range of altmetrics) dealing with the question of the specific impacts of altmetrics. For this, datasets are required which contain information about the importance of individual publications outside the area of science. This information should be produced by experts (and thus be reliable and valid). Unfortunately, the F1000 dataset does not contain this information. It would be particularly interesting to have information on the importance of publications for very specific segments of society (such as the economy, politics or culture). With this data, one could determine which specific altmetric impact one could measure in which segment of society.
It is well acknowledged that HCI research has a significant role to play in understanding how digital technology can facilitate and support new forms of civic engagement. Over the last five years, we have seen a wealth of work where technology has been used as a means for collecting community opinion [28, 45] to support community activists and community organisations to gather data [47] and facilitate discussion around political decision making [11] .
These emerging landscapes for HCI research typically require extensive working with and within communities [13, 44] , and often come laden with ideals around supporting new forms of democracy and participation in civic life. Furthermore, it involves placing greater emphasis not on just designing systems to collect public opinion, but to design systems for citizens, civic groups and local government to collect public opinion from others.
In this paper, we build on this prior work by detailing our experiences of collaborating with community organisations who used our 'Viewpoint' situated consultation technologies. We discuss fieldwork from two collaborative projects where our voting devices have been deployed to collect opinion on specific issues at different stages of campaigns and participatory governance exercises. Rather than focusing primarily on an evaluation of Viewpoint 'in use', we highlight the various trade-offs and decisions made before, and the making sense and use of collected data following deployment of the devices. Following [13, 14, 15] we highlight some of the human work that goes into planning and overseeing the use of consultation technologies for community organisations, and the ways in which the research team guided and influenced this process. Our reflections on this fieldwork highlight specific issues related to: the forming of the right questions to be posed on the devices; the identification of and gaining access to the right locations for promoting engagement and discussion; and the difficulties community organisations face in using and responding to the data and insights collected through novel consultation technologies.
Our contributions to the developing HCI discourse surrounding civics technology are two-fold. First, through rich ethnographic insights we highlight stakeholder (researcher and community partner) influence and responsibilities in deployments of community consultation technologies. Second, based on our two case studies, we highlight challenges and opportunities for HCI researchers working with communities and civic organisations, while problematising the perceived neutrality of community consultation technologies in contexts where only a privileged few set the questions, situate the devices and have access to the data.
The field of HCI has for many years dealt with issues to do with civic action, engagement and participation. A huge amount of work within the CSCW and CHI communities has examined how social media services are appropriated for civic discourse [11] , information sharing [42] , activism [30] , protest [46] , and action [25, 39] . Alongside studying the role of technology in relation to issues of civic importance, there has been increased attention paid to conducting in-the-wild studies of systems in community and civic contexts. Going back over 15 years, projects such as Civic Nexus [34] and CiVicinity [5] have highlighted the benefits of closely collaborating with communities to create Participatory design and systems that connect local actors and transform practices in voluntary and community sector organisations. More recently, a number of studies have explored how technology can support new forms of community engagement and participation in local decisionmaking. Much of this work has focused upon the evaluation of situated displays in public places to engage citizens in voting, consultation, and other forms of sharing and contributing to such processes (e.g. [8, 20, 22, 24, 43] ). For example, ongoing work in Oulu, Finland, has articulated the value of interactive public displays in engaging members of the public in commentating and giving feedback on planning proposals [23] . Taking a different approach, in the Bespoke project, Taylor et al. [45] deployed their Viewpoint technology as a simple means for local government representatives to set questions for community members to respond to. The ambition here was to promote wider participation, and a sense of increased efficacy, for community residents. Koeman et al. [28] took an approach to distributing voting boxes at multiple locations around communities. Again, like Taylor et al. [45] , they harness lightweight forms of engagement to promote participation in opinion sharing-however, they took a further step in visualising the results on a location-by-location basis, as well as in a 'neutral' ground, which promoted wider discussion around the contrasts and divisions within the community itself.
For a long time, the participatory design and community informatics literature-along with wider participatory research scholarship-has discussed, articulated and debated the challenges involved in working with community organisations and facilitating new practices and processes [2, 6, 34] . Issues such as these are becoming of increasing importance to the HCI scholarship on civic and community technology. This is especially so, given that HCI researchers are no longer just deploying technologies for opinion gathering and consultation-rather, in many respects, they are aiming to support others in developing such practices. This is particularly recognisable in Vlachokyriakos et al's [47] work on PosterVote, where the ambition was to build platforms to be appropriated and deployed by activists, rather than deploying it and evaluating it on their behalf. Beyond the technical and design features of the system under study, PosterVote raised questions related to the governance and ownership of the data collected and the influence of activists groups on the way people voted. In their work on crowdsourced cycling data, LeDantec et al. [12] noted issues of the provenance, legibility and meaningfulness of data generated by publics to those making planning decisions. Taylor et al. [43] -also raising issues of who owns and accesses communitygenerated data-note the ways in which residents make data meanigful by placing it into context. They also note the important role the research team played as a percieved neutral party to support dialogue and sensemaking around community-generated data, as well as providing the necessary skills and expertise to install and maintain devices and related infrastructures and archives. This is echoed by [44] who discuss the critical importance of building relationships with local residents and lead community members through the duration of projects and ensure skills and infrastructure are in place to sustain endeavours beyond the completion of the research project. In a similar vein, Hosio et al. [27] discuss the percieved value of situated displays in civic and community contexts, highlighting the range of additional costs and burdens they bring to the local government organisations who use them.
These examples in different ways pose questions about the responsibilities of different stakeholders in civic technology contexts where decision-making is a primary concern. They also raise issues related to the role of the researcher in these contexts, and whether they have a responsibility to not just to provide new tools with which to consult but also help organisations and individuals develop the skills, resources, capacity and practices to use these in a meaningful and sustainable manner. We build on the above by discussing our experiences of conducting field trials of distributed, multi-site community consultation technologies with two communty organisations. These deployments were intended to be led by our partners, as we will highlight, however, our community partners faced a number of conceptual and practical challenges in planning, overseeing and making sense of the insights from these deployments. Through our discussion of these projects we will highlight the ways in which the research team played an important role in carefully guiding and, at times, explicitly directing and managing parts of these deployments.
Our work built on the prior work of Taylor et al [45] and their original Viewpoint system. In the following, we provide an overview of this original work, followed by how our projects and version of the technology builds upon it.
The original Viewpoint technology was developed as part of the Bespoke project. The overarching project explored issues to do with community cohesion and political disengagement in a small city in North West England. Viewpoint allowed local councillors and community As noted in [45] , the choice of a simple interface situated in a public space proved to be successful in gathering high quantities of feedback. Across a two-month deployment, eight different polls received an average of over two hundred votes each, an order of magnitude higher than original expectations. However, Viewpoint was less successful in creating the kind of positive feedback loop that had been intended. Community members remained sceptical of whether any change would occur, and few meaningful responses or promises of action were given by the local government collaborators. Having to work closely with councillors to help them formulate a question that was capable of being answered through a binary choice, the rapid turnover on questions, and the lack of actionable issues with a burning need for input made this difficult to achieve. As since discussed extensively by Harding et al. [26] , this exposed a failure to integrate with existing council processes that might have created avenues for change. This was compounded by the way that Viewpoint placed the agenda firmly in the hands of those in positions of power, with community members acting as passive respondents.
Findings from the original Viewpoint suggest a number of future possibilities that we chose to take forward. First, the use of short-term, targeted deployments would allow the device to be deployed only when specific input was needed and actionable. This might also take advantage of the novelty effects that had been observed. Second, situated voting technologies might be more closely tied into existing practices. This could mean integrating with council feedback schemes, but it may also mean putting Viewpoint in the hands of community organisations who are already engaged with local authorities. For the most part, this repositioning only requires a change in how the device is used. However, to better support new deployment contexts, we redesigned Viewpoint with a focus on flexibility and portability. The redesigned device ( Figure 1 ) allowed greater flexibility in how questions could be presented and responded to. It made use of a physical rotary control rather than buttons to allow voters to respond through multiplechoice answers or points on a sliding scale. It also supported voice and video input if required, along with a touch-screen display to be enabled as and when deemed appropriate. Additionally, the devices were made considerably smaller, with the intention that they might be more easily moved between different locations, and a 3G modem was added as a backup in situations where Wi-Fi could not be provided.
We deployed the second generation of Viewpoint in two case studies, where the technology was used by community groups to elicit feedback on issues related to local planning and transportation developments. In the following sections we provide an overview of these case study contexts.
Our first case study involved working with the local chapter of an international movement that champions sustainable communities. When we approached them, the group were beginning to collect evidence to support the pedestrianisation of Acorn Road, a small shopping street that formed the centre of their neighbourhood. The group felt that the street was overly congested, making it dangerous for local pedestrians and cyclists who they felt most used the street, and that many of the cars could easily be re-routed to create a more pleasant environment.
The campaigners used the Viewpoint boxes to contribute to their ongoing consultations with people on Acorn Road. They developed two questions (discussed in the Findings) that were displayed on the devices for two weeks each, during the summer and during the autumn (to collect data from the student population). Three devices were deployed in the community: two in local supermarkets, with a third inside the neighbourhood's library. Simultaneously, the group carried out a street survey in which they stopped passers-by to ask a number of questions, one of which related to means of travel, and a traffic observation survey. Across the entire deployment, shoppers placed 2,040 votes in total. By contrast, the group's past attempts to collect feedback online had returned only a few dozen results.
Our second study was conducted in a small coastal town in Northern England comprised of approximately 6,000 residents. The town attracts large numbers visitors in the summer season and is currently experiencing major redevelopment, particularly in its harbour area. Beside this regeneration, which is primarily funded by the local Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA government, there is significant private investment. The development within the town is primarily driven by a Local Development Trust (henceforth 'the Trust'). The Trust was set up two decades ago as a response to the perceived lack of opportunities for development and employment in the town. It is responsible for attracting funding grants for community projects. One requirement the Trust has is to carry out public consultation as part of its applications to receive grants, along with further consultation to then allocate the budgets associated with community projects.
Significantly, there is a feeling amongst many residents in the town that regeneration is often focused on tourism, excluding the needs of those who live there. This is further compounded by high-levels of unemployment and closure of local industry over the last several decades. As such, this has led to a lack of trust that local and central government will act in their interests. These issues were echoed by the Trust, which has noted a substantial decline in levels of engagement in recent years which they put down to a feeling of disfranchisement from some residents. This is problematised further by a feeling that the consultations they perform tend to attract the same group of people every time, and the setting of consultation events at a fixed time and place leaves many people unable to attend.
These were important motivators for the Trust in using Viewpoint devices in their application scoping and project allocation processes. They developed three questions (again, discussed in the Findings) that were displayed on four boxes during the summer of 2015. Four devices were deployed in different locations around the town in areas where they expected to capture feedback from local people who would normally not engage in their consultations. Across the deployment of the Viewpoint devices, people registered 699 responses in total, with a significant number of these votes coming from devices located in parts of the town rarely engaged with by the Trust.
Initial contact with each group came via community engagement activities conducted by our research lab. The research team then met with each of the organisations to discuss their projects, modes of engagement and consultation, and the challenges they faced.
During the Acorn Road deployments, the devices and the progress of the campaign were monitored by both a researcher and the campaign group, although the primary source of data collected was automated interaction logs. The researcher and campaign group maintained regular contact through the deployment. For Ambit, we expanded on this approach to place more focus on in-situ observations. Semi-structured interviews were conducted with four 'custodians' (people working in locations where devices were deployed). These interviews were audiorecorded and focused on the use of the device by others as perceived by the custodians. These were supplemented by observations where the researcher would 'hang around' [48] and note interactions with the device and events that occur in these spaces. The researcher talked with users of the devices to ask them about why they participated and their response to the ongoing results presented on Viewpoint.
Overall, 84 hours of fieldwork observations were conducted and 22 conversations documented.
Each trial ended with a semi-structured interview with the representatives of each community partner, as well as device custodians for Ambit, where we discussed consultation results and how they may use the data gathered. The data collected (transcriptions of interviews and field notes) was then used as a corpus for thematic analysis [3, 9] . Data coding was driven by questions related to how choices and decisions that impacted on the consultation process were made during the projects. Coding of data was shared between the first and second authors and checked by the third author. Codes were then clustered into the themes presented in this paper.
Our analysis generated five themes. We organise our themes to present a comparative narrative of how the two field trials played out over time.
A critical aspect of our engagements across each study was working with the community organisations to establish the types of questions to be posed on the devices. Learning from our prior projects with the older Viewpoint boxes, each of our case studies was tied in one form or another to pre-existing campaigns and consultation processes. However, the stages within these processes at which the Viewpoints were used were rather different-this shaped the discussions around what questions may be asked, and how they should be asked and responded to. As such, what was initially assumed to be a simple endeavour in setting questions to ask the public, became complex decisionmaking processes in their own right.
In Acorn Road, the choice of overriding topic was determined by the pre-existing campaign. As noted, the ambitions behind the campaign were to advocate for the pedestrianisation of a local street. Initially the organisation considered using Viewpoint to directly ask residents whether they thought the street should be pedestrianised, with the intention that this could be presented to the council like a petition. However, prior to deployment concerns were raised about this direct line of questioning:
" This was a concern raised all the more by the fact that local businesses-who were opposed to pedestrianisation-were envisaged as being the most likely deployment locations, so some sensitivity was required. Instead, the group developed a new set of questions and possible responses that, as they articulated it, were more "objective" (Acorn Road campaigner). Two questions were asked: "What has been your main means of travel today?" followed by "How far do you live from here?" These questions sought to ascertain what percentage of the street's users were travelling by car and what proportion were local and might reasonably travel by foot or bicycle instead. The questions themselves were carefully chosen and reflected the many stakeholders that existed in this community issue. The decision to collect "objective" data about usage of the street rather subjective opinions about its future meant that data was felt to be unbiased (or, at least, less biased), and was envisaged as being taken more seriously by the complex range of actors involved in future decision making.
A similarly complex set of trade-offs around question setting occurred in the Ambit project. The Ambit project differed significantly from Acorn Road in that it occurred at a much earlier stage in a consultation process. As such, there was not a specific campaign for the organisation facilitating the trial to push-rather the Viewpoint devices were to be used as part of a scoping exercise for future project development. The main ambition here was to capture views from people about the locations and places in the town they felt needed investment. This initial broad consultation process would identify specific locations in the area to be targeted in a more focused piece of consultation work. Like Acorn Road, however, the language used in the questions posed was very carefully considered. While the Trust did have money for projects, they "did not want to raise expectations" (Trust representative) that this scoping process would lead to money being committed before current projects were complete. Initial questions suggested for the devices included words such as "funding", "investment" and "projects"-however, these were iterated to instead focus on the locations in the town people wished to "change" or where they "like to visit". These changes to how the questions were posed made them much more ambiguousat the same time it allowed the Trust to distance themselves from acting on the results of the consultation, should they feel unable to commit to working in the locations most identified in the responses.
Across both deployments the community groups had an agenda that in some way they wanted to obfuscate. As researchers, we had to find ways to fulfil the broad goals of each consultation, yet also encourage flexibility against some of the more conservative plans for using the technologies made by the community organisation. This continued when designing how citizens would respond to the questions posed on Viewpoint, which we discuss next.
While in both studies the same system was used, the process of providing responses to questions was very different. The way in which people were invited to respond represented the types of questions our collaborators defined and, again, the stage in their consultation process they found themselves to be in. In Acorn Road, the campaign group were very clear about the form of data they required and, as noted, had a strong desire for it to look objective. It needed to carefully compliment previous surveys they had conducted, and the data was to be used in a public report handed to the local council. The campaigners had a preference for collecting detailed information using a multistage questionnaire presented on Viewpoint. Ultimately, they had to be convinced by us that this was at odds with the design intent underpinning Viewpoint-that it offered lightweight and quick engagement-and that based on previous work it was unlikely people would stand and complete a longer questionnaire in a public place. This demonstrated a tension between the insights gained from our previous research experience and their desire for it to fit in with familiar frameworks of data.
The process of determining the form of response was rather more complex in Ambit, perhaps in part due to the scoping nature of the Trust's exercise. Initially, much like the campaign group, the organisation envisaged appropriating Viewpoint in relation to traditional consultation methods. They imagined using the box as an "interactive questionnaire" (Trust representative) where passers-by could switch through different questions and respond to them through scaled answers. However, when iterating imagined answers to these questions it was thought such an approach would lead to a very restrictive set of responses. Through a series of meetings, we came to an agreement that the Viewpoint boxes should represent the local area cartographically, where respondents could simply touch those parts of the town that correspond with their response to the question. Again, however, this process was not as Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA simple as may appear-there was anxiety among the Trust that capturing just a location on a map was "not enough" and that they needed to "know more" (Trust representative). There was a desire to capture additional comments from people who provided their response to the question. Ideas suggested at this stage included supporting video or audio feedback via the cameras and microphones built into the system. These were discounted by the Trust however due to perceived privacy concerns.
The issues encountered in both of the projects around defining the ways people responded to consultation questions highlighted issues around the legibility of even relatively simple civic technologies to community groups. Furthermore, it highlights the significant agency the research team had in advising and, in some respects, pushing ideas around what the technology was for and how it would best work. This is an issue we return to in more detail in the following section.
Through the two projects, the Viewpoint boxes were deployed across seven different locations for at least twoweeks at a time. Strategic selection of locations was important in both of the projects. Prior to each of the projects, it was assumed that good places for locating these boxes would be busy places. For the campaign group there was a concern for making sure the Viewpoint boxes were placed in carefully chosen locations in and around Acorn Road where they captured "a lot of footfall" but also a broad audience of passers-by. Two of the devices were located in local supermarkets-one a branch of the country's largest retailer and the other a newly opened branch of an upmarket chain. Both shops were heavily trafficked and the location of the devices, both just past the check-outs (see Figure 1) , helped to secure a large number of votes. In the Ambit project, the Trust wished to locate one of the boxes in a newly opened seafood centre, where again it was assumed they "would get a lot of people coming" (Trust representative). Indeed, these assumptions were confirmed as the seafood centre did capture the largest number of responses overall. The other locations where the devices were deployed blurred notions of public and private. In both projects, local libraries were used as locations for a device. Libraries were chosen as a legitimate space for the type of consultation processes both groups were engaging in-indeed, historically libraries in the UK often act as venues where redevelopment plans are displayed. Unlike shops and busy tourist destinations, libraries are also almost exclusively used by local residents, which has implications in terms of demographics that can be reached. In both projects, libraries were the locations with the fewest responses, yet they were appreciated for being able to reach parts of the community busier locations might not.
Another location used in the Ambit project was a local pub. Like libraries, community pubs are almost exclusively used by local residents, and they are places that have a unique social mix, considered to contribute to social capital and a healthy community [36] . Pubs are places void of institutional influence where citizens share information, often through vernacular, rather than formal interactions, but this sharing is a by-product of the focal activity of socialisation [17, 18] . While this may appear to be an unorthodox location for these devices, it was chosen due to being embedded in a different part of the town to the other devices in Ambit. We also assumed the chatter, gossip and complaints that may occur in such spaces might be usefully harnessed for the purposes of the consultation. During the deployment, the pub received the second highest quantity of responses-this was despite being deployed for the shortest period of time. More significantly, however, votes made at the pub were dramatically different to the three other locations in Ambit. In response to the 'place you would change' question, places where factories had closed down and job losses occurred were dominant. Our interviews and observations highlighted how bringing a Viewpoint to this location engendered conversations-and that the space was formed of regulars and hangers-around meant discussions were deeper and more heated than the impromptu and fleeting conversations seen at the other places the boxes were located.
It's worth noting that in all cases gaining access to the 'right' locations was not a simple affair. In the Ambit project a key criteria for the Trust was to elicit feedback from people who were not the "usual suspects". One of the core benefits seen in using the Viewpoint boxes was that they could be situated around the town in different locations, perhaps where dwellers and passers-by might not be those who would normally interact with the Trust. However, while the Trust desired to connect with the wider community, they were limited in brokering locations where the devices could be located. Indeed, early in the project, suggested locations were primarily based on their existing social networks. While in many cases the proprietors of these venues were happy to engage, the locations were not fitting with the stated aims of the consultation.
Again, the research team found themselves conflicted in the Ambit collaboration-should we let the Trust continue in their planned process of consultation, or should we push back and direct them to using other locations? It became clear at this stage that our collaborators simply did not have the social capital within the town with which to access certain places. What then followed involved the lead author taking ownership of a small number of boxes as a seemingly neutral party, in an attempt to engage new stakeholders across the town in the project. This process of wider engagement was, in itself, an incredibly time consuming and intensive process. It literally involved the researcher walking from one end of the town, visiting venues, and if appropriate spending time in each, before approaching staff or managers to explain the project, often with positive and supportive reactions.
Overall, the locating of the devices for the Acorn Road project was a simpler process-the campaign group used their existing links with local supermarkets and the local council to have the boxes installed. However, the ease of gaining initial consent masks a variety of issue that needed to be taken into account: health and safety regulations, pressure from managers regarding the appearance of a store, and local politics. As previously discussed, the choice of location even influenced the questions that could be asked, as it became increasingly important not to alienate traders who were also hosting the devices. These were issues experienced in Ambit as well, where despite the initial good will reported above, sometimes practical and infrastructural problems in specific shops, cafes and pubs meant Viewpoints could not be installed.
A further set of issues encountered during both studies related to the expectations our collaborators set around the value and validity of the data they would be collecting, and how they were able to use this going forward. In the case of the Acorn Road deployment, there was a very clear trajectory for the organisation from the collection of the data through to presenting it to the local authority alongside data collected by traditional methods. The Viewpoint devices were unique in capturing data over longer periods of time, while expending less human resource from the campaign. That the types of data collected were comparable to one another was of huge value here. On its own the data from Viewpoint wasn't seen as entirely "legitimate", but as one feature in a set of tools it was seen to be "very useful" (Acorn Road campaigner) in gathering a wider picture of the issue at hand. This was aided all the more that many of the results were "as expected":
In particular, the campaign group felt the devices validated their intuition that more people travelled via public transport to the local area, and that the majority of shoppers were likely to be students:
This is not to say that the trial did not process some unexpected results. One of the results that did surprise the campaigners was that despite a significant number of people travelling by car, the vast majority travelled from less than three miles away: In making sense of the data, the campaign group were not only forced to readdress their assumptions, but also change the focus of their campaign-softening their plans for full pedestrianisation, to a semi-pedestrianised space that still acknowledged the need for some parking and other access.
Compared to the Acorn Road campaign, the Viewpoint data in Ambit was somewhat more challenging for the Trust to make sense of. This was, in part, a result of the more ambiguous framing of the questions and the form of response determined at the start of the project. In this case, contributions on Viewpoints located where there would likely be many tourists were interpreted, perhaps unsurprisingly, as highlighting places that were primarily visitor attractions. Locations identified on boxes in primarily residential areas-such as the pub-instead were contextualised in the history of the town and ongoing concerns around employment opportunities. The data was used to construct a narrative about the different priorities people who used Viewpoint would have. These narratives were often based on ideas of who was living in specific parts of the town, what their imagined concerns and aspirations were and why they would choose specific places to change. Because the Viewpoints had not captured the provenance of these responses, the Trust's representatives made sense of these through relying on their own prior assumptions and knowledge.
While much of the data was used to reinforce what was already 'expected' by our partners-for Acorn Road judgements were made about the mode of travel of shoppers; for Ambit this was much more determined by the Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA imagined social class and background of the people responding in a particular place-it did leave space for ambiguity and interpretation, and even the groups reassessing their interpretations.
Throughout both of the studies, the Viewpoint deployments caused a significant amount of discussion and debate. This was somewhat a novelty effect-across many of the periods spent observing Viewpoint at a distance, members of the public would be seen staring at it, asking questions of staff in venues about what it was, what it was doing here, and how to use it. The leader of the campaign group noted that "it stimulated discussion, for sure" and that people "asked what's going to be done". All of the custodians interviewed at the end of the Ambit project reported how conversations that were born from curiosity typically then opened a debate about specific issues around the places displayed on the devices. In some cases these individuals saw it as a catalyst to discuss wider issues and in turn contest future decisions: 
Custodians of the devices also explained how people that may not normally be engaged in such processes were seemingly empowered by the boxes. For instance, our custodian in the public house explained how many people she knew well, but rarely discussed politics with, were suddenly inspired to comment on recent redevelopments based on the presence of the Viewpoint box. She remarked in surprise that older, opinionated, but "technophobic" regulars were seen to participate in giving their response. Another one of the locations hosting a device in Ambit regularly runs training sessions and courses for young people. The assumptions of the custodians in this space was that such a group would be reluctant to get involved in activities beyond their own training: "But they actually took
Clearly, the hope of promoting discussion and gathering data, from our collaborators perspectives, was to inform either their own decision making, or to put pressure on and influence the decisions of others. In Acorn Road, the Viewpoint consultation was a key step in a much longer process. Combined results of the Viewpoint survey and a street survey asking similar questions were presented to the city council as a report. The group's recommendation was softened based on the results, from full pedestrianisation of the street to a one-way street with shared space for cars, bikes and pedestrians. What followed was a period of consultation and protest lasting three years. After proposals were unveiled, local businesses complained about the loss of parking spaces, leading the council to launch a consultation on two different proposals. After the more radical proposal was chosen, concerns were raised about the validity of the vote and who was able to contributeconcerns that are echoed in our own findings around Viewpoint, which on its own lacked legitimacy. Interestingly, part of this consultation took place through the city's recently-launched online consultation platform, as well as through more traditional means. Two further petitions-one from each side of the argument-led to a reopened consultation and a revised plan that retained more parking spaces. At the time of writing, work is just beginning on the site, three years after the Viewpoint data was collected. Despite Viewpoint's role being dwarfed by the scale of the process, it is notable that there were clear points where Viewpoint and other technologies empowered citizens to drive or shift the agenda. There was similar hope that the Ambit deployment would also lead to some decision being made-even if this was just to determine more focused consultations in the next round. However, the discussions promoted around Viewpoint were in many respects born from a suspicion of the local council, or of those running the consultation. For example, early in the deployment one of the custodians commented on how some people were suspicious of places "missing" from the maps on the devices:
"People asked where places were, they were concerned about as they could not identify them on the map, so they had these preconceived ideas already [of deception and mistrust] and saw this as an opportunity to say something." (Ambit custodian 4)
Fears were raised that places that were missing had already been determined as not worthy of changing. Furthermore, as time went on, the data being captured by the Viewpoint boxes themselves appeared to reaffirm these divisions within the town: different Viewpoint sites captured very different impressions of what were important places to change; these would then be viewable and made visible to people interacting with the devices; this, in turn, promoted even further discourse around mistrust and division in the town. Critically, the Trust themselves-and not just local governors-were open for critique. They were described as a "very closed group", and indistinguishable from other institutions of power in the town: "the same people's on the development trust's as is on the harbour commissioners; they've all got their finger in little pies." (Ambit custodian 4). The lack of visibility with what was happening to the data they collect, coupled with the feeling some groups are excluded from any development has left people concerned about whose interests are being served.
The community groups in our studies were sometimes discussed as an extension of the civic authority, warranting the same distrust. Despite the studies being designed to support consultation, there were many unexpected results around broader issues of division and decision-making that were equally, if not more, interesting.
Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA
Our two case studies highlight some of the challenges for researchers and community organisations engaging in the use and deployment of situated consultation technologies. Some challenges were practical and technical in naturesuch as gaining access to appropriate locations that had the required space and infrastructure for the boxes to work, but most were conceptual, social and political. In the following sections we examine some of these issues. We draw out a number of key reflections on our experiences with Viewpoint and ask questions for the HCI and civic technology communities going forward.
One issue that was apparent in both of the case studies was the important role the researcher plays as an agent and as part of the infrastructure of civic technology deployments.
There is very often a tendency for the voice of the researcher to be 'written out' in the aid of objectivity, a criticism that HCI scholarship has faced in recent years [1, 7] . In our case it would be impossible not to acknowledge how critical the research team were in shaping the work conducted. We acted as critical friends to bounce ideas off-helping our collaborators to think through the questions they wished to pose, the places they wished to pose them in, and supporting them in understanding the particular affordances offered by the boxes. On other occasions we were more direct in our guidance. This was, we felt, to ensure that they maximised the potential of the technology. However, with it we also invoked a particular stance on what we saw as the 'right' and 'wrong' way to motivate people to participate in local decision making.
The participatory research literature highlights the importance of capacity building [29] and the negotiation of power and control in community research contexts [10] . In our examples we could see how a mutual exchange of skills and expertise informally supported the development of reflective practices around setting questions and inviting responses. Control over this had to be continually negotiated however, and a balance had to be found between offering our expertise and not taking ownership of the consultation. Often community partners have more at stake and more to lose than the research team, so in our case it was not surprising they resisted some of the ideas we brought. At different moments the social capital of the researchers or the community groups was more appropriate, and understanding this dynamic was an important element in maintaining positive and successful partnerships. Greater honesty and critical reflection on these issues is therefore needed, not just in civic technologies research, but in a broader range of participatory projects in HCI where such issues might arise.
The rhetoric underpinning much of the civic technology and digital democracy literature is that digital systems can support new relationships between citizens and states (e.g. [21, 41] ), and provide new mechanisms for decision-making (e.g. [19, 31, 37, 38, 40] ). In many respects it was this rhetoric that motivated our collaborators' use of the Viewpoints. In using the devices, both hoped to come to some agreement about the issues that faced them, to determine what 'should' be done about a busy road, or where the focus 'should' be of future community projects. However, in practice the results of both case studies raised more questions than answers. In the case of Ambit there were some occasions where people at different sites found commonalities with the views seemingly expressed elsewhere. Primarily however Viewpoint provided a platform for community members to express their concerns around the ongoing regeneration of the town, and a recent history of political dissatisfaction and economic disadvantage. The maps on the devices made visible social divisions and perceived and actual inequalities within this small community. In the Acorn Road trial the questions raised by the system were perhaps less divisive but equally as complex to deal withthey required the campaigners to re-evaluate their own perceptions of the problem at hand, to soften their political stance, and to find common solutions to the very different challenges and positions of a myriad of stakeholders.
Viewpoint was somewhat predicated on the idea that technology can provide lower barriers of entry to having a say and thus support the conditions for democratic processes to occur. This simplistic view ignores how such interventions fit into the much wider, complex network of processes and actors of varying degrees of power and influence at play. It also ignores pre-existing issues around trust between different parties, an issue that has been argued to be oft-discounted in digital voting and consultation literature [33] . As noted by Harding et al. [26] , this is not just mistrust of decision-makers and authorities by certain groups of citizens, but also mistrust from certain decisionmakers as to the legitimacy and value of contributions in certain formats or from specific groups of people. In our case it was clear that Viewpoint in some cases reaffirmed these issues of mistrust. Perhaps revealing these issues could be productively channelled in the long-term, but only if technologies like Viewpoint are designed in ways to account for this bigger picture and embedded as an actor in a carefully designed process of decision-making.
As with prior work [12, 43, 47] our studies also raised questions around who owned the data generated by the Viewpoint devices, and the subtle ways ownership and power over the data were deployed by our partnering organisations. Burgess [4] warns that the ongoing appropriation of deliberative engagement by institutional authorities can often serve to legitimise policy decisions set independently of public participation. It would be unfair to claim that this was the case in our studies-there was great will and desire from both of our partners to reach into new parts of the community, to consult a wide number of people, and to use the insights gathered in a meaningful and honest manner. However, because both of the projects had Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA very specific agendas involved it was hard for our collaborators not to let these shape the ways in which the consultations with the devices happened.
One observation here is that our desire to seek case studies where there were the results would the "actioned" (at least envisioned to be) meant our partners very carefully thought through the types of questions they would ask. While in part this was a process of ensuring they would ask good quality questions, as we saw it also involved them thinking through the potentially negative consequences of asking the wrong questions. In Ambit, the wrong question was one that would be seen to commit the Trust to spend project funds on a particular site in town; for the campaign group it was to make known their own values and opinions from the consultation. It could be argued that by choosing not to expose their ultimate intentions, the campaign group prevented citizens and local businesses from making their voices heard. We can imagine that these other stakeholders might want to collect their own data if they are in opposition to those conducting the polls, or to verify the data being collected; indeed, this was a desire and even an expectation expressed by the custodians of our devices in the Ambit project.
That the systems were deployed in clear decision-making processes also raised further questions around power, the use of citizen voice, and potentially on efficacy. Where the first generation Viewpoint device was designed with inbuilt mechanisms to support accountability and a sense of efficacy, the timescales and number of actors involved in these actionable contexts make this impossible. The slow timescales of decision-making processes and eventual outcomes are juxtaposed sharply against the quick, straightforward, lightweight interactions afforded by civic technologies, leaving a gap between engagement and action that may cause citizens to question 'what' is happening with the data. Fundamentally the length of these consultation processes means the rhetoric around feedback and voter efficacy become highly problematic.
In this paper we have reported on our experiences of working with community organisations that used our distributed, situated technologies as platforms for consulting their local communities. We have highlighted the diverse ways the Viewpoints promoted discussion and debate, facilitated the making of decisions, and exposed mistrust and contestation in the places they were deployed. In some cases this was promoted by the fact the boxes were installed in highly public locations, seen and engaged with, by a large number of people; it was also because they were distributed in a range of other locations as well, where existing practices of sociality and conviviality could be harnessed further. This is in keeping with recent work on similar systems (e.g. [28] ) that highlights the potential of these technologies to create a buzz and dialogue around local matters of concern. It also overlaps with scholarship on deliberative democracy [16, 32] , which states that political discourse are acts of everyday talk, and we should take the processes of decision-making to those sites where such talk occurs in civil society [35] .
However, it's critical to note that while we (the researchers) captured the buzz around the deployments, our collaborating community partners did not. The design of Viewpoint was such that it followed a simple framework of participation in the aid of lowering the barrier to entry. In doing so it purposely designed out the collection of "noise", privileging the idea that participation in local matters of concern can be captured at the press of a button or the tap of a map. If it were not for the performance of fieldwork around our systems then the richness and detail of conversations would be missed. While this may appear to be a moot point, it's a critical one in a context where the ambition is to create platforms that enable people to ask questions of others. In this context, it is critical to understand 'why' people say what they do, and to capture the wider discourse the questions posed provoke. Such additional layers of study would be practically difficult for our collaborators to conduct. Further, the perceived lack of objectivity of our collaborators from some of those being consulted may, in some respects, have made any such attempts meaningless. In this regard, the perceived neutrality of the researchers [43] eased people into sharing their views in a more candid manner.
One might argue that there were opportunities in our work to design in the capturing of such 'noise'. We could have invited people to give video or audio responses to questions. But beyond the privacy concerns bound up in this, there is a more poignant concern that such interactions lose the richness and discursive, dialogic element of debate.
In future work, we should perhaps look to designing systems that adapt to the conversations already taking place in society, rather than asking citizens to adapt to artificial interactions to express their views. With this comes an appreciation that it is the researcher's duty to proactively capture and convey this richness, so that community organisations-and the authorities and institutions to which they lobby-can acknowledge and use them appropriately in their decision-making processes.
We'd like to thank our community partners and the Viewpoint 'custodians' for giving their time to these projects. 

The recent widespread adoption of social network sites (SNS; boyd & Ellison, 2007) influences communication behavior in a variety of contexts, including political participation (Smith & Rainie, 2008) , identity construction (Liu, 2007) , collegiate teacher-student relationships (Mazer, Murphy, & Simonds, 2007) , and adolescent friendships (Lenhart & Madden, 2007) . Though users appropriate these sites for varied purposes, the maintenance of networked interpersonal relationships is their central attraction and function (Donath, 2007; Ellison, Steinfeld, & Lampe, 2007; Tufekci, 2008) . Accordingly, such sites are now receiving attention from interpersonal communication researchers, though a theoretical understanding of how SNS may contribute to relational closeness remains in infancy (Baym & Ledbetter, 2009) . Of the hundreds of SNS available on the Internet, Facebook is one of the most popular across a variety of demographic categories (Boyd & Ellison, 2007) . This article explores motivations toward self-disclosure and social connection as distinct yet related predictors of Facebook use within specific relationships. Though any number of specific interpersonal communication motivations might merit research attention, Facebook itself explicitly calls attention to these motivations in the site's slogan, prominently featured on the opening page: "Facebook helps you connect and share with the people in your life" (Facebook.com, 2009, emphasis added) . Facebook creator Zuckerberg (2008) acknowledged that the site's features are designed with these two motivations in mind. Separately, Ledbetter (2009b) identifies self-disclosure and social connection as fundamental motivations that foster online interpersonal communication more generally. Given decades of debate regarding interpersonal outcomes associated with online communication (for a review, see Walther & Parks, 2002) and that interpersonal communication scholars identify relational closeness as an outcome of practical and theoretical interest (Vangelisti & Caughlin, 1997) , the chief goal of this study is to test a theoretical model that elaborates how these two motivations might contribute to Facebook communication behavior (within specific interpersonal relationships) and, in turn, how such communication is associated with relational closeness.
Online communication's integration with offline social networks is seen clearly in the recent emergence of social network sites (SNSs), or "web-based services that allow individuals to (1) construct a public or semi-public profile within a bounded system, (2) articulate a list of other users with whom they share a connection, and (3) view and traverse their list of connections and those made by others within the system" (boyd & Ellison, 2007, p. 211) . Though Facebook originated in 2004 as an SNS exclusively for college student use, the site soon opened to corporate networks in early 2006 and then to the general public by the end of that year (boyd & Ellison, 2007) . As of this writing, Facebook remains one of the most popular SNSs across a variety of demographic categories (Hargittai, 2007 ). Yet before further considering the nature of interpersonal relationships on Facebook, we must address the ambiguous nature of the term friend when discussing SNS communication (boyd & Ellison, 2007) . Though colloquial and 30 Communication Research 38(1) affective components. These affective/cognitive orientations, in turn, influence behavior toward the attitude object. Given the diverse manifestations of online communication, some may question whether research can speak meaningfully about an attitude toward online communication as a whole. Without denying the value of examining attitudes toward specific technologies, a robust research tradition examines trait-like orientations toward tech nology at a more abstract level, identifying constructs such as online communication apprehension (Scott & Timmerman, 2005) , generalized problematic Internet use (Caplan, 2003) , and information reception apprehension from technology sources (Wheeless, Eddleman-Spears, Magness, & Preiss, 2005 ) that significantly predict technology use and related outcomes. This investigation follows this tradition, with the hope that such knowledge will help build theory that explains both current and future communication technologies (Sawhney, 2007) .
With this theoretical background in mind, Ledbetter (2009b) validates attitude toward online self-disclosure (OSD) and attitude toward online social connection (OSC) as two fundamental orientations influencing media-use patterns in interpersonal relationships, with similar concepts echoing in related lines of research (e.g., "disposition toward social grooming and privacy concerns, " Tufekci, 2008, p. 561) . Specifically, Ledbetter argues that these orientations address an individual's attitude toward the medium itself, which then influences both the formation and interpretation of online messages. That previous research recognizes both self-disclosure (Acquisti & Gross, 2006; Mazer et al., 2007) and social connection (Donath, 2007; Ellison et al., 2007) as core SNS behaviors further supports this line of argumentation; also, that Facebook's basic site structure aims to gratify both of these attitudinal orientations (Zuckerberg, 2008) further merits considering theoretical links between these motivations, communication behavior, and subsequent relational outcomes. We will review each of these orientations in turn.
OSD. Mazer and his colleagues (2007) provided perhaps the earliest peer-reviewed article on Facebook self-disclosure. Conceptualizing self-disclosure as "any message about the self that a person communicates to another" (Wheeless & Grotz, 1976, p. 47 ), Mazer and his colleagues identify several Facebook features that foster self-disclosure: "users post personal information such as pictures, hobbies, and messages to communicate with fellow students and instructors as well as friends and family" (p. 2). Building from Mazer and his colleagues' work and Petronio's (2002) treatment of self-disclosure as coordinating boundaries around private information, Walther and his colleagues (Walther, Van Der Heide, Kim, Westerman, & Tong, 2008) noted that self-disclosure occurs alongside information about the self provided by other users (e.g., through "wall" posts or comments on status messages). Wright and his colleagues (Wright, Craig, Cunningham, Igiel, & Ploeger, 2008) further validate the importance of self-disclosure behavior via Facebook, finding that breadth and depth of self-disclosure is associated with increased interdependence and predictability. Thus, as Facebook's own slogan claims, the site is indeed a location where users share information about the self with a proscribed set of others.
Communication researchers have long recognized the role of self-disclosure in healthy relational development (Petronio, 2002) , and Mazer et al. (2007) likewise report that Facebook self-disclosure can enhance the quality of teacher-student relationships. However, evidence from other studies of online communication suggests that generalized attraction to OSD may be associated with negative psychological and relational outcomes. Online communication scholars have long considered the antecedents and outcomes of identity formation and self presentation enacted via OSD (O'Sullivan, 2000; Turkle, 1995) , with several studies reporting that communicators often self-disclose more online than they do when face to face (Ho & McLeod, 2008; Joinson, 2001; Postmes, Spears, & Lea, 1998) . McKenna, Green, and Gleason (2002) focused on self-disclosure in online-only relationships, arguing that lack of social competence may account for heightened selfdisclosure online because those with poor social skills may prefer the greater control over communication behavior that online contexts afford:
Logically, those individuals . . . who have the social skills needed to communicate themselves well and effectively have little need to express their true selves or "Real Me" over the Internet. The rest of us should be glad that the Internet exists. . . . Thus we would expect people who are lonely or are socially anxious in traditional, faceto-face interaction settings to be likely to feel better able to express their true self over the Internet and so to develop close and meaningful relationships there. (p. 12) Thus, they argue that motivation to self-disclose online may produce beneficial relational outcomes, as online communication may provide the socially anxious with opportunities to build social skills and meaningful relationships (see also Valkenburg & Peter, 2008) .
Like McKenna et al. (2002) , Caplan (2003) agreed that poor social skills are associated with a preference for online communication (and particularly online self disclosure). Caplan (2007) identified lack of communication competence as a theoretical motivator, arguing that those with high social anxiety prefer online communication's "greater control over self presentation" and "less perceived social risk, than in traditional FtF communication" (p. 235 ). Yet Caplan (2003) challenged the claim that such use generates positive outcomes, demonstrating that preference for online communication is associated with depression, loneliness, and other negative psychosocial outcomes. Though Caplan's (2002) research initially focused on online and offline social life as separate social spheres, his recent research identifies communication competence as a more general influence on online communication behavior; in other words, Caplan (2007) does not theorize or test whether relational medium of origin or degree of multimodality influences online communication frequency. Relatedly, Spitzberg's (2006) overview of communication competence in online contexts concludes that loneliness and depression are related to online communication use in complex ways. Following these lines of theoretical development, Ledbetter (2009b) directly tested the association between OSD and generalized communication competence, finding a significant moderate inverse association between the two constructs.
To summarize, self-disclosure is an important Facebook communication behavior, and thus we would expect those with high OSD to use it more. Moreover, Caplan's (2003 Caplan's ( , 2007 OSC. In contrast to OSD, we argue that maintaining existing social connections (i.e., OSC) is a relationally healthier motivation for using online communication. Ledbetter (2009b) reports that both OSC and OSD exhibit similar patterns of association with online communication behavior, yet differ in their association with generalized communication competence: Though OSD is inversely associated with communication competence, OSC yields a positive association of nearly equivalent magnitude. This may suggest that communicatively competent people do not seek online communication because they wish to avoid discomfort attendant with face-to-face communication, but rather because they perceive online communication as a useful method for sustaining preexisting weak and strong social ties (Haythornthwaite, 2005) .
Other research supports our assertion that OSC is associated with positive relational outcomes. When countering claims that online communication (i.e., more generally than just SNS use) produces negative relational outcomes (Kraut et al., 1998; Nie et al., 2002) , scholars frequently provide empirical evidence demonstrating beneficial outcomes for the strength of both local and long distance social ties (Baym, Zhang, & Lin, 2004; QuanHaase, Wellman, Witte, & Hampton, 2002) . That SNSs likewise maintain social networks may sound tautological; nevertheless, recent research elaborates mechanisms via which SNSs foster such connections. For example, Ellison and her colleagues (2007) reported that Facebook social connections develop several types of social capital, Stern and Taylor (2007) reported that college students use Facebook to maintain social connections developed on campus and with old friends, and Baym and Ledbetter (2009) suggested that shared interests may motivate the formation of some SNS relationships.
In addition to Ledbetter (2009b) , other empirical evidence suggests that internal attitudinal factors influence attraction to online communication as a space for building social connections. Both Donath (2007) and Tufekci (2008) conceptualized SNS use as analogous to social grooming among primates (Dunbar, 1998) , advancing the claim that resources devoted to regular, brief contacts facilitate relational ties with other individuals in a social network. Tufekci noted that this desire for social grooming varies in magnitude across individuals, with some people valuing such behaviors and others considering them unnecessary; in Tufekci's study, those who generally desire social grooming were also more likely to use an SNS. Donath claims that this motivation arises from the nature of SNSs as "more temporally efficient and cognitively effective" for the purpose of "maintaining ties" (p. 231). Donath noted that this increased efficiency may facilitate formation of social supernets or social networks that are larger than those sustainable through other communication media. This line of theoretical development resonates with Parks' (2006) recent argument that all dyadic relationships are intimately constituted in webs of network ties, with individuals sustaining ties using several communication media (Sawhney, 2007; Walther & Parks, 2002) . Taken as a whole, then, this research indicates that many people use SNSs because they wish to maintain existing social ties and that this motivation, in contrast to OSD, is associated with positive relational and psychosocial outcomes. What remains less clear, however, is the extent to which OSC is associated with offline communication between Facebook Friends. Some research suggests that those who engage in social networking behavior when online are also likely to do so when communicating offline (Quan-Haase et al., 2002; Tufekci, 2008 We also expect a significantly positive association between Facebook communication frequency and offline communication frequency. Though media multiplexity theory (Haythornthwaite, 2005) suggests that tie strength is a moderator (i.e., such that strong ties communicate across many media whereas weak ties use fewer media), most studies find a significantly positive association between offline and online frequency with a specific relational partner (Baym et al., 2004; Ramirez & Broneck, 2009 
Thus far, we have considered relational outcomes associated with Facebook communication but have not specified these in testable hypotheses. In this investigation, relational closeness is our chief outcome of interest, as Vangelisti and Caughlin (1997) noted that relational closeness is a variable of interest in a wide variety of relationship types (including friendship, family, and romantic relationships). Though we acknowledge that closeness is not the only possible relational outcome worthy of investigation, it is also worth acknowledging that close relationships are important sources of social support (Burleson & MacGeorge, 2002) and that ongoing closeness promotes relational longevity (Ledbetter, Griffin, & Sparks, 2007) . Closeness has also received attention as an outcome variable associated with several forms of online communication behavior across diverse samples, including online relational maintenance (among U.S. college students; Ledbetter, 2009a) , duration of Internet use (among Israeli adolescents; Mesch & Talmud, 2006) , and both frequency of online communication and depth of online self-disclosure (among Dutch adolescents; Valkenburg & Peter, 2007) . Though scholars have not devoted as much attention to closeness across SNSs, Baym and Ledbetter (2009) reported that though relational quality (a variable conceptually similar to closeness) with SNS Friends tends to be low, frequency of SNS contact between Friends is positively associated with relational quality (even after controlling for contact across other media). In this study, we conceptualize 34
closeness as a subjective experience of intimacy, emotional affinity, and psychological bonding with another person (see Aron, Mashek, & Aron, 2004) ; given the foregoing literature, we predict that frequency of Facebook communication will uniquely and positively predict Friend closeness. Our conceptualization of closeness bears strong resemblance to Haythornthwaite's approach to strong and weak social ties in her theory of media multiplexity. Strong social ties include relationships such as those with friends, romantic partners, and family members; such relationships exhibit behavior that reflects emotionality, interdependence, and intimacy (i.e., a high level of closeness). By contrast, weak ties are "casual contacts" that are more loosely connected to an individual's social network and are not characterized by intimacy (Haythornthwaite, 2005, p. 128) . According to media multiplexity theory, the number of different communication media that dyad members use is strongly associated with whether a tie is weak or strong. Specifically, strong ties employ several media types, but weak ties use only one or two media. As Baym and Ledbetter (2009) As we argued earlier, previous research and theory (e.g., Caplan, 2007) suggests that online communication motivated by OSD is associated with negative outcomes. Nevertheless, some interpersonal communication theory (Altman & Taylor, 1973) and empirical research (Laurenceau, Barrett, & Pietromonaco, 1998) suggests that self-disclosure is positively associated with relational closeness. Thus, the computer-mediated communication and traditional interpersonal communication literatures offer divergent predictions regarding this association. Thus, we advance a research question:
Research Question 1: Does OSD indirectly (i.e., mediated via Facebook and offline communication constructs) predict relational closeness (with specific Facebook Friends)?
Though earlier online research characterizes online communication as reducing a sense of social connection, work countering this claim demonstrates that those who build social connections offline also tend to do so online and, consequently, experience positive relational outcomes (e.g., Quan-Haase et al., 2002 That extant literature suggests divergent outcomes from OSC and OSD implies that these motivations are inversely associated with each other. However, previous research reports a positive association between the two constructs (Ledbetter, 2009b) ; as such, it is theoretically unclear what outcomes arise from an individual who possesses high levels of both motivations. Following Caplan (2007) , one might speculate that problematic Internet use driven by OSD would reduce beneficial outcomes from increased social connections. Alternatively, following theoretical arguments that online social ties may enhance the social skills of the lonely and socially anxious (McKenna et al., 2002; Valkenburg & Peter, 2008) , it could stand to reason that OSC is associated with positive social outcomes regardless of an individual's level of OSD. In any case, the extant literature at least suggests the possibility of a meaningful interaction effect between these two constructs on online communication and relational closeness, though the available evidence does not permit a prediction of the nature of this association in advance. Thus, 

In order to capture a diverse sample of Facebook users, we recruited participants via three approaches. First, with the consent of the computing services department at a large Midwestern university, a random sample was drawn from the list of all students enrolled in undergraduate courses. Second, other participants were recruited through announcements on the Facebook pages of various members of the research team. Third, we posted a call for participants on the listserv of a professional organization interested in technology and communication. After discarding participants that indicated no Facebook usage (n = 27), these sampling techniques resulted in a group of 325 participants (75 men, 250 women) with 226 (69.5%) identifying themselves as undergraduate students. Participants' age ranged from 18 to 59 years (M = 23.4, SD = 6.0), and most participants (90.5%) reported their ethnic identity as White.

Recruitment procedures directed participants to a Web link containing an informed consent form, and upon acceptance, participants were taken to the secure online questionnaire. If the participants were Facebook users, the questionnaire instructed them to open their Facebook account in a separate window and load their profile. At the time of data collection (early 2008), Facebook profiles included a box at the left side of the screen that displayed Friends selected from a person's primary network. Although Facebook has not publicly discussed the algorithm behind Friend selection for this window, tests of the feature at the time seemed to indicate that Friend selection was at least pseudorandom (although it is worth noting that Facebook's recent site redesign seems to have altered this algorithm since data collection). This method of Friend selection was designed to move beyond the practice of participant friend selection common in friendship research (e.g., Johnson, Wittenberg, Villagran, Mazur, & Villagran, 2003; Ledbetter, 2009a) . The survey directed participants to complete several measures based on the first Friend who appeared in this box. At the end of the survey, participants had the option of entering their e-mail addresses for a chance to win one of four US$20 gift certificates from Amazon.com. These e-mail addresses were removed from the data set before analysis.
Online communication attitude. The self-disclosure and social connection subscales of Ledbetter's (2009b) generalized measure of online communication attitude assessed OSD and OSC, respectively. The self-disclosure subscale contains 7 items: "I feel less nervous when sharing personal information online"; "I feel like I can be more open when I am communicating online"; "I feel like I can sometimes be more personal during Internet conversations"; "When online, I feel more comfortable disclosing personal information to a member of the opposite sex"; "I feel less shy when I am communicating online"; "I feel less embarrassed sharing personal information with another person online"; and "It is easier to disclose personal information online." The social connection subscale contains 6 items: (Wheeless et al., 2005) . Participants responded on a 7-point Likert-type scale with response options ranging from 1 (strongly disagree) to 7 (strongly agree). Cronbach's alpha reliability was acceptable for both the OSD (.92) and OSC (.87) dimensions. Friend demographic information. Participants reported basic demographic information about the randomly chosen Friend. Most reported that their Friend was a member of the participant's sex (n = 193, 59.4%), though others reported on cross-sex relationships (n = 132, 40.6%). Age of the Friend ranged from 17 to 60 (M = 22.8, SD = 5.2), with length of relationship ranging from 1 month to 43 years (M = 4.3 years, SD = 5.2). Most participants reported that their Facebook Friend was, indeed, a friend (n = 204, 62.8%) or an acquaintance (n = 73, 22.5%), though a small number reported on a romantic partner (n = 11, 3.4%), a family member (n = 6, 1.8%) or did not specify the type of relationship (n = 31, 9.5%). Though most participants reported on local relationships (n = 221, 68.0%), some reported on long-distance relationships (n = 104, 32.0%).
Facebook communication. Informed by Lenhart and Madden's (2007) description of the methods of communication possible within Facebook, a 6-point Likert-type scale assessed frequency of Facebook communication with the Friend. This measure contains 7 items: "I write on my friend's wall," "I send my friend a private message," "I communicate with the friend in a Facebook group," "I 'poke' my friend," "I comment on one of my friend's photographs," "I comment on one of my friend's notes," and "I communicate with the friend through an application on Facebook." Participants responded on a 6-point Likert-type scale with response options ranging from 0 (never) to 5 (very frequently). Following Baym and Ledbetter's (2009) evidence that communication frequency on another SNS (Last.fm) exhibits unidimensional structure, we submitted all items to an exploratory factor analysis using the principal components extraction method with varimax (i.e., orthogonal) rotation. Using the criterion of eigenvalue >1.0 produced a unidimensional solution with all items loading above 0.60 (McCroskey & Young, 1979) . The 7 items also demonstrated strong internal reliability (a = .87), and thus were averaged to form a single measure of Facebook communication frequency with the Friend.
Offline communication. Several theorists in the field of computer-mediated communication urge examination of online communication alongside offline communication media (Baym et al., 2004; Sawhney, 2007; Walther & Parks, 2002) . Following this line of theoretical development, Ledbetter (2009b) factor-analyzed media use via a 6-point Likerttype scale structure (0 = never to 6 = very frequently) adopted from Scott and Timmerman (2005) , finding that face-to-face and telephone communication load onto the same factor of offline media use. We used the same instrument in this study to measure frequency of offline communication with the Friend, with an additional item measuring cellular-phone text messaging. These 3 items demonstrated good internal reliability (a = .85), and thus were treated as separate manifest indicators of a single latent construct in the confirmatory and structural models.
Relational closeness. Vangelisti and Caughlin's (1997) 7-item measure assessed relational closeness with the Facebook Friend. Sample items include the following: "How often do you talk about personal things with this person?" and "How close are you to this person?" Participants responded on a 7-point Likert-type scale with response options ranging from 1 (not at all) to 7 (very much). The measure demonstrated strong internal reliability (a = .93).
All hypotheses and research questions were addressed via structural equation modeling (SEM) using the LISREL 8.80 for Windows software package. Two chief advantages of SEM are holistic assessment of an a priori specified model, which is clearly advantageous for the model specified in this study ( Figure 1 ); in addition, SEM corrects for error variance and thus more accurately identifies parameters of interest. We assessed model fit using four frequently reported fit indices: (1) model chi-square, (2) the root mean square error of approximation (RMSEA), (3) the non-normed fit index (NNFI), and (4) the comparative fit index (CFI; Kline, 2005) . For the RMSEA statistic, lower values indicate better model fit, with 0.08 the traditional threshold for acceptable fit (and 0.05 for close fit). For the NNFI and CFI statistics, better fitting models achieve higher values, with 0.90 and 0.95 as traditional thresholds for acceptable and close model fit, respectively (Kline, 2005) .
As shown in Figure 1 , the hypothesized model contained 6 latent constructs: (1) attitude toward online self-disclosure (i.e., OSD), (2) attitude toward online social connection (i.e., OSC), (3) an interaction term for OSD and OSC, (4) Facebook communication frequency, (5) offline communication frequency, and (6) relational closeness. The OSD, OSC, Facebook communication, and relational closeness constructs were identified by creating three parcels ("aggregate-level [indicators] comprised of the sum (or average) of two or more items, responses, or behaviors"; Little, Cunningham, Shahar, & Widaman, 2002, p. 152) per latent construct. Given the unidimensional nature of these constructs, items were assigned to parcels by thirds (e.g., for the 6-item OSD measure, the first parcel contained Items 1 and 4, the second parcel contained Items 2 and 5, and the third parcel contained Items 3 and 6). Offline communication was identified by single-item indicators of face-to-face, telephone, and text messaging communication. The interaction effect was modeled by creating an orthogonalized interaction term, a method that more effectively removes multicollinearity than Baron and Kenny's (1986) method of mean-centering predictors prior to computing the interaction term. As described by Little, Card, Bovaird, Preacher, and Crandall (2007) , this necessitates forming a series of nine product terms between the mean-centered parcels for each construct (i.e., all possible multiplicative interactions between one of the three OSD parcels and one of the three OSC parcels). These product terms were then regressed onto the firstorder parcels, and their unstandardized residuals were saved. These unstandardized residuals were then combined into three parcels such that each interaction-term parcel contains only one instance of each of the first-order parcels (see Marsh et al., 2007) , resulting in indicators that are entirely orthogonal to the first-order indicators (e.g., Soliz & Harwood, 2006) . Table 1 presents the correlation matrix between the continuous study variables at the manifest level of measurement. Before latent variable analyses, an EM (expectation-maximization) algorithm imputed the trivial amount of missing data (less than 1%) in the data set (Vriens & Melton, 2002) . Consistent with standard two-step procedures for SEM (Kline, 2005) , confirmatory factor analysis (CFA) first evaluated the fit between the manifest indicators and their respective latent constructs. To evaluate potential covariates, a series of three metric invariance tests (Little, 1997) compared (1) male and female participants, (2) undergraduatestudent status (i.e., undergraduate versus nonundergraduate participants), and (3) local and long-distance friends. Specifically, this procedure invokes a sequential series of model constraints that evaluate equality of indicator loadings (i.e., weak metric invariance), equality of indicator means (i.e., strong metric invariance), and homogeneity of the variance/covariance matrix among latent constructs. These tests indicated both weak and strong metric invariance Thus, any apparent differences between groups are likely due to chance variation, and thus all groups should be analyzed in a single structural model (Ledbetter, 2009a) To further probe the nature of the association between the two components of online communication attitude and Facebook communication, the interaction effect was decomposed using the method described by Cohen, Cohen, West, and Aiken (2003) . To do this, we recomputed the structural model as a mean and covariance structures (MACS) model. As Kline noted, standard SEM lacks a mean structure (i.e., all latent variables are assumed to be standardized with a mean of 0), and thus information about means is lost. A mean structure is added to a structural model "by regressing exogenous or endogenous variables on a constant that equals 1.0" (2005, p. 287) . From the standpoint of regression analysis, this essentially adds intercept terms to both the manifest and latent variables. By identifying the model and mean structure via the contrast coding method described by Little, Slegers, and Card (2005) , we obtained intercepts and predicted values that reflect the original measurement metric of the manifest indicators, thus aiding interpretation of the interaction effect decomposition. Using these values to generate linear regression equations, we plotted the relationship between OSC and Facebook communication at three different levels of OSD (i.e., at the minimum value of 1, at the latent mean value of 3.63, and at the maximum value of 7). Figure 3 presents results of this decomposition.

Though OSC positively predicts Facebook communication when OSD is low, increased levels of OSD weaken the strength of this association. Specifically, OSC significantly predicts Facebook communication at both the minimum, B = 0.29 (95% CI = 0.15-0.44), b = .39 (95% CI = 0.20-0.58, p < .01), and mean, B = 0.17 (95% CI = 0.09-0.26), b = .23 (95% CI = 0.11-0.34, p < .01), levels of OSD, but the association is nonsignificant at a maximum OSD score: B = 0.02 (95% CI = -0.15 to 0.19) and b = .02 (95% CI = -0.20 to 0.25, p > .05). Examination of the graph indicates that the regression lines converge at an OSC value between the minimum and the mean. Solving the regression equations for this point of convergence reveals that it occurs when an individual's OSC score is 1.88. In other words, when an individual's OSC is slightly below a mean response of 2 (i.e., disagree), that individual's Facebook communication with a specific Friend will tend to be 0.77 (i.e., slightly below a mean response of 1, or very rarely) regardless of that individual's level of OSD. Taken as a whole, these results suggest that OSD has a moderate inverse association with Facebook communication when OSC is high, and OSC has a moderate positive association with Facebook communication when OSD is low. At low levels of OSC or high levels of OSD, the effect of the other independent variable becomes much weaker. 42
The initially hypothesized model also predicted that both offline and Facebook communication are positively associated with relational closeness. When controlling for the significantly positive covariance between these two latent constructs, Y = .72 (95% CI = 0.65-0.79, p < .01), both offline communication, B = 1.30 (95% CI = 1.02-1.57) and b = .70 (95% CI = 0.55-0.85, p < .01), and Facebook communication, B = 0.33 (95% CI = 0.13-0.53) and β = .18 (95% CI = 0.07-0.30, p < .01), emerged as separate predictors of relational closeness. Offline communication appeared to be a much stronger predictor than Facebook communication, and thus we formally tested the significance of this difference by creating a nested model with the relevant regression paths constrained to equality; this produced a significant decline in model fit, Dc 2 (1) = 24.28, p < .01, demonstrating that offline communication is indeed a stronger predictor of closeness.
In addition to direct effects on relational closeness, the model also leaves the possibility that online communication attitude (i.e., OSC and OSD) indirectly predicts relational closeness via Facebook communication. This possibility was tested via Preacher and Hayes's (2004) procedure for generating robust estimates of unstandardized regression weights with nonparametric bootstrapping, a technique in which "cases from the original data file are randomly selected with replacement to generate other data sets, usually with the same number of cases as the original" (Kline, 2005, p. 42) . After computing the structural model across these data sets, the unstandardized regression weight is defined as the mean of the products of the indirect path's component parameter estimates; statistical significance is then determined by (a) sorting these estimates in ascending order, and (b) when a = .05 and k represents the number of bootstrapped samples, obtaining the values that appear at .025 × k and .975 × k in the ordered list. These represent the boundaries of the confidence interval; if this interval does not contain zero, then the bootstrapped estimate is statistically significant. As bootstrapping does not assume normal distribution of unstandardized regression weights, the boundaries of the confidence interval are not necessarily symmetrical around the estimate. Standardized regression weight estimates were obtained via the covariance matrix of latent constructs from computation of a model based on bootstrapped estimates of the covariance matrix of manifest indicators.
Bootstrap analyses revealed that OSC, B = 0.08 (95% CI = 0.01-0.15) and b = .04 (95% CI = 0.01-0.08, p < .05); OSD, B = -0.04 (95% CI = -0.10 to -0.001) and b = -.02 (95% CI = -0.06 to -0.001, p < .05); and the interaction effect, B = -0.03 (95% CI = -0.07 to -0.001) and b = -.02 (95% CI = -0.04 to -0.001, p < .05), significantly and indirectly predicted relational closeness. As the contrast coding method of identification is not amenable to bootstrapping in LISREL, we could not decompose the interaction effect in the metric of the original variables. Rather, we conducted decomposition using information from bootstrapped models with latent construct variance fixed to 1.0, thus expressing the interaction effect in terms of construct standard deviations. The pattern of results from this decomposition was almost identical to the decomposition for Facebook communication (Figure 3) , such that OSC is a significant positive predictor of relational closeness at low, that is, two standard deviations below the mean, B = 0.14 (95% CI = 0.04-0.23) and b = .07 (95% CI = 0.02-0.13, p < .01), and mean, B = 0.08 (95% CI = 0.01-0.15) and b = .04 (95% CI = 0.01-0.07, p < .05), levels of OSD but not when OSD is high, that is, two standard deviations above the mean, B = 0.01 (95% CI = -0.08-0.11) and b = .01 (95% CI = -0.04-0.06, p > .05). Together, the direct and indirect effects explained 70.7% of the variance in relational closeness.
The overarching goal of this investigation was to test a theoretical model whereby trait-like attitudes toward online communication predict Facebook and offline communication, with these constructs then predicting relational closeness. Results generally supported the hypothesized model, with the exception of the speculated paths between online communication attitude and offline communication. More important, OSD functioned somewhat differently than predicted by some previous online communication research (e.g., Caplan, 2007; McKenna et al., 2002) , not only yielding an inverse main association with Facebook communication but also reducing the positive contribution of OSC to this dependent variable and, indirectly, to relational closeness. Taken as a whole, these results support media multiplexity theory (Haythornthwaite, 2005) yet suggest that the theoretical expectation that social anxiety fosters online communication (Caplan, 2007) 
One of Facebook's core functions is building connections within a social network (Zuckerberg, 2008) , and, as expected (Hypothesis 3), those who use online communication for that purpose (i.e., possess high OSC) are more likely to communicate with their Facebook Friends (Tufekci, 2008) . As the OSC variable addresses orientation toward a preexisting social network rather than just a dyad, this calls attention to the need to understand broader network-level forces when examining dyadic relationships. In other words, traditional interpersonal communication theory considers closeness as an outcome of dyad-and individual-level variables, whereas OSC is an individual-level variable that may bespeak group-and network-level realities. Though network forces no doubt operate offline as well (Parks, 2006) , Donath (2007) argues that SNSs facilitate creation of social supernets, or social networks, "with many more ties than is feasible without socially assistive tools" (p. 231); this may only augment group-and network-level effects on specific dyads. The social-relationships model (Kenny, Kashy, & Cook, 2006) permits statistical isolation of individual, dyadic, and group effects and thus may be an invaluable tool for identifying which effects are truly individual, truly unique to the dyad, or truly a reflection of broader social forces.
Similarly, drawing from previous research and theory indicating that social anxiety produces attraction to OSD (e.g., Caplan, 2007; McKenna et al., 2002) , we predicted that OSD would positively predict Facebook communication with a specific Friend (Hypothesis 1). Instead, OSD inversely predicted Facebook communication in the final model. This differs both from the positive zero-order association with SNS use reported in Ledbetter (2009b) and the nonsignificant zero-order association reported here (see Table 1 ). This suggests that, when examined in the context of a structural model that controls for the shared variance between Facebook and offline communication, OSD may not foster Facebook use as it does other forms of online communication. Interpreting this unexpected finding requires reconsidering the theoretical mechanisms that underlie the association between preference for OSD and online communication use. One approach is to consider the role of moderating variables. McKenna et al. examined how individuals selfdisclose within completely online relationships, whereas most Facebook friendships exist between individuals who also know each other offline ; thus, considering the moderating influence of a relationship's medium of origin (as well as current degree of multimodality) in future research might further explain this unexpected finding.
Alternatively (yet not necessarily in contradiction), as Caplan notes, the logic of the expectation that OSD positively predicts communication frequency rests in the communicator's desire to manage self-presentation and identity:
In almost all social interactions, people are motivated to engage in strategic self presentation and identity management and to avoid making undesired impressions on others. Social anxiety arises from the desire to create a positive impression of one's self in others along with a lack of self-presentational confidence. Most importantly . . . the self-presentational theory of social anxiety posits that, in order to increase their perceived self-presentational efficacy, socially anxious individuals are highly motivated to seek low-risk communicative encounters. (p. 235) Traditional forms of online communication (e.g., e-mail) provide such low-risk encounters, as the private and/or asynchronous nature of the communication medium permits almost complete control over self-presentation (Walther, 1996) . Yet Donath (2007 ) argued (and Tong, Van Der Heide, Langwell, & Walther, 2008 , empirically confirm) that users of an SNS are evaluated, in part, in terms of the nature of their social connections with others; thus, it stands to reason that identity management partially lies within the control of social network members (and outside the control of the individual). Recent empirical evidence supports this theoretical claim, finding that wall posts written by friends and the physical attractiveness of those friends influences perception of a Facebook profile's owner . Walther and his colleagues explained these findings in terms of the information's level of warrant, or "degree to which that information is perceived to be immune to manipulation from the target to whom the information pertains" (p. 32); a wall post by a Facebook Friend is an example of such high-warrant information. Because the information target cannot favorably alter that information, Walther, Van Der Heide, Hamel, and Shulman (2009) argue that others perceive that information as more trustworthy than low-warrant information. Recent empirical evidence supports this expectation. Taken together with Caplan's findings (2007) and results of the current study, it is possible that those who are socially anxious may prefer traditional forms of online communication because they wish to control their own self-presentation by avoiding high-warrant information. As Facebook's site design encourages proliferation of high-warrant information (Zuckerberg, 2006) , those with high OSD may avoid it in favor of other low-warrant forms of online communication.
This line of argument is further supported by decomposition of the interaction effect between OSD and OSC on Facebook communication (Research Question 1). Though OSC is a positive predictor of Facebook communication when OSD is low, this association is nonsignificant at high levels of OSD. That is, high levels of OSD tend to weaken the association between OSC and Facebook communication. As noted in the theoretical warrant, such a finding supports Caplan's argument that OSD is socially debilitating, perhaps reducing beneficial outcomes that might otherwise accrue from the desire to maintain preexisting relationships online. In other words, if preference for OSD does reflect a desire for greater control over self-presentation (Caplan, 2007) , such a motivation may override a person's desire to build online social connections. Following Walther and his colleagues' (2008) recent research, perhaps those who possess both high OSC and high OSD seek out forms of online communication that do not provide high-warrant information. In terms of theoretical development, this suggests that social anxiety is not necessarily associated with online communication as a whole but rather encourages use of media that lack high-warrant information, online or otherwise. Testing this theoretical claim via experiment is a clear direction for future research.
The expectations that self-disclosure (Hypothesis 2), social connection (Hypothesis 4), and the interaction between them (Research Question 3) would predict offline 46
communication were not supported in the final model. Though previous research reports significant zero-order associations between these constructs and face-to-face communication frequency (Ledbetter, 2009b) , a significant association did not emerge when modeling offline communication as a latent construct and controlling for variance shared with Facebook communication. Perhaps other structural/contextual variables (such as temporal ability to synchronize schedules for offline contact; Ling & Yttri, 2002) influence frequency of offline communication with specific Facebook Friends, and thus the hypothesized associations did not emerge.
As predicted by media multiplexity theory (Haythornthwaite, 2005) , both offline communication (Hypothesis 6) and Facebook communication (Hypothesis 7) positively predicted relational closeness. This replicates the pattern of results obtained by Baym and Ledbetter's (2009) study of Last.fm, a music-oriented SNS, suggesting that the predictions of media multiplexity theory apply across many types of SNSs. What remains unanswered is whether use of multiple SNSs with the same friend also additively contributes to relational outcomes; indeed, we are not aware of any study that examines SNS use as a multimodal phenomenon. But if researchers cannot fully understand online communication use apart from patterns of offline communication behavior (Baym et al., 2004 , and as our final model indicates), then one might expect that continually examining single SNSs in isolation may yield an incomplete theoretical picture of their role in interpersonal relationships. The measures employed here offer at least some of the tools necessary for such future research. It is important to note that our Facebook communication scale was developed and used in the present study before some recent alterations were made to Facebook's status message feature; when these data were collected, the status-message feature forced users to include the word "is" (e.g., "John is tired" was possible, whereas "John stayed up too late last night" was not), and the feature did not include the ability to attach direct comments to a status message. Even though the instrument demonstrated strong internal reliability and unidimensional structure (as did Baym & Ledbetter's measure of Last.fm communication), scholars should consider the status message as a possible scale item in future investigations, as well as any other new forms of communication developed on such a continuously evolving website.
Both dimensions of online communication attitude and the interaction effect between them produced significant indirect effects on relational closeness. OSD inversely predicted relational closeness (thus answering Research Question 1), and OSC emerged as a positive predictor (Hypothesis 8). This pattern of results supports our chief contention that OSC is a healthy, communicatively competent motivation for using online communication; however, motivation arising from OSD is associated with negative relational outcomes (Ledbetter, 2009b) . As such, this investigation provides empirical evidence consistent with the theoretical expectation that attraction to OSD produces not only negative psychosocial outcomes but also negative relational outcomes (Caplan, 2003) . These results are also consistent with the finding that Facebook communication better supports and facilitates the concept of bridging (versus bonding) of social capital . As with decomposition of the interaction effect's association with Facebook communication, OSD and OSC interact in such a way that high OSD reduces the positive indirect association between OSC and relational closeness to nonsignificance. This is consistent with media multiplexity theory (Haythornthwaite, 2005) as, to the extent that high OSD reduces OSC's association with Facebook communication, the theory suggests that losses in closeness would occur unless dyad members compensate with the addition of another medium. Such an interpretation also follows Haythornthwaite's finding that different social networks enact different hierarchies of media use.
The direct and indirect effects in the model explained a large amount of the variance in relational closeness (approximately 71%). Along with other recent empirical evidence (Baym & Ledbetter, 2009; Ledbetter, 2009c) , this suggests that media multiplexity is a parsimonious yet robust account of how media use is associated with strength of a relational tie. In turn, this further supports the importance of studying individual attitudinal factors that may foster or inhibit use of particular communication media. On a more practical level, these results refute some popular claims that SNSs reduce relational closeness (Henry, 2007; Tilsner, 2008) , as Facebook communication positively predicted relational closeness even when controlling for the contribution of offline communication. However, this finding must be interpreted in light of the significantly stronger association between offline communication and relational closeness (perhaps reflecting that relational maintenance is more temporally efficient via media with multiple nonverbal cues; Walther, 1996) .
Of course, any study must be interpreted within the limitations imposed by the research design. Though it is tempting to make causal inferences from analytic methods that model endogenous and exogenous variables, the cross-sectional nature of the data necessitates caution. Future longitudinal research might test the extent to which closeness predicts communication frequency or vice versa. Though a particular strength of the study is the inclusion of data beyond a college-student sample and establishment of metric invariance across groups, the sample is relatively homogeneous regarding racial and ethnic identity. Future research may consider cultural dimensions such as individualism and collectivism that have demonstrated associations with online communication in previous research (Lee & Choi, 2005; Zhang, Lowry, Zhou, & Fu, 2007) . Our sample also contained more women than men, even though our recruitment procedures were not sex specific in any respect. We do not possess an explanation for why more women completed our questionnaire, and nonsignificant metric invariance tests suggest this probably does not influence study results greatly. The study also explained a relatively small amount of variance in Facebook communication; this is perhaps to be expected when global trait-like constructs predict variables located within specific relational contexts. Future research might address this by more explicitly examining the degree of variance that exists within individual SNS networks; dyadic data analyses (Kenny et al., 2006) may also yield higher effect sizes by accounting for the attitudes of both friends.
It is also worth noting that this investigation did not directly measure participant social anxiety. Though multiple studies establish preference for OSD as positively associated with social anxiety and related constructs (Caplan, 2007; Ho & McLeod, 2008; Kelly & Keaten, 2007; McKenna et al., 2002; Morahan-Martin & Schumacher, 2003; Valkenburg & Peter, 2008 ) and thus warrants use as an interpretive heuristic in this investigation, it remains possible that other forces foster a positive attitude toward OSD, such as the desire to create a sense of relational immediacy (e.g., in teacher-student relationships; Mazer et al., 2007) or finding others who share rare or stigmatized conditions (Walther & Boyd, 2002) . To the extent that social anxiety is not perfectly associated with OSD, it remains possible that OSD is positively associated with relational outcomes if shared variance with social anxiety were controlled. Of course, verifying this speculation requires further empirical investigation.
Given their widespread proliferation and adoption, especially among younger users (Lenhart & Madden, 2007) , it stands to reason that SNSs will remain an important medium for maintaining social connections. The existence of these sites raises important questions regarding individual traits that might influence online communication frequency and the integration of dyads into larger social structures (Parks, 2006) . These results inform these broader projects by further identifying attitude toward online self-disclosure and social connection as two such traits that may produce divergent effects on both media use and, to some degree, subsequent outcomes in interpersonal relationships.
In this paper we describe a very simple, distributed mutual exclusion protocol by which a process can gain the right to execute for a fixed time interval A without interference from other processes. Our protocol is directly inspired by backoff protocols for multiple access channels; the collision detection protocol of Ethernet is the most well-known example. In Ethernet, a process wishing to send on an (apparently) empty channel simply does so. If it detects that its send collided with another process', it "backs off" for a random delay and tries again later. Here we use similar principles to derive a mutual exclusion algorithm for synchronous message-passing systems that is deterministically safe and that ensures entry to the "critical section'.' with probability one.
The performance of our protocol can generally be characterized in terms of amortized system response time [ 5 ] .
Bell Labs, Lucent Technologies 600 Mountain Ave., Murray Hill, NJ 07974 USA reiter @ research.bel1-labs.com
The amortized system response time is the mean delay that each o f t processes incurs before entering the critical section, assuming that all t (and no others) start contending at the same time. We prove an upper bound on the expected amortized system response time of O ( A t ) , thereby showing that our protocol is adaptive in that the amortized system response time is independent of the maximum number of processes that might contend. In addition, in the case of no contention, the delay a process incurs before entering the critical section is merely one round-trip message delay on the network, and thus is independent of A.
Fault tolerance is a feature of our protocol. We present our protocol in a system model with distinct clients and servers, motivated by the system in which we have implemented it, described later. Clients, which contend for mutual exclusion, may crash without affecting the protocol. In particular, since a client is granted exclusion for a fixed time period A-and there is no designated "unlock" operation that a client must perform-a client's failure after it succeeds in gaining exclusion does not preclude other clients from subsequently gaining exclusion after the A time period expires. Moreover, our protocol masks the arbitrary (Byzantine) failure of a threshold number of servers.
We use this mutual exclusion protocol to develop a protocol by which operations on a replicated object can be serially ordered. Despite the fact that this ordering protocol is deterministically safe even in an asynchronous systemand our mutual exclusion protocol is not-the mutual exclusion protocol is key to ensuring that operations are ordered and complete (with probability one) once the system stabilizes. Our ordering protocol orders arbitrarily many operations on the object as soon as a single contender gains access to the critical section.
Aside from always-safe operation ordering, we have found our mutual exclusion protocol useful for other tasks within the system that motivated it, called Fleet [ 161. Fleet supports highly available, shared data for clients using an infrastructure of servers that may suffer arbitrary (Byzantine) failures. In order to detect the presence of faulty servers, statistical fault detection algorithms mine for evidence of faulty servers in the responses they return [I] . Since detection is most accurate when data is accessed sequentially, Fleet attempts to serialize data accesses, and we employ the mutual exclusion protocol described here for this purpose. Our mutual exclusion protocol has the useful property that it remains probabilistically live even during periods of instability (asynchrony) in the system.' So, while fault detection may suffer during periods of instabiliity, the nonblocking properties of the Fleet data access protocols are never compromised.
The rest of this paper is structured as follows. We review related work in Section 2 and more precisely state our system model in Section 3. In Section 4 we describe our mutual exclusion protocol, and we outline certain optimizations to it in Section 5. We then develop our ordering protocol based upon it in Section 6. A proof of correctness for our ordering protocol can be found in Section 7.
In Singhal's taxonomy [21], the mutual exclusion protocol we present is a "Maekawa-type" protocol, following [12] . In this class of protocols, a process pi requests permission to enter the critical section from a set Qi of processes, such that Qi f l Qj # 8 for all i, j . Each process in a request set Qi grants exclusive access to the critical section until it is exited, and pi is allowed to enter the critical section only if all processes in Qi grant pi access. Due to the intersection property of request sets and exclusive locking at each process in a request set, only one process can be in the critical section at any time. Also due to these properties, however, Maekawa-type algorithms are generally prone to deadlock and consequently require extra messages to 'detect and recover from deadlocks. Deadlock-free Maekawa-type protocols, such as [ 10,201, have been proposed by strengthening the constraints on request sets so that for all i , j , eitherpi E Qj o r p j E Qi [21]. However, in our context, this strengthening is not possible because the clients requesting mutual exclusion are distinct from the servers that comprise the request sets. Clients cannot be added to reque., qt sets because they are transient and because the population of clients that might contend is not known a priori. The protocol that we present here works with Maekawa's original (weaker) intersection property Qi n Q j # 8 in the fault model addressed in [12]. At the same time, our protocol is not prone to deadlock.
'Formally, probabilistic liveness during periods of instability holds only if the scheduling adversary is nonadaptive. That is, for any execution, the scheduler chooses the distribution from which message delays will be drawn before the protocol execution begins; it cannot change this distribution in response to events in the execution. We omit further discussion of this issue here, except to note that in practice, this is an assumption we are willing to adopt for Fleet.
As discussed in Section I , we evaluate our protocol based on the amortized system response time that it achieves. This measure was introduced in the context of shared-memory mutual exclusion algorithms [ 5 ] , where there are examples boasting amortized system response times of O ( t ) or even O(1) (e.g., [3, 51). An alternative to using our protocol is to employ one of these algorithms, using a distributed protocol to emulate each shared variable it uses (e.g., [ 151). While the resulting algorithm would have superior amortized system response time (asymptotically), the performance in practice would be far worse than our prolocol in the main case we care about-i.e., contentionfree performance-due to the overheads of the variable emulation protocols. This also holds for backoff-style mutual exclusion algorithms explored for the shared-memory setting (e.g., [ 2 ] , which assumes even stronger objects than shared variables).
The manner in which we build upon our mutual exclusion protocol to order operations on data objects in an asynchronous system is similar to work of Fetzer and Cristian on consensus in the tinied asynchronous model [6], and the works of Lamport on Paxos [ 1 11 and of Keidar and Dolev on extended3-phase commit (E3PC) [8] . These works compose a (not necessarily safe) mutual exclusion protocol with a commit protocol to derive solutions to problems equivalent to our ordering problem. Paxos and E3PC, while building on mutual exclusion protocols, do not propose mutual exclusion implementations of their own. Fetzer and Cristian employ a mutual exclusion protocol that rotates the preference for access to the critical section among the possible contenders in sequence, but enables the next preferred contender to be bypassed without delay if that contender is unavailable. As such, its mutual exclusion primitive is also adaptive in the sense above. In order to achieve this, however, the: protocol relies on clock synchronization among the participating servers. Clock synchronization is not required by our protocol. More generally, however, our work contributes relative to all the above by providing a new and efficient mutual exclusion primitive, and by admitting arbitrary (Byzantine) server failures in our ordering protocol.
Our system model divides the set of processes into clients and servers. We assume a fixed, known set I / of n servers and an arbitrary, finite, but unknown number of clients. The protocol of a process is described in terms of event handlers that are triggered by the arrival of events such as the receipt of a message, the tick of a local clock, or input from a calling application. Once triggered, we assume that Ihe event handler runs to completion without delay.
Processes that obey their protocol specifications and receive (and handle) infinitely many events in an infinite run are called correct, Other processes are calledfuully. Up to a threshold b of servers may fail, and may do so arbitrarily (Byzantine failures); i.e., the event handlers of a faulty server may not conform to their specifications. While any number of clients may fail, clients are assumed to fail only by crashing, i.e., simply by no longer receiving events. This restriction of client failures to crashes may seem unrealistic when servers are presumed to fail arbitrarily. However, typically little can be done to protect an application from Byzantine clients, as such clients can always corrupt an object's data by submitting requests with incorrect content. One way of dealing with this in practice is to allow the creator of an object to prohibit untrusted clients from modifying objects using access control mechanisms that remain in force at correct servers provided that b or fewer servers fail (even arbitrarily). Thus, our assumption of client crashes in practice reduces to an assumption about the clients trusted to modify that object by its creator.
We assume that the local clock of each correct process ticks at the same rate as real time, so that a process can accurately measure the passage of a chosen, real-time timeout period. Since the timeout periods involved in our protocols would be very short in practice, this is a reasonable assumption. We do not assume that clocks at processes are synchronized.
Processes communicate by message passing. We assume that communication channels provide at-most-once message transmission: if pl and p2 are correct, then p2 receives any message m from pl at most once, and then only if pl sent m to pa. (Obviously, we further assume that p l never sends the same message twice to p 2 , which it can implement by, e.g., including a unique sequence number in the message.) There is a globally known constant 6. We say that a run is stable ut real time T if for each correct client, there exists a quorum of correct servers such that any message sent at time T' 2 T between that client and a server in that quorum arrives by time T' + 6. The definition of quorum that we use will be given in Section 4. A run is synchronous if it is stable at the time the run begins. Though processes communicate by message passing, we present our protocols in terms of remote operation invocations on servers, for simplicity of presentation. In synchronous runs, we also neglect the processing time of such remote invokations and assume that they complete instantaneously. We will return to explicit message passing events when necessary to prove correctness.
In this section we present our mutual exclusion protocol by which clients can contend for the opportunity to run for A time units without interference by other clients. More precisely, there is an operation contend that a client can invoke. When the invocation returns at the client, the client then has A time units in which to execute in isolation. After A time units pass, however, another client's contend operation may return, As discussed in Section 1, in addition to requiring mutual exclusion, we will also be concerned with the system response time.
The idea of the protocol is for clients to access servers simply to find out whether other clients are simultaneously contending. In order to provide mutual exclusion, every pair of clients must access 2b + 1 correct servers in common. If a client detects that another client is contending, it backs off for a random delay, chosen from a distribution that adapts to the number of contending clients. Intuitively, clients thus delay an amount of time proportional to the number of simultaneously contending clients, while eventually, when they sufficiently space their contentions, each succeeds. More precisely, the protocol is probabilistically live in a synchronous system, i.e., with probability 1 some client's contend operation returns.
The requirement that any two clients access at least 2b+ 1 common correct servers can be satisfied if each client queries the servers according to a special variant of masking quorum systems [ In this figure, ' ' 11' ' denotes concurrent invocation of statements, and " d t~ S" denotes the selection of an element of set S uniformly at random and assignment of that element to d. At a high level, the protocol executes as follows.
When presented with a request from a client, the server re- where s is a "retry value" that records the number of times the client has previously queried servers in this contend operation. That is, clients employ an exponential backoff strategy: the expected duration of a client's delay is proportional to twice its delay during its last retry.
The correctness of this protocol is proved easily in the following lemma: As discussed previously, t h e m e a s u r e o f quality on which w e focus for our mutual exclusion protocol is amortized system response time. T h e following l e m m a implies that the expected amortized s y s t e m response t i m e i s O(At). In t h e mutual exclusion protocol as presented in Figure 1 a n d analyzed in Lemma 2, client backoff w a s exponential a s a function of t h e n u m b e r o f retries in its contend operation.
Even though exponential backoff yields O(At) amortized system response time, analysis of backoff strategies in the context of multiple access channels shows that it performs less well in other measures than various polynomial backoff strategies (e.g ., [ 7 ] ) . While this analysis does not apply to our case directly, we expect that similar properties hold in our setting, and thus in practice it may be preferable to experiment with other backoff strategies.
In this section we sketch several possible improvements and optimizations to the mutual exclusion protocol. The implementation of the proposed ideas and the assessment of their practical implications are the subject of our ongoing work.
Avoiding backoff by breaking symmetry If the application is such that one client c repeatedly contends with little delay between contentions, then we can improve c's response time if c does not back off between consecutive try attempts. The backoff protocol will adequately space the other client's retries, and c's asymmetric strategy will enable it to gain mutual exclusion quickly.
In this optimization, each server maintains an internal data structure, called delayed reply fist, where it records IDS of the clients whose try requests arrive while the server is locked. As soon as the server's status becomes FREE, it goes through the records in the delayed reply list and sends FREE to the client with the lowest ID and LOCKED to everyone else. This optimization may allow the lowest ranking contending client a smooth entry to the critical section, without backoff.
It is possible to make the protocol safe even during instability periods if clients disregard those replies to their try() requests that arrive after 26 time units. However, in practice, this optimization can negatively affect the throughput of the applications whose implementation does not require the underlying mutex to be safe (e.g., the operation ordering presented in Section 6).
Parameterized contend In order to allow for better adaptation to changing system conditions and to application needs, it is possible to make A a parameter of the contend operation (and, consequently, of the try request) instead of being a system-wide constant. Both safety and the expected delay become parameterized by the actual A's employed.
As discussed in Section 1, one of the main applications for the mutual exclusion protocol of Section 4 is a protocol for serializing operations on replicas of an object in a distributed system. In order to perform an operation o on the replicated object, a client application submits the operation for execution. The properties that our ordering protocol satisfies are the following:
Order There is a well-defined sequence in which submitted operations are applied, and the result of each operation that returns is consistent with that sequence.
Liveness If a run is eventually stable, then every operation submitted by a correct client is performed with probability one, and if performed, its result is returned to the client.
Due to the Order and Liveness properties, our protocol emulates state machine replication [ 191. Among others, our implementation supports the following distinct features: First, the ordering responsibilities are delegated to the clients, which are not Byzantine by assumption. This way, we need not employ digital signatures or signaturelike cryptographic constructions, thus improving the performance and scalability of the protocol. Second, our protocol makes progress by updating only quorums of replicas, which helps to achieve better load balancing and enhances scalability. Third, our protocol supports nondeterministic operations, since each operation is applied at a client and the resulting object state is then copied back to servers.
Some modern protocols for implementing state machine replication in Byzantine environments (e.g., [ 17, 9, 41) assume a less restricted failure model by allowing arbitrary client failures. In these solutions, clients do not actively participate in the protocol, but serve merely as users that inject new operations into the server universe and collect responses. While this approach prevents Byzantine clients from interfering with the ordering protocol, it does not prevent attacks in which faulty clients corrupt object's data by submitting operations with arbitrary parameter values. Thus, in practice, the added value of providing protection against Byzantine client failures in terms of the system security guarantees is outweighed by the performance and scalability gain resulting from delegating ordering responsibilities to the clients.
The detailed client and server programs are shown in Figure 2 and Figure 3 respectively. The client program for submit(o) consists of two threads executed concurrently. The first thread, described in lines 2.3-7, simply submits the operation o to the servers for execution and awaits responses. The second thread, lines 2.8-32, invokes operations to create a new state and commits states in a serial order; we call this the ordering thread. I f f and g are: functions, then f1g denotes a function such that (flg)(o) == g ( o ) if g(o) # I and f ( o ) otherwise; see line 3.24. The following subsections contain details about operations, states, and ranks that are essential to understanding the ordering thread.
Our protocol works by applying an operation to ii state to produce a new state and a return result. A client submits an operation o to be performed by invoking submit(o). For simplicity of presentation, we assume that the same operation is never submitted by two distinct clients or twice by the same client. In practice, enforcing such uniqueness of operations can be implemented by each client labeling each of its operations with the client's identifier and a sequence number.
A state, denoted by (T (possibly with subscripts and/or superscripts), is an abstract data type that has the following interfaces:
a.version is an integer-valued field. It denotes the "version'' of the state. This field can be set by the protocol manipulating the state. A state's interfaces are assumed to satisfy the following properties. First, a.reflects(o) = t r u e iff doOp(o) vias invoked on some previous state. In practice, this can be implemented by recording within the state the highest operation sequence number already performed for each client. Second, if (T is the result of applying operations (via do0p) to a prior instance (T' such that d.reflects(o) = false, g.reflects(o) = true, and cT.version = d.version + -1, then a.response(o) is defined and returns the result for operation o. Note that by this assumption, a.response(o) can be eliminated ("garbage collected") when o.version is incremented. In this way, the size of (T can be limited.
Aside from the instance of garbage collection just mentioned, we do not further elaborate on garbage collection here. The primary data structures that grow in our protocol as presented in Figures 2 and 3 are (i) the record of which client operations have been performed (to compute u.reflects(o)) and (ii) a response function maintained at each server that records the response for each client operation (see lines 3.34,24). In practice, eliminating unnecessary data from these structures can be achieved, for example, by propagating information among servers in the background (e.g., using the techniques of [ 131) to convey when information about a given operation can be purged from the system. Other optimizations are possible, e.g., that trade off passing complete states versus update suffixes.
Each client executes the ordering thread of our protocol with an associated integer called its rank. We assume that no (WO clients ever adopt the same rank, which can be ensured, e.g., if each client's rank is formed with its identifier in the low-order bits. When invoking an operation on a server U in our protocol, a client always sends its current rank: as an argument to the invocation; this rank is denoted by r in u.get(r), u.propose(a, T ) and u.commit(a, r ) invocations. A server responds to only the highest-ranked client that has contacted it. In particular, if a server U is contacted by a client with a lower rank than another client to which it has already responded, then it throws a RankException that notifies the client of the higher rank under which another client contacted it. In order to get U to respond to it, the client will have to abort its current protocol execution, adjust its rank, and try again (starting at line 2.8).
The precise criteria that dictate when a client aborts its protocol run to adjust its rank are important to the liveness of our protocol. On the one hand, if a client aborts its protocol run based upon receiving a single RankException. then the client risks being aborted by a faulty server who in fact was not contacted by a higher ranking client. On the other hand, if the client requires b + 1 RankExceptions in order to abort, then the client may not abort even though b correct servers have been contacted by a higher-ranking client and thus will refuse to return responses to this client.
Our solution to this issue therefore mandates that the quorum system Q we employ satisfy the following property: For every B1, B2 s U with lBll = (Bzl = b, there exists Q E Q such that Q n (B1 U Bz) = 0. This restriction enables the client to complete its protocol run using quorums provided up to b correct servers respond with RankException. Consequently, whereas original masking quorum systems existed as long as n > 4b [ 141, this stronger constraint limits their existence to systems in which n > 6b (see Corollary 4.4 in [ 141). When a client is forced to adjust its rank due to receiving b + 1 RankExceptions, it does so by choosing a value larger than the maximum of all ranks reported by those RankExceptions.
We note that an alternative approach would be for clients to digitally sign (e.g., [IS] ) their ranks using a key available only to clients allowed to access the object (or a subset of them designated to execute the ordering protocol). When a scrver throws a RankException to a client, it passes the highest rank under which any client has contacted it, includ-1) submit (0):
2) waiting t true; contend(); ] until (pending = 0); a'.version t a'.version + 1;
ing the digital signature on that rank from that client. The client receiving the RankException can verify the validity of the rank by verifying the digital signature on it. In this implementation, a client can abort its protocol run based on a single RankException with which the client receives a validly signed rank, since a faulty server cannot forge signatures. This approach imposes overheads in terms of key management and computation, and we therefore opt against it. In particular, digital signatures tend to be relatively intensive to compute and verify. While for a small number r of clients, digital signatures can be emulated using message authentication codes, this approach does not scale well.
At a high level, the ordering thread of the protocol at a client works by first contending for mutual exclusion,, using the protocol of Section 4 (line 2.9). Once this contend returns, the protocol executes similarly to a 3-phase c:ommit protocol. It first invokes get on each server U in some quorum Qg" to obtain the states last committed to U (0:) and last proposed to U ((TE"); the rank proposer, of the: client who proposed a : " ; and the current set pending, of pending operations submitted to U . The client then computes the following values: a' is set to be the state with the highest version number that has been committed to some correct server ((i.e., at least b + 1 servers) in Qget (lines 2.14-15). 0 upc is set to be the state proposed to some correct server (Le., at least b + l servers) in Qg" by the h:ighestranking set of proposers (lines 2.16,33-43) .
completed is set to be the highest version number of all states that the responses from the servers iin Qget reveal to be committed at a full quorum. In part.icular, if b + 1 servers report proposed states atC with version numbers larger than U , then a state with version v must be committed at a full quorum (line 2.17).
'The client chooses which state (T to propose and commit to quorums based on these values. If oc has a version number larger than completed, then it will propose and com-. mit a' to ensure that c f gets committed to a full quorum (line 2.19). Its second choice will be to propose and commit the proposed state upc if its version number is larger than completed (line 2.21). Otherwise, it creates a new state by applying operations to a' (lines 2.23-24,44-5 1), and proposes and commits that state.
The protocol ensures that each newly proposed object state (T' is derived from the state CT that has been most recently committed by applying operations in the pending sets of correct servers to (T (line 2.24). This is guaranteed as follows. If 0 has been committed to a full quorum, then (T' = U at each correct server in that quorum. This implies that any client that succeeds in invoking get at a full quorum evaluates (T' to (T, and applies any pending operations to it. If, on the other hand, (T has not been committed at a full quorum, then it is possible for clients to evaluate ac to a prior state. However, since U must be proposed to a full quorum before it is committed, any client that invokes get on a full quorum evaluates nPC to 0. The client will therefore complete the commitment of a (bypassing (T' since completed 2 aC.version) and then continue by applying new operations to a to derive a'.
Kank is used to break ties between clients that attempt to propose different states simultaneously. Suppose that p and q each invoke get on a quorum of servers and obtain cc := a as above. 
In this section we prove that Order and Liveness are satisfied by our protocol. Let M denote a finite set of methods that for any object state U , a rank r and an operation o, consists of get(.), propose(a, r ) , commit(a, r ) and submit(o).
We assume that any method p E M can be invoked at a server u at most once throughout the execution. In practice such a requirement can be easily enforced using unique method identifiers composed of client identifier and the sequence number. Let p.rnnk be the rank with which method p is invoked. We consider the following system events: For a client p , let p.send(u, p ) be the client event that sends the method invocation p E M to server U, and let p.ret(u.p, p ) , be the client event triggered by the reply of the server U with return value p to a previously sent method p.
A server event is a computation performed upon receiving g e t , propose, commit, or submit invocations from a client. The event that occurs at a server U as a result of the invocation of a p E M is denoted u.p. The code executed by a correct server upon reception of such an invocation is the code of the corresponding server method (see Figure 3) . This code executes to completion (return) atomically, with the exception of submit ; submit executes atomically until the sleep command, and its return constitutes a separate event. A faulty server can perform arbitrary computation steps upon reception of client invocations.
We model the system execution as a countable set H of events partially ordered by -+ relation induced by the natural order of the method invocations and returns. We define the causal cone of an event e in H , denoted ccone(e, H ) , to be the subset of H such that Ve' E H , e' E ccone(e, H ) iff e' -+ e.
If a client p invokes method p on every server U E S C U in H , then we will unite all p.send (u,p) 
For the following lemma, we introduce the following definition. For any p.propose(a,r), we define its closest complete propose to be p'.propose(a', r') such that p.propose(u,r); and (iii) there does not exist a comp".propose(u", r " ) -+ p.propose(a, r ) . Note that any p.propose(a, r ) , other than the propose of a' at system initialization, has a closest complete propose, and that its closest complete propose is unique by Corollary 3.
(i) p'.propose(u', r') is complete; (ii) p'.propose(u', r') + H plete p".propose (a", r") (a2, r g ) ) . That is, we suppose the result holds for any q'.propsse(a', T ' ) E ccone(q.propose(a2, T Z ) ) , and we prove the result for q.propose(a2, T Z ) . Let p.propose(C, T ) be the first complete propose invocation in the causal chain leading to p.propose(a1, T I ) such that 5.version = u1.version. By the induction hypothesis for 4.2, C = a l .
According to the protocol, the value of a2 is computed based on the values of ( 0 ; ; ar, proposeru, pending,) returned by each server U in some quorum ~g~~(~z ) in response to g.get(r2) invocation. Furthemore, if U is correct, then the value of each a:, a,P' and proposer,, is determined by some (not necessarily complete) propose invocation p'.propose(a', T ' ) E ccone(q.get(r2)). By applying results of Lemma 3, Corollary 3 and the induction hypothesis, we conclude the following: If T' 3 f , then the closest complete propose ofp'.propose(a', T ' ) is eitherp.propose (5, F ) or the one that causally follows p.propose(6, T ) . Therefore, either U' = (TI, or U' is a state that extends U I with some previously submitted operations. Otherwise, if T' < F , then the closest complete propose of p'.propose(u', T ' ) causally precedes p.propose (5, F ) and therefore, a'.version 5 5.version.
Once we know the possible values of U : , u u p C and proposer, as returned by q.get(rz), and given that any two quorums intersect by at least 2b + 1 servers, we derive that the value of U 2 computed in lines 2.14-24 satisfies the lemma results.0 Theorem 1 (Order) There is a well-deJinfid sequelwe in which submitted operations are applied and the result of each operation that returns is consistent with that sequence. Theorem 2 (Liveness) If a run is eventually stable, then every operation submitted by a correct client is petjhrmed with probability one, and $performed, its result is rerurned to the client.
Proof :(Sketch) Once the system is stable, eventually some (correct client q returns from its invocation of contend with probability m e . This client executes for sufficiently long (if A is chosen adequately) in isolation of other clients. It either commits an existing state a to a full quorum or else extends U with operations in pending and proposes and commits the new state U' at a full quorum.0
Taxonomies and, in general, networks of words connected with transitive relations are extremely important knowledge repositories for a variety of applications in natural language processing (NLP) and knowledge representation (KR). In NLP, taxonomies such as WordNet [17] are widely used in intermediate tasks such as word sense disambiguation (e.g. [1] ) and selectional preference induction (e.g., [25] ) as well as in final applications such as question answering (e.g., [4] ) and textual entailment recognition (e.g. [5] ). In KR, taxonomies as well as other word networks are the bulk of domain ontologies.
To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29] ). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21] . These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26] ). The task is generally seen as a classification (e.g., [22, 27] ) or a clustering (e.g., [3] ) problem. This allows the use of machine learning models.
Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27] , are represented. These feature spaces are used to determine whether or not new word pairs coming from the text collection have to be included in existing knowledge repositories. Decision models are learnt * DISP University Rome "Tor Vergata" using existing knowledge repositories and then applied to new words or word pairs. Generally, these models use as features all the possible and relevant generalized contexts where words or word pairs can appear. For example, possible features in the word pair classification problem are "is a" and "as well as". Given the nature of the problem, these feature spaces can then be huge as they include all potential relevant features for a particular relation among words. Relevant features are not known in advance. Yet, large feature spaces can have negative effects on machine learning models such as increasing the computational load and introducing redundant or noisy features. Feature selection is the solution (see [11] ).
In this paper, we want to study how to improve performances of taxonomy learning methods by using feature selection. We focus on the probabilistic taxonomy learning model introduced by [27] as it uses existing taxonomies exploiting the transitivity of the isa relation. Leveraging on the particular model, we propose a novel way of using singular value decomposition (SVD) as unsupervised model for feature selection. In a nutshell, given the probabilistic model for taxonomy learning, we use SVD as a way to compute the pseudoinverse matrix needed in logistic regression. We will analyze if our method for using unsupervised feature selection positively affect performances.
Before staring, in Sec. 2 we will shortly review methods for taxonomy learning and for feature selection. We motivate our choice of working within the probabilistic setting. In Sec. 3, as SVD is the core of our method, we will then introduce SVD as unsupervised feature selection model. In Sec. 4 we then describe how we introduced SVD as natural feature selector in the probabilistic taxonomy learning model introduced by [27] . To describe how we use SVD as natural feature selector, we will shortly review the logistic regression used to compute the taxonomy learning model. We will describe our experiments in Sec. 5. Finally, in Sec. 6, we will draw some conclusions and describe our future work.
Extracting knowledge bases from texts is one of the major goal of NLP and KR. These methods can give an important boost to knowledge-based systems. In this section we want to shortly analyze some of these methods in order to motivate our choice to work within an existing probabilistic model for learning taxonomies. We also review the more traditional models for super-vised and unsupervised feature selection.
The models for automatically extracting structured knowledge, such as taxonomies, from texts use variants of the distributional hypothesis [12] exploit some induced lexical-syntactic patterns (originally used in [26] ).
The distributional hypothesis is widely used in many approaches for taxonomy induction from texts. For example, it is used in [3] for populating lattices, i.e. graphs of a particular class, of formal concepts.
Lexical syntactic patterns are also a source of relevant information for deciding whether or not a particular relation holds between two words. This approach has been widely used for detecting hypernymy relations such as in [13, 18] , for other ontological relations such as in [21] , or for more generic relations such as in [24, 28] . These learning models generally use the hypothesis that two words are related according to a particular relation if these often appear in specific text fragments.
Despite the wide range of models for taxonomy learning, only very few exploit the structure of existing taxonomies. The task is seen as building taxonomies from scratch. In [3] , for example, lattices and the related taxonomies are the target. Yet, existing taxonomies may be used to drive the process of building new taxonomies. In [19] , WordNet [17] and WordNet glosses are used to drive the construction of domain specific ontologies. In [22] , taxonomies are augmented exploiting their structure. Inserting a new word in the network is seen as a classification problem. The target classes are the nodes of the existing hierarchy. The distributional description of the word as well as the existing taxonomy structure is used to make the decision. This model is purely distributional. In [27] , a probabilistic model exploiting existing taxonomies is introduced. This model is purely based on lexicalsyntactical patterns. Also in this case, the insertion of a new word in the hierarchy is seen as a binary classification problem. Yet, the classification decision is taken over a pair of words, i.e., a word and its possible generalization. The probabilistic classifier should decide if this pair belongs or not to the taxonomy.
The probabilistic taxonomy learning models has at least two advantages with respect to the other models. The first advantage is that it coherently uses existing taxonomies in the expansion phase. Both existing and new information is modeled in the same probabilistic way. The second advantage is that classification problem is binary, i.e., a word pair belongs or not to the taxonomy. This allows to build a unique binary classifier. This is not the case for models such as the one of [22] , where we need a multi-class classifier or a set of binary classifiers. For these two reasons, we are using the probabilistic taxonomy learning setting for our study.
Yet, in applications involving texts such as taxonomy learning, machine learning models are exposed to huge feature spaces. This has not always positive effects. A first important problem is that huge feature spaces require large computational and storage resources for applying machine learning models. A second problem is that more features not always result in better accuracies of learnt classification models. Many features can be noise. Feature selection, i.e., the reduction of the feature space offered to machine learners, is seen as a solution (see [11] ).
There is a wide range of feature selection models that can be classified in two main families: supervised and unsupervised. Supervised models directly exploit the class of the instances for determining if a feature is relevant or not. The idea is to select features that are highly correlated with final target classes. Information theoretic ranking criteria such as mutual information and information gain are often used (see [8] ). Unsupervised models are instead used when the information on classes of instances is not available at the training time or it is inapplicable such as in information retrieval. Straightforward and simple models for unsupervised feature selection can be derived from information retrieval weighting schemes, e.g., term frequency times inverse document frequency (tf * idf ). In this case, relevant features are respectively those appearing more often or those more selective, i.e., appearing in fewer instances.
Feature selection models are also widely used in taxonomy learning. For example, attribute selection for building lattices of concepts in [3] is done applying specific thresholds on specific information measures on the attributes extracted from corpora. This models uses conditional probabilities, point-wise mutual information, and a selectional-preference-like measure as the one introduced in [25] .
A very important way of unsupervised feature selection is the application of the SVD. As this is the bulk of our methodology we will review how SVD can be used for this purpose. SVD has been largely used in information retrieval for reducing the dimension of the document vector space [7] . SVD, originally, is a decomposition of a rectangular matrix. Given a generic rectangular n × m matrix A, its singular value decomposition is A = U ΣV T where U is a matrix n × r, V T is a r × m and Σ is a diagonal matrix r × r. The diagonal elements of the Σ are the singular values such as δ 1 ≥ δ 2 ≥ ... ≥ δ r > 0 where r is the rank of the matrix A. For the decomposition, SVD exploits the linear combination of rows and columns of A.
There are different ways of using SVD as unsupervised feature reduction. An interesting way is to exploit its approximated computations, i.e. :
where k is smaller than the rank r. The computation algorithm [10] allows to stop at a given k different from the real rank r. The property of the singular values, i.e., δ 1 ≥ δ 2 ≥ ... ≥ δ r > 0, guarantees that the first k are bigger than the discarded ones. There is a direct relation between the informativeness of the i-th new dimension and the singular value δ i . High singular values correspond to dimensions of the new space where examples have more variability whereas low singular values determine dimensions where examples have a smaller variability (see [15] ). These latter dimensions can be then hardly used as efficient features in learning. The possibility of computing approximated versions of matrices gives a powerful method for feature selection and filtering as we can decide in advance how many features or, better, linear combination of original features we want to use.
In this section we will firstly introduce the probabilistic model (Sec. 4.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabilities of the model (Sec. 4.2). To describe this part we need to go in depth into the definition of the logistic regression and some ways of computing it.
In the probabilistic formulation [27] , the task of learning taxonomies from a corpus is seen as a maximum likelihood problem. The taxonomy is seen as a set T of assertions R over pairs R i,j . If R i,j is in T , i is a concept and j is one of its generalization (i.e., the direct or the indirect generalization). For example, R dog,animal ∈ T describes that dog is an animal according to the taxonomy T .
The main probabilities are then: (1) the prior probability P (R i,j ∈ T ) of an assertion R i,j to belong to the taxonomy T and (2) the posterior probability P (R i,j ∈ T | − → e i,j ) of an assertion R i,j to belong to the taxonomy T given a set of evidences − → e i,j derived from the corpus. These evidences are derived from the contexts where the pair (i, j) is found in the corpus. The vector − → e i,j is a feature vector associated with a pair (i, j). For example, a feature may describe how many times i and j are seen in patterns like "i as j" or "i is a j". These among many other features are indicators of an is-a relation between i and j (see [13] ). Given a set of evidences E over all the relevant word pairs, the probabilistic taxonomy learning task is defined as the problem of finding a taxonomy T that maximizes the probability of having the evidences E, i.e.:
In [27] , this maximization problem is solved with a local search. What is maximized at each step is the ratio between the likelihood P (E|T ) and the likelihood P (E|T ) where T = T ∪ N and N are the relations added at each step. This ratio is called multiplicative change ∆(N ) and is defined as follows ∆(N ) = P (E|T )/P (E|T ). The main innovation of the model in [27] is the possibility of adding at each step the best relation N = {R i,j } as well as R i,j with all the relations induced from R i,j , i.e., N = {R i,j } ∪ I(R i,j ) where I(R i,j ) are the relations induced using the existing taxonomy and R i,j . Given the taxonomy T and the relation R i,j , the
We will experiment with our feature selection methodology in two different models:
flat: at each iteration step, a single relation is added, i.e. R i,j = arg max Ri,j ∆(R i,j ) inductive: at each iteration step, a set of relations is added, i.e. I( R i,j ) where R i,j = arg max Ri,j ∆(I(R i,j )).
The last important fact is that it is possible to demonstrate that
where k is a constant (see [27] ) that will be neglected in the maximization process. This last equation gives the possibility of using the logistic regression as it is. In the next sections we will see how SVD and the related feature selection can be used to compute the odds.
We here show that the odds(R i,j ) in eq. 2 can be computed with logistic regression (Sec. 4.2.1). We then describe how we can compute logistic regression using a particular pseudo-inverse matrix (Sec. 4.2.2). Finally, we show that approximated pseudo-inverse matrices can be computed using SVD (Sec. 4.2.3).
Logistic Regression [6] is a particular type of statistical model for relating responses Y to linear combinations of predictor variables X. It is a specific kind of Generalized Linear Model (see [20] ) where its function is the logit function and the dependent variable Y is a binary or dichotomic variable which has a Bernoulli distribution. The dependent variable Y takes value 0 or 1. The probability that Y has value 1 is function of the regressors x = (1, x 1 , ..., x k ).
The probabilistic taxonomy learner model introduced in the previous section falls in the category of probabilistic models where the logistic regression can be applied as R i,j ∈ T is the binary dependent variable and − → e i,j is the vector of its regressors. In the rest of the section we will see how the odds, i.e., the multiplicative change, can be computed. We start from formally describing the Logistic Regression Model. Given the two stochastic variables Y and X, we can define as p the probability of Y to be 1 given that X=x, i.e.p = P (Y = 1|X = x) The distribution of the variable Y is a Bernoulli distribution. Given the definition of the logit(p) as logit(p) = ln 
where β 0 , β 1 , ..., β k are called regression coefficients of the variables x 1 , ..., x k respectively. It is obviously trivial to determine the odds(R i,j ) related to the multiplicative change of the probabilistic taxonomy model. The odds, the ratio between the positive and the negative event, can be determined as follows:
The remaining problem is how to estimate the regression coefficients. This estimation is done using the maximal likelihood estimation to prepare a set of linear equations using the above logit definition and, then, solving a linear problem. This will give us the possibility of introducing the necessity of determining a pseudo-inverse matrix where we will use the singular value decomposition and its natural possibility of performing feature selection. Once we have the regression coefficients, we have the possibility of estimating a probability P (R i,j ∈ T | − → e i,j ) given any configuration of the values of the regressors − → e i,j , i.e., the observed values of the features. Let assume we have a multiset O of observations extracted from Y × E where Y ∈ {0, 1} and we know that some of them are positive observations (i.e., Y = 1) and some of them are negative observations (i.e., Y = 0). For each pair, the relative configuration − → e l ∈ E appears at least once in O and can be determined using the maximal likelihood estimation P (Y = 1| − → e l ). Then, from the equation of the logit (Eq. 3), we have a linear equation system, i.e.:
where Q is a matrix that includes a constant column of 1, necessary for the β 0 of the linear combination of the values of the regression. Moreover it includes the set of evidences, i.e. Q = (1, − → e 1 ... − → e m ).
The set of equations in Eq. 5 are a particular case multiple linear regression [2] . As Q is a rectangular and singular matrix, the system (Eq.5) has no solution. This problem can be solved by the Moore-Penrose pseudoinverse Q + [23] . Then, we determine the re-
We finally reached the point where it is possible to explain our idea that is naturally using singular value decomposition (SVD) as feature selection in a probabilistic taxonomy learner. In previous sections we described how the probabilities of the taxonomy learner can be estimated using logistic regressions and we concluded that a way to determine the regression coefficients β is computing the Moore-Penrose pseudoinverse Q + . It is possible to compute the MoorePenrose pseudoinverse using the SVD in the following way [23] . Given an SVD decomposition of the matrix Q = U ΣV T the pseudo-inverse matrix is:
The diagonal matrix Σ + is a matrix r × r obtained calculating the reciprocals of the singular value of Σ.
We have now our opportunity of using SVD as natural feature selector as we can compute different approximations of the pseudo-inverse matrix. The algorithm for computing SVD is iterative (Sec. 3). The firstly derived dimensions are those with higher singular value. We can then decide how many dimensions we want to use. The first k dimensions are more informative than the k + 1. We can consider different k in order to obtain different SVD as approximations of the original matrix (Eq. 1). We can define different approximations of the inverse matrix Q + as Q + k , i.e.:
In this section, we want to empirically explore whether our use of SVD feature selection positively affects performances of the probabilistic taxonomy learner. The best way of determining how a taxonomy learner is performing is to see if it can replicate an existing "taxonomy". We will experiment with the attempt of replicating a portion of WordNet [17] . In the experiments, we will address two issues: determining to what extent SVD feature selection affect performances of the taxonomy learner and determining if, for the probabilistic taxonomy learner, SVD is better than other simpler models for supervised and unsupervised feature selection. We will explore the effects on both the flat and the inductive probabilistic taxonomy learner.
In the rest of the section we will describe: the experimental set-up (Sec. 5.1) and the results of the experiments in term of performance (Sec. 5.2).
To completely define the experiments we need to describe some issues: how we defined the taxonomy to replicate, which corpus we have used to extract evidences for pairs of words, which feature space we used, and, finally, the feature selection models we compared against. As target taxonomy we selected a portion of WordNet 2 [17] . Namely, we started from the 44 concrete nouns divided in 3 classes: animal, artifact, and vegetable. For each word w, we selected the synset s w that is compliant with the class it belongs to. We then obtained a set S of synsets. We then expanded the set to S adding the siblings (i.e., the coordinate terms) for each synset in S. The set S contains 265 coordinate terms plus the 44 original concrete nouns. For each element in S we collected its hypernyms, obtaining the set H. We then removed from the set H the 4 topmosts: entity, unit, object, and whole. The set H contains 77 hypernyms. For the purpose of the experiments we both derived from the previous sets a taxonomy T and produced a set of negative examples T . The two sets have been obtained as follows. The taxonomy T is the portion of WordNet implied by O = H ∪ S , i.e. T contains all the (s, h) ∈ O × O that are in WordNet and T contains all the (s, h) ∈ O × O that are not in WordNet. We have 5108 positive pairs in T and 52892 negative pairs in T . 2 We used the version 3.0
We then produced two experimental settings: a natural and an artificial one. In the natural setting we used only positive pairs in the training set. This is the natural situation when augmenting existing taxonomies. Only positive word pairs can be derived from existing taxonomies. Yet, negative pairs cannot. In the artificial setting we used both positive and negative examples.
To obtain the training and the testing sets, we randomly divided the set T ∪ T in two parts T tr and T ts , respectively, of 70% and 30% of the original T ∪ T .
As corpus we used ukWaC [9] . This is a web extracted corpus of about 2700000 web pages containing more than 2 billion words. The corpus contains documents of different topics such as web, computers, education, public sphere, etc.. It has been largely demonstrated that the web documents are good models for natural language [14] .
As the focus of the paper is the analysis of the effect of the SVD feature selection, we used as feature spaces both n-grams and bag-of-words. Out of the T ∪ T , we selected only those pairs that appeared at a distance of at most 3 tokens. Using this 3 tokens, we generated two spaces: (1) bag-of-word and (2) the bigram space that contains bigrams and monograms. For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in the context of features for learning a taxonomy.
Finally, we want define the feature selection models we compared against. As unsupervised feature selection models we used the term frequency times the inverse document frequency (tf*idf ). Instances − → e have the role of the documents. As supervised feature selection models we used the mutual information (mi). For all the feature selection models, we selected the first k features. Finally, we used a manual feature selection model based on the Heart's patterns [13] . In this model that we call manual, we used as features only the classical Hearst's patterns.
In the first set of experiments we want to focus on the issue whether or not performances of the proba-bilistic taxonomy learner is positively affected by the proposed feature selection model based on the singular value decomposition. We then determined the performance with respect to different values of k. This latter represents the number of surviving dimensions where the pseudo-inverse is computed. The features of this experiment are unigrams derived from a 3-sizedwindow. Punctuation has been considered. Figures 1 plots the accuracy of the probabilistic learner with respect to the size of the feature set, i.e. the number k of single values considered for computing the pseudoinverse matrix. To determine if the effect of the feature selection is preserved during the iteration of the local search algorithm, we report curves at different sizes of the set of added pairs. Curves are reported for both the flat model and the inductive model. The flat algorithm adds one pair at each iteration. Then, we reported curves for 40 and 80 added pairs. The curves show that accuracy doesn't increase after a dimension of k=400. For the inductive model we report the accuracies for around 40, 80, 130 added pairs. The optimal dimension of the feature space seems to be around 500 as after that value performances decrease or stay stable. SVD feature selection has then a positive effect for both the flat and the inductive probabilistic taxonomy learners. This has beneficial effects both on the performances and on the computation time.
In the second set of experiments we want to determine whether or not SVD feature selection for the probabilistic taxonomy learner behaves better than other feature selection models. We then fixed k to 600 both for the SVD selection model and for the other feature selection models. In this experiments, the original feature space is the bigram space. Figure 2 shows results. Curves report accuracies of the different models after n added pairs. In the natural setting, we compared our model against the tf * idf and the manual feature selection. Our SVD model outperforms both models of feature selection. The same happened against mutual information (M I) in the artificial setting. Our SVD way of selecting features seems to be very effective.
We presented a model to naturally introduce SVD feature selection in a probabilistic taxonomy learner. The method is effective as allows the designing of better probabilistic taxonomy learners. We still need to explore whether or not the positive effect of SVD feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in [27] .
Technological measures to mitigate climatic change include greenhouse gas (GHG) emission reductions and climate geoengineering options. Among these measures, solar radiation management (SRM) technologies such as placing sunshades in space and injecting sulfur aerosol into the stratosphere have been evaluated as having relatively large potential to contribute to the mitigation of climate change (The Royal Society, 2009 ).
However, while earlier studies dealing with strategies of climate change mitigation have focused on deriving optimal dynamic paths of the GHG emissions, especially carbon dioxide (CO 2 ), few have additionally considered the timing and scale of implementing SRM options. Though a pioneering study by Wigley (2006) shows plausible trajectories of the combination of CO 2 emissions reduction and SRM by stratospheric aerosol injection in the future, it lacks deep discussion of economics and risk management.
The present study aims at drawing desirable scenarios based on those combined points of view by using an integrated assessment model of climate and economy. For discussing the combination of CO 2 emissions reduction and SRM, the study pays special attention to the so-called "termination problem," i.e., the risk of adverse effects to climatic condition accompanied with a rapid global warming if the use of the SRM option is terminated for any reason after its implementation.

The 2007 version of the DICE model known as an integrated assessment model of climate change, DICE-2007 (Nordhaus, 2008 , is modified to deal explicitly with SRM options. The DICE model is available for public use through its developer's Web page and has served as the basis of most other economic models of climate change. The model is a nonlinear programming model that integrates a neoclassical macroeconomic growth model with the following three models: an emissions model that computes the amount of CO 2 emissions caused by economic production and the cost of mitigating the emissions, a climate model that simulates the flow and stock of CO 2 in the air and ocean and their impact on the changes in global mean atmospheric temperature, and a damage model that estimates the damage cost caused by a given rise in air temperature. The objective function is the total discounted sum of a representative individual's instantaneous utility stream. It is a one-region model that covers the entire world and derives the optimal dynamic paths of macro investment and CO 2 reduction rate. The total period of time is divided into 60 time periods, the first of which comprises the ten years centered on 2005.
Since radiative forcing that determines the greenhouse effect is controllable only by atmospheric CO 2 concentration in the DICE model, this study modifies the model to include SRM options as a factor controlling radiative forcing, as applied earlier in Kosugi (2010) . The two most important points of the modification are described as follows.
(i) Either placing sunshades in space or injecting aerosols into stratosphere is considered to be applicable. The balance of flow and stock of the sunshading materials is modeled; the service life of the materials, i.e., the period in which the materials stay in the area effective for SRM, is taken into account When we define the variables ) (t S and ) (t G as the mass stock of sun-shading materials accumulated in space or the stratosphere (Mt) and the mass flow of the materials lifted into space or the stratosphere (Mt/yr.), respectively, at time period t , and the parameter S δ as the depreciation rate of the sunshading materials accumulated in space or the stratosphere (yr.
-1 ), the balance of flow and stock of sunshades in space is modeled as:
noticing that a time period consists of ten years in the DICE model. Given the short staying period of injected aerosol in the stratosphere of a few years at the longest, the model for it is as follows:
(1')
(ii) The decrease in radiative forcing by implementing an option is assumed to be proportional to the up-mass stock of the sun-shading material. Letting ) (t F and ) (t F EX be total radiative forcing and its exogenous part due to non-CO 2 GHGs (W/m 2 relative to 1900) and ) (t M AT the mass of carbon in the atmosphere (GtC), this is modeled as:
where η and m denote the parameters connecting radiative forcing with temperature (°C/W/m 2 ) and the sunshade mass-effectiveness coefficient, i.e., the mass of the stock of sun-shading materials required to offset the increase in radiative forcing due to a doubling of the atmospheric CO 2 concentration (Mt/2×CO 2 ), respectively.
By using the calculated radiative forcing, the air temperature is estimated through the following simple climate model as in the original DICE model: (4) where variables ) (t T AT and ) (t T LO represent the global mean surface temperature and the temperature of the ocean depths (°C relative to 1900), respectively. Other modifications include: (iii) the cost of installing the sun-shading materials is subtracted from consumption; (iv) CO 2 emissions induced by installing the sun-shading materials are taken into account; (v) constraints to avoid an air temperature drop are imposed; the global mean air temperature is kept at no less than its 1900 value in the whole period and the rate of temperature decrease doesn't exceed 0.2 °C per decade; and (vi) the CO 2 mitigating trend is assumed to be continued; the rate of CO 2 mitigation is constrained not to decline with an elapse of time.
Among the variety of parameters in the model, the parameters used in the original DICE model were set to be the same as the reference values applied in the DICE-2007. Table 1 (a) shows a major set of extractions from those parameter settings.
The parameters introduced to incorporate SRM options in the model are set based on a survey of literature data (Hertzfeld, et al., 2005; Lenton and Vaughan, 2009; McClellan et al., 2010; Pearson, et al., 2006) as shown in Table 1 (b). Figure 1 shows the trajectory of the global mean air temperature calculated by using the modified DICE model described above. The figures hereafter show the results up to 2125 out of the whole time period calculated in the model. As seen from Figure 1 , the optimal path of SRM deployment follows the maximum allowable implementation starting from 2045 or 2015 if the space-sunshade installation or the stratospheric aerosol injection is applicable, respectively. This result implies that depending largely on an SRM option can be a more cost-effective measure for mitigating climatic change than facilitating CO 2 emissions reduction. In this case, as shown in Figure 2 (see "w/o temp. limit" in the figure) the global industrial CO 2 emission is allowed to rise steadily.
However, in the case of such a large dependency on SRM for mitigating climate change, we would be faced with the problem described below should the implementation of SRM be terminated. Space, w/o temp. limit Stratosph., w/o temp. limit Space, w/ temp. limit Stratosph., w/ temp. limit The broken lines in Figure 1 indicate the temperature increases after SRM termination at the respective time periods. More specifically, it shows the calculated global mean air temperature rise hypothesizing that the values of all the variables, e.g., CO 2 emissions, are the same as those calculated earlier through the model while no new sun-shading materials are placed into space or the stratosphere after each of the time periods. The abrupt rise in air temperature after the SRM termination is called the "termination problem," which has been described as one of the most serious risks concerning the use of SRM (Brovkin, et al., 2009 ).

For the safer use of SRM options, we need to avoid the risk of abrupt warming, which would occur in a situation where SRM implementation is terminated. The causes of termination could include unsuccessful continuous multilateral political negotiations regarding SRM or the unexpected revelation of a major adverse side effect of the SRM. Although such an occurrence is itself unforeseeable, the extent of the adverse effect brought about by the SRM termination can be estimated, and it is possible to control the use of SRM to keep the damage from unforeseen discontinuation at a certain allowable level.
Given the climate control recommendation by WBGU (2003) to constrain the rise in global average air temperature below 2 °C and the per-decade rate of temperature rise within 0.2 °C, a guideline for SRM use is derived such that the above condition holds even if SRM is terminated at any time.
The above guideline can be implemented in the model by introducing the following formulae. Let ) , ( t t S ′ be the group of variables representing the virtual dynamic path of the mass stock of sun-shading materials accumulated in space or the stratosphere (Mt) assuming an SRM termination at time t′ . For t t ′ < , clearly
while for T t t < ≤ ′ , setting the value of ) (t G to null in Eqs. (1) 
while for
, consistently with Eqs. (3) and (4), 
These two constraints should be applied for all t and t′ ; however, incorporating Eq. (10) for 3 < t makes the model infeasible, i.e., the rise in global mean air temperature in the next decade will inevitably be above 0.2 °C. We therefore apply Eq. (10) for 3 ≥ t . The total numbers of variables and constraints become 13 and 20 times, respectively, as many as those of the model before the extension. The computation time to find the utility maximizing solution is 41 seconds for the extended model when space-sunshades are assumed to be available as an SRM option, which is 27 seconds longer than the preextension when the model is solved by GAMS/ CONOPT3 (Brooke et al., 1992; Drud, 1994) with a PC based on the Intel(R) Core(TM) 2 Duo CPU P9300, 2.26GHz with 1.93 GB RAM.
The global mean air temperature calculated through the extended model is shown as the solid line in Figure 3 . Compared with Figure 1 , this figure suggests a moderate use of SRM, especially in the case of stratospheric aerosol injection, to lower the air temperature when we adopt the guideline introduced above. As in Figure 1 , the broken lines in Figure 3 indicate the trajectory of the temperature after an unexpected SRM termination at the respective time periods; we can confirm that, when the use of SRM is moderated to reflect the guideline of limiting the temperature rise that would occur by SRM termination, abrupt warming by SRM use termination is avoided. Figure 2 includes the optimal paths of the industrial CO 2 emissions when the constraint on the limit of temperature rise in case of SRM termination is adopted (see "w/ temp. limit") together with those without the limit of temperature rise explained in Section 2.3. The results imply that reducing CO 2 emissions is expected to play a more important role in mitigating climate change when we adopt the guideline of limiting temperature rise. Specifically, the amount of industrial CO 2 emissions should be kept at around the present level in the former half of this century and is expected to be reduced rapidly afterward, reaching only 20% of the 2005 levels by 2085. Figure 4 shows the calculated atmospheric CO 2 concentration, which steadily increases in this century and reaches 700 ppmv a century hence if the guideline of limiting the temperature rise in case of SRM termination is not adopted. With the limit of temperature rise in such a case, on the other hand, the increase in CO 2 concentration is expected to be mitigated to peak at 490 ppmv by 2075; afterward the concentration decreases to below 450 ppmv after 2125.
To observe the desirable combination of CO 2 emissions reduction and SRM for contributing to mitigating climate change derived under the guideline of limiting temperature rise in case of SRM termination, the decrease in radiative forcing by use of each measure to mitigate climate change, i.e., the difference from the radiative forcing compared to the case where no climate mitigation policy is implemented, is illustrated in Figure 5 assuming that stratospheric aerosol injection is usable as an SRM option.
CO 2 emissions reduction contributes more to lessening radiative forcing than SRM throughout the time periods addressed by the model, and the Space, w/o temp. limit Stratosph., w/o temp. limit Space, w/ temp. limit Stratosph., w/ temp. limit contribution of emissions reduction becomes much greater as time passes. Though we omit a figure corresponding to the case of using space-based sunshades instead of stratospheric aerosol injection, a similar tendency is observed for this case. 
SRM geoengineering is expected to be a lower-cost option of climate control compared to CO 2 emissions reduction, and may considerably contribute to the cost-effectiveness of global climatic change mitigation. However, this option is accompanied by the risk of rapid global warming if the implementation of SRM is unexpectedly terminated for any reason. As a guideline for the use of SRM to avoid the risk, this study suggests that the adverse effect should be controlled within an acceptable range in case of unexpected SRM termination at any time after its implementation. We incorporated the guideline into the integrated climate-economy model DICE by extending the model and quantitatively showed the contributions of CO 2 emissions reduction and SRM recommended to prevent global warming.
The extension of the model brings increases in the numbers of variables and constraint equations, resulting in a longer computation time to solve the model. The model is still solved within a minute using a PC because it incorporates a very simplified climate module; if we further extend the model to deal with geographic distribution of climate change, the computation time is estimated to increase, which may impose a barrier to practical evaluation.
Finally, it should be emphasized that there are some risks with the use of SRM other than those considered in the present modeling study. The quantitative results obtained from this study should be interpreted as the economic potential of SRM use assuming that such risks are low. If we needed to regard these risks as considerably high, more restrained use of SRM would be recommended.
Independent component analysis (ICA) is a computational and statistical technique with applications in areas ranging from signal processing to machine learning and more. Formally, if S is an n-dimensional random vector with independent coordinates and A ∈ R n×n is invertible, then the ICA problem is to estimate A given access to i.i.d. samples of the mixed signals X = AS. We say that X is generated by an ICA model X = AS. The recovery of A (the mixing matrix ) is possible only up to scaling and permutation of the columns. Moreover, for the recovery to be possible, the distributions of the random variables S i must not be Gaussian (except possibly one of them). Since its inception in the eighties (see [CJ10] for historical remarks), ICA has been thoroughly studied and a vast literature exists (e.g. [HKO01, CJ10] ). The theory is well-developed and practical algorithms-e.g., FastICA [Hyv99] , JADE [CS93] -are now available along with implementations, e.g. [CAS + ]. However, to our knowledge, rigorous complexity analyses of these assume that the fourth moment of each component is finite: E(S 4 i ) < ∞. If at least one of the independent components does not satisfy this assumption we will say that the input is in the heavy-tailed regime. Many ICA algorithms first preprocess the data to convert the given ICA model into another one where the mixing matrix A has orthogonal columns; this step is often called whitening. We will instead call it orthogonalization, as this describes more precisely the desired outcome. Traditional whitening is a second order method that may not make sense in the heavy-tailed regime. In this regime, it is not clear how the existing algorithms would perform, because they depend on empirical estimation of various statistics of the data such as the covariance matrix or the fourth cumulant tensor, which diverge in general for heavy-tailed data. For example, for the covariance matrix in the mean-0 case this is done by taking the empirical average (1/N )
where the {x(i)} are i.i.d. samples of X. ICA in the heavy-tailed regime is of considerable interest, directly (e.g., [Kid01b, Kid01a, SYM01, CB04, CB05, SAML + 05, WKZ09, JEK01, CS07, BC99]) and indirectly (e.g., [BG10, GTG09, WOH02] ) and has applications in speech and finance. We also mention an informal connection with robust statistics: Algorithms solving heavy-tailed ICA might work by focusing on samples in a small (but high probability) region to get reliable statistics about the data and avoid the instability of the tail. Thus, if the data has outliers, the outliers are less likely to affect such an algorithm.
Recent theoretical work [AGNR15] proposed a polynomial time algorithm for ICA that works in the regime where each component S i has finite (1 + γ)-moment for γ > 0. This algorithm follows the two phases of several ICA algorithms: (i) Orthogonalize the independent components. The purpose of this step is to apply an affine transformation to the samples from X so that the resulting samples correspond to an ICA model where the unknown matrix A has orthogonal columns. (ii) Learn the matrix with orthogonal columns. Each of these two phases required new techniques: (1) Orthogonalization via uniform distribution in the centroid body. The input is assumed to be samples from an ICA model X = AS where each S i is symmetrically distributed (w.l.o.g, see Sec. 2) and has at least (1 + γ)-moments. The goal is to construct an orthogonalization matrix B so that BA has orthogonal columns. In [AGNR15] , the inverse of the square root of the covariance matrix of the uniform distribution in the centroid body is one such matrix. (2) Gaussian damping. The previous step allows one to assume that the mixing matrix A is orthogonal. The modified second step is: If X has density ρ X (t) for t ∈ R n , then the algorithm constructs another ICA model X R = AS R where X R has pdf proportional to ρ X (t) exp(− t 2 2 /R 2 ), where R > 0 is a parameter chosen by the algorithm. This explains the term Gaussian damping. This achieves two goals: (1) All moments of X R and S R are finite. (2) The product structure of is retained. This follows from two facts: A has orthogonal columns, and the Gaussian has independent components in any orthonormal basis. Because of these properties, the model can be solved by traditional ICA algorithms.
The algorithm in [AGNR15] is theoretically efficient but impractical. Their orthogonalization uses the ellipsoid algorithm for linear programming, which is not practical. It is not clear how to replace their use of the ellipsoid algorithm by practical linear programming tools, as their algorithm only has oracle access to a sort of dual and not an explicit linear program. Moreover, their orthogonalization technique uses samples uniformly distributed in the centroid body, generated by a random walk. This is computationally efficient in theory but, to the best of our knowledge, only efficient in practice for moderately low dimension.
Our contributions. Our contributions are experimental and theoretical. We provide a new and practical ICA algorithm, HTICA, building upon the previous theoretical work in [AGNR15] . HTICA works as follows: (1) Compute an orthogonalization matrix B. (2) Pre-multiply samples by B to get an orthogonal model. (3) Damp the data, run an existing ICA algorithm. For step (1), we propose two theoretically sound and practically efficient ways below, orthogonalization via centroid body scaling and orthogonalization via covariance. Our algorithm is simpler and more efficient, but needs a more technical justification than the method in [AGNR15] . We demonstrate the effectiveness of HTICA on both synthetic and real-world data.
Orthogonalization via centroid body scaling. We propose a more practical orthogonalization matrix than the one from [AGNR15] (orthogonalization via the uniform distribution in the centroid body, mentioned before). First, consider the centroid body of random vector X, denoted ΓX (this is really a function of the distribution of X; formal definition in Sec. 2). For intuition, it is helpful to think of the centroid body as an ellipsoid whose axes are aligned with the independent components of X. The centroid body is in general not an ellipsoid, but it has certain symmetries aligned with the independent components. Let random vector Y be a scaling of X along every ray so that points at infinity are mapped to the boundary of ΓX, the origin is mapped to itself and the scaling interpolates smoothly. One such scaling is obtained in the following way: It is helpful to consider how far a point is in its ray with respect to the boundary of ΓX. This is given by the Minkoswki functional of ΓX, denoted p : R n → R, which maps the boundary of ΓX to 1 and interpolates linearly along every ray. We can then achieve the desired scaling by first mapping a given point to the boundary point on its ray (the mapping x → x/p(x)) and then using the function tanh, which maps [0, ∞) to [0, 1] with tanh(0) = 0 and lim x→∞ tanh(x) = 1 to determine the final scale along the ray, namely, tanh p(x). More formally, our scaling is the following: Let Y be tanh p(X) p(X) X. We show in Sec. 4.1 that B = Cov(Y ) −1/2 is an orthogonalization matrix when Cov(Y ) is invertible. In order to make this practical, one needs a practical estimator of the Minkowski functional of ΓX from a sample of X. In Sec. 4.1 and 5, we describe such an algorithm and provide a theoretical justification, including finite sample estimates. The proposed algorithm is much simpler and practical than the one described in [AGNR15] . In particular, it avoids the use of the ellipsoid algorithm by the use of a closed-form linear programming representation of the centroid body (Prop. 10, Lemma 11) and new approximation guarantees between the empirical (sample estimate) and true centroid body of a heavy-tailed distribution. In Sec. 4.1, we discuss our practical implementation and show results where orthogonalization via centroid body scaling produces results with smaller error. Orthogonalization via covariance. Previously, (e.g., in [CB04] ), the empirical covariance matrix was used for whitening in the heavy-tailed regime and, surprisingly, worked well in some situations. Unfortunately, the understanding of this was quite limited . We give a theoretical explanation for this phenomenon in a fairly general heavy-tailed regime: Covariance-based orthogonalization works well when each component S i has finite (1 + γ)-moment, where γ > 0. We also study this algorithm in experimental settings. As we will see, while orthogonalization via covariance improves over previous algorithms, in general orthogonalization via centroid body has better performance because it has better numerical stability; but there are some situations where orthogonalization via covariance matrix is better.
Empirical Study. We perform experiments on both synthetic and real data to see the effect of heavy-tails on ICA.
In the synthetic data setting, we generate samples from a fixed heavy-tailed distribution and study how well the algorithm can recover a random mixing matrix (Sec. 3).
To study the algorithm with real data, we use recordings of human speech provided by [Don09] . This involves a room with different arrangements of microphones, and six humans speaking independently. The speakers are recorded individually, so we can artificially mix them and have access to a ground truth. We study the statistical properties of the data, observing that it does indeed behave as if the underlying processes are heavy-tailed. The performance of our algorithm shows improvement over using FastICA on its own.
Heavy-tailed distributions arise in a wide range of applications (e.g., [Nol15] ). They are characterized by the slow decay of their tails. Examples of heavy-tailed distributions include the Pareto and log-normal distributions.
We denote the pdf of random variable Z by ρ Z . We will assume that our distributions are symmetric, that is ρ(x) = ρ(−x) for x ∈ R. As observed in [AGNR15] , this is without loss of generality for our purposes. This follows from the fact that if X = AS is an ICA model, and if we let X = AS be an i.i.d. copy of the same model, then X − X = A(S − S ) is an ICA model with components of S − S having symmetric pdfs. One further needs to check that if the components of S are away from Gaussians then the same holds for S − S ; see [AGNR15] . We formulate our algorithms for the symmetric case; the general case immediately reduces to the symmetric case.
For K ⊆ R n , K denotes the set of points that are at distance at most from K. The set K − is all points for which an -ball around them is still contained in K. The n-dimensional p ball is denoted as B n p . An important related family of distributions is that of stable distributions (e.g., [Nol15] ). In general, the density of a stable distribution has no closed form, but is fully defined by four real-valued parameters. Some stable distributions do admit a closed form, such as the Cauchy and Gaussian distributions. For us the most important parameter is α ∈ (0, 2], known as the stability parameter; we will think of the other three parameters as being fixed to constants.
We use the notation poly(·) to indicate a function which is asymptotically upper bounded by a polynomial expression of the given variables.
If α = 2, the distribution is Gaussian (the only non-heavy-tailed stable distribution), and if α = 1, it is the Cauchy distribution.
Definition 1 (Centroid body). Let X ∈ R n be a random vector with finite first moment, that is, for all u ∈ R n we have E(| u, X |) < ∞. The centroid body of X is the compact convex set, denoted ΓX, whose support function is h ΓX (u) = E(| u, X |). For a probability measure P, we define ΓP, the centroid body of P, as the centroid body of any random vector distributed according to P.
Note that for the centroid body to be well-defined, the mean of the data must be finite. This excludes, for instance, the Cauchy distribution from consideration in the present work.
In this section, we show experimentally that heavy-tailed data poses a significant challenge for current ICA algorithms, and compare them with HTICA in different settings. We observe some clear situations where heavy-tails seriously affect the standard ICA algorithms, and that these problems are frequently avoided by using the heavy-tailed ICA framework. In some cases, HTICA does not help much, but maintains the same performance of plain FastICA.
To generate the synthetic data, we create a simple heavy-tailed density function f η (x) proportional to (|x| + 1.5) −η , which is symmetric, and for η > 1, f η is the density of a distribution which has finite k < η − 1 moment. The signal S is generated with each S i independently distributed from f ηi . The mixing matrix A ∈ R n×n is generated with each coordinate i.i.d. N (0, 1), columns normalized to unit length. To compare the quality of recovery, the columns of the estimated mixing matrix,Ã are permuted to align with the closest matching column of A, via the Hungarian algorithm. We use the Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [ACY + 96]; the results have similar behavior and are not presented here.
Focusing on the third step above, where the mixing matrix already has orthogonal columns, ICA algorithms already suffer dramatically from the presence of heavy-tailed data. As proposed in [AGNR15] , Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l 2 -norm) to data from a related ICA model X R = AS R , where R > 0 is a parameter to be chosen. The independent components of S R have finite moments of all orders and so the existing algorithms can estimate A.
Using samples of X, we construct the damped random variable X R , with pdf ρ X R (x) ∝ ρ X (x) exp(− x 2 /R 2 ). To normalize the right hand side, we can estimate
If x is a realization of X R , then s = A −1 x is a realization of the random variable S R and we have that S R has pdf ρ S R (s) = ρ X R (x). To generate samples from this distribution, we use rejection sampling on samples from ρ X . When performing the damping, we binary search over R so that about 25% of the samples are rejected. For more details about the technical requirements for choosing R, see [AGNR15] . Figure 1 shows that, when A is already a perfectly orthogonal matrix, but where S may have heavy-tailed coordinates, several standard ICA algorithms perform better after damping the data. In fact, without damping, some do not appear to converge to a correct solution. We compare ICA with and without damping in this case: (1) FastICA using the fourth cumulant ("FastICA -pow3"), (2) FastICA using log cosh ("FastICAtanh"), (3) JADE, and (4) Second Order Joint Diagonalization as in, e.g., [Car89] . The Frobenius error of the recovered mixing matrix with the 'pow3' and 'tanh' contrast functions, on 10-dimensional data, averaged over ten trials. The mixing matrix A is random with unit norm columns, not orthogonal. In the left and middle figures, the distribution has η = (6, . . . , 6, 2.1, 2.1) while in the right figure, η = (2.1, . . . , 2.1) (see Section 3.2 for a discussion).
We now present the results of HTICA using different orthogonalization techniques: (1) Orthogonalization via covariance (Section 4.2 (2) Orthogonalization via the centroid body (Section 4.1) (3) the ground truth, directly inverting the mixing matrix (oracle), and (4) No orthogonalization, and also no damping (for comparison with plain FastICA) (identity). The "mixed" regime in the left and middle of Figure 2 (where some signals are not heavy-tailed) demonstrates a very dramatic contrast between different orthogonalization methods, even when only two heavy-tailed signals are present.
In the experiment with different methods of orthogonalization it was observed that when all exponents are the same or very close, orthogonalization via covariance performs better than orthogonalization via centroid and the true mixing matrix as seen in Figure 2 . A partial explanation is that, given the results in Figure 1 , we know that equal exponents favor FastICA without damping and orthogonalization (identity in Figure 2 ). The line showing the performance with no orthogonalization and no damping ("identity") behaves somewhat erratically, most likely due the presence of the heavy-tailed samples. Additionally, damping and the choice of parameter R is sensitive to scaling. A scaled-up distribution will be somewhat hurt because fewer samples The data was sampled with parameter η = (6, 6, 6, 6, 6, 6, 6, 6, 2.1, 2.1).
will survive damping.
While the above study on synthetic data provides interesting situations where heavy-tails can cause problems for ICA, we provide some results here which use real-world data, specifically human speech. To study the performance of HTICA on voice data, we first examine whether the data is heavy-tailed. The motivation to use speech data comes from observations by the signal processing community (e.g. [Kid00] ) that speech data can be modeled by α-stable distributions. For an α-stable distribution, with α ∈ (0, 2), only the moments of order less than α will be finite. We present here some results on a data set of human speech according to the standard cocktail party model, from [Don09] . The physical setup of the experiments (the human speakers and microphones) is shown in Figure 3 . To estimate whether the data is heavy-tailed, as in [Kid00], we estimate parameter α of a best-fit α-stable distribution. This estimate is in Figure 4 for one of the data sets collected. We can see that the estimated α is clearly in the heavy-tailed regime for some signals.
Using data from [Don09] , we perform the same experiment as in Section 3.2: generate a random mixing matrix with unit length columns, mix the data, and try to recover the mixing matrix. Although the mixing is synthetic, the setting makes the resulting mixed signals same as real. Specifically, the experiment was conducted in a room with chairs, carpet, plasterboard walls, and windows on one side. There was natural noise including vents, computers, florescent lights, and traffic noise through the windows. Figure 4 demonstrates that HTICA (orthogonalizing with centroid body scaling, Section 4.1) applied to speech data yields some noticeable improvement in the recovery of the mixing matrix, primarily in that it is less susceptible to data that causes FastICA to have large error "spikes." Moreover, in many cases, running only FastICA on the mixed data failed to even recover all of the speech signals, while HTICA succeeded. In these cases, we had to re-start FastICA until it recovered all the signals.
4 New approach to orthogonalization and a new analysis of empirical covariance
As noted above, the technique in [AGNR15] , while being provably efficient and correct, suffers from practical implementation issues. Here we discuss two alternatives: orthogonalization by centroid body scaling and orthogonalization by using the empirical covariance. The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm's approximation of the centroid body (as is done in [AGNR15] ). This removes the dependence on random walks and the ellipsoid algorithm; instead, we use samples that are distributed according to the original heavy-tailed distribution but non-linearly scaled to lie inside the centroid body. We prove in Lemma 3 that the covariance of this subset of samples is enough to orthogonalize the mixing matrix A. Secondly, we prove that one can, in fact, "forget" that the data is heavy tailed and orthogonalize by using the empirical covariance of the data, even though it diverges, and that this is enough to orthogonalize the mixing matrix A. However, as observed in experimental results, in general this has a downside compared to orthogonalization via centroid body in that it could cause numerical instability during the "second" phase of ICA as the data obtained is less well-conditioned. This is illustrated directly in the table in Figure 4 containing the singular value and condition number of the mixing matrix BA in the approximately orthogonal ICA model.
In [AGNR15] , another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work. Their procedure does not suffer from the numerical instabilities and composes well with the second phase of ICA algorithms. An impractical aspect of that procedure is that it needs samples from the uniform distribution in the centroid body. We described orthogonalization via centroid body in Section 1, except for the estimation of p(x), the Minkowski functional of the centroid body. The complete procedure is stated in Subroutine 1.
We now explain how to estimate the Minkowski functional. The Minkowski functional was informally described in Section 1. The Minkowski functional of ΓX is formally defined by p(x) := inf{t > 0 : x ∈ tΓX}. Our estimation of p(x) is based on an explicit linear program (LP) (10) that gives the Minkowski functional of the centroid body of a finite sample of X exactly and then arguing that a sample estimate is close to the actual value for ΓX. For clarity of exposition, we only analyze formally a special case of LP (10) that decides membership in the centroid body of a finite sample of X (LP (9)) and approximate membership in ΓX. This analysis is in Section 5. Accuracy guarantees for the approximation of the Minkowski functional follow from this analysis.
). Let U be a family of n-dimensional product distributions. LetŪ be the closure of U under invertible linear transformations. Let Q(P) be an n-dimensional distribution defined as a function of P ∈Ū . Assume that U and Q satisfy:
1. For all P ∈ U , Q(P) is absolutely symmetric.

of ICA model X = AS so each S i is symmetric with (1 + γ) moments. Output: Matrix B approximate orthogonalizer of A 1: for i = 1 : N do,
Let λ * be the optimal value of (10) with q = X (i) .
3. For any P ∈Ū , Cov(Q(P)) is positive definite.
Then for any symmetric ICA model X = AS with P S ∈ U we have Cov(Q(P X )) −1/2 is an orthogonalizer of X.
Lemma 3. Let X be a random vector drawn from an ICA model X = AS such that for all i we have
Proof. We will be applying Lemma 2. Let U denote the set of absolutely symmetric product distributions P W over R n such that E|W i | = 1 for all i. For P V ∈Ū , let Q(P V ) be equal to the distribution obtained by scaling
For all P W ∈ U , W i is symmetric and E|W i | = 1 which implies that αW , that is, Q(P W ) is absolutely symmetric. Let P V ∈Ū . Then Q(P V ) is equal to the distribution of αV . For any invertible linear transformation T and measurable set M, we have
. Thus Q is linear equivariant. Let P ∈Ū . Then there exist A and P W ∈ U such that P = AP W . We get
) is a diagonal matrix with elements E(α 2 W 2 i ) which are non-zero because we assume E|W i | = 1. This implies that Cov(Q(P)) is positive definite and thus by Lemma 2, Cov(Y ) −1/2 is an orthogonalizer of X.
Here we show the somewhat surprising fact that orthogonalization of heavy-tailed signals is sometimes possible by using the "standard" approach: inverting the empirical covariance matrix. The advantage here, is that it is computationally very simple, specifically that having heavy-tailed data incurs very little computational penalty on the process of orthogonalization alone. It's standard to use covariance matrix for whitening when the second moments of all independent components exist [HKO01] : Given samples from the ICA model X = AS, we compute the empirical covariance matrixΣ which tends to the true covariance matrix as we take more samples and set B =Σ −1/2 . Then one can show that BA is a rotation matrix, and thus by pre-multiplying the data by B we obtain an ICA model Y = BX = (BA)S, where the mixing matrix BA is a rotation matrix, and this model is then amenable to various algorithms. In the heavy-tailed regime where the second moment does not exist for some of the components, there is no true covariance matrix and the empirical covariance diverges as we take more samples. However, for any fixed number of samples one can still compute the empirical covariance matrix. In previous work (e.g., [CB04] ), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [CB04] also provided some theoretical analysis to explain this surprising performance. However, their work (both experimental and theoretical) was limited to some very special cases (e.g., only one of the components is heavy-tailed, or there are only two components both with stable distributions without finite second moment).
We will show that the above procedure (namely pre-multiplying the data by B :=Σ −1/2 ) "works" under considerably more general conditions, namely if (1 + γ)-moment exists for γ > 0 for each independent component S i . By "works" we mean that instead of whitening the data (that is BA is rotation matrix) it does something slightly weaker but still just as good for the purpose of applying ICA algorithms in the next phase. It orthogonalizes the data, that is now BA is close to a matrix whose columns are orthogonal. In other words, (BA)
T (BA) is close to a diagonal matrix (in a sense made precise in Theorem 5). Let X be a real-valued symmetric random variable such that E(|X| 1+γ ) ≤ M for some M > 1 and 0 < γ < 1. The following lemma from [AGNR15] says that the empirical average of the absolute value of X converges to the expectation of |X|. The proof, which we omit, follows an argument similar to the proof of the Chebyshev's inequality. LetẼ N [|X|] be the empirical average obtained from N independent samples X (1) , . . . , X (N ) , i.e., (
Lemma 4. Let ∈ (0, 1). With the notation above, for N ≥
Theorem 5 (Orthogonalization via covariance matrix). Let X be given by ICA model X = AS. Assume that there exist t, p, M > 0 and γ ∈ (0, 1) such that for all i we have
} for all i with probability 1 − δ when N ≥ poly(n, M, 1/p, 1/t, 1/ , 1/δ).
Proof idea. For i = j we have E(S i S j ) = 0 (due to our symmetry assumption on S) and E(
. Now by our assumption that (1 + γ)-moments exist, Lemma 4 is applicable and implies that empirical averageẼS i tends to the true average ES i as we increase the number of samples. The true average is 0 because of our assumption of symmetry (alternatively, we could just assume that the X i and hence S i have been centered). The diagonal entries of L are bounded away from 0: This is clear when the second moment is finite, and follows easily by hypothesis (c) when it is not. Finally, one shows that if in L the diagonal entries highly dominate the off-diagonal entries, then the same is true of
and so by Lemma 4, for i = j,
when N ≥ ( γ . Next, we aim to bound D 2 which can be done by writing
where
. Consider the random variable 1(s
≥ N p and use a Chernoff bound to see
and when k∈[N ] 1(s
Then with probability at least 1 − n exp(−N p/8),
all entries of D −1 are at least t 2 p/2. Using this, if N ≥ N 1 := (8/p) ln(3n/δ) then D 2 ≤ 2/pt 2 with probability at least 1 − δ/3.
Similarly, suppose that D 2 ≤ 2/pt 2 and choose 1 = min{
pt 2 } and
2 ≤ t 4 p 2 /8 with probability at least 1 − δ/3. Invoking (7), when N ≥ max{N 1 , N 2 }, we have
with probability at least 1 − 2δ/3. Finally, we upper bound 1/d i for a fixed i by using Markov's inequality:
so that 1/d i ≤ N 4 for all i with probability at least 1 − δ/3 when N ≥ N 3 := n/3δ. Therefore, when
for all i with overall probability at least 1 − δ.
We used the following technical result.
Lemma 6. Let · be a matrix norm such that AB ≤ A B . Let matrices C, E ∈ R n×n be such that C −1 E 2 ≤ 1, and letC = C + E. Then
This implies that if
2 ), then
In Theorem 5, the diagonal entries are lower bounded, which avoids some degeneracy, but they could still grow quite large because of the heavy tails. This is a real drawback of orthogonalization via covariance. HTICA, using the more sophisticated orthogonalization via centroid body scaling does not have this problem. We can see this in the right table of Figure 4 , where the condition number of "centroid" is much smaller than the condition number of "covariance."
5 Membership oracle for the centroid body, without polarity
We will see now how to implement an -weak membership oracle for ΓX directly, without using polarity. We start with an informal description of the algorithm and its correctness.
The algorithm implementing the oracle (Subroutine 2) is the following: Let q ∈ R n be a query point. Let X 1 , . . . , X N be a sample of random vector X. Given the sample, let Y be uniformly distributed in {X 1 , . . . , X N }. Output YES if q ∈ ΓY , else output NO.
Idea of the correctness of the algorithm: If q is not in (ΓX) , then there is a hyperplane separating q from (ΓX) . Let {x : a T x = b} be the hyperplane, satisfying a = 1, a T q > b and a T x ≤ b for every x ∈ (ΓX) . Thus, we have h (ΓX) (a) ≤ b and h ΓX (a) ≤ b − . We have
when N is large enough with probability at least 1 − δ over the sample X 1 , . . . , X N . In particular, h ΓY (a) ≤ b, which implies q / ∈ ΓY and the algorithm outputs NO, with probability at least 1 − δ.
If q is in (ΓX) − , let y = q + q ∈ ΓX. We will prove the following claim: Informal claim (Lemma 13): For p ∈ ΓX, for large enough N and with probability at least 1 − δ there is z ∈ ΓY so that z − p ≤ /10.
This claim applied to p = y to get z, convexity of ΓY and the fact that ΓY contains B σ min (A)B n 2 (Lemma 9) imply that q ∈ conv(B ∪ {z}) ⊆ ΓY and the algorithm outputs YES. We will prove the claim now. Let p ∈ ΓX. By the dual characterization of the centroid body (Proposition 10), there exists a function λ :
By Lemma 4 and a union bound over every coordinate we get P( p − z ≥ ) ≤ δ for N large enough.
Lemma 7 ([AGNR15]). Let S = (S 1 , . . . , S n ) ∈ R n be an absolutely symmetrically distributed random vector such that E(
). Let X be a random vector on R n . Let A : R n → R n be an invertible linear transformation. Then Γ(AX) = A(ΓX).
Lemma 9. Let S = (S 1 , . . . , S n ) ∈ R n be an absolutely symmetrically distributed random vector such that
Proof. From Lemma 7 we know ±e i ∈ ΓS. It is enough to apply Lemma 13 to ±e i with = / √ n and δ = δ /(2n). This gives, for any
Proposition 10 (Dual characterization of centroid body). Let X be a n-dimensional random vector with finite first moment, that is, for all u ∈ R n we have E(| u, X |) < ∞. Then
Proof. Let K denote the rhs of the conclusion.We will show that K is a non-empty, closed convex set and show that h K = h ΓX , which implies (8).
By definition, K is a non-empty bounded convex set. To see that it is closed, let (y k ) k be a sequence in K such that y k → y ∈ R n . Let λ k be the function associated to y k ∈ K according to the definition of K. Let P X be the distribution of X. We have λ k L ∞ (P X ) ≤ 1 and, passing to a subsequence k j ,
Thus, we have y = lim j y kj = lim j E((λ kj (X)X) = E(λ(X)X) and K is closed.
To conclude, we compute h K and see that it is the same as the definition of h ΓX . In the following equations λ ranges over functions such that λ : R n → R is Borel-measurable and −1 ≤ λ ≤ 1.
and setting λ * (x) = sgn x, θ ,
Lemma 11 (LP). Let X be a random vector uniformly distributed in {x
2. Point q ∈ ΓX iff there is a solution λ ∈ R N to the following linear feasibility problem:
3. Let λ * be the optimal value of (always feasible) linear program
with λ * = ∞ if the linear program is unbounded. Then the Minkowski functional of ΓX at q is 1/λ * .
1. This is proven in [McM71] . It is also a special case of Proposition 10. We include an argument here for completeness.
. We compute h K to see it is the same as h ΓX in the definition of ΓX (Definition 1). As K and ΓX are non-empty compact convex sets, this implies K = ΓX. We have
2. This follows immediately from part 1.
3. This follows from part 1 and the definition of Minkowski functional.
Input: Query point q ∈ R n , samples from symmetric ICA model X = AS, bounds s M ≥ σ max (A), s m ≤ σ min (A), closeness parameter , failure probability δ. Output: ( , δ)-weak membership decision for q ∈ ΓX.
1:
Check the feasibility of linear program (9). If feasible, output YES, otherwise output NO.
Proposition 12 (Correctness of Subroutine 2). Let X = AS be given by an ICA model such that for all i we have E(|S i | 1+γ ) ≤ M < ∞, S i is symmetrically distributed and normalized so that E|S i | = 1. Then, given a query point q ∈ R n , > 0, δ > 0, s M ≥ σ max (A), and s m ≤ σ min (A), Subroutine 2 is an -weak membership oracle for q and ΓX with probability 1 − δ using time and sample complexity poly(n, M, 1/s m , s M , 1/ , 1/δ).
Proof. Let Y be uniformly random in (
. There are two cases corresponding to the guarantees of the oracle:
• Case q / ∈ (ΓX) . Then there is a hyperplane separating q from (ΓX) . Let {x ∈ R n : a T x = b} be the separating hyperplane, parameterized so that a ∈ R n , b ∈ R, a = 1, a T q > b and a T x ≤ b for every x ∈ (ΓX) . 
we have
In particular, with probability at least 1 − δ we have h ΓY (a) ≤ b, which implies q / ∈ ΓY and, by Lemma 11, Subroutine 2 outputs NO.
• Case q ∈ (ΓX) − . Let y = q + q = q(1 + q ). Let α = 1 + q . Then y ∈ ΓX. Invoke Lemma 13 for i.i.d. sample (x (i) ) N i=1 of X with p = y and equal to some 1 > 0 to be fixed later to conclude y ∈ (ΓY ) 1 . That is, there exist z ∈ ΓY such that z − y ≤ 1 .
Let w = z/α. Given (12) and the relationships y = αq and z = αw, we have w − q ≤ z − y ≤ 1 . To conclude, remember that q ∈ (ΓX) − . Therefore q + ≤ √ nσ max (A) (from Lemma 7 and equivariance of the centroid body, Lemma 8). This implies
The claim follows.
Lemma 13. Let X be a n-dimensional random vector such that for all coordinates i we have E(|X i | 1+γ ) ≤ M < ∞. Let p ∈ ΓX. Let (X Proof. By Proposition 10, there exists a measurable function λ : R n → R, −1 ≤ λ ≤ 1 such that p = E(Xλ(X)). Let
By Proposition 10, z ∈ ΓY . We have E X (i) (X (i) λ(X (i) )) = p and, for every coordinate j,
By Lemma 4 and for any fixed coordinate j we have, over the choice of (X (i) )
whenever N ≥ (8M √ n/ ) 1 2 + 1 γ . A union bound over n choices of j gives: 
Infinite sequences of symbols are of paramount importance in a wide range of fields, ranging from formal languages to pure mathematics and physics. A landmark was the discovery in 1912 by Axel Thue, founding father of formal language theory, of the famous sequence 0110 1001 1001 0110 1001 0110 · · · .Thue was interested in infinite words which avoid certain patterns, like squares ww or cubes www, when w is a non-empty word. Indeed, the sequence shown above, called the Thue-Morse sequence, is cube-free. It is perhaps the most natural cube-free infinite word. A common way to transform infinite sequences is by using finite state transducers. These transducers are deterministic finite automata with input letters and output words for each transition; an example is shown in Figure 1 . Usually we omit the words "finite state" and refer to transducers. A transducer maps infinite sequences to infinite sequences by reading the input sequence letter by letter. Each of these transitions produces an output word, and the sequence formed by concatenating each of these output words in the order they were produced is the output sequence. In particular, since this transducer runs for infinite time to read its entire input, this model of transduction does not have final states. A transducer is called k-uniform if each step produces k-letter words. For example, Mealy machines are 1-uniform transducers. A transducer is non-erasing if each step produces a non-empty word; this condition is prominent in this paper.
Although transducers are a natural machine model, hardly anything is known about their capabilities of transforming infinite sequences. To state the issues more clearly, let us write x y if there is a transducer taking y to x. This transducibility gives rise to a partial order of stream degrees [6] that is analogous to, but more fine-grained than, recursiontheoretic orderings such as Turing reducibility ≤ T and many-one reducibility ≤ m . We find it surprising that so little is known about . As of now, the structure of this order is vastly unexplored territory with many open questions. To answer these questions, we need a better understanding of transducers.
The main things that are known at this point concern two particularly well-known sets of streams, namely the morphic and automatic sequences. Morphic sequences are obtained as the limit of iterating a morphism on a starting word (and perhaps applying a coding to the limit word). Automatic sequences have a number of independent characterizations (see [1] ); we shall not repeat these here. There are two seminal closure results concerning the transduction of morphic and automatic sequences:
(1) The class of morphic sequences is closed under transduction (Dekking [4] ).
(2) For all k, the class of k-automatic sequences is closed under uniform transduction (Cobham [3] ).
The restriction in (2) to uniform transducers is shown by the following example.
Let w ∈ { 0, 1 } ω be defined by w(n) = 1 if n is a power of 2 and w(n) = 0 otherwise. This sequence is 2-automatic. Let h be the morphism 0 → 0 and 1 → 01. Taking the image of w under h, that is h(w), yields a sequence that is no longer automatic (but still morphic). Here is a sketch that h(w) is not 2-automatic. Note that the i th digit in h(w) is 1 iff i = 2 n + n for some n. Suppose that M is a finite-state machine with the property that reading in each number i in binary yields the i th digit of h(w). Let N be large enough so that the binary representation of 2 N + N has a run of zeroes longer than the number of states in N. Then by pumping, N must accept a number which is not of the form 2 n + n.
In this paper, we do not attack the central problems concerning the stream degrees. Instead, we are interested in a closure result for non-erasing transductions. Our interest comes from the following easy observation: This motivates the question: how powerful is non-erasing transduction?
The main result of this paper is stated in terms of the notion of α-substitutivity. This condition is defined in Definition 8 below, and the definition uses the eigenvalues of matrices naturally associated with morphisms on finite alphabets. Indeed, the core of our work is a collection of results on eigenvalues of these matrices. We prove that the set of α-substitutive words is closed under non-erasing finite state transduction. We follow Allouche and Shallit [1] in obtaining transducts of a given morphic sequence w by annotating an iteration morphism, and then taking a morphic image of the annotated limit sequence. For the first part of this transformation, we show that a morphism and its annotation have the same eigenvalues with non-negative eigenvectors. For the second part, we revisit the proof given in Allouche and Shallit [1] of Dekking's theorem that morphic images of morphic sequences are morphic. We simplify the construction in the proof to make it amenable for an analysis of the eigenvalues of the resulting morphism.
Durand [5] proved that if w is an α-substitutive sequence and h is a non-erasing morphism, then h(w) is α k -substitutive for some k ∈ N. We strengthen this result in two directions. First, we show that k may be taken to be 1; hence h(w) is α k -substitutive for every k ∈ N. Second, we show that Durand's result also holds for non-erasing transductions.
We recall some of the main concepts that we use in the paper. For a thorough introduction to morphic sequences, automatic sequences and finite state transducers, we refer to [1, 8] .
We are concerned with infinite sequences Σ ω over a finite alphabet Σ. We write Σ * for the set of finite words, Σ + for the finite, non-empty words, Σ ω for the infinite words, and Σ ∞ = Σ * ∪ Σ ω for all finite or infinite words over Σ.
A morphism is a map h : Σ → Γ * . This map extends by concatenation to h : Σ * → Γ * , and we do not distinguish the two notationally. Notice also that
An erased letter (with respect to h) is some a ∈ Σ such that h(a) = ε. A morphism h : Σ * → Γ * is called erasing if has an erased letter. A morphism is k-uniform (for k ∈ N) if |h(a)| = k for all a ∈ Σ. A coding is a 1-uniform morphism c : Σ → Γ.
A morphic sequence is obtained by iterating a morphism, and applying a coding to the limit word.
+ be a word, h : Σ → Σ * a morphism, and c : Σ → Γ a coding. If the limit h ω (s) = lim n→∞ h n (s) exists and is infinite, then h ω (s) is a pure morphic sequence, and c(h ω (s)) a morphic sequence.
If h(x 1 ) = x 1 z for some z ∈ Σ + , then we say that h is prolongable on x 1 . In this case,
is a pure morphic sequence. If additionally, the morphism h is k-uniform, then c(h
Example 4. A well-known example of a purely morphic word is the Thue-Morse sequence. This sequence can be obtained as the limit of iterating the morphism 0 → 01, 1 → 10 on the starting word 0. The first iterations are 0 → 01 → 0110 → 01101001 → 0110100110010110 → · · · , and they converge, in the limit, to the Thue-Morse sequence. As the morphism h is 2-uniform, the sequence is also 2-automatic.
An example of a purely morphic word which is not automatic is provided by the Fibonacci substitution a → ab, b → a. Starting with a, the fixed point is abaababaabaababaababaabaababaabaababaaba · · · .
Definition 6. For a ∈ Σ and w ∈ Σ * we write |w| a for the number of occurrences of a in w. Let h be a morphism over Σ. The incidence matrix of h is the matrix M h = (m i, j ) i∈Σ, j∈Σ where m i, j = |h( j)| i is the number of occurrences of the letter i in the word h( j).
Theorem 7 (Perron-Frobenius). Every non-negative square matrix M has a real eigenvalue α ≥ 0 that is greater than or equal to the absolute value of any other eigenvalue of M and the corresponding eigenvector is non-negative. We refer to α as the dominating eigenvalue of M.
The dominating eigenvalue of a morphism h is the dominating eigenvalue of M h . An infinite sequence w ∈ Σ ω over a finite alphabet Σ is said to be α-substitutive (α ∈ R)
if there exist a morphism h : Σ → Σ * with dominating eigenvalue α, a coding c : Σ → Σ and a letter a ∈ Σ such that (i) w = c(h ω (a)), and (ii) every letter of Σ occurs in h ω (a).
Remark. Let us remark on the importance of the condition (ii) in Definition 8. Without this condition every α-substitutive sequence w ∈ Σ ω would also be β-substitutive for every β > α that is the dominating eigenvalue of a non-negative integer matrix.
This can be seen as follows. Let h : Σ → Σ * be a morphism with dominating eigenvalue α. Let a ∈ Σ such that w = h ω (a) exists, is infinite and contains all letters from Σ. Then w is α-substitutive. Now let β > α be the dominating eigenvalue of a non-negative integer matrix. Then there exists an alphabet Γ (disjoint from Σ, Γ ∪ Σ = ∅) and a morphism g : Γ → Γ * with dominating eigenvalue β.
ω (a) = w and the dominating eigenvalue of z is β.
Two complex numbers x, y are called multiplicatively independent if for all k, ∈ Z it holds that x k = y implies k = = 0. We shall use the following version of Cobham's theorem due to Durand [5] . Theorem 9. Let α and β be multiplicatively independent Perron numbers. If a sequence w is both α-substitutive and β-substitutive, then w is eventually periodic.
Example 11. The transducer (Σ, ∆, Q, q 0 , δ, λ) shown in Figure 1 can be defined as follows: Σ = ∆ = { 0, 1 }, Q = { q 0 , q 1 , q 2 } with q 0 the initial state, and the transition function δ and output function λ are given by:
We use transducers to transform infinite words. The transducer reads the input word letter by letter, and the transformation result is the concatenation of the output words encountered along the edges.
Definition 12. Let M = (Σ, ∆, Q, q 0 , δ, λ) be a transducer. We extend the state transition function δ from letters Σ to finite words Σ * as follows: δ(q, ε) = q and δ(q, aw) = δ(δ(q, a), w) for q ∈ Q, a ∈ Σ, w ∈ Σ * . The output function λ is extended to the set of all words Σ ∞ = Σ ω ∪ Σ * by the following definition: λ(q, ε) = ε and λ(q, aw) = λ(q, a) λ(δ(q, a), w) for q ∈ Q, a ∈ Σ, w ∈ Σ ∞ . We introduce δ(w) and λ(w) as shorthand for δ(q 0 , w) and λ(q 0 , w), respectively. Moreover, we define M(w) = λ(w), the output of M on w ∈ Σ ω . In this way, we think of M as a function from (finite or infinite) words on its input alphabet to infinite words on its output alphabet
ω and y ∈ ∆ ω , we write y x if for some transducer M, we have M(x) = y.
Notice that every morphism is computable by a transducer (with one state). In particular, every coding is computable by a transducer.
Definition 13. Let M = (Σ, ∆, Q, q 0 , δ, λ) and N = (Σ , ∆ , Q , q 0 , δ , λ ) be transducers, and assume that Σ = ∆. We define the composition N • M to be the transducer
Here δ and λ are the extensions of the transition and output functions of N to Σ * , respectively. Proposition 14. Concerning the composition relation on transducers and on finite and infinite words:
Definition 15. Let h : Σ * → Σ * be morphisms, and let Γ ⊆ Σ be a set of letters. We call a letter a ∈ Σ (i) dead if h n (a) ∈ Γ * for all n ≥ 0, (ii) near dead if a Γ, and for all n > 0, h n (a) consists of dead letters,
with respect to h and Γ. We say that the morphism h respects Γ if every letter a ∈ Σ is either dead, near dead, resilient, or resurrecting. (Note that all of these definitions are with respect to some fixed h and Γ.)
Lemma 16. Let g : Σ * → Σ * be a morphism, and let Γ ⊆ Σ. Then g r respects Γ for some natural number r > 0.
Proof. See Lemma 7.7.3 in Allouche and Shallit [1] .
Definition 17. For a set of letters Γ ⊆ Σ and a word w ∈ Σ ∞ , we write γ Γ (w) for the word obtained from w by erasing all occurrences of letters in Γ.
Definition 18. Let g : Σ * → Σ * be a morphism, and Γ ⊆ Σ a set of letters. We construct an alphabet ∆, a morphism ξ : ∆ * → ∆ * and a coding ρ : ∆ → Σ as follows. We refer to ∆, ξ, ρ as the morphic system associated with the erasure of Γ from g ω .
Let r ∈ N >0 be minimal such that g r respects Γ (r exists by Lemma 16). Let D be the set of dead letters with respect to g r and Γ. For x ∈ Σ * we use brackets [x] to denote a new letter. For words w ∈ {g r (a) | a ∈ Σ}, whenever γ D (w) = w 0 a 1 w 1 a 2 w 2 · · · a k−1 w k−1 a k w k with a 1 , . . . , a k Γ and w 0 , . . . , w k ∈ Γ * , we define
Here it is to be understood that
Let the alphabet ∆ consist of all letters [a] and all bracketed letters [w] occurring in words blocks(g r (a)) for a ∈ Σ. We define the morphism ξ : ∆ → ∆ * and the coding ρ : ∆ → Σ by
Remark. The requirement that g r respects Γ in Definition 18 guarantees for every a ∈ Σ that either g r (a) consists of dead letters only or g r (a) contains at least one near dead or resilient letter. In both cases, blocks(g r (a)) is well-defined. As a consequence ξ([w]) is well-defined for every [w] ∈ ∆.
Example 19. We let Σ = { a, b, c } and define a morphism g : Σ → Σ * by a → ab, b → ac and c → a. The word g ω (a) = abacabaabacababacabaabacabacabaabacababa · · · is known as the tribonacci word.
Let Γ = { a }, that is, we delete the letter a. The morphism g does not respect Γ since g(c) = a ∈ Γ * but g 2 (c) = ab Γ * . However, g 2 respects Γ: g 2 (a) = abac, g 2 (b) = aba and g 2 (c) = ab. The letter a is resurrecting and b, c are resilient with respect to g 2 and Γ. 
Then an application of the coding ρ yields ρ(
Example 20. We let Σ = { a,
where
Proposition 21. Let g : Σ * → Σ * be a morphism, a ∈ Σ such that g ω (a) ∈ Σ ω , and Γ ⊆ Σ a set of letters. Let ∆, ξ and ρ be the morphic system associated to the erasure of Γ from g ω in Definition 18. Then
We prove by induction on n that for all words w ∈ ∆ * , and for all n ∈ N, cat(ξ n (w)) = g nr (cat(w)). The base case is immediate. For the induction step, assume that we have n ∈ N such that for all words w ∈ ∆ * , cat(ξ
By the induction hypothesis, cat(ξ n+1 (w)) = g nr (cat(ξ(w))) = g nr (g r (cat(w))) = g (n+1)r (cat(w)). To complete the proof, note that by definition ρ([w a u]) = γ Γ (w a u) and thus ρ(w) = γ Γ (cat(w)) for every w ∈ ∆ * . Hence, for all n ≥ 1,
Definition 22. Let g, h : Σ * → Σ * be morphisms such that h is non-erasing. We construct an alphabet ∆, a morphism ξ : ∆ * → ∆ * and a coding ρ : ∆ → Σ as follows. We refer to ∆, ξ, ρ as the morphic system associated with the morphic image of g ω under h.
For nonempty words w = a 1 a 2 · · · a k ∈ Σ * we define head(w) = a 1 and tail(w) = a 2 · · · a k . We also define img(w)
We define the morphism ξ : ∆ * → ∆ * and the coding ρ : ∆ → Σ by
Notice here the ρ([a]) and u i , defined using head() and tail(), are well-defined since h is non-erasing and hence h(a i ) will be nonempty.
Here is an example illustrating Definition 22. Let g be the substitution from the Fibonacci word, g(a) = ab and g(b) = a. Further, let h be defined so that h(a) = bb and h(b) = a. As in Definition 22, let ξ and ρ be defined by
The point here is that applying ρ to the limit word ξ ω ([a]) is the same as h(g ω (a)):
Proposition 24. Let g, h : Σ * → Σ * be morphisms such that h is non-erasing, and a ∈ Σ such that g ω (a) ∈ Σ ω . Let ∆, ξ and ρ be as in Definition 18. Then
Proof. We define z : ∆ → Σ * by z(a) = ε and z([a]) = a for all a ∈ Σ. By induction on n > 0 we show
We start with the base case. Note that ρ(ξ([a])) = h(g(a)) = h(g(z([a]))) and ρ(ξ(a)) = ε = h(g(z(a))) for all a ∈ Σ, and thus ρ(ξ(w)) = h(g(z(w))) for all w ∈ ∆ * . Moreover, we have z(ξ([a])) = g(a) = g(z([a])) and z(ξ(a)) = ε = g(z(a)) for all a ∈ Σ, and hence z(ξ(w)) = g(z(w)) for all w ∈ ∆ * .
Let us consider the induction step. By the base case and induction hypothesis
Thus ρ(ξ n ([a])) = h(g n (a)) for all n ∈ N, and taking limits yields
Every morphic image of a word can be obtained by erasing letters, followed by the application of a non-erasing morphism. As a consequence we obtain:
Corollary 25. The morphic image of a pure morphic word is morphic or finite.
ω be a word and h : Σ → Σ * a morphism. Let Γ = { a | h(a) = ε } be the set of letters erased by h, and ∆ = Σ \ Γ. Then h(w) = g(γ Γ (w)) where g is the non-erasing morphism obtained by restricting h to ∆. Hence for purely morphic w, the result follows from Propositions 21 and 24.
Theorem 26 (Cobham [2] , Pansiot [7] ). The morphic image of a morphic word is morphic.
Proof. Follows from Corollary 25 since the coding can be absorbed into the morphic image.
The following lemma states that if a square matrix N is an extension of a square matrix M, and all added columns contain only zeros, then M and N have the same non-zero eigenvalues.
Lemma 27. Let Σ, ∆ be disjoint, finite alphabets. Let M = (m i, j ) i, j∈Σ and N = (n i, j ) i,j∈Σ∪∆ be matrices such that (i) n i, j = m i, j for all i, j ∈ Σ and (ii) n i, j = 0 for all i ∈ Σ ∪ ∆, j ∈ ∆. Then M and N have the same non-zero eigenvalues.
Proof. N is a block lower triangular matrix with M and 0 as the matrices on the diagonal. Hence the eigenvalues of N are the combined eigenvalues of M and 0. Therefore M and N have the same non-zero eigenvalues.
We now show that morphic images with respect to non-erasing morphisms preserve α-substitutivity. This strengthens a result obtained in [5] where it has been shown that the non-erasing morphic image of an α-substitutive sequence is α k -substitutive for some k ∈ N. We show that one can always take k = 1. Note that every α-substitutive sequence is also α k -substitutive for all k ∈ N, k > 0.
Theorem 28. Let Σ be a finite alphabet, w ∈ Σ ω be an α-substitutive sequence and h : Σ → Σ * a non-erasing morphism. Then the morphic image of w under h, that is h(w), is α-substitutive.
Proof. Let Σ = { a 1 , . . . , a k } be a finite alphabet, w ∈ Σ ω be an α-substitutive sequence and h : Σ → Σ * a non-erasing morphism. As the sequence w is α-substitutive, there exist a morphism g : Σ → Σ * with dominant eigenvalue α, a coding c : Σ → Σ and a letter a ∈ Σ such that w = c(g ω (a)) and all letters from Σ occur in g ω (a). Then h(w) = h(c(g ω (a))) = (h•c)(g ω (a))), and h • c is a non-erasing morphism. Without loss of generality, by absorbing c into h, we may assume that c is the identity. From h and g, we obtain an alphabet ∆, a morphism ξ, and a coding ρ as in Example 29. Let F be the Fibonacci word (generated by the morphism a → ab and b → a) and let T be the Thue-Morse sequence. We show that there exist no non-erasing morphisms g, h such that g(F) = h(T) and this image is not ultimately periodic. Let g and h be non-erasing morphisms. The Fibonacci word is ϕ-substitutive where ϕ = (1 + √ 5)/2 is the golden ratio, and the Thue-Morse sequence is 2-substitutive. By Theorem 28, g(F) is ϕ-substitutive and h(T) is 2-substitutive. Note that ϕ and 2 are multiplicatively independent: using induction on k ∈ N >0 it follows that every ϕ k is of the form a + b √ 5 for rational numbers a, b > 0. It follows by Theorem 9 that g(F) = h(T) implies that this word is ultimately periodic.
Remark. The restriction to non-erasing morphisms in Theorem 28 is important since every morphic sequence can be obtained by erasure of letters from a 2-substitutive sequence.
Nevertheless, we can use the above theorem to reason about morphic images with respect to erasing morphisms as follows. Let w ∈ Σ ω , and g : Σ → Σ * a morphism. Let Γ be the letters erased by g, and let h be the restriction of g to Σ \ Γ. Then h is non-erasing and g(w) = h(γ Γ (w)). Hence, if γ Γ (w) is α-substitutive, then so is g(w) by Theorem 28. As a consequence, it suffices to determine α-substitutivity of all sequences γ Γ (w) with Γ ⊆ Σ (using Definition 18 and Proposition 21).
In this section, we give a proof of the following theorem due to Dekking [4] .
Theorem 30 (Transducts of morphic sequences are morphic). If M = (Σ, ∆, Q, q 0 , δ, λ) is a transducer with input alphabet Σ and x ∈ Σ ω is a morphic sequence, then M(x) is morphic or finite.
This proof will proceed by annotating entries in the original sequence x with information about what state the transducer is in upon reaching that entry. This allows us to construct a new morphism which produces the transduced sequence M(x) as output. After proving this theorem, we will show that this process of annotation preserves α-substitutivity. Example 31. To illustrate several points in this section, we will consider the Fibonacci morphism (h(a) = ab, h(b) = a) and the transducer which doubles every other letter, shown in Figure 2. 
We show in Lemma 40 that transducts of morphic sequences are morphic. In order to prove this, we also need several lemmas about transducers which are of independent interest. The approach here is adapted from a result in Allouche and Shallit [1] ; it is attributed in that book to Dekking. We repeat it here partly for the convenience of the reader, but mostly because there are some details of the proof which are used in the analysis of the substitutivity property.
Definition 32 (τ w , Ξ(w)). Given a transducer M = (Σ, ∆, Q, q 0 , δ, λ) and a word w ∈ Σ * , we define τ w ∈ Q Q to be τ w (q) = δ(q, w).
Example 33. Recall the transducer M from Figure 2 . Let id : Q → Q be the identity, and let ν : Q → Q be the transposition ν(s) = t and ν(t) = s. For this transducer, τ w = id if |w| is even and τ w = ν if |w| is odd. We have Ξ(a) = (τ a , τ ab , τ aba , τ abaab , τ abaababa , . . .). In this notation,
Next, we show that { Ξ(w) : w ∈ Σ * } is finite.
Lemma 34. For any transducer M and any morphism h : Σ → ΣLemma 39. For all σ ∈ Σ, all w ∈ Σ * and all natural numbers n, if h
In particular, for 1 ≤ i ≤ , the first component of the i th term in h n (σ, Θ(w)) is s i .
Proof. By induction on n. For n = 0, the claim is trivial. Assume that it holds for n. Let h n (σ) = s 1 s 2 · · · s , and for 1
Concatenating the sequences h(s i , Θ((h n w)s 1 · · · s i−1 )) for i = 1, . . . , completes our induction step. 
This sequence z is morphic in the alphabet Σ × Q.
Proof. For (i), write h(x 1 ) as x 1 x 2 · · · x . Using the fact that h i ( ) = for all i, we see that
This verifies the prolongability. For (ii): if Θ(w) = Θ(u), then τ w and τ u are the first component of Θ(w) and are thus equal.
We turn to (iii). Taking w = in Lemma 39 shows that h
The image of this sequence under the coding c is
In view of the τ functions' definition (Def. 32), we obtain z in (2) . By definition, z is morphic.
Therefore, row k of N times v is
v b m a,b = αv a , since v is an eigenvector of M. Finally we note that the kth entry of v is v a by its definition. Hence multiplying v by N multiplies the kth entry of v by α for all k.
We have shown that v is a column eigenvector of N with eigenvalue α, so the (column) eigenvalues of M are all present in N. However, since a matrix and its transpose have the same eigenvalues, the (column) qualification on the eigenvalues is unnecessary.
If h is an annotation of h, then we have
Lemma 44. Let h, h be morphisms such that h :
Then every eigenvalue of h with a non-negative eigenvector is also an eigenvalue for h.
Proof. Let M = (m i, j ) i, j∈Σ be the incidence matrix of h and N = (n i, j ) i, j∈Σ×A be the incidence matrix of h. Let r be an eigenvalue of N with corresponding eigenvector v = (v (b, a) ) (b, a)∈Σ×A , that is, Nv = rv and v 0. We define a vector w = (w b ) b∈Σ as follows:
Hence Mw = rw. If w 0 it follows that r is an eigenvalue of M. Note that if v is non-negative, then w 0. This proves the claim.
* is an annotation of h : Σ → Σ * . Then the dominant eigenvalue for h coincides with the dominant eigenvalue for h.
Proof. By Lemma 43 every eigenvalue of h is an eigenvalue of h. Thus the dominant eigenvalue of h is greater or equal to that of h. By Theorem 7, the dominant eigenvalue of a non-negative matrix is a real number α > 1 and its corresponding eigenvector is nonnegative. By Lemma 43, every eigenvalue of h with a non-negative eigenvector is also an eigenvalue of h. Thus the dominant eigenvalue of h is also greater or equal to that of h.
Hence the dominant eigenvalues of h and h must be equal.
Theorem 46. Let α and β be multiplicatively independent real numbers. If v is a α-substitutive sequence and w is an β-substitutive sequence, then v and w have no common non-erasing transducts except for the ultimately periodic sequences.
Proof. Let h v and h w be morphisms whose fixed points are v and w, respectively. By the proof of Theorem 30, x is a morphic image of an annotation h v of h v , and also of an annotation h w of h w . The morphisms must be non-erasing, by the assumption in this theorem. By Corollary 45 and Theorem 28, x is both α-and β-substitutive. By Durand's Theorem 9, x is eventually periodic.
We conclude the section with an example of Theorem 30 and the lemmas in this section.
Example 47. We saw the Fibonacci sequence in Example 5:
We conclude our series of examples pertaining to this sequence and the transducer M which doubles every other letter (see Example 31 and Figure 2) . We want to exhibit h, following the recipe of Lemma 40. First, some examples of how h works: It turns out that only a few elements from this A end up appearing in the expressions for h(σ, Θ(w)): It is convenient to abbreviate some of the elements of Σ × A: Let us use x as an element of {a, b}, and also write (x, Θ( )) as x 0 , (x, Θ(a)) as x 1 , (x, Θ(b)) as x 2 and (x, Θ(ab)) as x 3 . It turns out that we do not need to exhibit h in full because only eight points are reachable from a 0 . We may take h to be
The fixpoint of this morphism starting with a 0 starts as Recall that λ : Σ × Q → ∆ * = Σ * in our transducer doubles whatever letter it sees while in state s and copies whatever letter it sees while in state t. That is, λ(x s ) = xx, and λ(x t ) = x. Thus when we apply the morphism λ to the sequence z, we get λ(z) = aa b aa a bb a bb a aa b aa a bb a bb a aa b aa b aa a bb a aa b aa b · · · As we saw in the proof of Theorem 30, this sequence aabaaabbabbaaabaaabbabbaaabaabaaabbaaabaab · · · is exactly M(x).
We have re-proven some of the central results in the area of morphic sequences, the closure of the morphic sequences under morphic images and transduction. However, the main results in this paper come from the eigenvalue analyses which followed our proofs in Sections 3 and 4. These are some of the only results known to us which enable one to prove negative results on the transducibility relation . One such result is in Theorem 46; this is perhaps the culmination of this paper.
The next step in this line of work is to weaken the hypothesis in some of results that the transducers be non-erasing. Although our results can be used to reason about erasing morphisms, see Remark 3, this does not help us with erasing transducers since annotating a morphism can yield an unbounded large alphabet. As a consequence, to reason about erasing transducers, we need to understand better what form of annotated morphisms arise from transducers, and how these interact with the erasure of letters (Proposition 21).
Stroke and seizures-induced neurodegeneration share a number of biological processes, including increased neuronal activity, neuronal plasticity, inflammation, and apoptosis [1, 2] . Separation of effects of these processes on gene expression, identification of participating transcription factors, and comparison of transcriptional regulation between the two pathological conditions remain a challenging task. Global gene expression following stroke and seizures were compared before at a single time-point [3] , but no comparison of time-series gene profiling datasets from the two conditions was reported to date.
Alter et al. first introduced a concept that orthogonal components (eigensystems) resulting from the singular value decomposition (SVD) of time-series gene expression dataset [4, 5] may help to separate concurrent effects of different processes and regulators on gene expression. These authors proposed that an eigen array may reflect a genome-wide input from a particular regulator, with the corresponding eigen gene reflecting this regulator's activity across the samples (arrays). For an illustration of the SVD nomenclature, when applied to gene expression -see Additional file 1.
A number of recent studies concentrated on usefulness of eigengenes [6] [7] [8] [9] [10] , whereas the properties and interpretation of eigenarrays remained relatively less explored. We previously suggested that conservation of eigenarrays between related biological systems may identify eigensystems of biological origin [11] . In the same work, utilizing a comparative SVD approach we identified an eigensystem conserved between hippocampal development and differentiation of hippocampal neurons in vitro. Analysis of cis-regulation of that eigensystem revealed that it reflected exit of neural precursors from the cell cycle and beginning of neuronal differentiation, regulated by transcription factors E2f1 and Nr2f1 [12] .
Bayesian Networks (BN) learning approach is a wellestablished method of modelling gene regulation and interactions between gene regulatory motifs, starting from gene expression data [13] or gene expression and genomic sequence data [14] [15] [16] [17] [18] [19] [20] . The use of linear regression in analysis of gene cis-regulation is grounded in the linear response model of gene regulation [21, 22] .
Here, we report a time-series dataset from gene expression profiling in the rat MCAO model of stroke, and compare these data to the published time-series dataset from the kainate-induced seizures model [23] . By comparative SVD approach, followed by Bayesian network analysis of cis-regulation, we identified two conserved eigensystems separating the effects of different well-defined biological processes on gene expression and regulated by distinct sets of transcription factor binding sites. The results obtained on either dataset were validated on the other.

We compared two time-series gene expression datasets from experimental rat models of stroke and epilepsy, which were the transient middle-cerebral artery occlusion (MCAO) and the kainate-induced seizures, respectively. The MCAO dataset was generated in our laboratory and probed gene expression in the cortex of the ischemic hemisphere at four time-points (6, 12, 24, 48 h ) following a 90 minutes occlusion of the right middle-cerebral artery in adult anesthetized rats, and included sham-operated animals as controls. The kainate dataset, published by Koh and co-workers [23] probed gene expression in the hippocampus of adult rats at five time-points (1, 6, 24, 72, 240 h) following the injection of kainate -a neurotransmitter analogue inducing seizures, which can last for several hours, followed by a seizure-free latent period.
As immobilization of a conscious animal and injection alters gene expression in the brain, this dataset included a control time-series following the injection of saline.
The overall design of our study is illustrated in Figure  1 . We transform each dataset (MCAO, kainate) separately by SVD ( Figure 1A ) and identify eigenarrays conserved between the two systems ( Figure 1B ). This is followed by analysis of biological function using Gene Ontology (GO), and gene cis-regulation using Bayesian networks (BN) and our TRAM database of putative regulatory regions and motifs. These analyses are performed separately for either dataset and then the results for the corresponding eigensystems are compared (GO terms) or statistically cross-validated (BN results) on the other dataset. The cross-validation between the stroke and seizures data is not contradictory with the goal of gaining information by comparison of the two, because the two experimental models can be assumed -on biological grounds -to share some, but not all, regulatory mechanisms. Note that features specific for one model can be identified, as for each model we separately account for the multiplicity of testing. The eigenarrays resulting from the SVD of either dataset were compared by correlation analysis performed for the genes common between the two datasets. (C) For the emerging conserved eigensystems 2 and 3, separately for all the genes in either dataset, we studied their functional Gene Ontology (GO) associations and employed Bayesian Networks (BN) to study their cis-regulation. The results obtained on one dataset were then compared (GO) or statistically tested (BN) on the other.
The global temporal changes in gene expression following MCAO in the stroke model are dominated by the top three eigensystems (Figure 2A ). The eigengene of the first eigensystem in the MCAO dataset (M1, "M" to indicate MCAO) is constant in time (data not shown) in the log-expression space and thus represents the average level of expression across all the conditions. The second eigengene (M2) represents an increased expression, as compared to control value, at 12-48 h following MCAO, with a peak at 12 h ( Figure 2B ). The third eigengene (M3) represents a complex pattern with an increase in gene expression at 12 h followed by down-regulation of expression at 24 h and further drop at 48 h ( Figure 2C ). Notably, the three top eigengenes indicate no changes in gene expression at 6 h after MCAO, which is in agreement with our earlier PCR results showing no changes in mRNA levels of a smaller number of genes [24] .
The global temporal pattern of gene expression following kainate-induced seizures in adult rats is dominated by the top three eigensystems ( Figure 2D ), of which the first again represents the magnitude (data not shown). The second eigengene (A2), represents an increased expression following the injection of kainate; starting at 1 h, largest at 6 and 24 h, returning to the baseline level at 72 and 240 h; and no change at any time-point after the injection of saline ( Figure 2E ). The third eigengene (A3) represents an increased expression at 1 and 6 h after the injection of kainite; followed by strong decrease in expression at 24 h, continuing, but less pronounced, also at the 72 and 240 h ( Figure 2F ).
Despite their overall similarity, the corresponding eigengenes are distinct between the two experimental models. In particular, eigengenes M2 and M3 show no change in expression at 6 h following the MCAO, in contrast to eigengenes A2 and A3, showing an increase at 6 h following the injection of kainate.
The kainate datasets comprised of expression profiles for 2786 genes (distinct Ensembl gene_stable_id) that significantly changed expression and the stroke dataset consisted of 2392 such genes, with 737 genes common between the two datasets. The correlation analysis revealed that the top three eigenarrays (compared for the common genes) were highly correlated ( Figure  2G ). The correlations between the respective first, second, and third eigenarrays were 0.87, 0.84, and 0.63, respectively. Note that the correspondence between the three conserved eigenarrays was one-to-one. Given the length (737) of the correlated vectors, these correlations are highly significant (p-values: 10 -229 , 10 -197 , 10 -83 , respectively, assuming independence of genes). This indicates that the top three eigenarrays are highly conserved between the two datasets. Figure 2H -I shows directly genes' loadings of the respective second (H) or third (I) eigensystem in the two datasets, sorted on their average loading in both datasets. This sorting of the genes aids visualization of the eigenarrays conservation, but is not in any way a reason for it, as the correlations shown in Figure 2G were computed before the sorting (and would not be affected by it, anyway). The tangent-like shape of the plots reflects the bell shape of the distributions of genes' loadings of eigensystems 2 and 3.
Further, we focus on eigensystems 2 and 3 characterized by conservation of their eigenarrays occurring despite differences between the corresponding eigengenes ( Figure 2B vs. E, C vs. F). This suggests that the two eigensystems reflect regulatory inputs operating in both systems, but with different kinetics and relative strengths.
A universal reason underlying co-regulation of genes is participation of their products in a common biological process. To assess if the contribution of the eigensystems 2 and 3 to the gene expression profiles is associated with biological functions, we analyzed the Gene Ontology "biological process" annotations of all genes in either dataset, ranked on the loadings of the respective eigensystems 2-3.
In both experimental models, the positive loading of the second eigensystem was significantly associated with overlapping GO terms describing the inflammatory response to the brain injury ( Figure 3A, B) . Additionally, in the MCAO system the positive loading of eigensystem M2 was also significantly associated with GO terms describing programmed cell death (apoptosis).
In the kainate system, the positive loading of eigensystem A3 was highly significantly associated with several overlapping GO terms describing neuronal activity, such as: synaptic transmission, transmission of the nerve impulse ( Figure 3C ). No such association was detected for third eigensystem (M3) from the SVD on the MCAO dataset, following its initial filtering (ANOVA p-value < 0.05). However, when the GO analysis was repeated for the third eigenarray in the SVD result on the MCAO dataset filtered at ANOVA p-value < 0.5 and thus containing more genes, there was a clear association between the loadings of the third eigensystem and GO terms describing neuronal activity ( Figure 3D ). Loosening of the p-value threshold was possible, because the top three eigensystems were extremely robust to the change of the p-value threshold, with eigenarrays correlations > 0.999 between vectors of length 2786 for the change of the threshold from 0.05 to 0.5 (data not shown). Comparison of the singular values ( Figure 2A vs. 2D) indicates that the relative contribution of the conserved third eigensystem (reflecting neuronal activity) was higher in the kainate system, while the relative contribution of the conserved second eigensystem (reflecting inflammation and/or apoptosis) was higher in the stroke.
Conservation of eigenarrays suggests that they reflect regulatory mechanisms, possibly operating at the level of transcription regulation. To identify such mechanisms, we employed Bayesian networks, previously successfully applied to modelling transcriptional regulation [14, 15, [17] [18] [19] [20] . We follow the above approaches in general, but several essentials are specific to our methodology:
• Regulation of gene expression is analysed separately for each conserved eigensystem. In the subspace of a given eigensystem gene expression is binarized into up-and down-regulation, according to the sign of its loading. (Figure 4B , D).
• Our combinatorial model of cis-regulation takes into account fragmentation of metazoan cis-regulatory regions into multiple conserved non-coding sequences (CNSs) [25, 26] , and distinguishes between co-occurrence of several TF-binding motifs in the Figure 3 Functional Gene Ontology annotations associated with the conserved eigensystems (A-D) Association of loadings of the conserved eigensystems with the functional annotations from the GO "biological process" ontology were analyzed by Wilcoxon sign rank test using RankGOstat [55] . Twenty GO terms most associated with a given eigensystem, and their association FDR q-values are shown as bar plots. For the plots the q-values were log10-transformed and multiplied by +1 or -1, to reflect association with the positive or negative loadings of a particular eigensystem. GO terms with overlapping meanings (identified by human inspection) are indicated by the same colour of the bars, with red marking terms related to "synaptic transmission", blue marking terms similar to "inflammatory response", and black marking terms describing cell death/apoptosis.
same CNS and their co-occurrence in the same gene ( Figure 4A , C). Following previous work [27, 28] , we term every possible subset of the motifs present in the same CNS a composite motif.
• Regulatory mechanism is predicted by learning Bayesian networks with an exact algorithm. Computations are performed by double application of the BNFinder program [29] . The first run selects the most promising composite motifs (possibly single motifs), while the second run selects the sets of such composite motifs that best predict the sign of the loading the chosen eigensystem ( Figure 4E , F).
Four BN analyses were performed, separately for each conserved eigensystem in either experimental model (M2, A2, M3, A3). BN scores were directly converted to q-values -the false discovery rate [30] analogue of p-values, by comparing each feature's score on the Figure 4 Bayesian network model of fragmented cis-regulatory regions (A, C) Sequence preprocessing consists of extracting instances of composite motifs i.e. sets of (up to three motifs) in the same conserved non-coding sequence (CNS), from the flanks of transcription start sites of all human-rat orthologous genes. (B, D) Expression data preprocessing consists of SVD, followed by discretization of expression into up-and down-regulation in the subspace of a particular conserved eigensystem -based on the sign of its loading. (C, D) Composite motifs and expression data are combined in one dataset, in which the data records correspond to genes. (E) This dataset becomes an input for our Bayesian networks (BN) learning algorithm, which identifies sets of composite motifs most associated with the sign of loadings of a given eigensystem. (F) The final output consists of a ranking of such sets, with conditional probability distributions representing their impact on a given eigensystem. BN learning was performed independently for each of the eigensystems: A2, A3, M2, M3; on the data for all the genes in the respective dataset. Eigensystem A3 is shown as an example.
original data to the distribution of scores from 1000 BN analyses on permuted data -each following an independent random permutation and assignment of expression values to the genes' putative cis-regulatory regions. The conservation of the two eigensystems between the stroke allowed for selection of best features on one dataset (we choose up to ten features with training q-values < 0.05) and then testing them on the other -containing the data for largely different genes. The training and testing were performed for the conserved second ( Figure 5A , B) and third eigensystem ( Figure 5C , D) in both directions. During the test we used the same q-values as during the training, i.e. they were corrected for all the hypotheses ever looked at on the test dataset. We note that this is a very stringent correction, as only up to 20 hypotheses Figure 5 BN analysis of cis-regulation for the conserved eigensystems. The four tables (A-D) present the results of BN analysis of cisregulation for the conserved second and third eigensystems from either dataset, followed by testing of highest-ranking features on the corresponding eigensystem from the other dataset. In each panel, the column Feature lists up to 10 nonempty sets of composite motifs with highest BN score and q-value < 0.05 on the indicated training dataset. Note that single motifs are included in the set of composite motifs. BN score of a composite motif set is the ratio of its posterior probability to the posterior probability of the empty set. The corresponding q-value derives from the permutational test. The shaded columns give the values of BN score and the corresponding q-value for the same feature computed on the other (test) dataset. Red color marks the cells with the test q-values < 0.05 for the features that also had training q-value < 0.05 and the descriptions of such features are given in bold. The q-values take into account the multiplicity of testing for each dataset separately, so it is possible to identify the features significant for one dataset only. (E, F) The conditional probability tables for the pairs of motifs: {AP1F, SATB} (E) and {EGRF, LHXF} (F).
were considered for each eigensystem during the test stage (up to ten for either direction of the comparison).
Antagonistic effects of motifs binding AP1 and SATB on gene expression following the stroke BN search identified just one feature, namely the motif AP1F -a family of binding sites for the transcription factor AP1 (Additional file 2) as the feature significantly (q-value < 0.05) associated with the positive sign of eigensystem A2 in the kainate model ( Figure 5A , columns: "Training: Kainate"). Notably, this feature was significantly associated with the corresponding eigensystem M2, when tested on the dataset from the MCAO model ( Figure 5A , column Test: MCAO). The choice of the MCAO data as the training dataset resulted in identification of 7 significant features, of which the second was again AP1F, and only this feature was significant also in the cross-system test on the kainate dataset ( Figure 5B , columns "Test: kainate"). All remaining features identified as significant (q-value < 0.05) on the training datasets included AP1F as one motif, and two of them were pairs of AP1F with another motif in the same gene. Of the features significant in the MCAO system, particularly interesting is the pair {AP1F, SATB} -a set of two motifs co-occurring in the same gene, which have antagonistic effects on expression in the subspace of eigensystem M2. The presence of motif AP1F in the absence of SATB in the same gene was associated with the positive sign of M2 loading, while the presence of SATB in the absence of AP1F was associated with the negative M2 loading ( Figure 5E ).
Identification of known and new regulators/targets for the eigensystem reflecting synaptic activity BN search identified a number of features as highly significantly (q-value < 0.001) associated with the sign of M3 loading during the training on the kainate dataset. The ten highest-ranking features, ranked on their BN score were tested on the MCAO dataset ( Figure 5C ). Of the top ten features significant on the kainate dataset, four were also significant on the MCAO dataset. All of these features, marked in bold in Figure 5C , were pairs of motifs co-occurring in the same gene. All these pairs contained LHXF as one motif, with EGRF, AHRR, ZF5F or ZBPF as the other motif. The highest-ranking featurethe pair EGRF and LHXF in the same gene, but neither motif of its own, was 79% specific for the positive sign of eigensystem 3 ( Figure 5F ). When the training was performed on the MCAO dataset, several features significantly (q-value < 0.001) associated with the sign of M3 were identified ( Figure 5D ). Importantly, out of the top ten features identified on the stroke dataset, nine were also significantly associated with the same sign of M3 on the kainate dataset. The features significant in the cross-system test were either single motifs (AP1R, PARF, CREB, AHRR) or pairs of motifs in the same gene. All these pairs contained AP1R as one motif, with PARF, AHRR, ZF5F, EGRF, E4FF as the other motif. Three motifs, namely EGRF, ZF5F, AHRR were common between the top ten features identified during training on the kainate and the MCAO datasets.
We wanted to check if a model taking into account motif multiplicity would allow a more precise prediction of the value of expression. Therefore, we applied a linear regression analysis to the motifs identified by BN analysis as significant in both systems, and additionally the motif SATB significant in the MCAO system only. For the reasons detailed in the Materials and methods, we always performed a weighted linear regression, with the average loadings in groups of genes with the same motif count as the response variable, and the weights set to the numbers of genes in each group, as suggested by Faraway [31] .
The regression analyses were performed separately for the MCAO and the kainate datasets. The linear regression confirmed that the AP1F and SATB motifs had antagonistic effects on expression in the subspace of eigensystem M2 (Figure 6A-C) . The count of motif SATB per gene had a clear linear (R 2 = 0.91) and highly significant (p = 1.6 × 10 -5 ) effect on the group-average expression in the subspace of eigensystem M2 ( Figure  6A ). In agreement with the earlier BN result, the count SATB had no effect on loading of eigensystem A2 (data not shown). The inhibitory effect of SATB on gene expression in the MCAO system was specific for eigensystem M2, with no inhibition of expression in the subspace of any other eigensystem (data not shown).
The count of motif AP1F had a significant, positive and possibly linear effect on the average expression in a subspace of the second eigensystem, both in the MCAO (p = 0.0019, R 2 = 0.64) and in the kainate dataset ( Figure 6B , p = 0.00059, R 2 = 0.71). Remarkably, when the effect of AP1F count on M2 loading was analyzed separately for the genes with and without motif SATB, the effect became more apparent for the genes without motif SATB ( Figure 6C , p = 0.00044, R 2 = 0.76), while the effect was nullified for the genes with the motif SATB ( Figure 6D) .
The linear regression revealed that the count of motif CREB had a highly significant and approximately linear effect on the average expression in a subspace of the third eigensystem in the kainate ( Figure 7A eigensystem, in particular in the MCAO model, where it had no effect on the loadings of the eigensystem M2 (data not shown).
The effects of motif multiplicity on gene expression prompted us to investigate by the linear regression if a related variable -the count of conserved non-coding sequences (CNSs) per gene had an effect on gene expression. That we found was true in both experimental models (Figure 7E, F) . Similarly to the effect of CREB count, the effect of CNS count was highly specific for the third eigensystem (data not shown). However, when the effect of CNS count was analyzed in a bivariate linear regression model, together with that of CREB, the effect of the CNS was completely (MCAO) or nearly completely (kainate) dependent on the CNSs' content of Creb-binding motifs ( Figure 7E, F) .
Here, we demonstrated that eigensystems conserved between stroke and seizures separate effects of inflammation/apoptosis and synaptic activity on gene expression. The contribution of the eigensystem 3 reflecting synaptic activity was relatively greater (compared to eigensystem 2) in the seizures model, in agreement with higher electrical activity of neurons. Remarkably, our analysis of cis-regulation revealed that the these two functionally well-interpretable eigensystems were regulated by distinct sets of transcription factors, with AP1 and SATB regulating the eigensystem reflecting inflammation/apoptosis, and numerous TFs including Creb and Egr regulating the eigensystem reflecting neuronal synaptic activity.
Activation of transcription factor AP1 following the kainate-induced seizures and cerebral ischemia is well established [32, 33] . In particular, Timp1 was shown to be the target of AP1 following kainate-induced seizures [34] . The mRNA profiles of Timp1 in both systems ( Figure 6E, F) are highly similar to the profiles of the respective second eigengenes, which is compatible with our identification of AP1 as the key regulator of this eigensystem. It is well established that activation of Mapk8-Jun/AP1 signalling pathway has a predominantly pro-apoptotic effect in neurons [35] , however, only few Mapk8-AP1 targets genes have been identified. Therefore, demonstrating the importance of the number of AP1-binding motifs per gene and the simultaneous absence of SATB motif for gene activation contributes to identification of AP1 target genes.
We report novel and exciting finding that presence of the motif binding Satb1 prevents -in a motif number dependent manner -transcriptional activation in the stroke system. Satb1, which is the best characterized MAR-binding protein, has recently emerged as a key factor integrating higher-order chromatin architecture and gene regulation -reviewed in [36] . Depending on cell type and locus, its effect on chromatin looping may either activate transcription, as described for Th2 interleukin gene cluster [37] , or inhibit transcription, as for the MHC class 1 locus [38] and tentatively for our eigensystem M2. A hypothetical mechanism, in which genes in longer chromatin loops, or at the peaks of the loops, are more accessible to binding or activation by AP1, is depicted in Figure 6G . Proteolytic degradation of Satb1 occurs during early phases of apoptosis [39] [40] [41] . In the current work, the effect of SATB motif on expression was limited to the MCAO eigensystem 2 associated with the apoptosis.
Our analysis of cis-regulation of conserved eigensystem 3 -reflecting neuronal (synaptic) activity correctly predicted the known role of Creb/Atf/E4f1 and Egr as key regulators of neuronal activity regulated genes, important for neuronal plasticity and memory -for review, see [42, 43] . CREB motif binds transcription factors of the Creb family [43] [44] [45] , while E4FF motif binds transcription factors from the Atf family. EGRF binds transcription factors of the Egr family [46, 47] . PARF binds PAR/bZIP family of TFs (Dbp, Hlf, Tef, and Vbp1). The motifs binding Creb, Atf and Vbp1 are similar (Additional file 2) and these transcription factors have been shown to bind to overlapping sites [48] . A loss of the PAR/bZIP transcription factors results in seizures [49] . Using classical experimental methods, about a hundred Creb target genes have been identified, of which about half encodes neuronspecific proteins -reviewed by Lonze & Ginty [44] . A genome-wide chromatin immunoprecipitation study by Impey et al. identified Creb binding genes in the neuron-like differentiating rat pheochromocytoma PC12 cells [50] . When this set of genes was analyzed in our datasets, we found a clear association between Crebbinding to the gene and the positive loading of the third eigensystem ( Figure 7D) . Thus, the experimental data of Impey and co-authors support our in silico results, demonstrating an importance of the presence of CREB motif for gene up-regulation in the subspace of eigensystem reflecting neuronal activity.
Much experimental evidence supports an important role of Egr transcription factors in brain function. Transcription factors from the Egr family are induced in the rat hippocampus following kainate-induced seizures with kinetics closely resembling eigengene A3 (data not shown) and regulate expression of Arc [51] , a gene important for neuronal plasticity and memory formation [52] . Transcriptional activation of Egrs was also demonstrated following brain ischemia -reviewed in [47] . In addition to Creb and Egr, our BN analysis identified several novel tentative transcriptional regulators of the eigensystem reflecting synaptic activity ( Figure 5 and Additional file 2).
We demonstrate linear effects of the counts of the motifs SATB and CREB on log-expression in subspaces of the respective regulated eigensystems following the MCAO. These findings are in agreement with the predictions of the linear response model of gene regulation [21] . Moreover -because this model is valid only for TFs operating within the same cell -the observed agreement is revealing of the underlying biology ( Figure 7F) . First, it suggests that Satb and Creb operate within the same cells, namely neurons. This prediction is in agreement with our previous experimental data that majority of the cells undergoing apoptosis in the MCAO system are neurons [24] . Second, our results suggest that neuronal apoptosis is triggered by inflammation occurring in other cell types, namely microglia and astrocytes. This could explain why effects of inflammation and apoptosis are reflected by the same eigensystem, uncorrelated to the one reflecting effects of synaptic transmission.
The observed linear effect of CNSs' count per gene on log-transformed gene expression, depending on their content of CREB, is very interesting in the context of high specificity of this effect (data not shown) for the conserved eigensystem reflecting neuronal synaptic activity. Lee et al. [53] reported relatively greater cumulative length of CNSs in the upstream regions of genes involved in development, cell communication, neural functions and signaling processes, and suggested that this may reflect their greater regulatory complexity. We suggest, as another possibility, that neuronal genes need more CNSs (putative enhancers) to accommodate CREB motifs needed for responsiveness to rapidly changing synaptic activity.
Our results, demonstrating conservation of eigenarrays of temporal log-expression profiles, between hippocampus following seizures and cortex following the stroke, corroborate and extend recent findings of Oldham et al. [54] . These authors applied SVD to clusters ('modules' in their terminology) of expression profiles identified separately for several brain regions, and demonstrated conservation of 'module membership' between the corresponding clusters from different regions. As the 'module membership' is closely related to the first eigenarray of each cluster, their findings imply conservation of the first eigenarrays between the corresponding clusters. Our results demonstrate conservation of eigenarrays that occurs genome-wide for three eigensystems, two of which reflect distinct well-defined biological processes and are regulated via different sets of transcription factor binding sites.
Eigensystems conserved between stroke and seizures separate effects of different biological processes on gene expression, exerted via distinct sets of transcription factor binding motifs. Motif recognized by the nuclear matrix attachment region-binding protein Satb1 blocks AP1-driven transcriptional activation. The effects of motifs binding Creb and Satb1 on gene expression conform to the assumptions of the linear response model of gene regulation.
Gene expression profiling in the MCAO system Animals and surgical procedures
The experimental protocol was approved by the Local Animal Care and Use Committee and conforms to the national guidelines for the care and use of animals in research. 3-months old male Wistar rats weighing 270-320 g were used.
The MCAO (a middle cerebral artery occlusion) surgeries were performed under general halothane anaesthesia. Transient MCAO was induced with the intraluminal filament method (3-0 nylon monofilament suture) as described before [24] . A filament was withdrawn after 90 min. of MCAO to allow reperfusion, the incision was closed and anaesthesia discontinued. Sham-operated animals were subjected to the similar surgery with exception of MCA occlusion.
At various times after reperfusion, sham-operated and MCAO subjected rats were anesthetized with an overdose of pentobarbital and decapitated. Brains were rapidly removed, bisected at the midline and dorsolateral fragments of cerebral cortex containing MCA territory was dissected from the ipsilateral to occlusion (right) and contralateral (left) hemisphere. Total RNA was extracted from the samples using a phenol-guanidine thiocyanatebased method (TRI REAGENT, Sigma, Germany) and cleaned using RNeasy Total RNA kit (Qiagen, Germany) according to the manufacturer's recommendations followed by DNAse treatment. The amount and quality of the RNA was determined by spectrophotometry and capillary electrophoresis. The microarray hybridizations were conducted in the microarray facility of the Institute of Oncology, Maria Sklodowska-Curie Memorial Cancer Center, Gliwice Branch, Gliwice, Poland. Each time-point (6, 12, 24, 48 h ) and sham-operated (sh) group consisted three animals per group; RNAs from each individual were separately labelled and analyzed by microarray hybridization, for a total of 15 microarray hybridizations. The experiment was loaded to ArrayExpress (accession E-MEXP-2222).
The published dataset of Wilson et al. [23] from expression profiling in the hippocampus of adult rats with Affymetrix RG-U34A chip was downloaded from the NIH Neuroscience Microarray Consortium http://arrayconsortium.tgen.org/, projects: Koh-7K08NS002068-05-3, Koh-2K08NS002068-04. These datasets probed gene expression in the hippocampus of adult (P30) and young (P15) rats at 5 time-points (1, 6, 24, 72, 240 h) following the intraperitoneal injection of kainate (treatment) or saline (control). Only animals with nearly continuous seizures for more than half an hour were included in that study. Age-specific doses of kainate (3 mg/kg at P15, and 10 mg/kg at P30) were used that had been determined previously to result in < 25% mortality while inducing seizures in >60% of the animals. At the time of RNA isolation the animal could be seizing or during the latent period. Each condition was probed by three microarray hybridizations. The kainate data from both projects were pre-processed together, and the MAS5 detection calls for both ages were used together for the P/A/M filtering described below. Subsequently, the Mas5 signal data only from the adult rats (10 conditions) were used in the current work.
The CEL files from the MCAO experiment and separately the CEL files from the kainate experiment (from the young and adult rats together) were pre-processed with the MAS 5.0 algorithm as implemented in the affy R Bioconductor package (Irizarry et al. 2002) . Only the profiles of the probesets detected (MAS 5 call: Present or Marginal) in all hybridizations for at least one condition in a given experiment were used. The profiles from either experiment identified by probe set identifiers were mapped to the Ensembl 39 gene_stable_ids. Separately for either dataset, we computed a single average MAS5 signal profile for each gene_stable_id, resulting in gene expression matrices: (11012 × 15) for the MCAO system, and (3908 × 30) for the Adult rats from the kainate system. These data matrices were log2 transformed and analyzed separately by ANOVA. For further analysis from either dataset we selected the genes with the respective ANOVA p-value < 0.05. The average log2 expression profiles of these genes over the three biological replicates were computed, resulting in matrices: M (2786 × 5) for the MCAO system, and A (2392 × 10) for the kainate system.
The SVD analysis and the comparison of eigenarrays between two datasets were performed essentially as previously described [11] . Briefly, SVD was performed separately on matrices M, A, resulting in matrices u M (2786 × 5), m genes common between these two datasets. This resulted in matrices u MA and u AM . We calculated the Pearson correlation coefficient r between each pair of columns of u MA and u AM . The two-sided p-values corresponding to these correlations were obtained from the Student t distribution, with the t statistics calculated with the formula t = r[d /(1-r
2 )] 1/2 , where d is the number of the degrees of freedom.
GO terms associated with loadings of conserved eigensystems were identified, separately for either dataset, using RankGOstat [55] , available at http://gostat.wehi. edu.au/. The lists of gene symbols (Ensembl display_id), together with loadings of a particular eigensystem for a given (ANOVA-filtered) dataset were used as the input files. Default options (Wilcoxon Signed Rank test, Benjamini False Discovery Rate correction for multiple testing) were used, with the RGD database chosen as the source of GO annotations and the analysis was restricted to the "biological process" ontology. The result files were saved, parsed and converted to graphics using local scripts.
We used conserved non-coding sequences (CNSs) between human and rat as putative regulatory regions. For each human-rat orthologous gene pair (ortholog_one2one and apparent_ortholog_one2one) in Ensembl release 39, a flank of 20 kilobase (kb) of the genomic sequence from -10 kb to + 10 kb from the transcription start site were aligned using the AVID global alignment algorithm [25] . Sequence windows at least 100 base-pairs (bp) long with ≥ 75% identity were selected as putative regulatory regions. This resulted in the identification of 49425 CNSs for 9099 orthologous gene pairs in the human and rat genomes. A large proportion of similarly identified human-rodent CNSs was shown experimentally to function as enhancers [26] . The input genomic sequence and annotation data, and the results of this analysis were stored in a relational database named TRAM (Transcription regulatory Regions And Motifs), built on the open MySQL platform. The average length of the CNSs was 190 +/-SD 136 bp.
Instances of transcription factor binding motifs were predicted for all the vertebrate nucleotide distribution matrices of the Matrix Family Library version 6.2 using the program MatInspector [56] (Genomatix). Default thresholds, optimized for each motif as described in [57] were used. Search was performed for all CNSs in the TRAM database, separately for the human and the rat sequence, resulting in identification of 1679998 vertebrate motif instances in the human and 1601216 in the rat. The motif library contained 464 vertebrate nucleotide distribution matrices grouped into 151 matrix families [57] . Motifs identified with matrices from the same family were treated as the same nonredundant (n-r) motif identified by the family name. An instance of a n-r motif X in a given CNS is defined as conserved, if both the human and the rat sequence of this CNS contain at least one instance of X (not necessarily in the same AVID-aligned position). According to this definition, TRAM contains 1061884 instances of conserved n-r motifs. Only the conserved n-r motifs, referred to as "motifs" in the main text, were used in further analysis.
A composite motif X_Y_... is defined to have an instance in a CNS if this CNS contains at least one instance of each of the conserved n-r motifs X, Y, ... . Note that every single motif is also a composite motif.
In our model of transcription regulation the set of Bayesian network vertices is split into two subsets: cisregulatory features (composite motifs) and expression patterns (sign of the loading of a particular eigensystem). Furthermore, all the edges lead from cis-regulatory features to a particular expression pattern. In order to identify these relationships, we learn Bayesian networks from a dataset joining cis-regulatory and expression data for each gene. The input dataset joins presence or absence of every composite motif with the sign of loading of a single conserved eigensystem ( Figure 4C, D) .
In Step 1 of our procedure (not illustrated) over 100 promising composite motifs (built of up to three motifs) associated with the sign of the chosen eigensystem are identified. Only these selected composite motifs are then used as the input for the Step 2 ( Figure 4E ) identifying the best sets of composite motifs and their conditional probability distributions ( Figure 4F ). Each set of composite motifs has a q-value derived from 1000 random permutations of gene labels. For each permutation we created a new cis-regulatory dataset (with gene labels permuted accordingly) and learned the optimal composite motif set. Both steps of a learning procedure were performed with the BNFinder software [29] -a Python package for learning Bayesian networks from data. BNFinder implements the polynomial time learning algorithm dedicated to dynamic Bayesian networks, as well as to static ones with constraints forcing the network acyclicity [58] , as is the case here.
We used the Bayesian-Dirichlet equivalence (BDe) [59, 60] criterion with priors on the conditional probability distributions according to [59] . A prior on the network structures is proportional to the product of penalty parameters over all the edges in the graph of the refined model. Furthermore, penalty parameters increase with composite motif size. This choice results in a preference for sparse graphs, and thus protects our procedure from overfitting. BN score of a composite motif set was computed as the ratio of its posterior probability to the posterior probability of the empty set. To permit the cross-system validation of BN scores, the sets of composite motifs selected during Step 1 for the corresponding eigensystems (e.g. A2-M2) from either dataset were combined to form their union, which was then used during Step 2.
The motif count per gene was defined as the number of instances of conserved non-redundant motifs in the rat sequences of all the CNSs assigned to this gene. Only the genes with at least one CNS were used in the univariate regression analysis when the count of a particular motif was used as the regressor variable. When the CNS count, or CNS count and the motif count, were used as the regressor variable(s), the genes with zero CNS count were also included during the analysis
The single gene loadings of eigensystems 2, 3 were not normally distributed, which precludes statistical interpretation of the results of the regression with singlegene loadings used as the response variable. Therefore, in linear regression analysis, we decided to use the average loadings of a particular eigensystem in groups of genes with the same motif count as the response variable. In the regression analysis on the average values we confirmed the approximate normality of the residua (Additional file 3). Since the average values for different motif counts were computed from different numbers of observations, generally decreasing with the motif count, which was accompanied by changing variance of the loadings, we employed the Goldfeld-Quandt (GQ) test to detect the existence and magnitude of heteroskedasticity. Results of this test indicate that (i) for MCAO data heretoscedastic errors were detected (p < 0.05) in all the regression models. Therefore, we used weighted least squares approach, with weights set to the number of genes in each group [31] , which is the well-known solution to the heteroskedasticity problem. The regression analysis was performed in Mathematica 7, and the GQ tests were performed in R.
In the scientific literature, many approaches to fault diagnosis have been proposed since 1980. The FDI approach, which focuses on fault detection in dynamical systems, was summarized in (Blanke, Kinnaert, Lunze and Staroswiecki, 2006) . Related papers in this journal deal with the design of redundancy relations (Shumsky, 2007) as well as with the use of fuzzy logic (Dalton, Klotzek and Frank, 1999; Koscielny, Syfert and Bartys, 1999; Lopez-Toribio, Patton and Uppal, 1999) and neural networks (Korbicz, Patan and Obuchowicz, 1999; Witczak, 2006) . The DX approach focuses on diagnosis reasoning. It is summarized in (Hamscher, Console and De Kleer, 1992) . Recently, a bridge approach between FDI and DX was proposed (Cordier, Dague, Lévy, Dumas, Montmain, Staroswiecki and Travé-Massuyès, 2000; Nyberg and Krysander, 2003; Ploix, Touaf and Flaus, 2003) . Thus, tools for solving diagnosis problems are now well established. However, designing an efficient diagnosis system does not start after the system design but it has to be done during the system design. Indeed, the performance of a diagnostic system highly depends on the number and location of actuators and sensors. Therefore, designing a system that has to be diagnosed requires not only relevant fault diagnosis procedures, but also efficient sensor placement algorithms. Madron and Veverka (1992) proposed a sensor placement method which deals with a linear system. This method makes use of the Gauss-Jordan elimination to find a minimum set of variables to be measured. This ensures the observability of variables while simultaneously minimizing the cost of sensors. In this approach, the observable variables include the measurable variables plus the unmeasured but deductible variables. Another method of sensor placement was proposed in (Maquin, Luong and Ragot, 1997) . This method aims at guaranteeing the detectability and isolability of sensor failures. It is based on the concept of the redundancy degree in variables and on the structural analysis of the system model. The sensor placement problem can be solved by an analysis of a cycle matrix or by using the technique of mixed linear programming. Commault, Dion and Yacoub Agha (2006) proposed an alternative method of sensor placement where a new set of separators (irreducible input separators), which generates sets of system variables in which additional sensors must be implemented to solve the considered problem, is defined.
However, all these methods are not suitable for the design of systems that include a diagnosis system because, in this context, the goal of sensor placement should be to make it possible to monitor hazardous components. The sensor placement algorithm should compute solutions that satisfy detectability and diagnosability properties where detectability is the possibility of detecting a fault on a component and diagnosability is the possibility of isolating a fault on a component without ambiguities with any other faulty components. Few methods have focused on this problem.
Travé-Massuyès, Escobet and Milne (2001) proposed a method based on consecutive additions of sensors, which takes into account diagnosability criteria. The principle of this method is to analyze the physical model of a system from a structural point of view. This structural approach is based on Analytical Redundancy Relations (ARRs) (Blanke et al., 2006) . However, this method requires an a priori design of all the ARRs for a given set of sensors. Recently, Frisk and Krysander (2007) proposed an efficient method based on a Dulmage-Mendelsohn decomposition (Dulmage and Mendelsohn, 1959; Pothen and Chin-Ju, 1990 ). Nevertheless, this method only applies to just-determined sets of constraints while most practical systems are under-determined when sensors are not taken into account and over-determined afterwards.
This paper presents a new sensor placement algorithm that takes into account detectability and diagnosability specifications. It applies to systems for which only the structure is known. Thanks to this algorithm, sensor placements satisfying diagnosability objectives can be computed without designing all the ARRs, which is still an open problem. It applies to any system described structurally and does not assume just-determination. Section 2 details the main concepts that are useful to model systems for sensor placement. Then, Section 3 presents how the sensor placement problem is formulated. Section 4 introduces tools for analyzing structural matrices. These tools are then used in Section 5 to determine diagnosability properties directly from the analysis of structural matrices. Section 6 proposes basic algorithms for extracting blocks with useful properties from structural matrices, and Section 7 shows how to use these algorithms to compute sensor placements that satisfy diagnosability specifications. Finally, Section 8 presents an application to an electronic circuit.
Let us introduce the concepts and the formalism used in the paper to formalize the sensor placement problem. Behavioural knowledge starts with phenomena. A phenomenon is a potentially observable element of information about the actual state of a system. It is modelled by an implicitly time-varying variable, which has to be distinguished from a parameter that is model-dependent. Generally speaking, even if a phenomenon is observable, it is not possible to merge it with data because in fault diagnosis data are only known provided that some actuators or sensors behave properly. Phenomena V (t) = {. . . , v i (t), . . . } are linked to a phenomenological space F(T, V ) = {V (t); t ∈ T }, where T stands for a continuous or discrete time set. At any given time t in T , these phenomena belong to a domain dom(t, V ) = dom(V (t)) representing all the possible values that the phenomena may have. Consequently, when considering all t ∈ T , {dom(V (t)); t ∈ T } represents a tube in the timed phenomenological space F(T, V ).
All the phenomena have thus to be considered as unknown because observable phenomena are not observations. Let us introduce the concept of a data flow to model actual data recorded on a system. A data flow models data provided by a source of information concerning a phenomenon. A data flow concerning a phenomenon v is denoted by val (t, v) with val(t, v) ∈ dom(t, v) . It corresponds to a trajectory belonging to the tube {dom (t, v) ; t ∈ T } (see Fig. 1 ). When information about v is coming from different sources, the different data flows can be denoted by val i (t, v) . Formally, a data flow provided by a component c can be linked to a phenomenon: ok(c) → ∀t ∈ T, val(t, v) = v, which means that if the component named c is in the mode ok, then the data val (t, v) correspond to the actual value of the phenomenon v at any time t ∈ T . In fault diagnosis, a system is not supposed to remain in a given mode. Indeed, diagnostic analysis aims at retrieving the actual behavioral modes of the components of a system. At minimum, two modes are defined: the ok mode, which corresponds to the expected normal behavior, and the cf mode, which is the complementary fault mode: it refers to all the behaviours that do not fit to the expected normal behavior. Sometimes, specific fault modes may be modelled (de Kleer and Williams, 1992; Struss, 1992) . They are denoted by a specific label, e.g., the leak mode. Consider, e.g., a pipe where ok and leak are modelled. It yields M odes(pipe) = {ok, leak, cf }, where cf (pipe) refers to the behaviours that do not correspond to ok(pipe) or to leak(pipe).
Except for the complementary fault mode, behavioural modes are modelled by cause-effect relationships between phenomena, which are represented by constraints. Each constraint refers to a set of mappings containing unknown variables and known data flows. Generally speaking, a mapping over dom(t, V ) is defined from one subspace dom(t, V 1 ) to another dom(t, V 2 ), where
is a mapping representing a constraint k that models, for example, a component c 1 in mode mode 1 and a component c 2 in mode mode 2 , we have
where the data flow val(V 3 ) is considered as being included in the mapping. But constraint is not strictly equivalent to mapping. A constraint corresponds to a set of equivalent mappings. Firstly, although mappings to multidimensional spaces could be used, they are difficult to manage. It is better to break them down into one-dimensional mappings. In the following, one-dimensional mappings modelling a constraint k are named realizations of k. Moreover, several realizations of a constraint may be equivalent. Let κ i be a realization from V \{v} to {v}. There may be equivalent realizations defined on V that also model the constraint. Therefore, the notion of constraint can be extended to represent all the equivalent realizations representing a given subset of dom(V ). In the following, a constraint k will be understood as a set of equivalent realizations. It is summarized by the set of variables occurring in the realizations: var(k). It is assumed that if k is a constraint, for all v ∈ var(k), there is an equivalent realization κ i : dom (t, var(k) 
To summarize, a system Σ is composed of a set of constraints K Σ and a set of behavioural modes M odes(Σ) related to components in Σ. var(K Σ ) is the set of variables, named port in (Chittaro and Ranon, 2004) , which models observable phenomena involved in Σ. Indeed, by extension, the set of variables appearing in a set of constraints K is denoted by var(K) = k∈K var(k). Each constraint κ ∈ K Σ is linked to a mode m ∈ M odes(Σ) by a first order relationship: m → κ. For the sake of simplicity, in this paper, it is assumed that:
• only ok modes are considered in the sensor placement,
• each constraint κ ∈ K Σ models one mode and, conversely, that a mode can be modelled by at most one constraint.
The sensor placement problem then consists in defining the variables of var(Σ) that have to be measured to facilitate the detection and identification of ok modes from M odes (Σ) . These modes are denoted by M odes ok (Σ). From a mathematical point a view, it is a kind of combinatorial problem. The next section proposes a precise problem formulation.
Let us present an intuitive formulation of the problem. Full definitions are given afterwards. The solving of a diagnostic problem is generally decomposed into two consecutive steps. The conflict or symptom generation, also called fault detection in the automatic control community, and the diagnostic analysis, also called fault isolation. The first step relies on consistency tests among minimal testable subsets of constraints 1 K ∈ K Σ that include data flows (often called OBS for observations). Let K be the set of minimal testable subsets of constraints. If K ∈ K is a set of constraints leading to a test which is inconsistent, this means that, at least, one of the modes corresponding to the constraints of K is not actual. It is therefore important to trace the constraints belonging to a minimal testable subset K because this makes it possible to solve the second sub-problem: the diagnostic analysis, which provides global conclusions in terms of modes about the actual system states. The performance of a diagnostic system is highly dependent on the set K and, consequently, dependent on the set K Σ , which highly depends on the dataflows, i.e., on the observations. Additional sensors lead to addtional constraints in K Σ and, therefore, to new sets in K. K can be obtained from combinations of constraints from K Σ using possible conflict generation (Pulido and Alonso, 2002) , a bipartite graph (Blanke et al., 2006) , the Dulmage-Mendelsohn decomposition (Krysander, Aslund and Nyberg, 2008) or elimination rules (Ploix, Désinde and Touaf, 2005) . Basically, once K has been generated, it is possible to compute the performance of the diagnostic system in terms of detectability, discriminability or discernability, and diagnosability. Irrespective of whether or not the performance satisfies the requested performance requirements, the set K Σ is modified and the process is conducted once again until the requested performance is reached. However, this process requires lots of computations because the generation of K is time consuming. Moreover, up to now, no one of these algorithms has been proved to be complete.
Another approach to sensor placement is proposed in this paper. It does not require the computation of K from K Σ . It directly solves the following problem by studying the structure of Σ: Let K Σ be a set of constraints modeling the ok modes of a system Σ. Let var(K Σ ) be the variables appearing in K Σ . The problem to be solved is as follows: What are the complementary constraints modelling sensors dedicated to variables from var(K Σ ) that have to be added to satisfy requested diagnosability performance requirements?
Let us precise the problem formulation by defining the concept of a testable subset or a subsystem (TSS) of constraints and its relationship with the concept of the ARR. Definition 1. Let K be a set of constraints and v a variable in var(K) characterized by its domain dom (v) . K is a solving constraint set for v if, using K, it is possible to instantiate v with a value set S such that S ⊂ dom (v) . A solving constraint set for v is minimal if there is no subset of K, which is also a solving constraint set for v. A minimal solving constraint set K for v is denoted by K v.
Definition 2. Let K be a set of constraints. K is testable if and only if there is a partition
If this property is satisfied, it is indeed possible to check if the value set S 1 deduced from K 1 is consistent with the value set S 2 deduced from K 2 :
Adding any constraint to a testable set also leads to a testable set of constraints. Only minimal testable sets are interesting.
Definition 3. A testable set of constraints is minimal if it is not possible to keep testability when removing a constraint.
A global testable constraint that can be deduced from a TSS is called an analytical relation (ARR). Let
. .} be the set of all the testable subsystems that can be deduced from K Σ according to (Blanke et al., 2006; Ploix et al., 2005) . Because of the assumed one-to-one relationships between constraints and components, the notions of detectability and discriminability can be extended to constraints.
Let K be a set of TSSs coming from (Struss, Rehfus, Brignolo, Cascio, Console, Dague, Dubois, Dressler and Millet, 2002) (Struss et al., 2002) 
Obviously, nondetectability implies nondiscriminability. (Struss et al., 2002; Console, Picardi and Ribando, 2000) 
In order to formulate the sensor placement problem, the notion of a terminal constraint has to be introduced.
Definition 7. A terminal constraint k is a constraint that satisfies card(var(k)) = 1, where var(k) is the set of variables appearing in the constraint k.
A terminal constraint usually models a sensor or an actuator. It is thus a major concept in sensor placement. Note that if a candidate sensor measures not only one variable v but a combination of several variables
where v * is a virtual measurable variable, has to be added into K Σ . Then, the solving is similar to the standard problem.
In fault diagnosis, sensor placement has to satisfy specifications dealing with detectability and diagnosability. Because a one-to-one relation between components and constraints is assumed, what is true for components is also true for constraints. In the following, only constraints will be considered: the analogy with components is implicit. In this paper, complete specifications are considered. Partial specifications can also be managed: they will be presented in a forthcoming paper. These complete specifications consist of a partition of the constraint set K Σ into the following subsets:
• the set of constraints K diag that must be diagnosable,
• the set of subsets of constraints K nondis = {. . . , K i , . . .} for which each set K i must be nondiscriminable but detectable,
• the set of constraints K nondet that must be nondetectable,
Complete specifications K diag , K nondis and K nondet for sensor placement problems are meaningful if the following two properties are satisfied:
1. Sets in specifications must not overlap one another to make sense. Constraint sets have to satisfy
2. The union of all the components appearing in K diag , K nondis and K nondet has to correspond to K Σ :
If these properties are satisfied, the complete specifications are qualified as consistent in K Σ . Satisfying the diagnosability specifications requires information delivered by sensors. Let K Σ represent the system Σ with additional sensors where K Σ contains the constraints K Σ of the system Σ plus the additional terminal constraints modelling the additional sensors. Therefore, solving a sensor placement problem consists in determining additional terminal constraints in K Σ that lead to the satisfaction of complete specifications.
In the next sections, diagnosability properties of structural matrices are established and used for the design of a sensor placement satisfying diagnosability specifications.
Before pointing out diagnosability properties, some basic properties of structural matrices have to be established.
. .} can be represented by a structural matrix M Σ , which is an incidence matrix representing the mapping
According to the definition, a TSS is a minimum set of constraints K such that there is at least one variable for which two different minimal solving sets can be found. A minimal solving set leading to a variable v corresponds to a value propagation (Apt, 2003) starting usually, but not necessarily, by terminal constraints and leading to v. Therefore, a TSS can also be seen as two distinct value propagations leading to a given variable. This point of view has been adopted as a theoretical tool to develop proofs.
Let k 1 and k 2 be two constraints. The propagation of a variable v between k 1 and k 2 is possible only if v ∈ var(k 1 ) ∩ var(k 2 ). The variable v is qualified as propagable between k 1 and k 2 : v is a link between k 1 and k 2 . In the corresponding structural matrix, this link is represented by a thick line:
Consider now a system defined by
Terminal constraints k 4 and k 5 model sensors or actuators. Each terminal constraint contains known data. Figure 2 represents examples of propagations that lead to a TSS with a bipartite graph. But in a bipartite graph, links do not appear clearly: they correspond to alternate paths (or chains) with the pattern 'constraintvariable-constraint'. Links appear more clearly in structural matrices as lines linking two constraints. In the fol- lowing structural matrices, the variables surrounded by a circle represent the variables that can be instantiated twice. The relevance of links remains obvious in Example 2, where a propagation does not start by a terminal constraint. The paths corresponding to propagations of solving sets were drawn. Variable v 2 was instantiated twice.
Once again, paths may be reduced to links (thick lines).
The following example points out another structural matrix with two propagations leading to variable v 3 :
The concept of linked constraints has to be formalized because discriminability depends on this concept. Before defining linked constraints, the concept of interconnected constraints has to be introduced. The constraints of a system Σ may be modelled by a non-directed bipartite graph (K Σ , var(K Σ ), E Σ ), where E Σ is the set of edges. Each edge e = (k, v) reflects v ∈ var(k).
with constraints at extremities (see, e.g., (Bollobás, 1998) 
To point out the link with bipartite graph theory, if K is interconnected by V in K Σ , V is necessarily a complete coupling for K with respect to variables. The notion of a linked set of constraints can now be introduced. 
The shape of a structural matrix dealing with linked constraints is drawn in Fig. 3 . The concept of linked constraints is strongly connected with discriminability.
Proof. Indeed, because variables in V only appear in the constraints belonging to K, the only way for propagating variables is to use the constraints in K and the variables in V . What is more, because there is a tree (K, V, E) ⊂ (K Σ , var(K Σ ), E Σ ) with constraints at extremities, instantiating all the variables in V involves at least the achievement of the propagations defined by the tree.
Therefore, all the constraints are invariably found together in the TSS. In order to improve the clarity of these explanations, let us introduce the notion of stump variables.
Definition 10. A set of variables var(K) appearing in a set of constraints K but not in the other constraints of K Σ (i.e., K Σ \K) are named stump variables in K Σ with respect to K. They are denoted by var stump (K, K Σ ).
For instance, the set of variables V that link a set of constraints K belong to the stump variables
A set of constraints cannot be used to generate a TSS if they are linked and if there are additional variables that cannot be propagated. These constraints are qualified as isolated. Detectability depends on this concept.
it is linked by V and if there is at least one variable in var(K)\V that does not belong to other constraints of K Σ (i.e., K Σ \K). If the set contains only one constraint, the link condition disappears.
The shape of a structural matrix dealing with isolated constraints is shown in Fig. 4 . The concept of isolated constraints is strongly linked with detectability.
Proof. The constraints K isolated in K Σ by V will always come together in the TSS because, by definition, they are linked by V . Because of the fact that, in isolated constraints, there is at least one additional variable in var(K) which does not appear in other constraints (i.e., K Σ \K), it is not possible to instantiate this variable and, therefore, this set of constraints cannot be involved into a TSS: constraints K are thus non-detectable.
This section aims at setting up a direct link from sets of constraints to detectability and diagnosability properties.
Firstly, it is obvious that adding additional constraints connected to all the variables var(k) appearing in a constraint k ensures the diagnosability of k. Lemma 3 can be directly applied to all the constraints of a constraint set.
In Lemma 2, a relationship between isolated constraints and the detectability property has been presented. The next lemma generalizes the previous results.
Lemma 4. A sufficient condition for a subset of constraints K ⊂ K Σ to be non-detectable is that there is a sequence
Proof. The case of K 1 has been discussed in Lemma 2: because the constraints in K 1 are isolated in K Σ , they are non-detectable and therefore cannot be included in the TSS. Then, the remaining candidate constraints for the TSS belong to K Σ \K 1 . Because K 2 is isolated in K Σ \K 1 , they are non-detectable. The reasoning can be extended to any K i . Consequently, the constraints in K = i K i are non-detectable. Figure 5 indicates the shape of a structural matrix of non-detectable constraints.
Consider, e.g., a system modelled by the following structural matrix:
Assume that the set K = {k 1 , k 2 , k 3 } is required to be non-detectable. In this example, there exists a pair ({k 1 } , {k 2 , k 3 }) such that each element K i satisfies Lemma 4. If there are no additional terminal constraints containing v 1 , v 2 and v 3 , the subset K is necessarily nondetectable.
Proof. This lemma is a direct application of Lemma 1 to several sets of constraints.
Consider, for example, a system modelled by the following structural matrix:
} is a constraint subset that should be non-discriminable. Because the constraints k 1 , k 2 , k 3 and k 4 are linked by V = {v 1 , v 2 , v 3 }, Lemma 5 is satisfied. Therefore, k 1 , k 2 , k 3 and k 4 are nondiscriminable provided that no additional terminal constraints contain a variable of V .
The following theorem collects the results of Lemmas 3, 4 and 5. 
K i belonging to K nondis = {K 1 , . . . , K m } such that ∀K i = K j , K i ∩ K j = ∅,
Proof. The proof relies on the resulting structure of the structural matrix, which directly stems from Corollary 5 as well as Lemmas 4 and 5. Note that Point 2 could also be stated for the whole set of constraints K Σ . However, it is not useful to include non-detectable constraints, which will not appear in the resulting TSS: it would be less conservative. Because of Lemmas 4 and 5, the variables of var(K diag ) cannot contain variables appearing in the variables involved in (1) and (2), that is to say, in var stump (K nondet , K Σ ) and in
Because the variables of V candidate can be instantiated with measured values, all the constraints of K diag are diagnosable following Corollary 5.
The point which has to be proved is that, in specifications, K nondis defines non-discriminable but detectable sets and not only non-discriminable sets as in Lemma 5: the detectability of sets in K nondis has to be proved. The variables var(K i ) of a constraint set K i ∈ K nondis can be decomposed into two sets: V constraint set K i is necessarily detectable. Because this result holds for any K i ∈ K nondis , it proves the theorem.
A block containing the set of isolated constraints subsets of K Δ in K Σ and the isolating variables, considering only the varcostiables V Δ Require:
Satisfying the assumptions of Theorem 1 guarantees that the specifications are satisfied. However, because the theorem provides only a sufficient condition for diagnosability, the number of additional terminal constraints is not necessarily minimal. It has to be checked afterwards.
In the next section, an algorithm for extracting blocks from a structural matrix is presented. This algorithm is required by methods for sensor placement based on complete specifications.
Before presenting an algorithm for extracting blocks from a structural matrix K Σ , let us introduce some notation. Firstly, the notion of a block is formalized: a block is a couple defined by block = (K, V ) where block.cons = K and block.var = V stand respectively for a set of con- 
A set of blocks is denoted by the symbol B. By extension, the block resulting from the merging of sets of blocks B is denoted by merge(B). Figure 7 represents the dependency scheme between the methods that are defined. The main algorithm is named findBlocks (Algorithm 1). It extracts the different blocks that appear in Theorem 1, considering only the variables V Δ .
In order to describe the methods findIsolatedBlocks() and findLinkedBlocks(), the notions of Knode and buffer of Knodes are introduced. A Knode is a couple of constraint sets:
A buffer is a special First-In First-Out buffer. The basic functionalities are buffer .push(Knode) and buffer .pop(). They respectively correspond to adding a Knode in the buffer and getting a Knode from the buffer.
Using these notions, the algorithm findIsolatedBlocks() (Algorithm 2) extracts the set of isolated blocks from a set of constraints K Δ ⊆ K Σ , considering only the variables V Δ . According to Lemma 4, the constraints belonging to the resulting blocks are not detectable.
This algorithm depends on the findIsolatingVariables() method. It is given by Algorithm 3.
The algorithm findLinkedBlocks() (Algorithm 4) extracts the set of linked constraints from a set K Δ ⊆ K Σ , considering only the variables V Δ . The structure of this algorithm is very closed to that of Algorithm 2. Accord-
A set of blocks, where each one corresponds to a linked but not isolated set of constraints, and its corresponding linking variables, considering only the variables V Δ Require:
ing to Lemma 5, the constraints belonging to the resulting blocks are not discriminable. This algorithm depends on the findLinkingStumpVariables() method, which is given by Algorithm 5.
Finally, according the Fig. 7 , the algorithms findIsolatingVariables() and findLinkedBlocks() depend on two methods findStumpVariables() (Algorithm 6) and isInter-
The top-level method findBlocks(K Σ ) leads to the blocks depicted in Fig. 6 . These results are very useful to support the sensor placement. Indeed, constraints belonging to B diag .cons are already diagnosable. Therefore, finding a sensor placement satisfying the specifications requires that the specified In much the same way, the constraints merge(B nondis ).cons ∪ B diag .cons are already detectable. Therefore, finding a sensor placement satisfying the specifications requires that the specified K spec nondet should be included in merge(B nondet ).cons:
A method for optimal sensor placements satisfying diagnosability specifications is proposed in this section. This method deals with complete specifications: K spec diag , K spec nondis and K spec nondet (see Section 4). There may be several sensor placements that satisfy diagnosability specifications. In order to select the most interesting one, a criterion based on the cost of the sensor placement is considered. Introduce the following notation: The cost of the measurement of a variable v is denoted cost (v) . By extension, the cost of the measurement of a set of variables V is defined as cost(V ) = v∈V cost (v) .
Adding sensors amounts to adding terminal constraints (see Definition 7). Indeed, as mentioned in Section 3, a sensor measuring a variable v is modelled by the constraint val (t, v) = v, where val(t, v) is a datum coming from the sensor. Therefore, structurally speaking, a sensor measuring v is modelled by a terminal constraint k satisfying var(k) = {v}. The constraint k will be denoted by k sensor (v) . By extension, the terminal constraints modelling sensors measuring variables V are denoted by K sensor (V ). 
The method to solve these complete specifications can be decomposed into two steps: the determination of candidate variables for sensor placements using Theorem 1, and the reduction of the candidate variables in order to find the minimal cost sensor placement that satisfies the complete diagnosability specifications using a branchand-bound algorithm. Figure 8 presents the dependency scheme of the method.
The findCandidates() (Algorithm 8) method is based on Theorem 1. It takes into account the specifications to determine a set of variables to be measured. If these variables are measured, the complete specifications will be satisfied. This algorithm depends on the findLinkingVariables() method, which is given by Algorithm 9. This algorithm uses the results issuing from Algorithm 5 to find a subset of variables linking a subset of constraints K Δ , considering only the variables V Δ .
In this algorithm, the cost of variables is considered. This algorithm depends on the sortVariables() method, which sorts a list of variables according to measurement costs in descending order.
A subset of the candidate variables may also lead to the satisfaction of the specifications. A branch-and-bound algorithm is used to select the most interesting candidate variables to be measured in order to find an optimal sensor placement. Before defining the optimisation algorithm, it is necessary to be able to check if the complete specifications are satisfied for a given subset of candidate variables.
True is the sensor placement satisfies the specifications Require: The optimality criterion for a feasible sensor placement defined by V measured is given by cost(V measured ). The branch-and-bound search algorithm is implemented in the placeSensor() method (Algorithm 11) using a simple First-In First-Out buffer of nodes of variables.
In this section, the special case of a dynamical system modelled by recurrent or differential equations is discussed. Then, an example is presented. 
Dynamical systems. The sensor placement method relies on structural modelling. Therefore it should be suitable for most systems. Let us examine the special case of dynamical systems. Generally speaking, a model is said to be dynamic if either:
• a variable appears several times in a system but at different time, stamps, or
• a variable and some of its derivatives or summations (whatever the order is) appear in the system.
The first case mainly concerns time-delays and discrete time recurrent systems. According to Section 3, each variable stands for a tube in a phenomenological space. Therefore, a time delay, modelled by y(t + Δ) = x(t), is a constraint that establishes a link between two tubes: {dom(y(t + Δ)); ∀t} and {dom(x(t)); ∀t}. Therefore, even if the two variables model the same phenomenon, in the structural model they cannot be merged. Consider now the following discrete-time recurrent model:
k ∈ N, where T e stands for the sampling period.

The phenomenon modelled by x appears twice. Therefore, the constraint must be implicitly completed by a time delay between variables x((k + 1)T e ) and x(kT e ). Structurally speaking, these constraints are modelled by the following structures:
Moreover, if the tube corresponding to x((k + 1)T e ) appears only once in these constraints (which is usually the case in practice), constraints k 1 and k 2 can be merged:
The second case mainly concerns integration and differential equations. Consider, e.g., the following model: and var(k 2 ) = {x, dx dt, x 0 }. In the same way as timedelays, the constraints k 1 and k 2 can be merged to obtain the following structure: var(k 12 ) = {u, x} or, if the initial condition is considered, var(k 12 ) = {u, x, x 0 }. This result remains true for summations and derivatives of any order.
Consequently, these kinds of dynamical systems can be handled just like other systems.
8.2. Example. The method presented in this paper has been applied to a sensor placement for an electronic circuit (Fig. 9) . It is modelled by the following constraints:
with
The corresponding structural matrix is given by Table 1. Suppose that the costs of the measurements are
Consider the following complete specifications:
In order to check if the specifications K nondet are satisfiable, Algorithm 2 is used with K Δ = {k 1 , k 4 , k 10 }, K Σ and V Δ = var(K Σ ). Algorithm 2 computes the following sets of isolated constraints: {{k 10 , k 1 } , {k 4 }}. The specifications K nondet are consequently satisfiable. Algorithm 2 also provides the isolated variables
In order to check if the specifications K nondis are satisfiable, Algorithm 9 is used with two subsets,
. Algorithm 9 computes the linking variable subsets V 1 = {i 1 } and V 2 = {v 3 }.
In order to find the candidate variables to be measured to satisfy the specifications, Algorithm 8 is used. It yields terminal constraints that correspond to the measurements of variables
In order to find the cheapest sensor placement that satisfies the specifications, Algorithm 11 is used. It yields
In order to validate the result, the method proposed in (Ploix et al., 2005) has been used to design all the ARRs. It has led to the fault signature given by Table 2 . According to these results, the constraint sets that cannot be discriminated are {k 2 , k 6 } and {k 7 , k 8 }. The constraint set that cannot be detected is {k 1 , k 4 , k 10 } and the diagnosable constraints are {k 3 , k 5 , k 9 , k 11 , k 12 }. Applying the function Φ : K Σ −→ C Σ , it is obvious that the components that cannot be discriminated are {c 2 , c 6 } and {c 7 , c 8 }, the components that cannot be detected are {c 1 , c 4 , c 10 }, and the diagnosable components are {c 3 , c 5 , c 9 , c 11 , c 12 }.
Suppose now that the specifications are given by K nondis = {{k 2 , k 3 } , {k 7 , k 8 }} ,
In order to check if the specifications K nondet are satisfiable, Algorithm 2 is used with K Δ = {k 1 , k 10 }, K Σ and V Δ = var(K Σ ). Algorithm 2 computes the following sets of isolated constraints: {{k 10 , k 1 }}. The specifications K nondet are consequently satisfiable. Algorithm 2 also provides the isolating variables V isolated = {i 4 , v 2 }.
In order to check if the specifications K nondis are satisfiable, Algorithm 9 is used with the two subsets K Δ1 = {k 2 , k 3 } and K Δ2 = {k 7 , k 8 }, considering V Δ = var(K Σ \ V isolated ). Because Algorithm 9 computes the linking variable subset V 1 = {∅} for the constraint subset K Δ1 = {k 2 , k 3 }, there is no solution that satisfies these specifications.
The results presented in this paper demonstrate that it is possible to design optimal sensor placements satisfying diagnosability criteria without designing the ARR a priori.
A new approach to sensor placement has been proposed that makes it possible to satisfy diagnosability specifications. It is thus possible to specify the performances that a diagnostic system has to meet and then to compute where the sensors should be placed.
The presented lemmas, theorems and algorithms are general and can be reused to develop other methods for sensor placement that deal with various kinds of specifications, e.g., a set of components that have to be at least detectable and another one of those that have to be diagnosable. The provided tools apply to any system including dynamical systems described by recurrent or differential equations because they are based on a structural approach: only the variables appearing in constraints are considered. However, the generality of the structural approach is paid by possible over-estimation depending on the nature of constraints: it is well known that it relies on the conditioning of constraints. But solutions taking into account the nature of constraints can only be specific.
An algorithm for sensor placement managing complete specifications has been presented. It deals with elements that have to be diagnosable, discriminable and nondetectable. Thanks to the proposed algorithm, cost optimal sensor placement satisfying complete diagnosability specifications is possible without designing the ARR a priori. This is a very important feature since it is no longer necessary to design all the possible ARRs assuming all the variables are measured.
This approach manages only specifications dealing with models of the normal behaviour. It does not take into account specific fault models such as a leak in a pipe. Therefore, if such models are considered, the sensor placement algorithm will lead to an over-estimation of the required sensors. Taking into account specific fault models may lead to a reduction of the required sensors. Nevertheless, fault models cannot be easily taken into account in sensor placement methods. It is still an open problem.
Wireless sensor networks (WSNs) have attracted a lot of attention. Recently, research efforts have been dedicated to power management [9] , routing [6] , deployment and coverage [2] , and localization [1] . Nowadays, many WSN systems have adopted ZigBee [10] as their communication protocol. ZigBee adopts the physical (PHY) and the medium access control (MAC) layers defined by IEEE 802.15.4 [5] and extends to network, application, and security services. ZigBee supports three network topologies, star, tree, and mesh. Regardless of network topology, there is a coordinator responsible for initializing, maintaining, and controlling the network. Star networks can only cover small areas, but tree and mesh networks can cover larger fields by allowing multi-hop communications. The backbone of a tree/mesh network is formed by one coordinator and multiple routers. An end device must associate with the coordinator or a router. In a tree network, routing can be done in a stateless manner based on nodes' 16-bit short addresses.
To form a ZigBee network, addressing is the first work to be done. ZigBee suggests a distributed address assignment mechanism (DAAM), which enforces some addressing rules. By this mechanism, the coordinator needs to decide three parameters: the maximum number of children of a router (C max ), the maximum number of child routers of a router (R max ), and the maximum network depth (L max ). While simple, the scheme may prohibit a node from accepting isolated child routers/devices. A node is an orphan node when it cannot associate with any parent router but there is still unused address space in the network. Some heuristics algorithms are proposed to reduce orphans [7] .
Reference [3] proposes an address borrowing scheme. When a new node sends an association request to a router of a ZigBee network and this router does not have any free address, it will ask other neighboring routers to lend an unassigned address to serve the new node. Park et al. [8] propose a distributed borrowing addressing scheme (DBAS) by allowing a parent node to borrow a maximum unused address space to alleviate the orphan problem.
The main goal of our work is to alleviate the orphan problem by a distributed address assignment scheme with address borrowing. We improve the borrowing scheme of [8] by allowing a parent router to borrow a flexible subtree of address space from a neighbor. We also define the detailed procedure for our address-borrowing scheme, which is compatible with the ZigBee standard. Our solution solves addressing and routing issues altogether. When a new node tries to associate with a parent router with no unassigned address, the new node will estimate the number of orphan nodes in its neighborhood and this parent router will ask its 2-hop neighbors to lend a suitable address space to serve the new node. Then this new node can use the ZigBee DAAM to serve its children. We also show that routing and maintaining the borrow and lend lists can be easily done. In ZigBee, address assignment is done in a distributed manner. To form a network, the coordinator determines C max , R max , and L max first. Note that the children of a router include both routers and end devices. So C max ≥ R max and up to C max − R max children must be end devices. Addresses are assigned in a top-down manner. The coordinator takes 0 as its address and divides the remaining address space into R max + 1 blocks. The first R max blocks are to be assigned to its child routers and the last block has C max −R max addresses, each to be assigned to one child end device. The similar process is adopted by each child router to partition its given address space in a recursive manner. From C max , R max , and
which is the size of one address block to be assigned to a child router [10] :
The value of d is 0 for the coordinator and is increased by one as we go down the tree.
A. Basic Idea Fig. 1 (a) shows a ZigBee address assignment example. Router y cannot accept router x as its child because it already has the maximum R max = 3 child routers, making x an orphan node. The orphan problem [7] refers to the scenario where some nodes in Fig. 1 (a) cannot connect to the network even though there are free addresses in the network.
We try to alleviate the orphan problem by allowing addressborrowing. We define a new Borrowing Information Base (BIB) attribute called MaxBorrowingNumber (B max ), which can be carried by any reserved field. B max is the maximum number of addresses that a router can borrow. A router can serve up to R max + B max child routers and up to C max − R max + B max child end devices. Here, these B max children can be routers or end devices. Fig. 1(b) shows an example where y has borrowed the address block rooted at 36 of depth 2 from router z. It also shows that the address block is assigned to router x. Then router x can assign them to its children.
Below, we propose a fully automated, distributed ZigBee address assignment scheme to facilitate such borrowing behaviors. Then we further show how to conduct address-based routing in the network.
During the formation process, each node x maintains the following variables:
• A x : the address of x in a ZigBee network.
• R(x) and E(x): the child routers and the child end devices currently associating with x, respectively.
• L r (x) and L e (x): the lend lists of child routers' addresses and child end devices' addresses, respectively, which x has lent out.
• B r (x) and B e (x): the borrow lists of child router's addresses and child end device's addresses, respectively, which has borrowed.
• p(x): the address of x's parent in a ZigBee network.
• o(x): the estimated number of orphans in x's neighborhood.
• d x : the depth of x in the ZigBee network. Note that there are two interpretations for the value of d x . Under the normal situation, d x is the actual depth value of x. However, d x is the "depth" of the address counting from its original location (not x's location). For example, the d z of z in Fig. 2 is 1 since originally address 107 is the child of address 0 (the coordinator t) even though its current physical depth in the ZigBee tree is 4. Similarly, the d y of y in Fig. 2 is 2 since it is the child of z, and its physical depth in the ZigBee tree is 5.
• s x : the state of A x 's location. We allow an address block to lent out if it is not a borrowed address black. Note that a child address block of a "borrowed" address block is also considered a "borrowed" address block. That is, a borrowed address block connot be further lent out to other nodes, and so is its sub-blocks. We set s x = "original" if A x is not a borrowed address; otherwise, we set s x = ICC'14 -W7: Workshop on M2M Communications for Next Generation IoT "borrowed". For example, in Fig. 2 , s z = "borrowed" since A z = 107 is borrowed from the coordinator. Since A y is a child of A z , s y = "borrowed", too. On the other hand, A w is not a borrowed address, so s w = "original". 2) Associating Scheme: To start a new network, a coordinator t first sets the following parameters: C max , R max , L max , and B max . Then t assigns A t = 0, d t = 0, and s t = "original", and sets R(t), E(t), B r (t), B e (t), L r (t), and L e (t) as empty sets. The detailed associating scheme is presented below.
(a) Router v receiving a MLME-ASSOCIATION-REQ request from a node u:
is free, and assigns the address A u to u by replying a MLME-ASSOCIATION response with A v , d v and s v to u. 
(ii) Otherwise, x ignores the request. (g) Router x receiving an ASK-ADD-Confirm with the lending address A y from router v: Router x updates L r (x) if this address would be assigned to a child router; otherwise, it updates L e (x). In the example of Fig. 1(b) , z lends address 36 to y, so its lend list L r (z) = {(36, y = 54)}. On the other hand, y should update its borrow list B r (y) = {(36, z = 1, 1)}. Fig. 3 shows the flow chart for an orphan node x to request a router node v to lend an address block to serve it. Next, we analyze the depth of the ZigBee tree after borrowing. Since we do not allow recursive borrowing and a node can only inquire its 2-hop neighbors for borrowing, the maximum depth of the ZigBee tree formed by ABS is L max + 2.
3) Disassociation Scheme: When a node wants to disassociate with its parent router, it should broadcast a MLME-DISASSOCIATION request to its parent and descendants to ensure that its address block can be re-used and its descendants can try to reassociate with other parents. The detailed disassociating scheme is presented below.
• Node v receiving a MLME-DISASSOCIATION request from node u:
, and B e (v). If this address is a borrowed one, then v sends a Return-ADD with the address A u to its original lender. This address A u then can be reused by other nodes. (b) If u is v's parent, then v sends a MLME-DISASSOCIATION request to its children. Since v is not an orphan, it should try to reassociate with the ZigBee network. (c) Otherwise, v ignores the request.
• Router x receiving a Return-ADD with A u from node v: Then x removes A u from L r (x) and L e (x). Fig. 4 shows the flow chart of the above procedure. 
With the above address borrowing scheme, ZigBee can still support very simple address-based routing as follows. When router v receives a packet with a destination address A dest , it accepts the packet if A v = A dest . Otherwise, v forwards this packet as follows.
1) If dest is a child router or grandchild of v, then v forwards this packet to a router as follows.
. Then v forwards this packet to its child router A r if A r / ∈ L r (v); otherwise, v forwards this packet to the borrower who borrows A r from v.
for some node x in B r (v), v forwards this packet to its child router x. 2) If dest is a child end device, then v forwards this packet to one of its child end device as follows.
; otherwise, v forwards this packet to the borrower who borrows A r from v. b) If A dest ∈ B e (v), v forwards the packet to this child end device. 3) Otherwise, v forwards the packet to its parent p(v) if v is not the coordinator, and v ignores this packet if v is the coordinator. Fig. 5 is an example that a packet is transmitted from node 108 to node 45. This packet is transmitted to node 0 in accordance with the step 3) of the routing scheme first. Then node 0 forwards it to node 1 according to the step 1.a) of the routing scheme. Node 1 finds that address 45 should be one of its grandchildren, and this address is lent to node 54. According to the step 1.a), node 1 then forwards this packet to node 54 by looking address 45 up in the lend list of node 1. When node 54 receives the packet, it looks this address up in the borrow list and then forwards it to the child router 36 according to the step 1.b). Finally node 36 forwards this packet to node 45 in accordance with the step 1.a).
To verify the benefit gained from our ABS, we evaluate the average number of orphans of our scheme as compared to other approaches. We assume the following simulation environment. The monitoring region is 500 × 500m 2 where 2000 routers are randomly deployed and the coordinator is set in the center. The communication range of each router is set to 50m.
For comparison, we design three modifications of our ABS by relaxing the borrowing strategy in the step (e) of associating scheme. A parent router can ask for borrowing from its 1-hop neighbors (ABS-1), 2-hop neighbors (ABS-2), and 3-hop neighbors (ABS-3). We also set DBAS-1 and DBAS-2 as the strategies of asking its 1-hop, and 2-hop neighbors, respectively.
We vary the maximum network depth (L max ) to see how this influences the average number of orphans. We set B max = 2. Fig. 6(a) shows the results. The average number of orphans is inversely proportional to the tree depth limit L max . Regardless of L max , our schemes always outperform other approaches. The average number of orphans by ABS-2 is fewer than that by ABS-1, since ABS-2 can serve further nodes. Fig. 6(b) shows the average number of orphans with different R max . Regardless of borrowing strategy, the higher R max = C max makes nodes join the network more easily, and thus fewer orphans.
ABS always outperform DBAS, because it allows a parent router to borrow a flexible address space through counting orphans. On the contrary, DBAS always lets a parent router borrow a maximum address space, exhausting address quickly.
Finally, we measure the impact of B max on the average number of orphans. B max affects the number of children that a router can serve in the ZigBee tree. In Fig. 7 , we observe that larger B max cause fewer orphans. 
To relieve the orphan problem in a ZigBee network, we have proposed the address-borrowing scheme (ABS) by allowing a node to borrow unused address spaces from neighbor nodes. We have shown that it effectively decreases the number of orphans in a ZigBee network when the network cannot expand due to the constraints of C max , R max , and/or L max . The unit of lending/borrowing is a subtree of address space under the original definition in ZigBee, thus reducing the cost of routing tables and the requirement of storage spaces in router nodes. We have also suggested a light-weight routing algorithm for ABS which follows the original ZigBee address-based routing strategy with a slight modification. In addition, the maximum depth of the ZigBee network is predictable. Simulation results also show that ABS can befit original ZigBee address assignment mechanism and thus effectively reduces orphans.
Current trends in object-oriented software construction, namely MDA (Model-Driven Architecture)-based approaches, promote designing high-level models that represent domain and application concepts ("Platform Independent Models"). These models, typically described in UML (Unified Modeling Language), are further on mapped to the target implementation platform ("Platform Specific Models"). Modeling has thus become a key activity within the software process whereas large efforts are currently spent in developing automated tools to assist it.
Formal Concept Analysis (FCA) has already been successfully applied to the analysis [1] and restructuring [2] [3] [4] [5] [6] [7] of conceptual class models: it helps reach optimal hierarchical organization of the initial classes by discovering relevant new abstractions. However, providing far-reaching abstraction mechanisms requires the whole feature set of UML to be covered, inclusive those encoding relational information (e.g., UML associations), whereas such features clearly outgrow the scope of standard FCA.
Making FCA work on UML models is the global aim of our study. Here, we propose a new relationally-aware abstraction technique, ICG (Iterative Cross Generalization), which works on several mutually related formal contexts that jointly encode a UML class diagram. It performs simultaneous analysis tasks on the set of contexts where inter-context links are used to propagate knowledge about the abstractions from a context into its related contexts (and thus broaden the discovery horizon on those contexts).
The paper recalls the basics of FCA (Section 2) before providing a motivating example (Section 3). Our recent FCA-based framework for processing relational data is presented in Section 4. In Section 5 we specify ICG while emphasizing the role UML meta-model plays in data description within ICG. Experiments done in the framework of industrial projects are then reported (Section 6) with a discussion of benefits and difficulties in applying ICG.
Formal concept analysis (FCA) [8] studies the way conceptual structures emerge out of observations. Basic FCA considers an incidence relation over a pair of sets (objects, further denoted by numbers) and (attributes, denoted by lower-case letters). Binary relations are introduced as formal contexts ! " ! $ #
. An example of a context, Foo, is provided in Figure 1 is a complete lattice with joins and meets based on intersection of concept intents and extents, respectively. The lattice of the Foo context is drawn in Figure 1 on the right (as a Hasse diagram).
Research on applications of FCA has yielded a set of meaningful substructures of the concept lattice. For instance, in object-oriented software engineering, the assignments of specifications/code to classes within a class hierarchy is easily modeled through a context, and applying FCA to a particular hierarchy may reveal crucial flaws in factorization [2] and therefore in maintainability. The dedicated substructure that specifies a maximally factorized class hierarchy of minimal size is called the Galois sub-hierarchy (GSH) of the corresponding context. Mathematically speaking, the GSH is made out of all the extremal concepts that contain an object/attribute in their extents/intents:
. Moreover, as practical applications of FCA may involve processing of non-binary data, many-valued contexts have been introduced in FCA. In a many-valued context 6 x V " A t #
, each object w is described by a set of attribute -value pairs S 7 9 G # , meaning that is a ternary relation that binds the objects from , the attributes from and the values from . The construction of a lattice on top of a many-valued context requires a pre-processing step, called scaling, which basically amounts to encoding each non-binary attribute by a set of binary ones.
3 Improving UML models: a motivating example A highly simplified example introduces the problem domain. Consider the UML model in Figure 2 . A class Diary is associated to a class Date through the association orderedBy. Class Date has three attributes (or variables) day, month and year and two methods including isLeapYear() and a comparison method (Date). Another class Clock is linked to Time class via the association shows. Class Time is described by the three attributes hour, min and sec, and by a method To infer a more elaborate UML model, we apply an approach that may be summarized as follows. On the one hand, we process various sorts of UML entities such as attributes, methods and associations, as first-class formal objects and assign a formal context to each entity sort. Moreover, we use relational attributes to express links between entities and model them as inter-context binary relations.
On the other hand, we use a repeated scaling along the relational attributes to propagate the knowledge about possible generalizations between related contexts. Thus, the concept construction process amounts to alternating scaling and proper construction until stability in concept structures is reached.
In Figure 4 , three many-valued formal contexts describe classes, associations and methods as first-level formal objects, respectively. Here, UML class attributes are not processed as objects for simplicity sake, but in the general case they are. Note that some formal attributes (e.g. originType) are relational ones while others are not (e.g. originMultiplicity or name). Figure 5 shows the main relational attributes of the example. The method lattice ( Figure 7 ) is now used as a scale for the formal attribute has owned by classes. Thus, if a class has a method © in the initial many-valued context, then it owns all the formal attributes has:m in the scaled class context where m stands for a method concept whose extent contains the formal object representing © . The resulting scaled class context and its lattice (with top and bottom dropped) are shown in Figure 8 . The lattice includes a new concept c1 which obviously represents comparable objects, hence it could be called Magnitude.
Our knowledge about the concept structure on classes has thus grown and the new abstractions can be used as descriptors that could, whenever shared, induce potential abstractions on related contexts. For example, the method context could be fed with the knowledge about Magnitude thus prompting a re-consideration of its con- (1):Time). The resulting concept lattice remains isomorphic to that of Figure 7 , however its concepts are explicitly related to existing concepts on classes, e.g., the top concept intent is bound to c1 via typeOfParam(1):c1. The same procedure can be applied for scaling the association context, revealing that the two formal objects can be generalized by a new association which ends into the c1 concept. The scaling of isOrigin and isDestination from the class context, using the augmented association lattice, introduces a new generalization of Diary and Clock (representing devices which manipulate magnitudes). The resulting set of abstractions, re-interpreted in UML, is shown in Figure 9 . The associations orderedBy and shows are linked to the new association manipulate by the constraint subset which indicates a specialization relationship. Because of this constraint, their names are now prefixed by the symbol "/", used in UML for highlighting elements that derive from others.
To sum up, we may claim that the apparent commonalties between the classes Time and Date have led to the constitution of common superclass, Magnitude. The discovery of this class has been propagated to both method and association contexts where new abstractions have been created to reflect the existence of Magnitude. In the following, we summarize the key elements of our relational FCA framework. A detailed description could be found in [10] . As in classical FCA, heterogeneous datasets, i.e., ones made out of several sorts of individuals, are introduced through a family of contexts, one per sort of formal objects. Here, a set of binary relations (or set-valued functions) is added to data description, which map objects from a context to sets of objects from another one.
A relational context family . By bringing those attributes to the objects from , conceptual scaling allows new concepts to occur in which the members of the extent share abstractions of the initial values rather than values themselves. Clearly, the choice of scale attributes has a direct impact on the structure of the target concept lattice: different attribute sets may lead to different lattices.
The same principle may be applied to the processing of relations which are basically object-valued attributes: given a relation q { | } 1 P and w s | the set q e S w # could be replaced by a collection of binary attributes that characterize it. As the entire process is ultimately aimed at detecting commonalties in the abstractions that conceptually describe the target objects, the scaling binds scale attributes to existing concepts on the co-domain context rather than to formal attributes of this context (see the attributes typeOfParam(1):cX from Section 3). Moreover, as we argued in [10] , the most natural choice for the scale lattice of q is the lattice of the context c since it embeds the most precise knowledge about the meaningful abstractions on the set "
. However, in specific situations smaller structures, such as the GSH, may be more appropriate.
Consider an object 
("wide"). The "narrow" scheme clearly fits lattice-shaped scales whereas the "wide" one suits also less extensive concept structures.
To sum up, the encoding by concepts rather than by formal attributes from the destination context eases the interpretation of the formal concepts discovered in the source context | . Moreover, such an encoding fits a step-wise discovery of the scale concepts as illustrated in Section 3: the formation of some new concepts within , one per context l , such that the concepts reflect both shared attributes and similarities in object relations, i.e., common concepts in the co-domain context. Obviously the relational scaling helps to reduce the lattice construction on relational data to the binary case so that the same algorithmic procedures could be applied. However, unlike conventional constructions, some RCF may require a step-wise construction process due to the mutual dependencies between contexts, as it was shown in the UML model analysis. Indeed, having aligned scales with actual concept hierarchies on the destination contexts, an apparent deadlock occurs whenever two contexts are connected both ways by a pair of relational attributes (or chains of such attributes). For instance, in Figure 5 , the class context is doubly connected to the method one by the initial attribute pair (typeOfParam(i), has).
To resolve the deadlocks resulting from circularity in the relational structure of a RCF, we apply a classical fixed-point computation mechanism that proceeds stepwise. Its grounding principle lies in the gradual incorporation of new knowledge gained through scaling: the computation starts with uniformly nominal scales for all relational attributes and at each subsequent step uses the previously discovered concept structures within the respective co-domain contexts as new and richer scales.
Technically speaking, the global lattice extraction process associated with a RCF alternates between relational scaling and lattice construction (see Algorithm 1) . At the initial step, relations are ignored (line 5), hence the lattices at this stage (line 6) are not impacted by the relational information and rather reflect common non-relational attributes. At the following step these lattices are used as new scales for a first-class relational scaling thus providing new possibilities for generalizations (lines 10-11). The scaling (line 10) / construction (line 11) steps go on until the global set of concepts stabilizes, i.e., for each context r . Stabilization of the process can be deduced from the fact that the formal objects do not change over the steps; the concept number of the lattice associated with ! " A t #
is bounded by
, which gives a bound to the scaling of relational attributes.
Out:
² array of lattices ) 3:
while not halt do 8:
Algorithm 1: Construction of the set of concept lattices corresponding to a RCF.
Once the lattices of all contexts are available, a post-processing step clarifies the links between concepts induced by relational scale attributes. In fact, many concept intents will present redundancies: a concept , only those corresponding to minimal Cl will be preserved. For instance, in Figure 8 , the attribute has:m3 is redundant in the intent of class concept c2 since c2 also owns has:m1, whereas m3 is a super-concept of m1 in the method lattice.

In class diagrams, classes are associated to structural features (attributes) and behavioral features (operations and methods). In Figure 10 (top) the main elements of attribute and method description are presented: visibility (+, -and #); attribute types e.g. String, Point or Color which can be classes; return type; parameter type list; multiplicity for many-valued attributes (like color), the multiplicity is a set of integer intervals restricting the value number (for color, multiplicity 1..* expresses the fact that color has one or more value); static status (underlined feature); derived status (introduced by /). Figure 10 (bottom) also illustrates the main aspects of UML associations. An association is composed of at least two association ends. When it has a name (for example place order), the name is followed by a triangle which establishes the direction for reading this name: a person places an order and not the other way round. An association end is typically characterized by: a type (the class involved in this end), for example Person and Order are the two end types of the association place order; a visibility; a multiplicity; a navigability (shown through an arrow next to the type end); a white or black diamond which indicates an aggregation or a composition. An association end is sometimes provided with a role name which gives more accurate semantics to objects when they are involved in the link, e.g.
for a person in association © 7 0 7 0 Ê G . When the association owns variables and methods it is considered as an association class, e.g. Access is an association class that supports the variable passwd. Using the UML meta-model to guide the context construction The definition of UML is established by the UML meta-model, that is a model that defines the language for models. The UML meta-model is described through a subset of UML, and is given with well-formedness rules in the formal language OCL (Object Constraint Language), as well as with semantics in natural language. Part of this meta-model [12] relevant to our problem, that considers the classes and their features, is shown in Figure 11 . The meta-class Class specializes Classifier, and as such, inherits from the possibility to own Features (Attribute or Method). An Attribute includes the meta-attributes initialValue, multiplicity, visibility, changeable; it has a type via the meta-association that links it to Classifier. A Method has the meta-attributes body, isQuery, visibility, and is composed of an ordered set of Parameters. An Association is composed of several AssociationEnds which have a type which is a classifier. AssociationEnds are described by a type (a classifier), and several meta-attributes including isNavigable, isOrdered, aggregation and multiplicity.
As a meta-description of UML, the meta-model naturally contains the good abstractions for determining the right formal contexts: meta-classes are straightly interpreted as formal objects, while meta-attributes and ends of meta-associations are their formal attributes. Nevertheless, such an approach can lead to the manipulation of many tables of data, and to the use of descriptors that generate too numerous uninteresting concepts. Parameter for example is preferably included in the description of methods. Associations should be described by an ordered set of association ends, but if we consider only binary and directed associations, as often suggested in modeling [13] , we can avoid having a specific formal context for association end description. Conversely, if we want to inspect all possible generalizations of associations in the general case, a formal context describing association ends would be relevant. Notice that the algorithm may propose to factorize role names depending on the way the designer has named the associations: association names, role names or both. This corresponds to the formal attributes name, nameOrigin and nameDestination of the formal context on associations. The multiplicity on the side of the class @Authentication context is also properly factorized into a 1..* multiplicity. On the other hand, one may question the factorization of 0..1 and 1 multiplicities into * (it could have been factorized into 0..1) but this is an internal choice of the algorithm that could be fine-tuned.
We presented a new FCA-based technique (ICG) which processes several mutually related formal contexts and sketched its application to UML class diagram restructuring. Experiments on industrial-scale projects established the feasibility of our approach (execution time and semantic relevance of the results) and highlighted the crucial role of parameter tuning and appropriate user interface. A key track of improvement is the separation of formal attributes that guide the construction of new abstractions (e.g. names, types of attributes, association ends, etc.) from secondary ones that only help to increase the precision (e.g., multiplicity or navigability). Another current concern is the integration of a domain ontology into the ICG framework that should enable the comparison of symbolic names used by the designer. This is crucial for any automated reconstruction technique such as our, because terms are not uniformly used over UML diagrams, many synonymy, homonymy or polysemy situations occur. Although Objecteering offers an operational user interface for ICG there is a large space for improvement. First, designers that are FCA neophytes would benefit from an automated assistance in tool fine-tuning. Second, navigation and edition tools should help make the entire ICG process more interactive and thence more purposeful, e.g., by supporting run-time filtering of the discovered abstractions.
1+o (1) )-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here n is the number of nodes in the network and D is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized (1 + o (1) In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the "hitting set argument" commonly used for shortest paths computation in var- * A full version of this paper is available at http://arxiv.org/ abs/1504.07056 † This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. The research leading to this work has received funding from the European ious settings, and (ii) a simple, deterministic, construction of an (n o (1) , o (1) )-hop set of size O(n 1+o (1) ). We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g. ruling sets [26] , source detection [39] , and partial distance estimation [38] . Our hop set construction also leads to single-source shortest paths algorithms in two other settings: (i) a (1 + o (1) every node to know how far it is from s. The unweighted version -the breadth-first search tree computation -is one of the most basic tools in distributed computing, and is well known to require Θ(D) time (e.g. [44] ). In contrast, the only available solution for the weighted case is the distributed version of the Bellman-Ford algorithm [5, 23] , which requires O(n) time to compute an exact solution. In 2004, Elkin [16] raised the question whether distributed approximation algorithms can help improving this time complexity and showed that any α-approximation algorithm requires Ω((n/α) 1/2 / log n + D) time [17] . Das Sarma et al. [12] (building on [45, 35] ) later strengthened this lower bound by showing that any poly(n)-approximation (randomized) algorithm requires Ω(n 1/2 / log n + D) time. This lower bound was later shown to hold even for quantum algorithms [19] .
Since running times of the formÕ(n 1/2 + D) 1 show up in many distributed algorithms (e.g. MST [36, 45] , connectivity [53, 46] , and minimum cut [43, 25] ) it is natural to ask whether the lower bound of [12] can be matched. The first answer to this question is a randomized O(
-time algorithm by Lenzen and Patt-Shamir [37] 2 . The running time of this algorithm is nearly tight if we are satisfied with a large approximation ratio. For a small approximation ratio, Nanongkai [42] presented a randomized ( 
The running time of this algorithm is nearly tight when D is small, but can be close toΘ(n 2/3 ) even when D = o(n 2/3 ). This created a rather unsatisfying situation: First, one has to sacrifice a large approximation factor in order to achieve the near-optimal running time, and to achieve a (1 + o(1)) approximation factor, one must pay an additional running time of D 1/4 which could be as far from the lower bound as n 1/8 when D is large. Because of this, the question whether we can close the gap between upper and lower bounds for the running time of (1 + o(1))-approximation algorithms was left as the main open problem in [42, Problem 7.1] . Secondly, and more importantly, both these algorithms are randomized. Given that designing deterministic algorithms is an important issue in distributed computing. This leaves an important open problem whether there is a deterministic algorithm that is faster than BellmanFord's algorithm, i.e. that runs in sublinear-time.
Our Results.
In this paper, we resolve the two issues above. We present a deterministic (1+o(1))-approximation O(n 1/2+o(1) +D 1+o(1) )-time algorithm for this problem (the o(1) term in the approximation ratio hides a 1/ polylog n factor and the o(1) term in the running time hides an O( log log n/ log n) factor). Our algorithm almost settles the status of this problem as its running time matches the lower bound of Das Sarma et al. up to an O(n o(1) ) factor. Since an α-approximate solution to SSSP gives a 2α-approximate value of the network's weighted diameter (cf. Section 2), our algorithm can (2 + o(1))-approximate the weighted diameter within the same running time. Previously, Holzer et al. [31] showed that for any > 0, a (2 − )-approximation algorithm for this problem requiresΩ(n) time. Thus, the approximation ratio provided by our algorithm cannot be significantly improved without increasing the running time. The running time of our algorithm also cannot be significantly improved because of the lower bound of Ω(n 1/2 / log n + D) [12] for approximate SSSP which holds for any poly(n)-approximation algorithm.
Using the same techniques, we also obtain a deterministic (1 + o (1) (1))-approximation of the diameter requiresΩ(n) time in the worst case [30] in the congested clique.
Our techniques also lead to a (non-distributed) streaming algorithm for (1 + o(1))-approximate SSSP where the edges are presented in an arbitrary-order stream, and an algorithm with limited space (preferablyÕ(n log W ), when edge weights are in {1, 2, . . . W }) reads the stream in passes to determine the answer (see, e.g., [40] for a recent survey). It was known thatÕ(n log W ) space and one pass are enough to compute an O(log n/ log log n)-spanner and therefore approximate all distances up to a factor of O(log n/ log log n) [22] (see also [21, 3, 20, 18] ). This almost matches a lower bound which holds even for the s-t-shortest path problem (stSP), where we just want to compute the distance between two specific nodes s and t [22] . On unweighted graphs one can compute (1 + , β)-spanners in β passes and O(n 1+1/k ) space [20] , and get (1 + )-approximate SSSP in a total of O(β/ ) passes. In 2006, McGregor raised the question whether we can solve stSP better with a larger number of passes (see [1] ). Very recently Guruswami and Onak [27] showed that a p-pass algorithm on unweighted graphs requiresΩ(n 1+Ω(1/p) /O(p)) space. This does not rule out, for example, an O(log n)-passÕ(n)-space algorithm. Our algorithm, which solves the more general SSSP problem, gets close to this: it takes O(n o(1) log W ) passes and O(n 1+o(1) log W ) space.
Our crucial new technique is a deterministic process that can replace the following "path hitting" argument: For any c, if we pickΘ(c) nodes uniformly at random as centers (typically c = n 1/2 ), then a shortest path containing n/c edges will contain a center with high probability. This allows us to create shortcuts between centers -where we replace each path of length n/c between centers by an edge of the same length -and focus on computing shortest paths between centers. This argument has been repetitively used to solve shortest paths problems in various settings (e.g. [54, 28, 15, 4, 48, 49, 13, 14, 41, 7, 37, 42] ). In the sequential model a set of centers of sizeΘ(c) can be found deterministically with the greedy hitting set heuristic once the shortest paths containing n/c edges are known [55, 33] . We are not aware of any non-trivial deterministic process that can achieve the same effect in the distributed setting. The main challenge is that the greedy process is heavily sequential as the selection of the next node depends on all previous nodes, and is thus hard to implement efficiently in the distributed setting 4 . In this paper, we develop a new deterministic process to pickΘ(c) centers. The key new idea is to carefully divide nodes intoÕ(1) types. Roughly speaking, we associate each type t with a value wt and make sure that the following properties hold: (i) every path π with Ω(n/c) edges and weight Θ(wt) contains a node of type t, and (ii) there is a set of O(n/c) centers of type t such that every node of type t has at least one center at distance o(wt). We define the set of centers to be the collection of centers of all types. The two properties together guarantee that every long path will be almost hit by a center: for every path π containing at least n/c edges, there is a center whose distance to some node in π is o(w(π)) where, w(π) is the total weight of π. This is already sufficient for us to focus on computing shortest paths only between centers as we would have done after picking centers using the path hitting argument. To the best of our knowledge, such a deterministically constructed set of centers that almost hits every long path was not known to exist before. The process itself is not constrained to the distributed setting and thus might be useful for derandomizing other algorithms that use the path hitting argument.
To implement the above process in the distributed setting, we use the source detection algorithm of Lenzen and Peleg [39] to compute the type of each node. We then use the classic ruling set algorithm of Goldberg et al. [26] to compute the set of centers of each type that satisfies the second property above. (A technical note: we also need to compute a bounded-depth shortest-path tree from every center. In [42] , this was done using the random delay technique. We also derandomize this step by adapting the partial distance estimation algorithm of Lenzen and Patt-Shamir [38] .)
Another tool, which is the key to the improved running time, is a new hop set construction. An (h, )-hop set of a graph G = (V, E) is a set F of weighted edges such that the distance between any pair of nodes in G can be (1 + )-approximated by their h-hop distance (given by a path containing at most h edges) on G = (V, E ∪F ) (see Section 2 for details). The notion of hop set was defined by Cohen [10] in the context of parallel computing, although it has been used implicitly earlier, e.g. [54, 34] (see [10] for a detailed discussion). The previous SSSP algorithm [42] was able to construct an (n/k, 0)-hop set of size kn, for any integer k ≥ 1, as a subroutine (in [42] this was called shortest paths diameter reduction 5 ). In this paper, we show that this subroutine can be replaced by the construction of an ( (1) ). Our hop set construction is based on computing clusters which is the basic subroutine of Thorup and Zwick's distance oracles [51] and spanners [51, 52] . It builds on a line of work in dynamic graph algorithms. In [6] , Bernstein showed that clusters can be used to construct an (n o (1) , o(1))-hop set of size O(n 1+o (1) ). Later in [29] , we showed that the same kind of hop set can be constructed by using a structure similar to clusters while restricting the shortest-path trees involved to some small distance and use such a construction in the dynamic (more precisely, decremental) setting. The con-struction is, however, fairly complicated and heavily relies on randomization. In this paper, we build on the same idea, i.e., we construct a hop set using bounded-distance clusters. However, our construction is significantly simplified, to the point that we can treat the cluster computation as a black box. This makes it easy to apply on distributed networks and to derandomize. To this end, we derandomize the construction simply by invoking the deterministic clusters construction of Roditty, Thorup, and Zwick [47] and observe that it can be implemented efficiently on distributed networks 6 . We note that it might be possible to use Cohen's hop set construction instead. However, Cohen's construction heavily relies on randomness and derandomizing it seems significantly more difficult.
We start by introducing notation and the main definition in Section 2. Then in Section 3 we explain the deterministic hop set construction in, which is based on a variation of Thorup and Zwick's clusters [51] In Section 4, we give our main result, namely the (
In that section we explain the deterministic process for selecting centers mentioned above, as well as how to implement the hop set construction in the distributed setting.
In this paper we consider weighted undirected graphs. For a set of edges E, the weight of each edge (u, v) ∈ E is given by a function w (u, v, E) .
Whenever we define a set of edges E as the union of two sets of edges E1 ∪ E2, we set the weight of every edge (u, v) ∈ E to w(u, v, E) = min(w(u, v, E1), w(u, v, E2)). We denote the weight of a path π in a graph G by w(π, G) and the number of edges of π by |π|.
Given a graph G = (V, E) and a set of edges F ⊆ V 2 , we define G∪F as the graph that has V as its set of nodes and E∪ F as its set of edges. The weight of each edge (u, v) is given by
We denote the distance between two nodes u and v, i.e., the weight of the shortest path between u and v, by d (u, v, G) . We define the distance between a node u and a set of nodes 
We denote the hop-distance between two nodes u and v, i.e., the distance between u and v when we treat G as an unweighted graph, by hop (u, v, G) . (u, v, G) . When G is clear from the context, we use D instead of D(G). We note that this is different from the weighted diameter, which is defined as WD(G) = max u,v∈V (G) d (u, v, G) . Throughout this paper we use "diameter" to refer to the hop diameter (as it is typically done in the literature).
Given any graph G = (V, E), any integer h, and ≥ 0, we say that a set of weighted edges F is an (h, )-hop set of G if
where H = (V, E ∪ F ). In this paper we are only interested in (n o (1) , o(1))-hop sets of size O(n 1+o(1) ). We refer to them simply as "hop sets" (without specifying parameters).
In our algorithm we will repeatedly use the following established weight-rounding technique [9, 55, 6, 41, 7, 42 ] to scale down edge weights at the cost of approximation. 
An important subroutine in our algorithm is a procedure for solving the source detection problem [39] in which we want to find the σ nearest "sources" in a set S for every node u, given that they are of distance at most γ from u. Ties are broken lexicographically. The following definition if from [38] 
{(d(u, v, G), v)|v ∈ S ∧ d(u, v, G) ≤ γ}((d(u, v, G), v) < (d(u, v , G), v ) ⇐⇒ (d(u, v, G) < d(u, v , G))∨(d(u, v, G) = d(u, v , G)∧v < v ) ,
Lenzen and Peleg designed a source detection algorithm for unweighted networks [39] . One can also run the algorithm on weighted networks, following [38, proof of Theorem 3.3], by simulating each edge of some weight L with an unweighted path of length L. Note that nodes in the paths added in this way are never sources. We also use another source detection algorithm: Roditty, Thorup, and Zwick [47] also solve a variant of the source detection problem with γ = ∞ in their centralized algorithm for computing distances oracles and spanners deterministically. They reduce the source detection problem to a sequence of single-source shortest paths computations on graphs with some additional nodes and edges. Their algorithm can easily be generalized to arbitrary γ. Theorem 2.4 (implicit in [47] The classic result of Goldberg et al. [26] shows that in the distributed setting, for any c ≥ 1, we can compute a (c, cλ)-ruling set deterministically in O(c log n) rounds, where λ is the number of bits used to represent each ID in the network. Since it was not explicitly stated that this algorithm works in the CONGEST model, we sketch an implementation of this algorithm in the full version of this paper (see [ 
In this section we present a deterministic algorithm for constructing an (n o (1) , o(1))-hop set.
The basis of our hop set construction is a structure called cluster introduced by Thorup and Zwick [51] . Consider an integer p such that 2 ≤ p ≤ log n and a hierarchy A of sets of nodes (Ai) 0≤i≤p such that A0 = V , Ap = ∅, and A0 ⊇ A1 ⊇ . . . ⊇ Ap. We say that a node v has priority i if v ∈ Ai \ Ai+1 (for 0 ≤ i ≤ p − 1). For every node v ∈ V we define the restricted cluster up to distance R as
where i is the priority of v. For every node v ∈ V , every 0 ≤ i ≤ p − 1, and every R ≥ 1, we define the restricted i-bunch up to distance R as
Clusters and bunches are dual concepts, i.e., u ∈ C (v, A, R, G) if and only if v ∈ 0≤i≤p−1 Bi(u, A, R, G). In our hop set construction we will use sets of edges obtained from the clusters in the straightforward way: every node has an edge to each node in its cluster. Note that the size of such a set of edges (which in turn influences the size of our hop set) is at most v∈V |C (v, A, R, G)|, the size of all clusters, which is equal to
A second motivation for keeping cluster sizes small is that in the models of computation considered in this paper also the time needed for constructing all clusters will depend on both R and the size of the clusters. Given a hierarchy of sets A, the clusters can be computed as follows. First, for every 1 ≤ i ≤ p − 1, compute the distance of every node to its closest node in Ai by constructing a shortest-path tree in a modification of the graph where all nodes of Ai are contracted to a single source node. Second, compute the cluster of every node v by constructing a shortest-path tree up to distance R from v under the following restriction: only let a node u to join the tree if d(u, v, G) < d(u, Ai+1, G) . This additionally computes the distances between v and every node in its cluster.
If randomization is allowed, bunches of small size can be obtained as follows [51] : If we set A0 = V and Ap = ∅, and for each 0 ≤ i ≤ p − 2 we obtain Ai+1 by picking each node from Ai with probability (ln n/n) 1/p , then the expected size of each i-bunch is at most n 1/p and thus the expected size of all bunches (and hence all clusters) is O(pn 1+1/p ). Roditty, Thorup and Zwick [47] also give a deterministic algorithm for setting A such that the resulting clusters will have the same asymptotic size. Following their algorithm we iteratively compute Ai's such that the restricted i-bunch of every node will have size q =Õ(n 1/p ). We set A0 = V and, given Ai, we construct Ai+1 as follows. Using a source detection algorithm, we first determine for each node v the set L(v, Ai, R, q, G), which among all nodes of Ai at distance at most R from v contains the q closest ones. We now find a set Ai+1 ⊆ Ai of size |Ai+1| ≤ |Ai|/n 1/p such that, for every node v, L(v, Ai, R, q, G) contains at least one node of Ai+1. This restricts the size of each i-bunch to q. Finding such a set Ai+1 of minimum size is exactly the hitting set problem. By probabilistic arguments there exists a hitting set of size |Ai|/n 1/p . Once all sets L(v, Ai, R, q, G) are known, we can compute an approximation of the minimum hitting set using a greedy heuristic. The guarantees of our algorithm for computing clusters can be summarized as follows.
A = (Ai) 0≤i≤p , where V = A0 ⊆ A1 ⊆ · · · ⊆ Ap = ∅, such that v∈V |C (v, A, R, G)| = O(pn 1+1/p ).

We now explain how to construct the hop set. We omit many details, which can be found in the full version of the paper.
Assume we are given a hierarchy of sets A and the corresponding clusters and consider the set of edges F containing for every node u edges to all nodes v in its cluster with weight equal to the distance from u to v. Using an analysis similar to [52] and [29] we can show that if the number of priorities p is small enough the following holds after adding the edges of F to the graph: for every ∆ sufficiently smaller than the distance range R we can, for all pairs of nodes u and v find a path from u to v with O ((p + 1) d(u, v, G) /∆ ) edges that overestimates the distance from u to v by a multiplicative error of (1 + ) and an additive error of n o (1) .
and parameters ∆ ≥ 1 and 0 < ≤ 1. Then F has sizẽ O(pn 1+1/p ), where p = (log n)/(log (4/ )) , and in the graph H = G ∪ F , for every pair of nodes u and v, we have
Procedure 1: HopReductionAdditiveError(G, ∆, ) Input: Graph G = (V, E) with non-negative integer edge weights, ∆ ≥ 1, 0 < ≤ 1 Output: Hop-reducing set of edges F ⊆ V 2 as specified in Lemma 3.2
Consider a shortest path π from u to v with h edges and weight R ≥ ∆. With the hop reduction of Procedure 1 we can compute a set of edges that reduces the number of hops from u to v toÕ(R/∆) (at the cost of approximating the distance). This is not yet sufficient for computing the desired hop set because R might be as large as nW . Instead we would like to reduce the number of hops toÕ(h/∆) as h can be at most n. We can achieve this by using the weight-rounding technique of Lemma 2.1: for every distance range of the form 2 j . . . 2 j+1 , we scale down the edge weights by a certain factor ρj (depending on j, h, and ) and run Procedure 1 on this scaled-down version of G to obtain a set of edges Fj. The set j Fj will then provide a reduction toÕ(h/∆) hops. Additionally, if h is sufficiently larger than ∆, then the additive error inherent in the hop reduction of Procedure 1 can be counted as an additional multiplicative error of . 
We now use the following iterative approach in which we repeatedly apply the hop reduction of Procedure 2 with ∆ = n o (1) . We first compute a set of edges F1 that reduces the number of hops in G from h0 = n to h1 = h0/∆ = n/∆. We then add all these edges to G and consider the graph HopReduction(G, ∆, h, , W ) Input: Weighted graph G = (V, E) with integer edge weights from 1 to W , ∆ ≥ 1, 0 < ≤ 1 Output: Hop-reducing set of edges F ⊆ V 2 as specified in Lemma 3.3
We apply Procedure 2 again on H1 to compute a set of edges F2 that reduces the number of hops in H1 from h1 to h1/∆ = n/∆ 2 . Now observe that the set of edges F1 ∪ F2 reduces the number of hops in G from n to n/∆ 2 and in general, after i iterations, the number of hops is reduced to n/∆ i . This process stops when the number of hops reaches the bound h ≥ n 1/p ∆/(p + 2) of Lemma 3.3. We show that by repeating this process p = Θ( log n/ log (1/ )) times we can compute a set F that reduces the number of hops to n/∆ p = n o (1) .
Input: Weighted graph G = (V, E) with integer edge weights from 1 to
2 be the set of edges computed by Procedure 3 for a weighted graph G = (V, E) and a parameter 0 < ≤ 1. Then, for p = (log n)/(log (108
The main computational cost for constructing the hop set comes from computing the clusters in Procedure 1, which is used as a subroutine repeatedly. If 1/ ≤ polylog n, then (1) and Procedure 3 will compute an (n o (1) , o(1))-hop set of size O(n 1+o(1) log W ); it will performÕ(log W )
cluster computations each with p = Θ( log n/ log (1/ )) priorities up to distance range O(n o(1) ) on a graphs of size O(m 1+o(1) log W ). The concrete time complexity depends on the model of computation we want to consider (see the implementation in Section 4.2). As for each cluster computation the priorities are set deterministically, our whole algorithm is deterministic.
Our algorithm consists of two parts, presented in two sections: In Section 4.1 we give a deterministic algorithm for constructing an overlay network such that it is sufficient to compute SSSP on this network. A randomized version of this result was given in [42] . In Section 4.2 we present a more efficient algorithm for computing SSSP on this overlay network using Procedures 1 to 3 from before. We finish the computation in the same way as in [42] .
An overlay network (aka landmark or skeleton) [42, 50, 37] is a virtual network G of nodes and "virtual edges" that is built on top of an underlying real network G; i.e.,
such that the weight of an edge in G is an approximation of the distance of its endpoints in G. The nodes in V (G ) are called centers. Computing G means that after the computation every node in G knows whether it is a center and knows all virtual edges to its neighbors in G with their weights. We show in this subsection that there is aÕ(n 1/2 )-time algorithm that constructs an overlay network G ofÕ(n 1/2 ) nodes such that a (1+o(1))-approximation to SSSP in G , can be converted to a (1 + o(1) )-approximation to SSSP in G, as stated formally below.
Theorem 4.1. Given any weighted undirected network G and source node s, there is anÕ(n 1/2 )-time deterministic distributed algorithm that computes an overlay network G and some additional information for every node with the following properties.
• Property 1: |V (G )| =Õ(n 1/2 ) and s ∈ V (G ).
• Property 2: For every node u ∈ V (G), as soon as
In Theorem 4.2 of the full version of [42] , the following randomized algorithm that achieves the result above was given 7 . In the first step of [42] , the algorithm selects each node to be a center with probabilityΘ(1/n 1/2 ) and also makes s a center. By a standard "hitting set" argument (e.g. [54, 13] ), any shortest path containing n 1/2 edges will contain a center with high probability. Also, the number of centers isΘ(n 1/2 ) with high probability. In the second step, the algorithm makes sure that every node v knows theΘ(n 1/2 )-hop distances between v and all centers using a light-weight bounded-hop single-source shortest paths algorithm from all centers in parallel combined with the random delay technique to avoid congestion.
We derandomize the first step as follows: In Section 4.1.1 we assign to each node u a type t(u) such that every path π containing n 1/2 edges contains a special node u with 2 t(u) =  O( w(π, G) ). This is comparable to the property obtained from the hitting set argument, which would be achieved if we made the special node of every path a center. However, this might create too many centers. Instead we select some nodes to be centers using the ruling set algorithm, as described in Section 4.1.2, which outputs a small set of centers and we can show that every special node u is at distanceÕ (2 t(u) ) to one of the centers. Thus, while we cannot guarantee that the path π contains a center, we can guarantee that it contains a node that is not far from a center. To derandomize the second step, we use the recent algorithm of Lenzen and Patt-Shamir [38] for the Partial Distance Estimation (PDE) problem together with the above Procedures 1 to 3, as explained in Section 4.1.2. The parameters used by our algorithm in the following are = 1/ log
, and k = (1 + 2/ )k. Recall that λ is the number of bits used to represent each ID in the network.
For any integer i, we let ρi = Proof. Let l = |π|/h ≥ 1/ and let x and y denote the endpoints of π. Partition π into the path πx consisting of the (l − 1)h edges closest to x and the path πy consisting of the |π| − (l − 1)h edges closest to y. Further partition πu into l − 1 non-overlapping subpaths of exactly h edges, and expand the path πy by adding edges of πx to it until it has h edges. Thus, there are now l paths of exactly h edges each and total weight at most 2w(π, G). It follows that there exists a subpath π of π consisting of exactly h edges and weight at most 2w(π, G)/l ≤ 2 w(π, G). Let u and v be the two endpoints of π and let i be the index such that
Computing Types of Nodes.
To compute t(u) for all nodes u, it is sufficient for every node u to know, for each i, whether |B(u, Gi, h )| ≥ h. We do this by solving the (S, γ, σ)-detection problem on Gi with S = V (G), γ = h and σ = h, i.e., we compute the list L(u, S, γ, σ, G) for all nodes u, which contains the σ nodes from S that are closest to u, provided their distance is at most γ. By Theorem 2. 
Having computed the types of the nodes, we compute ruling sets for the nodes of each type to select a small subset of nodes of each type as centers. Remember the two properties of an (α, β)-ruling set T of a base set U : (1) all nodes of T are at least distance α apart and (2) each node in U \ T has at least one "ruling" node of T in distance β. We use the algorithm of Theorem 2.6 to compute a (2h + 1, (2h + 1)λ))-ruling set Ti for each graph Gi where the input set Ui consists of all nodes of type i. The number of rounds for this computation is O(h log n) =Õ(n 1/2 ). We define the set of centers as V = ( i Ti) ∪ {s}. Property (1) allows us to bound the number of centers and by property (2) the centers "almost" hit all paths with n 1/2 edges. We prove the following lemma in the full version of our paper. 
Next, we compute for every node u and every center v a valued(u, v) that is a (1 + o(1) The goal is that each node u knowsd(u, v) for all centers v. In particular we also computed(u, v) for all pairs of centers u and v. As in Section 4.1.1, we do this by solving the source detection problem on a graph with rounded weights. 
Equations (1) and (3) it then follows that
We define our final overlay network to be the graph G where the weight between any two centers u, v ∈ V (G ) iŝ d(u, v) (as computed in Section 4.1.2). Additionally, for every node u ∈ V (G) we store the value ofd(u, v) to all centers v ∈ V (G ). All steps for computing G above takeÕ(n 1/2 / ) rounds and |V (G )| =Õ(n 1/2 ). It is thus left to prove Property 2 in Theorem 4.1. This is similar to the standard path-hitting argument (see the full version for details): The shortest path from node u to s can be partitioned into subpaths of at most n 1/2 hops with nodes u1, u2, . . . , ut hitting these paths (where t ≤ n 1/2 and ut is nearest to u) such that each ui has a center vi nearby (by Lemma 4.3). The distance from vi to vi+1 (1 + o (1))-approximates the distance between ui and ui+1.
We now show how to simulate the hop set algorithm presented in Section 3 on an overlay network G , whose set of nodes V (G ) are the centers, to compute a hop set of G (not of G) and how to compute approximate shortest paths from s in G using the hop set. Throughout the algorithm we will work on overlay networks with the same nodes as G , but which might have different edge weights as, e.g., Procedure 2 calls Procedure 1 and Procedure 1 calls Clusters on overlay networks with modified edge weights. Thus, we will use G to refer to an overlay network on which the subroutines run.
Computing Bounded-Distance Single-Source Shortest Paths.
We will repeatedly use an algorithm for computing a shortest-paths tree up to distance R rooted at s on an overlay network G , where R = O(n o (1) ). At the end of the algorithm every center knows this tree. We do this in a breadth-first search manner, in R + 1 iterations. Like in Dijkstra's algorithm, every center keeps a tentative distance δ(s, u) from s and a tentative parent in the shortest-paths tree, i. to all other centers a message (u, δ(s, u), v) where v is the parent of u. Using this information, every center u will update ("relax") its tentative distance δ(s, u) and its tentative parent.
Clearly, after the L th iteration, centers that have distance L+1 from s will already know their correct distance. Thus, at the end of the last iteration every center knows the shortest- 9 More precisely, there is a designated center (e.g. the center with lowest ID) that aggregates and distributes the messages (via upcasting and downcasting on the breadth-first search tree of the network G), and tells other centers when the iteration starts and ends. Computing Priorities, Clusters, and Hop Sets.
To compute the hop set of G we simulate the algorithm of Section 3 on the overlay network. We sketch the main idea here and refer to the full version of our paper for details. The main idea is that the main computational cost for computing the hop set comes from repeatedly calling its subroutine for computing bounded-distance clusters. We observe that computing these clusters on the overlay network basically needs the same analysis as the bounded-distance shortestpath trees discussed above. In particular, for computing the priorities of centers deterministically, we use the source detection algorithm of Theorem 2.4 as a subroutine, which reduces the problem to computing a series of bounded-depth shortest-path trees. Technically, this reduction would require us to add some nodes to the overlay network, which we can avoid by simulating the behaviour of the additional nodes by centers that are already present in the network. We can argue that a single cluster computation takesÕ(RD + N 1+o (1) ) rounds and thus the hop set can be computed deterministically in O(n o(1) D + n 1/2+o(1) ) rounds.
Final Steps. (1) ) rounds by the same method as in Lemma 4.6 in the full version of [42] . The details are given in the full version of our paper. 
[5], [6] , [7] , [8] ). Terrestrial SLAM algorithms often assume within the AQUA robot. We combine range information exa predictable vehicle odometry model in order to assist tracted from stereo irnagery with 3DOF orientation from an in the probabilistic association of sensor information with inertial measurement unit (IMU) within a SLAM algorithm salient visual features. The underwater domain necessitates to accurately estimate dense 3D models of the environment solutions to the SLAM problem which are more dependent and the trajectory of the sensor. It is important to note that upon sensor information than is traditional in the terrestrial although the AQUASENSOR does not process the collected domain. This has prompted recent research in robotic vehicle data in realtime, the algorithms used to analyze the data are design, sensing, localization and mapping for underwater being developed with realtime performance in mind. vehicles (see [9] , [10] , [11] , [12] ).
The AQUASENSOR is sealed within a custom underwater housing that permits operation of the device to a depth II. AQUA AND THE AQUASENSOR of 30 metres. The sensor is operated either via an IEEE One vehicle that requires robust and versatile sensing 802.1 Ig wireless link for surface operation, or via waterproof strategies to extract 3D models of the environment is the switches mounted on the exterior of the sensor. The wireless AQUA robot (see Figure 1 and [13] ). The AQUA robot is interface onboard the sensor provides status information a visually guided autonomous robot developed through a and sarnple imagery to offboard devices. LEDs mounted on collaboration between researchers at Dalhousie University, the exterior of the sensor provide status information to the McGill University, and York University. The vehicle is operator. The LEDs and waterproof switches are the only a hexapod capable of amphibious operation. On land, its communication possible with the device when submerged.
legs provide foot-ground contact that propels the vehicle. Although designed primarily to be integrated within the Underwater these same legs act as flippers or fins to drive AQUA housing, the AQUASENSOR was also designed to the vehicle both along the surface of the water and at depths be deployed independently of the AQUA vehicle. Although of up to 30 metres. In order to meet the sensing needs of bulky when handled in the terrestrial domain, it is easily the AQUA robot a series of visual-inertial sensors [14] have operated by a single diver underwater (Figure 2(b) ). been constructed. The primary goal of these sensors is to III. OBTAINING LOCAL SURFACE MODELS collect high framerate multi-camera video imagery coupled Upon return to the surface, data from the AQUASENSOR with synchronized time-stamped inertial information. This is offloaded to higher performance computers and a larger data is later processed off-line to obtain 6DOF ego-motion disk array for processing. Recovering accurate depth from and 3D models of the environment. Although we describe stereo imagery is performed by leveraging the optimized the standalone sensor package here, the long-term goal is sum-of-squared differences algorithm implemented in the to incorporate the sensing hardware and algorithms into the Point Grey Triclops library'. This provides a dense set of 3D vehicle itself.
points per acquired image frame. To estimate the motion of
In order to permit the sensor package to be operated the camera, point sets from different times are combined into independently of the robot itself, an independent sensor a common reference frame. The change in the 6DOF position package known as AQUASENSOR (Figure 2 ) has been and orientation of the sensor between frames is estimated by developed. The hardware design goal for the AQUASEN-tracking interesting features temporally, using only features SOR is to provide a compact, fully-contained unit that is that correspond to estimated 3D locations in both framnes. (1) First, "good" features are extracted from the reference i=1 camera at time t using the Kanade-Lucas-Tomasi feature The rotation, R(.), and scale, s, are estimated using a linear tracking algorithm (see [15] , [16] ) and are tracked into least-squares approach (detailed in [18] ). After estimating the subsequent image at time t + 1. Using the disparity the rotation, the translation is estimated by transfomning the map previously extracted for both time steps, tracked points centroids into a common frame and subtracting. that do not have a corresponding disparity at both time t The final step is to refine the rotation and translation and t + 1 are eliminated. Surviving points are subsequently using a noninear Levenberg-Marquardt minimization [19] triangulated to determtne the metric 3D points associated over six parameters. For this stage we parameterize the with each disparity.
In underwater scenes, many objects and points are visually and translation parameters by minimizing the transformation similar and thus many of the feature tracks will be incorrect. error Dynamic illumination effects, aquatic snow, and moving n objects (e.g. fish) increase the number of spurious points
(2) that may be tracked from frame-to-frame. To overcome these problems, we employ robust statistical estimation techniques to label the feature tracks as belonging to either a static or In practice, we find that the minimization takes few iterations non-static world model. This is achieved by estimating a to minimize the error to acceptable levels and as such rotation and translation model under the assumption that the does not preclude realtime operation. This approach to pose scene is stationary. The resulting 3D temporal correspon-estimation differs from the traditional Bundle-Adjustment dences are associated with stable scene points for the basis approach [21] in the structure-from-motion literature in that of later processing.
it does not refine the 3D locations of the features as well
We represent the camera orientation using a quatemion as the trajectory. We chose not to refine the 3D structure to and compute the least-squares best-fit rotation and trans-limit the number of unknowns in our minimization and thus lation for the sequence in a two stage process. First, provide a solution to our system more quickly. 
The reconstruction algorithm is summarized below: two sources of error in the estimate of 6DOF pose. First, the 1) Perform stereo algorithm and extract 3D point cloud point-cloud registration computed from feature tracks can at time t never be perfect. As such, there is always a small residual 2) Track salient features from time t to t -1 error in the registration per frame which accumulates over 3) Prune 2D features to only the ones with a correspondtime. Second, the intrinsic camera parameters are not known ing 3D point perfectly and any error in these parameters introduces an 4) Estimate 3D vision-only pose change using RANSAC, error in each 3D point. In particular, the radial distortion CAM qb, Tt estimate of the lens is prone to error and as a result the 5) Compute IMUq6 and qt as above per-point error is non-uniform over the visual field. This 6) Refine qt, Tt using Levenberg-Marquardt minimization can introduce an artificial surface curvature in the 3D point for a few iterations (2-3) clouds which is subtle but noticeable when many frames are 7) Apply the pose to the point cloud and add the point registered. This effect can be seen in Figure 3(a) . Here, the cloud to the octree data structure registration error is small and the points line up very well 8) Extract the surface mesh using a constrained elastic creating a visually "correct" model when viewed closely, surface-net algorithm [22] or the Marching Cubes however after registering many frames it can be seen that algorithm [23] there is an introduced curvature to the recovered surface. D. Local surface models To help counteract this effect, we utilize the 3DOF IMU to provide more information about the orientation of the device.
The IMU provides us with a quatemion representing absolute accuracy of the reconstruction system and to create 3D orientation in 3D space. We enforce the fact that the change models of real-world objects in the field. Results from in orientation as computed from the visual system must be field experiments near Holetown, Barbados show the reconconsistent with the change in the absolute orientation of struction of a coral bed and sections of a sunken barge the device. This is accomplished by transformning the IMU lying in the Folkstone Marine Reserve. Sample qualitative orientation change, reconstructions from underwater sequences are shown in Figure 4 . As in the 2D case, the problem of segmenting a 3D range of applications where the raw data must be segmented dataset can be expressed as a graph cut problem, with the cut into semantically important structures for later processing. dividing the foreground voxels from the background voxels.
Consider the recovered surface model shown in Figure 4 . The 3D segmentation process begins by converting the input For some applications -such as investigating the cause of 3D polygon or point cloud representation into a discrete 3D the sinking -the surface that belongs to the wreck may be voxel grid. A graph is created, where a node in the graph of interest while for other applications -such as monitoring is used to represent each non-empty voxel. Adjacent nonecosystem health -the surface that belongs to the coral may empty voxels are considered to be connected nodes. A sink be more salient. For these and similar applications automatic node and a source node are added to the graph with edges and semi-automatic tools are required to segment the dataset to each voxel node. Weighting heuristics are applied to the into appropriate components. It is interesting to observe that voxel graph in a similar fashion to that performed on the this problem is not unique to the 3D environment. through the 3D voxel scene. An arbitrary number of voxels EDGE WEIGHT CALCULATIONS FOR VOXEL GRAPH. may be marked by the user and the view of the 3D scene can be translated and rotated to allow the user to mark voxels that may be hidden in some views. In addition to each voxel possessing a color value, a voxel has a state attribute. This being more likely to be marked as foreground, and similarly state attribute can be one of the following four values: empty, for background. colored, foreground, or background.
A penalty term is assigned to adjacent nodes along the
The task of marking the voxels is simply an instance of segmentation boundary proportional to the difference tfl the common problem of 3D ray-picking. When a user wishes color The edge weight between these adjacent nodes thus to mark a voxel, the 2D window-coordinate of the mouse increases in proportion with the color similarity of the two position is converted into a 3D ray directed into the 3D voxel nodes, so that similar color nodes are less likely to be on grid. Each voxel intersected along the ray into the grid is different sides of the segmentation boundary. A small color tested. The first non-empty voxel encountered is tagged. difference between adjacent voxel nodes results in a large A graph is then constructed in which a node exists in the weight for their shared edge, and a large color difference graph for each non-empty voxel and edges are constructed results in a small weight.
between nodes that represent adjacent cells in the voxel Table I nodes that are connected to the source in the resulting cut are labelled as foreground nodes, and all nodes that are a) Create a graph node.
connected to the sink are labelled as background nodes. The b) Create an edge from this node to each node that dataset is now fully labelled and can be redisplayed using corresponds to an adjacent voxel. this semantic information. c) Create an edge from the source terminal node to Once the nodes in the graph have been marked the surface this node.
reconstruction can be performed. Points contained within d) Add an edge from this node to the sink terminal voxels with corresponding graph nodes marked as foreground node. are identified as foreground points. The remaining points If the node is tagged as foreground, then the edge from the are background points. A variation of the Marching Cubes source terminal is assigned a weight of oo. If the node is algorithm [23] is applied to the foreground points to generate tagged as background, then the edge from this node to the a 3D texture-mapped triangle mesh representation of the sink terminal is given a weight of oc.
foreground object. Graph edge weights are established so that edges connecting similar voxels have a high weight, while edges B Example segmentation connecting dissimilar voxels have a low weight. Similar to Figure 6 shows the results of segmenting a coral growth the method used by [24] the colors in the foreground and from the 3D mesh recovered from a wreck in the Folkstone background seeds are clustered using the k-means method Marine Reserve, Barbados. The growth in the lower left [25] .
hand view of Figure 6 (a) was segmented by having the For each node in the graph, the minimum distance from its user identify sample regions of the mesh as corresponding color to the foreground and background colors is computed. to either foreground or background (Figure 6(b) ). The 3D This information regarding the likelihood of a node being Lazy Snapping algorithm was then used to automatically foreground or background is represented as the weight be-identify regions as being foreground (here the coral growth) tween the node and the source and sink nodes. This has the or the background (everything else). Elements of the raw effect of making nodes with similar colors to the foreground point cloud that correspond to the foreground volume were then used to construct a textured mesh surface using the be used to overcome many of these difficulties. Results from Marching Cubes algorithm. The resulting segmented coral land-based experiments demonstrate the accuracy of our growth is shown in Figure 6(c)-(d 
With the advent of Intelligent Transportation System (ITS) [1] , automobile companies have started deploying more Electronic Control Units (ECUs) in vehicles for engine management, air bag controlling, automatic brake systems and air condition control. Along with these enhancements the increase in vehicles on roads and streets has also made it compulsory to implement safety applications for vehicles. Vehicular Ad Hoc Networks (VANETs) [2] are gaining scope because they are providing numerous applications like traffic safety, driver assistance, entertainment, internet access, automatic toll payment and many more for roadside environments.
Vehicular Ad Hoc Networks (VANETs) have similarities to Mobile Ad Hoc Networks (MANETs) like random topology and short communication range. These two characteristics depict that messages could not be directly delivered to destinations rather a message shall be routed by intermediate nodes to given destination. So routing protocol is very important in Copyright ⓒ 2014 SERSC both VANET and MANET environments and almost having same requirements with some changes. But the second side of coin, there are some major differences which are also available for these two networks. In Mobile Ad Hoc Networks (MANETs) environments mobile nodes can move in any direction without any geographical and spatial constraints. But in Vehicular Ad Hoc Networks (VANETs) mobile nodes have to follow some patterns because of roads, streets and buildings in their way of movement. Second difference in VANETs is that the connectivity among vehicles, size of network and speed of node are highly dynamic as compared to MANETs.
Thus by doing some little changes to MANET routing protocols, these protocols can be used for better performance in VANETs. This paper has presented new routing protocol for VANETs by modifying the existing MANET routing protocol AODV for achieving better performance in terms of routing overheads, no. of advertisements and packet delivery ratio. Second section of this paper highlights literature survey which explains the performance comparison of various MANET routing protocols. Third section of this paper give details regarding MANET routing protocols, section four elaborates new routing protocol AODV-FN (Forget Neighbors), design and implementation of new routing protocol is given in next section. Simulation results for various parameters of network performance are given in the following section and final section contains conclusion and future scope of this research.
Asma et al. [3] , compared DSR, AODV and DSDV. DSDV has higher routing load than AODV and DSR but has lower throughput than both of these. AODV and DSR have performed well in all comparisons but in some cases AODV outperforms DSR. But DSR was better when evaluated in terms of average end to end delay. Changing the packet size has affected the performance of AODV and DSR but had no effect on DSDV. Authors of Monarch project [4] have evaluated DSR, DSDV, AODV and TORA in terms of periodic advertisements, source routing, on demand route discovery and hop by hop routing. They have observed that DSR performs well for all parameters.
In [5] , authors compared AODV and DYNMO protocols for path optimally, routing overheads, packet delivery ratio, throughput and end to end delay. In these comparisons DYNMO shows better performance than AODV because it can handle different traffic patterns and mobility conditions. Paul et al., [6] , have done analysis of AODV and DSR in VANET environment in terms of node speed and node density. Higher packet delivery ratio (PDR) has been recorded in lower density and lower speed. Packet loss has been also decreased. Author compared AODV and DSR with respect to CBR and TCP. Packet delivery ratio of AODV is average under high speed conditions.
Three main classes of routing protocols which are available for MANETs: first, proactive routing protocols; second is reactive routing protocol and last one is hybrid routing protocols.
In this category of routing protocols a table is maintained for every reachable node in the network. These protocols update this table time to time. Advantage of this kind of routing protocol is that whenever a node wants to send data it can find path to destination node very easily. The examples of these kinds of protocols are Fisheye State Protocol (FSR) [7] , Optimized Link State Routing (OLSR) [8] and Topology Dissemination Based on Reverse Path Forwarding (TBRPF) [9] .
Optimized Link State Routing (OLSR) is routing protocol in which node routing table maintains paths to all nodes of network and at the time of change in topology it changes the tables according to change occurred. So due to this characteristic of this protocol it is not appropriate for highly dynamic VANET environments. On the other hand Fisheye State Protocol (FSR) is good for VANET environment because it does not send message on link failure. But major drawback of FSR is its processing time and storage needs for routing table. In Topology Dissemination Based on Reverse Path Forwarding (TBRPF) each node periodically updates the paths to other nodes by sharing control messages to each other. Problem in this protocol is flooding of control message whenever a change occurs.
In this category of routing protocols path to destination is searched only when its need arises. So these protocols are also known as "on demand" routing protocols. Search to the destination node ends only when link to that node found or the path to given node is not available from the current node.
Major difference in reactive and proactive routing protocols is that, routing overheads for reactive routing protocols are very less because they find a path to a node only when it has demanded. So a little overhead occur on control messages. This is reason why proactive routing protocol cannot be used in VANETs where topology changes very frequently. Examples of protocols based on reactive strategy are: Temporally Ordered Routing Algorithm (TORA) [10] , Dynamic Source Routing (DSR) [11] , protocol and Ad hoc On Demand Distance Vector (AODV) [12] routing protocol.
In Dynamic Source Routing (DSR), link to the destination is searched and established only when requested to do so. But this link maintenance method is not able to repair broken link and performance of protocol decreases when mobility is very high. In Temporally Ordered Routing Algorithm (TORA), when number of nodes increase more than thirty then performance of this protocol degrades. So this protocol is suitable only for small networks. AODV from reactive protocol category is best suitable for mobile ad hoc networks. A path from source to destination is determined only on demand and paths to all nodes are not maintained. The main benefit of using AODV routing is that it uses sequence numbers to check the freshness of the route. And this is also a loop free protocol and is suitable for different sizes of networks and high mobility conditions. It comes to market with different implementations like AODV-UU [13] and Kernel-AODV.
Hybrid routing protocol category contains both proactive and reactive features. These are designed in such a manner that scalability of proactive routing protocol is improved by lowering the routing overheads. In this various nodes which are close to each other to make backbone for network so that routing overheads can be minimized. Zone Routing Protocol (ZRP) [14] is type of hybrid routing protocol. In this whole network is divided into zones of different sizes. It uses reactive protocol strategy in inter-zone environment and uses proactive technique for intra-zone route discovery. It suffers from the latency problem while finding new routes.
In our paper reactive routing protocol is used because it does not store routing table for whole network hence uses less memory which is very suitable for VANETs. Because embedded systems for vehicles have very less memory. That's why protocol which uses more memory for routing table storage is not so effective in VANET environments.
Ad hoc On Demand Distance Vector (AODV) [15] routing protocol is best routing protocol for VANET environment because it uses less memory for routing table storage. Second reason for choosing AODV is that it is available in Network Simulator 2 (NS2) [16] . AODV uses four major messages for their working:
RREQ (route request message ) 2) RREP (route reply messages) 3)
HELLO (notifications) 4) RERR (routing error messages)
RREQ is used for route discovery to a given destination. RREP is a reply from destination node or from intermediate node which has a valid route to the destination. HELLO messages are sent on regular intervals to find active neighbors. And RERR messages are sent to source node when a link is broken on which it had sent some packet.
There is no problem with first three types of messages but fourth type of messages is very problematic in AODV routing protocol. Because it is generated when there is a link failure in the network. VANETs are very dynamic in nature so link loss is very high due to high speed and mobility of vehicles. So these messages in AODV are major source of congestion. This paper has explained this problem in next section and solved it by new routing protocol AODV-FN.

AODV sometimes uses old links to its neighbors for sending the packets. Most of the times neighbors change their location very frequently due to high speed so their old paths too those neighbors become invalid but AODV is unaware of it. In AODV nodes use high value for expiry for their neighbors so the paths to these neighbors are marked as valid in node's local table until the RERR message arrive but in reality those paths are invalid.
So whenever a source node sends packets to its neighbors for forwarding them to destination, in response to this RERR messages are generated about unavailability of that path, because neighbors are not present at their old positions and they have moved to some other location. Now on reception of RERR source node mark those paths invalid in its routing table and sends HELLO message for new neighbors and on receiving REPLY HELLO it again finds path to destination by sending fresh RREQ message to new neighbors. Now suppose time taken in arrival of RERR from broken link is t error. Time taken to find neighbor by HELLO and time taken to search new route to destination are t hello and t newroute respectively, then total time to find route to a node is t total : t total = (t hello + t newroute )
But total time taken for finding new route after an error is t aftererror :
(ii) so extra time taken due to error in link loss is t error. This time is for one request only but there are many RERR messages in whole network. Suppose total number of errors in network is E n then total extra time for network is t extra :
This time is very high which reduces packet delivery ratio (PDR) of AODV and also increases the routing overheads due to large number of RERR messages. The main reason behind all these RERR messages is high neighbor expiry time due to which nodes uses old links to their neighbors which are not at their previous known locations due to their mobility.
The problem in AODV was, it used to maintain the neighbors which were intermediate nodes in recent packet sending event of current node. It does not invalidate its neighbors very quickly and trust the last few paths used in packet delivery process but VANET environments are highly dynamic so all near nodes are changing their locations very fast. To solve this problem of the AODV this paper had proposed a new protocol AODV-FN (AODV-Forget Neighbors) which reduces the neighbor's expiry time for a node's neighbors. Before going to that we should understand some key terms used in AODV.
It is time after which a source node concludes that its neighbor has expired and it changes the validation for old paths to that node. But it does not mean that if that node is present in our neighborhood we will not consider it our neighbor.
Allowed hello loss means number of HELLO messages protocol is assuming to fail due to physical characteristics of network. This is generally very less in various protocols because HELLO messages have very less failure rate.
Current time is real world time of environment in which vehicle is moving. This is generally taken form C++ scheduler function in NS 2 simulator.
It is time gap after which a node sends HELLO messages to its neighbors for its presence and to know their presence in the neighborhood. It should not be too small because it will reduce the bandwidth of network by sending large number of HELLO messages. It should not too large because it will reduce the performance by insufficient number of HELLO messages.
Below given line describe how we have reduced neighbor expiry time:
nb_expire= current time + (.25 * ALLOWED_HELLO_LOSS * HELLO_INTERVAL) (IV)
New protocol AODV-FN will perform better than the old one because of following changes:
Copyright ⓒ 2014 SERSC  Each node will find new neighbors of it at present time and use them to discover fresh paths to destination. It will store them in its list for time given in equation (iv). This process will repeat for n nodes in network without leading to erroneous old paths.
 Now recent routes will not be used by the node because on expiry of neighbors it will invalidate all the old routes to those neighbors. Maintenance of table is for less time as compared to AODV.
 Each time precursor list will not be followed because it can have broken paths, those can lead to error messages. Less RERR messages will be generated because each time protocol will find new trustworthy paths to destinations.
 Total number of E n (no. of errors across network) as given in equation (iii) will be reduced. Due to this total extra time t extra will be almost nil. All these changes will help new protocol to enhance performance in terms of Packet Delivery Ratio (PDR), Advertisements and Routing load. All these parameters are improved in new routing protocol AODV-FN as compared to existing AODV routing protocol. The results of simulation in different traffic conditions have shown in section sixth.
For implementation of above said routing protocol we have used mainly two simulators:
SUMO (Simulation of Urban Mobility) Network Simulator 2 (NS2).
One of the foremost requirements while simulating VANETs is that we should keep the mobility model as real as possible. Real world traffic environment contains roads, junctions, traffic lights, dead ends and crossings. So these all real world aspects should be preserved in simulation. For this purpose we have chosen SUMO [17] . SUMO is a freeware simulator for simulation of real time traffic environments. It can import maps from various sources like OpenStreetMap, Tiger database and from many other sources. In our implementation we have taken map from OpenStreetMap. Many maps were shortlisted in beginning but in the end a map with good combination of straight lines and intersections has been chosen.
Then mobility trace file for this traffic environment which could be run in NS-2 has been created with the help of MOVE which is built on top of SUMO. Because SUMO can create simulation environment for real road network but could not show communication among vehicles. Simulation screenshot of SUMO is given in following Figure 1.
NS2 is a popular open source tool which based on 802.11 MAC layer of wireless communication networks. It is an object oriented tool which runs on occurrence of events in the network. We have used ns-2.35 version of NS-2 in which AODV is built in. NS2 can run .tcl files generated from real time traffic in SUMO and can show visual output of communication and message passing with the help of nam (network animator).
Various steps of simulation are shown in blew figure.
Step 1: In first step SUMO is used for simulation of real traffic. Different files are generated during this simulation process like .net (network files), .rou (route files), .con (connection files) etc.
Step 2: In this step MOVE which is a part of SUMO is used to convert SUMO output files to .tcl (mobility file), because NS2 could run only these .tcl files.
Step 3: Now NS2 uses .tcl file to show communication environment using .nam file on nam console. Second file generated by NS2 is .tr (trace file) which contains various parameters of simulation like generated TCP packets, dropped packets, sending time, receiving time and many other aspects of communication. In NS2 we can apply different routing protocols on traffic (AODV, AODV-FN in our case).
Step 4: Now from output files of NS2 we can see communication by .nam using nam console or can generate different results from .tr file.
In this way by applying AODV and new routing protocol AODV-FN we have generated various .tr files and then used them to conclude results for routing load, advertisements generated and packet delivery ratio. These results are shown in next section. In NS-2 parameters given below in table 1 are used to compare results of AODV and new routing protocol AODV-FN. 
Each simulation result is compared for different number of nodes 24, 40 and 60. Different numbers of node are taken to check performance for different traffic conditions. Comparison has done in terms of routing load percentage, number of advertisements generated and packet delivery ratio.
Routing packets are number of routing information messages generated by a protocol to manage communication. These are used to check availability of neighbors and error reporting messages. Now advertisements are number of routing messages generated by protocol during total simulation time. On the other hand, routing load is calculated from number of advertisements divided by total number of messages (routing messages + data messages) generated during simulation time. It is clear from above definition that routing load percentage should be less for better performance of protocol Figure 4 routing load of AODV-FN is less in every case for different number of nodes. For 24 nodes and 60 nodes it is below 1%, it means our new routing protocol will provide more bandwidth for data packets which will improve the communication among nodes without suffering from any bandwidth limitation. Due to less routing load in AODV-FN packet delivery ratio of new protocol has increased as compared to AODV. Below given Figure 5 shows packet delivery ratio for both these protocols for different number of nodes. For 24 nodes and 60 nodes AODV-FN's PDR is more by approximately 6% and 9%, which is huge difference as compared to AODV. Results of advertisements, routing load and packet delivery ratio show how by reducing RERR messages in old protocol our performance has been improved. 
This paper has presented an enhanced version of existing MANET routing protocol AODV by reducing neighbor expiry time, which shows dramatic improvements in performance of old routing protocol. Due to less expiry time for neighbors source nodes are using the broken paths very less, because after a short time it assumes that its neighbor has been moved to another location and it invalidates all paths to various destinations through that neighbor. Now each time originator of the message finds new trustworthy neighbors and sends packets via them without any error. In this way we have designed AODV-FN, a new routing protocol that has less routing overheads and generating fewer advertisements than AODV. Packet delivery ratio of new routing protocol is also improved.
In future we can enhance the performance of this protocol to higher peaks by adding new features according 802.11 MAC layer. Researcher can also suppress HELLO messages and can stop their blind broadcasting. These enhancements can further improve the performance of AODV.
Information Systems (IS) are now utilized in a wide range of application areas, and correspondingly diverse types of systems have emerged and various methodological approaches to IS design have been developed. Since automation of office work is one of the fastest growing application areas of IS [3, 11] , many research efforts have been conducted during the last few years to provide some guidelines with systematic foundations for the design of Office Information Systems (OIS) [1, 4, 6, 8, 13, 14, 15, 17, 19, 24-26, 29, 31] . In developing new methodological approaches to OIS design, identification and consideration of the different facets and original aspects of the office environment constitute a fundamental issue [20, 21] . These original features are particularly relevant in the early phases of the design process, commonly called the "conceptual" or "logical" phases.
The purpose of this paper is to provide a review of existing methodologies and models for OIS conceptual design, stressing their characteristic aspects and comparing their features. A discussion of areas of interest related to office models G. Bracchi and B. Pernici can also be found in [22] , where office models are classified according to different views of the office, or according to the diverse aspects of office work, into the following categories: procedural models, information-flow models, database models, decision-making models, and behavioral models. This paper emphasizes the first three categories.
In Section 2 the main concepts related to OIS design methodologies are discussed, and, in Section 3, some sample OIS design methodologies described. In performing office requirements specification, the design methodologies make use of conceptual models that are usually specifically conceived for OIS. These models are examined, described, and classified in Section 3, together with the design methodologies. Finally, in Section 4, the required features for OIS design methodologies and models are discussed and the previously classified techniques evaluated.

The conceptual design of an office information system constitutes a complex task. In fact, in designing an OIS, it is first necessary to define office and business goals, which are not always evident, in order to understand what the work globally performed in the office is, and how the system to be designed will affect the work. The analysis of office work that must be carried out in order to gain such knowledge is also complex, due to the nature of office work itself, which consists of a large number of operational and decision support activities--in general, loosely structured--with many possible anomalies.
The goals of a methodology for office information system conceptual design are thus manifold. The main aim is to make the OIS design process easier and more reliable. In fact, it is recognized that even if it does produce a considerable overhead during the system design process, an analysis methodology usually increases the rationality, quality, and maintainability of office systems implementation. It does so by providing conceptual and practical guidance to the analyst in understanding an existing office and in designing the corresponding computer-assisted procedures [26] .
Thus, the first goal of an OIS conceptual methodology is that of obtaining a description of the office. A complete and formal description of all aspects of office work is unfeasible owing to the large number of exceptions, special cases, shortcuts, and so on, which are hard to identify and formalize. However, the model used in a methodology should describe as many aspects of the office as possible in an unambiguous way. This description should be useful and comprehensive not only to the analyst or system designer, but also to the people in the office (users, managers or OIS planners) who requested (or are affected by) the implementation of the system, enabling them to validate the system, suggest modifications, and identify inconsistencies.
A second aim of conceptual methodologies is to locate those functions in office work that are only loosely related to the goals of the enterprise. Analysis of these functions allows us to classify them into two basic categories: those which are not related to actual office work, but are still necessary for social and organiza-tional reasons, and those functions which really require reconsideration. Existing office models usually concentrate on the latter; for instance, it may be that some tasks that are no longer useful in achieving current office goals are still performed only because of habit. It is then necessary to identify and correct these procedures before implementation of a new system. Social functions must also be predicted, since they will very likely have to be modified according to the modified nature of the activities.
Finally, a conceptual methodology should act as guide in providing some technical solutions suitable to the office under study that take into consideration the set of analyzed problems. In addition to this, a methodology should also offer some indication of the criteria to follow in evaluating possible solutions and in choosing design tools, and should provide direct interfaces to the subsequent implementation phases.
A fundamental element of a methodology is the type of office conceptual view that is adopted during the analysis of the office. Just as in conventional information systems design, several types of conceptual views, which lead to different approaches in the analysis of office work, can be considered. These views can be classified as technical, organizational, and socio-technical.
A technical view examines office work in great detail, considering the operations that are performed and measuring them, usually in terms of execution time. The goal of such an approach is to identify optimal methods to perform the work. However, what is measured is mainly the physical dimension of office work, in spite of its intellectual nature. This results in analyzing the data, which are manipulated, and in measuring productivity mainly in terms of throughputs, instead of considering global office performance.
In an organizational view, the global organizational structure of the office is considered and business goals are examined. Hence, the hierarchical structure of the organization is reflected in the office model.
Finally, a socio-technical view considers the office in terms of the tasks to be performed by each unit into which the enterprise is subdivided. Each unit performs some sort of control on the work, which must remain within a predefined schema. A unit has resources and memory on which to base present and future actions. A set of decision rules is used to perform controls and to take into consideration goals and constraints in the execution of tasks. Office work is usually not examined with the high level of detail reached by the technical approach, but is instead seen as a set of activities executed by several groups. The persons working inside each group are examined, together with the available resources. Thus it is possible to consider not only the technologies to be used, but also the operators who utilize them; the joint optimization of the technical and social aspects can lead to a more usable and thus a globally more effective system.
Some general approaches to OIS design are discussed above; in order to derive specific requirements for design methodologies, and owing to the peculiar nature of office work, it is useful to summarize how office systems differ from conventional IS. These differences must be reflected in the design process.
Office data. The conceptual models on which OIS design methodologies are based must be semantically rich and must consider all office elements, with their particular features. The types of data used in conventional IS, such as character, string, and numeric data, are not sufficient in the office environment, since other types of data, consisting of unstructured data contained in messages, letters, texts, annotations, graphics, and oral communications, are currently employed and must be supported. A characteristic of office data is that information elements are most frequently used together in groups (as in independently stored documents), instead of being manipulated as single pieces of information put together into a form; for instance, it is often useful to store in its completeness a document of type "letter."
The time factor. Another aspect essential in the specification of office elements is the time factor, needed to determine the life time of documents or of single operations, or to specify the duration of activities and timing constraints; it is used for scheduling activities, calendar functions, and performing control operations on the OIS. Time data must be defined in the system together with all the operators that can be applied to it. Flexibility is also needed in the definition of time constraints, which are usually rather lax. For instance, a business letter must be answered within an approximate time interval.
Office activities. Not only data are unstructured in OIS, but also some of the activities that are performed on the data. The same action can be performed in several ways, provided that the obtained result is the required one. Most office activities, moreover, are performed once some form of instruction has been given to the worker who is to perform it. This instruction is usually incomplete and, to perform the work, knowledge, which is only implied in the instructions, is needed. Flexibility in performing activities is essential for achieving office goals, due to the high number of possible abberations. Every abberation that can arise during office work should be supported by the system, at least in a minimal way; for instance, it is necessary to determine what to do if a person needed for a certain decision is absent.
Complexity in an OIS is higher than in a conventional IS design for operational activities. There are a large number of elements in each office which are related through several connections. In general, the elements that are needed to perform office work are distributed among several office workers in the same or in different departments, and can also be located externally to the office environment. Thus one of the main office functions is communication among office workers and with the external world [28] .
Office evolution. Another aspect that must be considered in the analysis of OIS elements is the inherently high level of evolvability. The most unstable aspects of an office are the constraints defined on its elements; for instance, the authorization for a worker to perform an operation. Activities can also be changed, in time, due to evolving ways to process information, for instance. Document types can also vary (although at a lower frequency) owing to new legal requirements. Usage characteristics. OIS are highly interactive: the interface with the users should be adequate to the kind of work to be performed, and dialog with the system should be possible for even the casual user. In fact, OIS usage implies a direct contact between office workers and the system. Users tend to be nonprofessional and very diversified. Thus the need to design a user-friendly system, which requires the development of functions for high-level users also; for instance, decision support facilities should be made available to those at the top management level. Another implication is that some knowledge roles in the office are replaced by the use of OIS.
Filtering. One of the functions of an OIS is filtering the large amount of data in order to provide office workers with specific information. Filtering can be useful, for instance, in both discarding uninteresting mail or in handling it automatically, and in selecting data that must be analyzed to make a decision.
Reminding. In conventional offices, the arrangement of papers, books, and notes on the desk and on office shelves has the function of reminding workers of activities to be performed and their different priorities [18] . In an automated office, where most of the information is hidden from the user, the system must provide the function of reminding--which, in conventional offices, is supported by the allocation of papers. This function can be improved by the automatic scheduling of activities (tasks to be performed, meetings to attend, and so on).
based on cooperation among the elements in the office, to achieve the goal (not always well defined) of an efficiently operating system. Typically, the work is done by different workers in distinct steps. Various functions such as communication, data processing, information manipulation and retrieval, personal assistance, and task management are linked together and used by the same worker in a fast sequence (these functions may also interrupt each other).
are not yet established and are evolving quickly; hence, office methodologies should be as independent as possible of implementation details, particularly in the early phases of OIS design. On the basis of the features of an OIS that have been outlined in this section, some well-known office conceptual methodologies and models are described and evaluated in the rest of this paper.
The design phases of the OIS are similar to phases that can commonly be found in conventional IS methodologies for the design of databases and information systems (see Figure 1 ) [23] . In the requirements analysis phase, the office work is studied and requirements are then formally specified, using a conceptual model of the office. Another phase is concerned with the optimization and implementation of systems. The evaluation of the currently running system and of the new requirements produces subsequent modifications.
Each OIS design phase shows many differences when compared to conventional IS design, since the original characteristics of office systems (summarized in Section 2) have to be taken into consideration. These differences are particularly 
Phases of the OIS design process.
extensive and meaningful in the early, conceptual design phases, namely in requirements analysis and in requirements specification. Thus the focus of this paper is on these two early phases. In the literature, the problem of formally specifying office elements in a model has been investigated in more depth than that of proposing a complete methodological approach to OIS design. Hence, there are few methodologies in existence that offer complete frameworks, instead they emphasize only a subset of the OIS design phases. But, on the other hand, many conceptual office models exist on which the requirements specification phase can be based, and some of them also consider the subsequent step of the logical optimization of office description.
Even if the models are not explicitly included in a complete methodological framework, they still often assume a specific methodological approach. In particular, the requirements analysis phase can be based on approaches also common to conventional IS design, such as technical analysis, decision analysis, social or behavioral analysis, and prototype implementation for experimention [9] ; correspondingly, some elements in the models are emphasized more than others.
In this section, several OIS design methodologies are briefly illustrated and some of the most relevant conceptual models are outlined and classified. All of the presented models and methodologies are considered independently of their specific usage environments. In fact, it should be possible to distinguish between models used for procedurally describing office work and models which are implementation independent [15] . The latter are based on languages which are designed as documentation tools, and describe office functions in an implementation-independent way. The former (procedural) models focus on current task structure, rather than functions. This paper considers both types of models together, since the distinction is not sharp, and languages used for documentation can sometimes be used for specification of prototypes also, or even of the final system.
Three sample OIS design methodologies are illustrated here: OFFIS, Office Analysis Methodology (OAM), and MOBILE-Burotique. These three methodologies are not really directly comparable: their purpose and phases of application in the design process are different, as illustrated in Figure 2 . Moreover, while OAM and OFFIS are specific methodologies, MOBILE-Burotique is more of a meta-methodology, which provides ingredients for a real methodology.
OFFIS. OFFIS is a system designed to facilitate an interactive and iterative office analysis and design process, providing the planner/designer of the automated office with a flexible method for analyzing system features and constraints [16] . OFFIS was developed at the Department of Management Information Systems at the University of Arizona. The system is based on the OFFIS model. The elements of the model can be specified in the OFFIS language by planners/ designers who are office personnel or managers. The language has a simple syntax and is nonprocedural.
In the specification of office requirements the system is divided into sections, which describe categories in the office (for instance, departments or types of documents). The elements specified with statements of the language describe requirements and constraints. These specifications lead to an incremental development of the office model. The OFFIS system, besides providing for model building, also provides the possibility of analyzing, through a set of rules, the consistency and completeness of the office system, and of obtaining the corresponding reports. The specifications are collected in the OFFIS database and used by the analyst (see Figure 3) . The requirements analysis is based on a technical approach to requirements collection. The basic elements in an OFFIS model are objects, attributes, and relations. Objects are static elements that describe data and agents. Attributes are related to objects. Relations describe interconnections among objects, with the main purpose of expressing possible operations. Relations can also express conditions, durations, and organizational hierarchical connections in terms of persons that must report to others. The information flow is expressed through the relations, and a weight is provided to measure their importance.
OFFIS methodology is principally based on a technical model in which offices and office elements are studied in great detail. However, it also takes into consideration some of the organizational divisions in the offices, thus enriching the technical approach to the design. Several design tools are provided, mainly for the analysis of the correctness of the conceptual model resulting from user requirements specification. Some specifications in the OFFIS language, extracted from [16] , are shown in Figure 4 .
Office Analysis Methodology. The Office Analysis Methodology (OAM) was developed by the Office Automation group at MIT [26] . It is based on the analysis of the activities performed in the organization. The goal of OAM is to understand office work in terms of functions, activities, flows, tasks, and so on, and to specify this knowledge in a systematic way. The methodology is directed to the analysis of semistructured problems at a managerial level in order to identify the business goals of the organization. The office analysis process investigates why functions are performed, what they do, and how they are implemented. A general schema describing how office analysis is conducted using OAM is shown in Figure 5 . The first step of the methodology consists of identifying and listing for each function its resources and tasks. The second step analyzes the procedures in the office without considering exceptions. The third step classifies exceptions into main categories. The fourth step identifies conflictual situations and ad hoc decisionmaking processes. The fifth step consists of a revision of the office model, involving validation of the model by office managers, definition of user interfaces, and general exception handling. The last step consists of preparation of documentation. EXTERNAL-NAME ACCOUNTANTS; Fig. 4 . The OFFIS language [16] . Example language specification. 
OAM approaches the office with an organizational model. Office functions are examined top-down; the office manager and planner are interviewed first, and then office activities are examined in greater detail, following the office hierarchical level. In contrast, the integration of the office system with the organization and the other systems is performed bottom-up. The results of the requirements analysis phase can be specified in the Office Specification Language (OSL). OSL [14] is a high-level and problem-oriented language, which embeds the concepts of hierarchical abstraction and resource. It provides the advantage of using a nonambiguous description of the office. This description is implementation independent, emphasizing office functions rather than specific' operational tasks. An OSL processing system which would automatically generate an office information system from the language description is under study.
The Organization-Methodology Group, part of the KAYAK project at INRIA (France), has established and experimented with the MOBILE-Burotique methodology for OIS design [10] . Human and organizational factors are considered together with the technical aspects of the design. The office issues to which the system must refer are productivity, economic assessment of the proposed technical solutions, and acceptance of the system by the organization and its office workers. These issues imply the following series of consequences for the technological aspect of the project: an OIS should be modular (that is, composed of integrated functional modules) and simple to use, and it should provide a series of measures of the work performed. MOBILEBurotique is a meta-methodology; in fact, it provides a series of types of instruments for observation and analysis. They can be classified into instruments for conducting an overall observation of the organization, instruments for analyzing the procedures performed in the organization, and instruments for supporting office analysis, such as simulation. Some constraints are given for the implementation of these instruments (i.e., their cost, the usefulness and reliability of the obtained results, the time required to use them, and their acceptability to the organization). The most suitable instruments can then be chosen via a costbenefit analysis. In this methodology there is an attempt to take into account social factors in system design. Moreover, it provides some guidance in each of the OIS design phases.
Office conceptual models can be classified, on the basis of the fundamental elements that they take into consideration, into the following main categories:
data-based models, process-based models, agent-based models, and mixed models.
In Table I , some well-known existing conceptual methodologies and models are grouped into the above categories (the classification of methodologies is derived from the type of the underlying conceptual model for requirements specification). Models are listed chronologically within each class, and are examined with respect to both general characteristics and to particular features.
Most of the models of Table I are formal, since they provide the formal description of at least some of the office elements. All models have the goal of describing office elements and activities, but some also offer features for guidance in restructuring activities. In Table I tive and analytical models. Descriptive models provide specification of office elements and office work, leaving to the analyst the task of restructuring office procedures, when necessary. Analytical models provide instead, together with office description, some facilities for supporting automatic restructuring of office procedures. For each model, Table I specifies the type of requirements analysis that is adopted (which can be technical, decisional, social, or based on a prototype). The last two columns of Table I synthesize the characteristics of the models, listing their basic elements with a few comments.
In the rest of this section, features held in common by each class of conceptual models will be discussed, and one of the models will be illustrated more extensively to provide a clarifying example.
Data-based models. Generally, data-based models group data into forms, which are similar to paper forms in the traditional office. Types of data and the operations on data (storage, retrieval, manipulation, transmission) are the basic elements of these office conceptual models. Office activities are then seen as a series of operations on data.
An important example of a data-based model is Office-by-Example (OBE), developed by Zloof [31, 32] . OBE is a language for describing and manipulating office objects of different kinds; it is an extension of a well-known query language fo relational databases (Query-By-Example) and of the System for Business Automation [30] . The data structures on which OBE is based are two-dimensional objects, in the form of tables, which can be relations, forms, reports, hierarchical structures, documents, menus, and so on. The system allows the user to define (program) its own objects and to use complex data types such as time, graphics, and text. OBE has evolved from the consideration of simple forms to the above mentioned types of two-dimensional structures, although keeping the aspect of objects similar to that of paper forms. In a prototype OBE system [32] , several basic office functions are defined with a unique language. Such functions include word processing, querying, automatic triggering of office activities on temporal conditions and/or events, data processing, authorizations, communication, and creation and manipulation of documents. An example of how a program can be written to send predefined letters to selected persons, upon certain conditions, is displayed in Figure 6 [32] .
The main purpose of data-based models is to represent the office from the viewpoint of objects manipulated by office workers (agents), in a way similar to traditional offices, where work is primarily based on documents. It is probably for this reason that the early, conceptual office models were mainly data-based, used to design office work-stations--where the work of a single user at a time is supported--connecting users through a communication network (see, for instance, OFFICETALK-ZERO [11] and OMEGA [5] ). The general work flow in these models is not under system control and single users are supported by the system in their individual activities only.
Process-based models analyze and describe office work by looking at different activities performed concurrently by the users and the system.
There are different types of models of this class: SCOOP, developed by Zisman [29] , is chronologically the first one, and provides a particulary clear illustration Trigger program to check SALES table and send letters [32] .
Basic SCOOP elements [29] .
) a2 al of the characteristics of process-based models. It applies to the office environment concepts already used in other areas for the analysis of concurrent processes, and is based on an extension of conditions/actions and Petri nets. In this formalism, certain conditions trigger the appropriate actions. Different conditions may be neces.sary to trigger an action, and a single action can cause several conditions. The conditions can be based on time (a certain date or the elapsing of a certain interval) and/or events. For instance, an initial condition triggers the execution of an activity whose termination is an event; this event, with the possible addition of other conditions, triggers new activities, thus establishing an order of execution for activities. Processes are described through Petri nets, where conditions (circles) are set as preconditions of activities (bars) to be triggered, as shown in Design Requirements of Office Systems
• 165
information necessary for the advancement of activities are specified in a system knowledge base through production rules. A production rule is generally of the following form: if (condition) then (body). The "if" part is a logical expression and the "body" specifies which operations are to be performed if the logical expression is true (operations such as activation of other rules, setting of parameters, and manipulations on data). Several other models of this category now exist; some of the more significant ones are listed in Table I . Also, the conceptual model on which the OAM methodology is based is a process-based model. In fact, office work in OAM is described in terms of functions, activities, tasks and flow of activities, where the resources used in the system are seen in relationship with activities. The goal of process-based models is that of representing office activities in a coordinated way: thus the approach is not founded (as in data-based models) on operations performed by single users, but instead on an integrated vision of all the activities performed in an office in Order to execute certain tasks, with the purpose of a general control of office work.
Agent-based models. An office can also be modeled from the viewpoint of the functions performed by active elements of the office environment (the agents). An agent-based model (for instance, the Structural Office Model [2] ) describes the office by associating to the different agents a set of functions (i.e., the different roles that they take in performing their tasks, the domain Within which they are authorized to act, and the set of relationships that link them to other agents). Every agent has also his or her personal data as a personal database. Hence, the description of office data and activities is dependent On a third element (besides data and processes) assumed as basic in the system; that is, the set of office workers and their organizational structure. Those actions that are performed automatically by the system are also considered as performed by particular agents. This approach is different from the other two because its goal is that of examining office workers' roles, the delegation of roles in the office, and so on; while data and activities are considered only in relation to their executors. Thus, owing t0 the evolution of and changes in office work, it is easier, in this type of model, to describe the structural modifications in the office.
Mixed models. Mixed models explicitly assume more than one type of element as the basis for system specification, and define the relationships among these elements. The SOS (Semantic Office System), developed at the Politecnico di Milano by the authors of this paper [7] , is an example of a mixed model. SOS classifies office elements into three different submodels: the static, the dynamic, and the evolution submodels. The static submodel contains the specification of data related to office work--basic types of static office elements ate documents, dossiers, and agents. The dynamic submodel contains the specification of operations and activities performed in the office. The evolution submodel specifies, through two sets of rules, both the normal evolution of office work and the possible structural modifications of office tasks. Some of the rules support office activities with information useful in performing operations or in making decisions, other rules trigger the automatic execution of some operations, still other rules definitively specify static and dynamic constraints on data and the authorizations to manipulate it. In the definition of all system elements, abstractions 27] are used to specify generalizations, aggregates, and associations of elements. The purpose is to specify office element semantics together with their structures. Most of the recent office models belong to the mixed category, since it provides more complete specification of different types of fundamental elements in the office and of their interrelationships. It should be noted that some of the models which in the early days belonged to one of the three categories mentioned above have since evolved into the mixed category. Examples of this transformation are OFFICETALK-D [12] ; a data-based model, OFFICETALK-ZERO [11] ; and a process-based model, Information Control Nets [8] .
In this section some requirements for methodologies and models for OIS design are illustrated, and the models presented in Section 3 are evaluated accordingly. OIS design methodologies can be evaluated according to different types of features, which, using a classification similar to the one used for conventional IS, can be included in one of the following groups: technical features, usage features, project management features, and economic evaluation features. Since this paper has for the most part considered technical models for OIS design and implementation, these models will now be evaluated according to a few technical features, with results that are particularly relevant for OIS, on the basis of the discussion in Section 2.
Technical features include technical characteristics of the methodology and/or model, their basic elements, and the way in which they are specified. For the sake of brevity, the features that are discussed here have been selected in order to focus on the most relevant and original aspects of the OIS conceptual design process, without, however, providing an exhaustive list of characteristics, including those common to conventional IS. In this way, the technical features specified for OIS models in Table II also constitute a set of requirements for evaluating the extensibility of a conventional IS model to OIS design. The features in Table II are concerned with types of data, types of activities, and functions. Data features deal with the categories of documents--particular document types for specific applications, operations on documents that extend the usual operations on simple data types, abstractions [27] , and unstructured data--with the time factor and the specification of events. Activity features deal with the specification of controls and with synchronizing mechanisms on activities performed by the system, the possibility of specifying unstructured activities (e.g., decision making) and abstractions on activities. Two important functions that an OIS should provide are communications and exception handling (the models should give primitives for them). Another important feature is the task level that can be reached in the description by using a certain model. All models describe elements at the operational level, some of them (including all the process-based models) provide a description at the management level, and a few provide the description of planning activities to be supported by the system. 
• 167 Table II shows that data types concerned with documents are usually well defined in data-based models, with the exception of abstractions, which are usually more carefully considered in recent mixed models. Data types concerned with control • G. Bracchi and B. Pernici aspects such as times and events are present only in the most recent data-based models, but are commonly dealt with in process-based and mixed models, where control data are fundamental in the definition of the control aspects of activities (which, on the other hand, are usually neglected by data-based models). Activities are poorly handled in pure data-based models, while process-based models allow a sophisticated description; some control of activities is permitted in the recent data-based models. Description of complex and unstructured activities is also possible in all of the most recent models that are based on processes, agents, and in those that are of the mixed type. The treatment of functions, however, is not as dependent on the type of model. While communication is a characteristic of data-based models (inherited by mixed models also), and exception handling is more characteristic of process-based models, there are other functions (not considered in Table II ) which are almost independent of the type of model. Some functions are concerned wtih support operations (such as support in development, use, and evolution of the system), others deal with decision support based on data and tools provided by the system. It can be observed that these functions are often provided by data-based models; in fact, because these models are oriented to those office activities that are performed by individual workers at a workstation, support functions are essential, since they actually support the basic activities of the individual user. On the other hand, other types of models, those based on a more integrated and general approach to office work, do not necessarily require any support for individual workers. There are other features that are important in an OIS: modularity in the specification of different types of elements in the model {on which the evolvement capability of the model depends); simplicity of specification, including comprehensibility; implementability of the system; and the existence of a graphic representation in the model.
Simplicity and implementability favor data-based models, which do not capture complex features of OIS, and are thus easier to implement and use. Graphics are not always used in OIS models; in fact, when the models specify a complex reality, their graphic description can become very difficult.
A summary is derived from the analysis and comparison of the models. It can be seen from Table II that models based on data provide the possibility of defining sophisticated data types for documents, while lacking primitives to describe events.
Models based on processes, on the other hand, allow precise specification of temporal conditions and events, but are often inadequate in the definition of data types; when this definition is used, it is often performed in a generic way.
Models based on agents perform well as far as activity and function definitions are concerned, but the strong dependence of all office elements on the one main definition (agents) causes a rather poor definition of all the other types of data. Mixed models show good technical features, but are still lacking in usage features and in graphic representation.
We conclude that future developments will increasingly integrate the characteristics of the first three types of models into new mixed models; however, such global representations still require wide research in order to be practically useful for supporting a complete OIS design methodology.
There are two main sorts of solution concepts for strategic games: equilibrium concepts and what might be called "effective" concepts. One interpretation of the equilibrium concepts, for example Nash equilibrium, tacitly presupposes that a game is played repeatedly (see, e.g. [13, page 14] ). Thus the standard condition for Nash equilibrium in terms of the knowledge or beliefs of the players [3] -the so-called "epistemic analysis" of Nash equilibrium -includes a requirement that players know the other players' strategy choices. Consider the left-hand game in Figure 1 , in which each player has two choices L and R and both players get payoff of 1 if they coordinate, and 0 otherwise. Then there are two Nash equilibria 1 : both play L or both play R. But this does not translate by itself into an effective strategy for either player reasoning in isolation, without some exogenous information.
In contrast, effective solution concepts, for example the iterated elimination of strictly dominated strategies, are compatible with such a "oneshot" interpretation of the game. Thus the epistemic analysis of the iterated elimination of strictly dominated strategies does not require that the players know each other's strategy choice.
A strategy s i is strictly dominated if there is an alternative strategy t i such that no matter what the opponent does, t i is (strictly) better for i than s i . Say that a player is sd-rational if he never plays a strategy that he believes to be strictly dominated. What the iterated elimination of strictly dominated strategies does in general require, see [4] , is then that players have common true belief that each other is rational, that is: they are rational, believe that all are rational, believe that all believe that all are rational, etc.
In the right-hand game in Figure 1 , the column player, on first looking at her choices L or R is, superficially, in the same situation as before: choose L and risk the opponent playing D or choice R and risk the opponent playing U . However, this time the row player can immediately dismiss playing D on the grounds that U will always be better, no matter what the column player does. So if the column player knows (or believes) this, then he cannot rationally play R, and so must play L.
In this paper we study the logical form of epistemic characterisation results of this second kind, so we give formal proof-theoretic principles to justify some given effective or algorithmic process in terms of common belief of some form of rationality. We will introduce two formal languages. One, L O , is a first-order language, that can be used to define 'optimality conditions'. Avoiding playing a strictly dominated strategy is an example of an 'optimality condition'. Another one is choosing a best response.
However, as observed in [2] for all such notions there are two versions: 'local' and 'global'. Notice that in our informal description of when s i is strictly dominated by t i we did not specify where i is allowed to choose alternative strategies from. In particular, since we are thinking of an iterated procedure, if t i has been eliminated already then it would seem unreasonable to say that i should consider it. That intuition yields the local definition; the global definition states the opposite: that player i should always consider his original strategy set from the full game when looking to see if a strategy is dominated.
A motivation for looking at global versions of optimality notions is that they are often mathematically better behaved. On finite games the iterations for various local and global versions coincide [1], but on infinite games they can differ. In a nutshell: an optimality condition φ i for player i is global if i does not 'forget', during the iterated elimination process, what strategies he has available in the whole game. The distinction is clarified in the respective definitions in L O .
An optimality condition φ induces an optimality operator O φ on the complete lattice of restrictions (roughly: the subgames) of a given game. Eliminating non-φ-optimal strategies can be seen as the calculation of a fixpoint of the corresponding operator O φ . Furthermore, common belief is characterised as a fixpoint (cf. Note 3 below). Viewed from the appropriate level of abstraction, in terms of fixpoints of operators, this connection between common belief of rationality and the iterated elimination of nonoptimal strategies becomes clear.
We define a language L ν that describes things from this higher level of abstraction. Each optimality condition defines a corresponding notion of rationality, which means playing a strategy that one believes to be φ-optimal. L ν is a modal fixpoint language with modalities for belief and optimality, and so can express connections between optimality, rationality and (common) belief.
We say that an operator O on an arbitrary lattice (D, ⊆) is monotonic when for all A, B ∈ D, if A ⊆ B then O(A) ⊆ O(B). The global versions of relevant optimality operators, in particular of the operators corresponding to the best response and strict dominance, are monotonic. This is immediately verifiable in L O by observing that the relevant definition is positive.
Our first result is a syntactic proof of the following result, where φ is a monotonic optimality condition: 2 Theorem 1 Common true belief of φ-rationality entails all played strategies survive the iterated elimination of non-φ-optimal strategies.
Although this theorem relies on a rule for fixpoint calculi that is only sound for monotonic operators, the semantics of the language L ν allows also for arbitrary contracting operators, i.e. such that for all A, O(A) ⊆ A. We are therefore able to look at what more is needed in order to justify the following statement (cf. [4, Proposition 3.10]), where gbr-rationality means avoiding avoiding strategies one believes to be never best responses in the global sense:
Theorem 2 (Imp) Common true belief of gbr-rationality implies that the players will choose only strategies that survive the iterated elimination of strictly dominated strategies.
This theorem connects a global notion of gbr-rationality with a local one, referred to in the iterated elimination operator. Our language allows for arbitrary contracting operators, and their fixpoints to be formed, and we exhibit one sound rule connecting the resulting fixpoints with monotonic fixpoints.
Our theorems hold for arbitrary games, and the resulting potentially transfinite iterations of the elimination process. The syntactic approach clarifies the logical underpinnings of the epistemic analysis. It shows that the use of transfinite iterations can be naturally captured in L ν , at least when the relevant operators are monotonic, by a single inference rule that involves greatest fixpoints.
The relevance of monotonicity in the context of epistemic analysis of finite strategic games has already been pointed out in [5] , where the connection is also noted between the iterated elimination of non-optimal strategies and the calculation of the fixpoint of the corresponding operator.
To our knowledge, although several languages have been suggested for reasoning about strategic games (e.g. [7] ), none use explicit fixpoints (except, as we mentioned, for some suggestions in [5] ) and none use arbitrary optimality operators.
Therefore they are not appropriate for reasoning at the level of abstraction that we suggest when studying the epistemic foundations of these "effective" solution concepts. For example while [7, Section 13] does provide some analysis of the logical form of the argument that common knowledge of one kind of rationality implies not playing strategies that are strictly dominated, the fixpoint reasoning is done at the meta-level. What [7] provides is a proof schema, that shows how, for any finite game, and any natural number n, to give a proof that common knowledge of rationality entails not playing strategies that are eliminated in n rounds of elimination of non-optimal strategies.
The more general and elegant reasoning principle is captured by using fixpoint operators and optimality operators. Another important advan-tage to our approach is that we are not restricted in our analysis to finite games. This means in particular that our logical analysis covers the mixed extension of any finite game.
Our use of transfinite iterations is motivated by the original finding of [12] , where a two-player game is constructed for which the ω 0 (the first infinite ordinal) and ω 0 + 1 iterations of the rationalizability operator of [6] differ.
A strategic game is a tuple (T 1 , . . . , T n , < 1 , . . . , < n ), where {1, . . . , n} are the players and each T i is player i's set of strategies, and < i is player i's preference relation, which is a total linear order over the set of strategy profiles T = n i=1 T i . Note that we assume arbitrary games, rather than restricting to games in which T is finite. To depict games it is sometimes easier, as we did in Figure 1 , to write down a number for the players' "payoffs", rather than just a preference ordering. We use some standard notation from game theory, writing s −i for (s 1 , . . . s i−1 , s i+1 , . . . s n ) and (s i , t −i ) for the strategy profile (t 1 , . . . t i−1 , s i , t i+1 , . . . s n ), as well as S −i for j =i S j . A restriction of the game (T 1 , . . . , T n , < 1 , . . . , < n ) is a sequence S = (S 1 , . . . , S n ) with S i ⊆ T i for all players i, i.e. a (possibly empty) subgame in which the payoff information is left out.
The language we use for specifying optimality conditions is a firstorder language, with variables V = {x, y, z, . . .}, a monadic predicate C, a constant o and a family of n ternary relation symbols · ≥ i · ·, where i ∈ [1..n]. So L O is given by the following inductive definition:
We use the standard abbreviations → and ∨, further abbreviate ¬a
An optimality model (G, G ′ , s) is a triple consisting of a strategic game G = (T 1 , . . . , T n , < 1 , . . . , < n ), a restriction G ′ of G, and a strategy profile s ∈ T . G will be used to interpret the predicate C, and s will be the interpretation of o. An assignment for (G, G ′ , s) is a function α assigning a strategy profile in T to each variable, and s to o. The ternary satisfaction relation |= between optimality models, assignments and formulas of L O is defined inductively as follows, where α is an assignment for (G, G ′ , s), and |= the complement of |=:
A variable x occurs free in φ if it is not under the scope of a quantifier ∃x; a formula is closed if it has no free variables. An optimality condition for player i is a closed L O -formula in which all the occurrences of the atomic formulas a ≥ j c b are with j equal to i. Intuitively, an optimality condition φ i for player i is a way of specifying what it means for i's strategy in o to be an 'OK' choice for i given that i's opponents will play according to C −i and that i's alternatives are C i .
In particular, we are interested in the following optimality conditions:
The optimality conditions listed define some fundamental notions from game theory: lsd i says that o i is not locally strictly dominated in the context of C; gsd i says that o i is not globally strictly dominated in the context of C; and gbr i says that o i is globally a best response in the context of C.
The distinction between local and global properties, studied further in [2] , is clarified below. It important for us here because the global versions, in contrast to the local ones, satisfy a syntactic property to be defined shortly.
First, as an illustration of the difference between gbr i and gsd i , consider the game in Figure 2 . Call that game H, with the row player 1 and the column player 2. Then we have
We say that an optimality condition φ i is positive when any subformula of the form C(z), with z any variable, occurs under the scope of an even number of negation signs (¬). Note that both gbr i and gsd i are positive, while lsd i is not. As we will see in a moment, positive optimality conditions induce monotonic optimality operators, and monotonicity will be the condition required of optimality operators in Theorem 1 relating common knowledge of φ-rationality with the iterated elimination of non-φ strategies.
Henceforth let G = (T 1 , . . . , T n , < 1 , . . . , < n ) be a fixed strategic game. Recall that a restriction of the game G is a sequence S = (S 1 , . . . , S n ) with S i ⊆ T i for all players i. We will interpret optimality conditions as operators on the lattice of the restrictions of a game ordered by component-wise set inclusion:
Given a sequence φ giving an optimality condition φ i for each player i, we introduce an optimality operator O φ defined by
Consider now an operator O on an arbitrary complete lattice (D, ⊆) with largest element ⊤. We say that an element S ∈ D is a fixpoint of
We define by transfinite induction a sequence of elements O α of D, for all ordinals α:
We
Not all operators have fixpoints, but the monotonic and contracting ones (already defined in the introduction) do: Proof. For (i), it is enough to know that for every set D there is an ordinal α such that there is no injective function from α to D.
Note that the operators O φ are by definition contracting, and hence all have outcomes. Furthermore, it is straightforward to verify that if φ i is positive for all players i, then O φ is monotonic.
The following classic result due to [14] also forms the basis of the soundness of some part of the proof systems we consider. 3 
where νO is the largest fixpoint of O.
We shall need the following lemma, which is crucial in connecting iterations of arbitrary contracting operators with those of monotonic operators. It also ensures the soundness of one of the proof rules we will introduce.
Proof. By Note 1(i) the outcomes of O 1 and O 2 exist. We prove now by transfinite induction that for all α
from which the claim follows, since by Note 1(iii) we have
By the definition of the iterations we only need to consider the induction step for a successor ordinal. So suppose the claim holds for some α.
The second assumption implies that O 1 is monotonic. We have the following string of inclusions and equalities, where the first inclusion holds by the induction hypothesis and monotonicity of O 1 and the second one by the first assumption
Recall that G is a game (T 1 , . . . , T n , P 1 , . . . , P n ). A belief model for G is a tuple (Ω, s 1 , . . . , s n , P 1 , . . . , P n ), with Ω a non-empty set of 'states', and for each player i, s i : Ω → T i and P i : Ω → 2 Ω . The P i 's are possibility correspondences cf. [4] . The idea of a possibility correspondence P i is that if the actual state is ω then P i (ω) is the set of states that i considers possible: those that i considers might be the actual state. Subsets of Ω are called events. A player i believes an event E if that event holds in every state that i considers possible. Thus at the state ω,
Given some event E we write G E to denote the restriction of G determined by E:
In the rest of this section we present a formal language L ν that will be interpreted over belief models. To begin, we consider the simpler language L, the formulas of which are defined inductively as follows, where i ∈ [1..n]:
with φ i an optimality condition for player i. We abbreviate the formula i∈[1..n] rat φ i to rat φ , i∈[1..n] i ψ to ψ and i∈[1.
Formulas of L are interpreted as events in (i.e. as subsets of the domain of) belief models. Given a belief model (Ω, s 1 , . . . , s n , P 1 , . . . , P n ) for G, we define the interpretation function · : L → P(Ω) as follows:
gives the set of states that i considers possible at ω, so rat φ i is the event that player i is φ i -rational, since it means that i's strategy is optimal according to φ i in the context that the player considers it possible that he is in. The semantic clause for i was mentioned at the begin of this section and is familiar from epistemic logic:
i ψ is the event that player i believes the event ψ . O φ i ψ is the event that player i's strategy is optimal according to the optimality condition φ i , in the context of the restriction G ψ .
Then clearly rat φ is the event that every player i is φ i -rational; O φ ψ is the event that every player's strategy is φ i -optimal in the context of the restriction G ψ ; and ψ is the event that every player believes the event ψ to hold.
Although L can express some connections between our formal definitions of optimality rationality and beliefs, it could be made more expressive. The language could be extended with, for example, atoms s i expressing the event that the strategy s i is chosen. This choice is made for example in [7] , where modal languages for reasoning about games are defined. The language we introduce is not parametrised by the game, and consequently can unproblematically be used to reason about games with arbitrary strategy sets.
We will use our language to talk about fixpoint notions: common belief and iterated elimination of non-optimal strategies. Let us therefore explain what is meant by common belief . Common belief of an event E is the event that all players believe E, all players believe that they believe E, all players believe that they believe that. . . , and so on. Formally, we define CB(E), the event that E is commonly believed, inductively:
Notice that B 1 (E) is the event that everybody believes that E (indeed, we have B 1 ψ = ψ ), B 2 (E) is the event that everybody believes that everybody believes that E, etc.
'Common belief' is called 'common knowledge' when for all players i and all states ω ∈ Ω, we have ω ∈ P i (ω). In such a case the players have never ruled out the current state, and so it is legitimate to interpret i ψ as 'i knows that ψ'.
Both common knowledge and common belief are known to have equivalent characterisations as fixpoints, and we will exploit this below in defining them in the modal fixpoint language which we now specify.
We extend the vocabulary of L with a single set variable denoted by X and the contracting fixpoint operator νX. (The corresponding extension of first-order logic by the dual, inflationary fixpoint operator µX was first studied in [8] .) Modulo one caveat the resulting language L ν is defined as follows:
The caveat is the following:
-φ must be ν-free, which means that it does not contain any occurrences of the νX operator.
This restriction is not necessary but simplifies matters and is sufficient for our considerations.
To extend the interpretation function · to L ν , we must keep track of the variable X. Therefore we first extend the function · : L → P(Ω) to a function · | · : L ν × P(Ω) → P(Ω) by padding it with a dummy argument. We give one clause as an example:
We use this extra argument in the semantic clause for the variable X:
Those formulas whose semantics we have so far given define operators. More specifically, for each of them ψ | · is an operator on the powerset P(Ω) of Ω. We use this to define the clause for νX:
When X does not occur free in ψ, we have ψ | E = ψ | F for any events E and F , so in these cases we can write simply ψ . Note that νX.ψ is well-defined since for all E we have ψ ∧ X | E = ψ | E ∩ X | E ⊆ E, so the operator ψ ∧ X | · is contracting.
We say that a formula ψ of L ν is positive in X when each occurrence of X in ψ is under the scope of an even number of negation signs (¬), and under the scope of an optimality operator O φ i only if φ i is positive.
Note 2. When ψ is positive, the operator ψ | · is monotonic.
Then by Tarski's Fixpoint Theorem and Note 1(iii) we can use the following alternative definition of νX.ψ in terms of post-fixpoints:
Let us mention some properties the language L ν can express. First notice that common belief is definable in L ν using the νX operator. An analogous characterization of common knowledge is in [9, Section 11.5]. From now on we abbreviate the formula νX. (X ∧ ψ) with ψ a formula of L to * ψ. So L ν can define common belief. Moreover, as the following observation shows, it can also define the iterated elimination of non-optimal strategies. Note 4. In the game determined by the event νX.O φ X , every player selects a strategy which survives the iterated elimination of non-φ-optimal strategies.
Proof. It follows immediately from the following equivalence, which is obtained by unpacking the relevant definitions:
Consider the following formula:
By Notes 3 and 4, we see that (1) states that: true common belief that the players are φ-rational entails that each player selects a strategy that survives the iterated elimination of non-φ-optimal strategies.
In the rest of this section we will discuss a simple proof system in which we can derive (1). We will use an axiom and rule of inference for the fixpoint operator taken from [11] and one axiom for rationality analogous to the one called in [7] an "implicit definition" of rationality. We give these in Figure 3 , where, crucially, ψ is positive in X, and all the φ i 's are positive. We denote here by ψ[X → χ] the formula obtained from ψ by substituting each occurrence of the variable X with the formula χ. Assuming given some standard proof rules for propositional reasoning, we add the axioms and rule given in Figure 3 to obtain the system P.
Axiom schemata
χ → νX.ψ νInd Fig. 3 . Proof system P A formula is a theorem of a proof system if it is derivable from the axioms and rules of inference. An L ν -formula ψ is valid if for every belief model (Ω, . . .) for G we have ψ = Ω. We now establish the soundness of the proof system P, that is, that its theorems are valid.
Lemma 2. The proof system P is sound.
Proof. We show the validity of the axiom ratDis:
Let (Ω, s 1 , . . . , s n , P i , . . . , P n ) be a belief model for G. We must show that rat φ → ( χ → O φ χ) = Ω. That is, that for any χ the inclusion rat φ ∩ χ ⊆ O φ χ holds. So take some ω ∈ rat φ ∩ χ . Then for every i ∈ [1..n], φ i (s i (ω), G P i (ω) ), and P i (ω) ⊆ χ . So by monotonicity of φ i , φ i (s i (ω), G χ ), i.e. ω ∈ O φ i χ as required.
The axioms νDis and the rule νInd were introduced in [11] ; they formalise, respectively, the following two consequences of Tarski's Fixpoint Theorem concerning a monotonic operator F :
Next, we establish the already announced claim. Theorem 1. The formula (1) is a theorem of the proof system P.
Proof. The following formulas are instances of the axioms ratDis (with ψ := * rat φ ∧ rat φ ) and νDis (with ψ := (X ∧ rat φ )) respectively:
Putting these two together via some propositional logic, we obtain
which is of the right shape to apply the rule νInd (with χ := * rat φ ∧rat φ and ψ := O φ X). We then obtain
which is precisely the formula (1).
It is interesting to note that no axioms or rules for the modalities or O were needed in order to derive (1), other than those connecting them with rationality. In particular, no introspection is required on the part of the players, nor indeed is the
In the language L ν , the rat φ i are in effect propositional constants. We might instead define them in terms of the i and O φ i modalities but to this end we would need to extend the language L ν . One way to do this is to use a quantified modal language, allowing quantifiers over set variables, so extending L ν by allowing formulas of the form ∀Xϕ. Such quantified modal logics are studied in [10] . It is straightforward to extend the semantics to this larger class of formulas:
In the resulting language each rat φ i constant is definable by a formula of this second-order language:
The following observation then shows correctness of this definition. To complete our proof-theoretic analysis we augment the proof system P with the following proof rule where we assume that χ is positive in X, but where ψ is an arbitrary ν-free L ν -formula:
χ → ψ νX.χ → νX.ψ Incl
The soundness of this rule is a direct consequence of Lemma 1.
To formalize the statement Imp we need two optimality conditions, gbr i and lsd i .
To link the proof systems for the languages L O and L ν we add the following proof rule, where each φ i and ψ i is an optimality condition in L O , and O φ X → O ψ X is a formula of L ν .
The soundness of this rule is a direct consequence of the semantics of the formulas O φ X and O ψ X. We denote the system obtained from P by adding to it the above two proof rules and standard first-order logic rules concerning the formulas in the language L O , like ∃y ∀xφ ∀x ∃yφ by R. We can now formalize the statement Imp as follows:
(rat gbr ∧ * rat gbr ) → νx.O lsd x.
The following result then shows that this formula can be formally derived in the considered proof system. Theorem 2. The formula (5) is a theorem of the proof system R.
Proof. The properties gbr i are monotonic, so the following implication is an instance of (1):
(rat gbr ∧ * rat gbr ) → νx.O gbr x.
Further, since the implication gbr i → lsd i holds, we get by the Link rule νx.O gbr x → νx.O lsd x, from which (5) follows.
Corollary 2. The formula (5) is valid.
We have studied the logical form of epistemic characterisation results, for arbitrary (including infinite) strategic games, of the form "common knowledge of φ-rationality entails playing according to the iterated elimination of non-φ ′ properties". A main contribution of this work is in revealing, by giving syntactic proofs, the reasoning principles involved in two cases: firstly when φ = φ ′ (Theorem 1), and secondly when φ entails φ ′ (Theorem 2). In each case the result holds when φ is monotonic. The language L ν that we used to formalise this reasoning is to our knowledge novel in combining optimality operators with fixpoint notions. Such a combination is natural when studying such characterisation results, since common knowledge and iterated elimination are both fixpoint notions. The language L ν is parametric in the optimality conditions used by players. It is therefore built on the top of a first-order language L O used to define syntactically optimality conditions relevant for our analysis.
The government of the United Kingdom is currently reaffirming its promise to link every school, college, university, and public library in the country to the "national grid for learning" at a cost in excess of £100 million (Howells, 1998) . These links will be free, and access to all who require it guaranteed. Every child will have his or her own e-mail address to access learning resources worldwide. As well as this access, the Prime Minister is 'online' so that the public can pose questions electronically and, presumably, expect a reply.
This type of initiative together with the exponential increase in the availability of information and communications technologies (ICT) has created opportunities for teachers to exploit a new tool. However, the research base for exploiting these new tools, as well as examples of good professional practice, is in its infancy and still requires considerable thought and empirical investigation (Barnard, 1998) .
In the world of education the burgeoning in the availability of ICT has created exciting opportunities for its exploitation. However, ICT, like all new tools, provides a challenge to established thinking. The attractiveness of ICT to education is a two-edged sword. While it may lead to the provision of additional resources for education, it also leads to expectations that are often problematic to deliver (Bottino, Forcheri, & Molfino, 1998) . The educational advantages claimed for ICT are often not translated into meaningful learning activity (Barnard, 1998) , particularly in the specialized field of what can be termed school technology, and it is important, therefore, that these advantages are identified and justified by practitioners as well as being explained through learning theory. Technological developments occur in two ways: (a) as a solution to a known problem or (b) as a spin-off from other research and a search is made to find uses for them; technology becomes available and then opportunities are sought to employ its potential in the classroom. This approach is not the most appropriate strategy; it is frequently ineffective and sometimes leads to the early discarding of a potentially useful tool. The reasons for this vary. It could be that technologies have not been sufficiently developed and that they are being used before teething problems have been rectified. Or it may be that they are too "sophisticated" for the job at hand. In other words, the teaching and learning strategies may be over-engineered and busy teachers have no time to de-bug software or persevere with inefficient approaches.
It is, therefore, essential that as well as providing a new tool we should try to explain its application within current learning theories (Wild & Quinn, 1998 ) so that we are using it from a position of authority, based upon a sound knowledge base, and not relying upon serendipity. In other words, the design and manufacture of learning materials, particularly when utilizing new technologies, should be purpose built and not be media led (Dyne, Taylor, & Boulton-Lewis, 1994) .
And within technology education we have even more particular issues to address. Technology within the context of education can be described and defined in several ways. For many it has to do with using technology to enhance the efficiency of the educational process. Within this context is the use of the personal computer and all that goes with it: its use as a word processor, highly efficient calculator, database, and communication system. In other words, the personal computer becomes a library and
George Shield access system to the world's store of knowledge as well as a manipulator of that knowledge.
A slightly different justification for technology education is the understanding that one can acquire knowledge of a range of other subjects through the study of technology. For example, by building a model bridge in cardboard or wood or even modeling it on a personal computer, learners will apply mathematical skills and understand scientific concepts through their application (McCormick & Murphy, 1998) . Some would take this argument further and claim that other more ephemeral attributes such as communication skills are gained through technologists explaining their solutions to others, and ethical and moral problems are confronted by debating controversial issues.
While these two descriptions of the value of technology education are both valid and widely held, the most common view is that technology education should be about the acquisition of a thorough grounding in technological principles. This understanding of the activity features prominently in the curriculum of schools through the requirements of the National Curriculum in the United Kingdom and similar directives and recommendations in other countries (e.g., Botswana Ministry of Education, 1996;  International Technology Education Association (ITEA), 1997). While the other considerations, those of "learning through technology" are more general imperatives, they are rarely addressed as prime objectives by technology teachers.
If broad definition that this article was written.
While it should be remembered that a number of other curriculum subject areas also lay claim to a wide range of desirable attributes with problem solving, social awareness, and knowledge acquisition featuring prominently in their aims and objectives, technology education is perhaps unique in that the results of such problem-solving activity is often translated into tangible artifacts or solutions.
A further factor to note is that such practical activity is not necessarily employed to explain a scientific concept or justify an aesthetic principle, but one that constructs the technological reality of schoolroom learning. It transforms scientific experiments with string and meter rules into machines found on building sites or dockyards and applies the aesthetic principles used in art studios to the creation of functional and attractive artifacts. Involvement in designing and making activities thus enables a number of technological concepts to be established or existing ones enhanced.
This demand may be said to require of teachers and learners in the subject area a much wider range of skills (both professional and pedagogical) than are often expected elsewhere in the curriculum.
Education is seen, certainly at its higher levels, to be concerned with developing the ability to explain and predict the outcomes of innovative situations as they occur (Wild & Quinn, 1998) . This ability is necessary to solve problems and comes from a combination of experiential and academic learning and is acquired through the skill of being able to make appropriate judgments based on personal reflection. Another common definition of learning, which stems from behaviorist theory, is that learning takes place when a relatively permanent change in behavior occurs. This definition takes the word behavior to mean any observable change that takes place. In other words, if someone can now do something (e.g., remember a fact, demonstrate a skill, perform an operation) that he or she couldn't do before, it is said that learning has taken place. Behaviorism underpins much learning that takes place formally and informally and has also led to a great deal of current educational practice in assessment and evaluation. Constructivists attempt to explain the principles of learning by encompassing the understanding that knowledge is constructed by the learner in the context of his or her environment. It is therefore acquired when the learner actively tries to make sense of new experiences based upon his or her previous understanding (Bruner, 1972) .
Sociocultural theories rely heavily upon the value of communication in the learning process (Meadows, 1998) . This can be between teacher and learner in the formal sense, but it may also be between peers and others that occurs within a normal social context. Language is therefore extremely important to allow for successful interaction and, hence, learning to take place. The principles behind scaffolding and the zone of proximal development (ZPD) fall within these theories (Gredler, 1992; Kincheloe & Steinberg, 1993; Tharp & Gallimore, 1988) .
These more complex theories of learning do not totally exclude behaviorist theories that propose the independence of knowledge from social and cultural influences, as such instrumental approaches are useful for understanding the basis of some teaching strategies, particularly those concerned with lower level skills (Atkins, 1993 Examples of these theories of learning and their application provided by McLoughlin and Oliver (1998) . These theories are very rarely used independently of each other to explain learning: Most skilled teachers are simply adept at knowing when and where to employ them, often subconsciously, to produce the most effective results.
Strategies based upon behaviorism can be used effectively for factual and rote learning, and teachers use this theory frequently by rewarding a learner with encouragement or other more tangible signs of approval. Such basic learning theories are also often used in programmed learning where a student is rewarded through an encouraging comment before moving on to the next learning objective.
It is in this type of learning that the use of ICT is immediately apparent. The computer games that are so highly addictive to teenagers are perfect examples of learning behavior being progressively rewarded as each level of the game is mastered. This learning is not restricted to the cognitive field in which the game is mastered but also in the area of psychomotor skills when the reflexes of learners are constantly refined to produce ever faster reactions to visual stimuli.
The student's mastering of basic technological terms, descriptions of components, and understanding of theory behind technical processes can be achieved through structured programs delivered through CD-ROMs or similar media. We can, therefore immediately see a place for ICT in technology education, both as a source of information and also, if structured effectively, a context or structure for learning simple skills and concepts.
Obviously, such teaching and learning strategies are not sufficient for all learning. They are, however, often needed at some time to service processes that will enable learners to acquire basic information to undertake higher order activities including problem solving (Atkins, 1993) .
Different learning objectives may require different teaching and learning strategies to achieve them. Some aspects of learning require basic low-level information as a preliminary activity before the more complex can be internalized. Often the rote learning of factual information is essential before a learner can be engaged in problem solving or those higher order activities deemed more desirable (Underwood & Underwood, 1990) . While behaviorism is said to have a number of views, this view of learning drives a lot of current educational practice where competencies and standards have become established indicators of achievement.
Thus, in technology education we have a subject that inherently has a philosophy that is overlaid by effective learning theories. By the very nature of students being involved in design and make tasks, they are learning through real-life contexts and strategies that in many other school subjects have to be artificially created. Therefore, we must think carefully before we change a learning experience, which can provide a worthwhile education in its own right. The links between teacher and taught are of crucial importance. Satisfactory learning often takes place when the teacher identifies where the learning blockage occurs. In other words, by determining where the difficulty is occurring in the student, the teacher can rectify the wrong concept or build upon work already understood. Based upon this understanding, Andaloro and Bellomonte (1998) suggested that a way forward is to use the computer to model the student's learning strategies to signify where they are likely to encounter difficulties in the future and thereby build up a learning program for each student, ensuring that the ICT programmers' emphasis recognizes that learners are different in the learning strategies they employ and the material is adapted accordingly. This activity can be used as a precursor to the Other Factors use of design packages and simulations. Much is currently being made of the use of computer-aided design and simulation packages to aid student problem solving, but without an understanding of the learning profile of the child (which the good teacher uses in traditional contexts), most of this activity may not be used to its best advantage and is directed to task achievement rather than the development of a learning skill (i.e., they may assist a student to design a specific product but not necessarily teach that student how to develop design skills).
Learning is a personal activity. It depends upon a series of factors that are often very difficult to control and manipulate. Some of these factors are related to the individual, including cognitive processing style and learning style. Some people learn better within a group situation; others by reading the printed word. Some learn through graphic symbols (Thompson, 1990 ); others through instruction. Other factors include the learning strategy employed (often controlled by the teacher) and the expected outcomes. Lord (1998) 
A number of learning tasks can be readily aided through the routine use of ICT. Simple skill acquisition, knowledge building, and modeling through simulations can give practice to aid creative development. However, when the tasks are related more to higher cognitive tasks, the benefits become more problematic. Passey (1998) suggested that in developing higher order skills, ICT has a more restricted role than with work in lower order domains, with the concomitant suggestion that work in the higher domains requires more in the way of teacher intervention. The continuum lies between the teacher being assisted by the technology and the teacher teaching to the technology.
Most learning theories have much in common, such as the need for motivation and consideration of individual differences in learners. Students are not all interested in the same topic, do not have the same physical or psychological characteristics, and do not come from the same environment. These individual differences are clearly evident in design and technology and particularly in their design project work (Atkinson, 1998; Wu, Custer, & Dyrenfurth, 1996) . This would indicate that the most effective way forward would be an individualized learning program for each student where the acquisition of knowledge, skills, and values could be tailored to each student's special needs. This student-centered approach in technology education, sometimes called the investigative learning approach (Sellwood, 1991) , creates heavy demands on the teacher and, consequently, it is often modified to ensure that it is manageable within the classroom context. The resulting curriculum and its implementation is a compromise between the resources available and what is required by the student (Barlex, 1993) . This concept of individualized and differentiated learning as an ideal methodology has strong advocates and yet is not very common in school (Thomas, 1992) because teachers, understandably, find it difficult to determine and meet the needs of both the better able and those with learning difficulties. They also find this methodology time consuming when working under pressure to transmit facts and achieve observable changes in behavior (Kyriacou, 1992 ) such as design folios or technological reports and records for assessment purposes.
Some teachers have already recognized the use of a personal computer can facilitate individualized learning, particularly as a source of information. The provision of information is, of course, not sufficient for learning to take place as it does not necessarily lead to understanding. For example, the importance of cultural and social interaction is stressed by Bruner (Wood, 1988) as necessary for cognition (Jenkins, 1994) . (Technology teachers often take advantage of this understanding and build such work into their programs. Hill and Smith [1998] described a program of manufacturing technology education in which they base the work on community needs specifically to harness this student involvement with others.) However, the additional dimension of the individual's cognitive makeup (Salomon, 1991) is also important in the development of technological concepts. It is this combination that forms the basis of individuality that could explain the value of ICT with some learners and yet totally fail to connect with others. The social interaction that is essential to many learners may take a unique form with others. It is possible that the interaction becomes one step removed so that the relationship is at second hand and is mediated through the technological hardware. McLoughlin and Oliver (1998) demonstrated that students working in groups using personal computers interact in such ways that cognitive abilities and concepts are developed much like face-to-face interactions. The interaction between learners using the Internet could be as valid as the interaction between teacher and learner in a traditional setting. In an Australian study, Williams and Williams (1997) recognized the validity of this interaction and also identified some practical problems when engaged in collaborative designing using remote interaction. Clayden, Desforges, Mills, and Rawson (1994) elaborated upon the view that learning is a product of negotiation, a constant initiation into socially constructed webs of beliefs, thus requiring much more than the transmission of knowledge. This socially constructed web, however, may not necessitate a physical presence. It is the quality of the interaction that is important, not the means of exchange.
One way forward is to switch our attention from the design of software packages (which act solely as storehouses of information) to an interactive problem-based environment in which the student assumes the key. Currently, where it is common practice to produce learning materials that are uniform for all learners the learners must "fit in" with the suggested activities. The learner should not have to adjust to the equipment available; the learning task should always be the dominant factor (Beardon, Malmborg, & Yazdani, 1998) and the software designed to this end. In this model, the first task of the learning package is to develop a picture of both the student's learning strategies as well as an analysis of the student's existing knowledge and cultural base (Tweddle, 1998) .
With this profile in place, the learning task can be tailored to the student's capabilities rather than the student having to fit in with the software designer's generalized understanding of how learning should take place (Andaloro & Bellomonte, 1998) . The creation of these rich learning environments will also have to ensure that texts, reference sources, multimedia, and communication facilities are fully integrated.
While it can be seen that major benefits can accrue from the use of ICT in technology education, it will not be sufficient on its own to provide a meaningful program of learning activities that can deliver the full range of desired outcomes. Technology education as it is practiced in schools is essentially a practical activity and the making element is fundamental to the learning activity. Other theoretical models must be identified to explain the shortcomings.
What appears to be a fundamental difficulty in the utilization of ICT in this subject area is the basic belief that in technology we are involved in education through the use of materials (i.e., if the connection is broken between the content and the process through which it takes place, the subject's raison d'etre is nullified). A computer simulation cannot be used as a substitute. Baird (1990) (Gredler, 1992) learned. The reason for this understanding can be illustrated through an information processing model of learning shown in Figure 4 . While such models are hypothetical, they provide a valuable insight into the value of technology education as a process as well as a body of knowledge.
These theories are derived from models in which data, developed from perceptual cues, are processed in a logical fashion to provide desired outcomes through the systematic ordering and restructuring of new information. In information processing models of learning, information is received through our sensory organs where some of it is lost and some of it is filtered for its importance before being passed to the short-term memory store. (When working in a workshop or studio, the range of senses employed by the learner is increased. The tactile and olfactory senses employed when working with resistant materials together with sounds generated must all help to build more accurate concepts than those obtained from working solely with the printed word or even a computer simulation). At this stage, information is consciously worked upon and sometimes used for routine operations. Data or information that is recognized to be of greater value is subjected to transformation and transferred to the long-term store for appropriate concepts that "make sense'" and for use when needed. This is obviously a very complex process that relies upon the accuracy of the interpretation of the perceptual cues that are received from our sensory organs and also the ability of the brain to recognize appropriate schemata or connections. The learning process can therefore be enhanced if the learner uses a range of sense organs (Eisner, 1985) to help form the concepts under development. The wider the range and the more accurate the inputs, the more effective the learning is.
While information processing models of learning are useful in explaining how changes in behavior in the cognitive domain may occur, these theories are not solely concerned with that domain because the sensorimotor skills necessary for the implementation of much technological/scientific/physical activity are said to have much in common with the mental skills used in categorizing and processing knowledge for other forms of activity (Welford, 1971) . We could, therefore, have an understanding that encompasses and explains a lot of activities found in technology education. Again, it is important to stress that while an information processing model is of value, it cannot be the whole story. Dyne et al. (1994) suggested that information processing explains in part the learning that takes place while acknowledging that what they term "student approaches to learning" (SAL) as an essential component. Learning occurs both within the student as well as within the teaching\learning context.
We are often confused by the virtues of ICT. Its advantages for data retrieval and routine, lower order activities are obvious and often valuable. However, the application of ICT to technological problem solving or other higher level research activities still leaves much work to be done.
In summary, there appears to be at least five stages or levels in the use of ICT in technology education:
• Level 1 is the development of routine skills such as word processing or graphics packages as an aid to clarification and communication.
• Level 2 is the use of ICT to search databases such as CD-ROMs and the WWW as a powerful library. It is important to realize, however, that it is not suffi cient to give students practice in using such sources; they also require skills in finding the information and in discrimination of the results.
• The third level is the adoption of existing programs to gain a deeper understanding of the power of ICT to "number crunch," to control mechanisms, or for modeling simple solutions to attainable problems. • In Level 4 the learner moves from the direction and guidance of the teacher to the development of cre ative thought, possibly not until senior levels of school or university education. The PC now becomes an extension of the brain.
• The fifth and possibly most difficult level to attainand the one in which most research needs to be done-is related to the development of understand ing of the cognitive processes of the learner and the application of this knowledge to specific tasks. At this stage we may be able to develop generic creative abilities rather than the ability to understand specif ic problems. When we can utilize this capability, we will begin to be able to exploit the power of ICT in the learning environment.
• While it is obvious ICT can be used to aid learning, the real breakthrough will occur when truly interactive packages provide rich learning environments.

ANY real-world optimization problems involve multiple conflicting objectives [1] , [2] , known as multiobjective optimization problems (MOPs), which can be mathematically formulated as follows:
where X is the search space of decision variables with x = (x 1 , . . . , x D ) denoting the decision vector [3] . Due to the conflicting nature of the objectives, there does not typically exist a single solution that can minimize all the objectives simultaneously. Instead, a set of nondominated solutions can be obtained as the trade-offs between different objectives [4] . Suppose x A and x B are two solutions of an MOP illustrated by (1), solution x A is known to Pareto dominate solution x B (denoted as x A ≺ x B ), if and only if f i (x A ) ≤ f i (x B ) (∀i ∈ {1, 2, . . . , M}) and there exists at least one objective f j (j ∈ {1, 2, . . . , M}) satisfying f j (x A ) < f j (x B ). The collection of all the Pareto optimal solutions in the decision space is called the Pareto optimal set (PS), and the projection of PS in the objective space is called the Pareto optimal front (PF). To solve MOPs, a variety of multiobjective evolutionary algorithms (MOEAs) have been proposed during the past two decades [5] , including the Pareto-based MOEAs [6] - [8] , the decomposition-based MOEAs [9] , [10] , and the indicatorbased MOEAs [11] , [12] , etc. Despite that most existing MOEAs have been well assessed on the MOPs with a small number of decision variables, their performance degenerates dramatically on MOPs with hundreds or even thousands of decision variables, i.e., the large-scale MOPs (LSMOPs) [13] . As the number of decision variables increases linearly, the volume (as well as complexity) of the search space will increase exponentially, and thus leading to the curse of dimensionality [14] , [15] . In recent years, there has been an increasing interest in large-scale multiobjective optimization [16] , [17] . Existing approaches for large-scale multiobjective optimization can be roughly classified into three different categories as follows.
The first category is known as the decision variable analysisbased approaches. A representative algorithm of this category is the MOEA based on decision variable analysis (MOEA/DVA) [18] , where the original LSMOP is decomposed into a number of simpler sub-MOPs. Then, the decision variables in each sub-MOP is optimized as an independent subcomponent. Similarly, the decision variable clustering-based large-scale evolutionary algorithm (LMEA) [19] also divides the decision variables into two types using a clustering method. Then, the convergence-related and diversity-related variables are optimized using two different strategies by focusing on convergence and diversity, respectively.
The second category applies the cooperative coevolution (CC) framework [20] . For example, the third-generation cooperative coevolutionary differential evolution algorithm [21] maintains several independent subpopulations. Each subpopulation is a subset of the equal-length decision variables obtained by variable grouping (e.g., random grouping [22] , linear grouping [23] , ordered grouping [24] , or differential grouping (DG) [25] ). All the subpopulations work cooperatively to optimize the LSMOPs in a divide-and-conquer manner.
The third category is based on the problem transformation, where the original LSMOP is transformed into a simpler MOP with a relatively small number of decision variables. The weighted optimization framework (WOF) is representative in this category [26] . In WOF, the decision variables are divided into a number of groups, each of which is assigned with a weight variable. As a consequence, the optimization of the weight variables in the same group can be regarded as the optimization of a subproblem in a subspace of the original decision space.
There are also some other approaches that do not fall into the above three categories, e.g., the recently proposed competition mechanism-based multiobjective particle swarm algorithm (CMOPSO) [27] . Instead of adopting explicit decision variable analysis or grouping, the algorithm is motivated to implicitly enhance the swarm diversity of particle swarm optimizer (PSO) for solving LSMOPs using a pairwise competition strategy [28] . Despite that these existing approaches as introduced above can improve the performance of MOEAs on LSMOPs to some extent, the development of large-scale multiobjective optimization is still in its infancy. Particularly, most of the existing algorithms suffer from a low computational efficiency, in terms of both computation time and function evaluations. To accelerate the computational efficiency of existing MOEAs on large-scale multiobjective optimization, we propose a generic framework, termed largescale multiobjective optimization framework (LSMOF). The main new contributions are summarized as follows.
1) A novel problem reformulation method is proposed. It reformulates the original LSMOP into a low-dimensional single-objective optimization problem (SOP) with some direction vectors and weight variables, aimed at guiding the population toward the PS. Different from existing dimension reduction techniques, there is no decision variable analysis or grouping process in our proposed method. Since the reformulated problem characterized by the weight variables has a lower dimensionality than the original problem, the computational efficiency can be significantly improved. 2) A bi-directional weight variable association strategy is proposed to enhance the performance of the proposed framework for tracking the PS in the decision space effectively. This strategy not only increases the population diversity of the reformulated problem to avoid local optima, but also eliminates the potential nonuniform search caused by the divergence of the unidirectional vectors. 3) A two-stage strategy is adopted in our proposed LSMOF.
At the first stage, the decision space reconstructionbased single-objective optimization is used to push the population toward the PS efficiently. Then, the second stage spreads the candidate solutions over the approximate PS evenly. The rest of this paper is organized as follows. In Section II, we briefly recall some related work on large-scale multiobjective optimization, and the motivation of this paper is also elaborated. The details of the proposed LSMOF for largescale multiobjective optimization are presented in Section III. Experimental settings and comparisons of LSMOF with the state-of-the-art heuristic algorithms on the benchmark problems are presented in Section IV. Finally, the conclusion is drawn in Section V.
In this section, we first recall some concepts and definitions in large-scale multiobjective optimization. Then some related work about the decision variable analysis, decision variable grouping, and problem transformation are illustrated. Finally, the motivation of this paper is elaborated.
Definition 1: f (x) is called a partially separable with k components iff [25] , [29] 
where
According to Definition 1, an SOP is known as partially separable if the decision variables can be divided into a number of subcomponents and optimized independently. The main idea of decision variable analysis is intuitive. First, the interdependence between the pairwise decision variables is detected based on Definition 2 by different techniques, e.g., perturbation [30] , interaction adaption [31] , [32] , modeling [33] , or randomization [34] . Then, the relationship between a specific decision variable and the optimization problem is analyzed. To be specific, a decision variable can be related to convergence, diversity, or both of them. Finally, the decision variables of different types can be optimized using different strategies.
In MOEA/DVA [18] , the decision variables are divided into three types according to their control properties of convergence or/and spread. The three types of decision variables are defined as follows.
1) Position Variable: Decision variable x j in x is called a position variable iff changing x j in x will generate a new solution x satisfying that x ⊀ x and x ⊀ x . 2) Distance Variable: Decision variable x j in x is called a distance variable iff changing x j in x will generate a new solution x satisfying that x ≺ x or x ≺ x . 3) Mixed Variable: Decision variables do not fall into any of above two types. Similarly, the decision variables in LMEA are also clustered into convergence-related variables and diversity-related variables [19] . By dividing the decision variables into different types, the algorithms are able to adopt different optimization strategies to focus on convergence and diversity, respectively. Nevertheless, a crucial disadvantage of the decision variable analysis is the high computational cost, especially when there is a large number of decision variables. For example, it takes up to 7 577 615 function evaluations for LMEA to perform decision variable analysis on an LSMOP with 1000 decision variables (i.e., the 1000D DTLZ1 problem), which is prohibitively expensive in practice.
In the CC-based algorithms, the decision variables are divided into a number of groups and optimized in a cooperative coevolutionary manner. Assuming that the grouping technique aims to divide D decision variables into k groups, some representative grouping strategies are summarized as follows.
1) Random Grouping [22] : The decision variables are randomly divided into k even groups. 2) Linear Grouping [23] : The decision variables are assigned to k groups in order, i.e., x 1 , . . . , x D/k are assigned to the first group, x D/k+1 , . . . , x 2D/k are assigned to the second group, and so forth. 3) Ordered Grouping [24] : For a selected solution, the decision variables are sorted by their absolute values in ascending order. The D/k decision variables with the smallest absolute decision variables are assigned to the first group, and the rest may be deduced by analogy. 4) DG [35] : In contrast to the above three grouping techniques which are based on some heuristic strategies, DG techniques take the variable interactions into consideration when performing grouping [25] , where the interacting decision variables are divided into the same group. Without prior knowledge about the interactions among the decision variables or the number of groups, the performance of CC-based algorithms can be influenced by the selection of different grouping techniques.
Inspired by the grouping mechanism in the CC framework, the problem transformation strategy is proposed to improve the efficiency of CC-based algorithms on large-scale multiobjective optimization [26] . Instead of optimizing different subpopulations with fixed decision variables, the problem transformation strategy assigns a weight variable to the original decision variables in each group. Then the optimization of the decision variables is transformed to the optimization of the weight variables, which has significantly improved the efficiency of the algorithm.
Given a candidate solution
where w i (i ∈ [1, k] ) is a weight variable and k is the number of groups. In this way, the optimization of the D decision variables is transformed to the optimization of a problem with k decision variables [26] .
Despite that the transformation strategy is able to reduce the dimensionality of the decision space to a certain extent, it suffers from two main drawbacks. First, since the transformed subproblems are optimized separately, the correlations between different weight variables are not considered. Second, since the performance of the transformation strategy heavily depends on the grouping technique adopted therein, its computational efficiency and stability remain to be improved.
While most existing approaches in the literature mainly focused on the optimization performance, little work has been dedicated to improving the computational efficiency. As a result, the computational budgets for solving LSMOPs could be expensive in terms of computation time as well as the number of FEs.
For example, an experimental comparison is conducted on the bi-objective LSMOP8 problem with 200 decision variables using LMEA, MOEA/DVA, and NSGA-II, where the decision variables of the test problem are mixed/interacting. The plot of the convergence profiles of the mean IGD values achieved by NSGA-II, MOEA/DVA, and LMEA on the problem is displayed in Fig. 1 . It can be observed that NSGA-II performs much better in the early stage of the evolution. Meanwhile, NSGA-II performs overall better than MOEA/DVA and similarly to LMEA, which implies that the decision variable analysis adopted by MOEA/DVA and LMEA do not work effectively. However, in order to perform the variable analysis, it will cost much more additional FEs and computation time [19] . Moreover, since the grouping-based approaches are highly dependent on the grouping results, an unsuitable grouping may lead to complete failure of an algorithm [20] .
To address the above issues, this paper proposes a problem reformulation-based framework, termed LSMOF, for largescale multiobjective optimization. Without using any grouping technique or decision variable analysis method, our LSMOF shows competitive optimization performance and computational efficiency compared to the existing approaches in the literature.
The main scheme of the proposed LSMOF is presented in Algorithm 1. To begin with, the population of the embedded MOEA is initialized. Then a two-stage strategy is adopted, where the first stage aims to find several quasi-optimal solutions near the PS and the second stage spreads them over the approximate PS evenly. At the first stage, the decision space is reconstructed with the assistance of population P, and the original LSMOP is reformulated into a low-dimensional SOP Z ; then, a single-objective optimizer (e.g., the differential evolution (DE) algorithm [36] ) is used to optimize Z . The above problem reformulation and single-objective optimization repeat until the maximum number of FEs is reached. For simplicity, we allocate 50% of the whole FEs to each stage, i.e., tr is set to 0.5. At the second stage, the original LSMOP is optimized by the embedded MOEA with the population P obtained at the first stage. Note that the LSMOF framework shares the same environmental selection operator with the embedded MOEA, and thus we will not enter the details of it. In the following sections, we will introduce the other two main components in Algorithm 1, i.e., problem reformulation and single-objective optimization. Z ← Problem_Reformulation(P, r, Z)
A, t ← Single_Objective_Optimization(Z ) 6: P ← Environmental_Selection(A P, N) 7: t ← t + t 8: end while 9: /*********Second Stage*********/ 10: P ← Embedded_MOEA (P, N, Alg, Z)
Problem reformulation is a crucial component of the proposed LSMOF (step 4 in Algorithm 1), which reformulates the original LSMOP into an SOP with relatively small-scale weight variables. To be specific, the proposed problem reformulation consists of three steps: 1) the bi-direction weight variable association; 2) the weight variable-based subproblem construction; and 3) the objective space reduction, where the first two steps aim to reconstruct the decision space.
To guide the search of the algorithm toward the PS, a set of well converged and evenly distributed candidate solutions is used during the decision space reconstruction. For simplicity, we directly use the environmental selection in the embedded MOEA to select r solutions from the current population P as the reference solution set. Afterwards, each reference solution is associated with two direction vectors and two weight variables. This operation aims to specify the search directions in the decision space and guide the population toward the PS. 
where l max = ||t − o|| is the maximum diagram length in X. 
where λ 11 and λ 11 are two weight variables. Note that since the weight variables are between 0 and 0.5, each weight variable will only range in half of the original search space. Hence, the bi-directional weight variables will cover the entire search space without overlapping, which also enables the parallelization of the search for enhancing the efficiency of the proposed algorithm.
To be specific, each reference solution is associated with bidirectional vectors instead of a unidirectional vector, which is out of two main considerations. First, the bi-directional vectors can enhance the population diversity, thus reducing the possibility that there is no intersection between a given directional vector and the PS. For instance, if the reference solution locates around the boundary of the PS or the PS locates around a corner of the decision space, the unidirectional vector may disjoint with the PS, but the bi-directional vectors are more likely to have at least one intersection with the PS. Second, the bi-directional vectors can eliminate the nonuniform search caused by the divergence of the unidirectional vectors. To further illustrate the advantage of the proposed bi-directional weight variable association strategy, an example is given in Fig. 3 . Generally, it has a better chance to locate the Pareto optimal solutions on the PS by adopting the bi-directional vectors than the unidirectional vectors. Furthermore, the experimental comparisons between our proposed algorithm with bi-directional vectors (LSMOF) and that with unidirectional vector (LSMOFU) are given in the supplementary materials. The results indicate that LSMOF has an overall superiority over LSMOFU, thus verifying the effectiveness of adopting bi-directional vectors.
2) Subproblem Construction: Given a reference solution set of size r, once each reference solution is associated with two direction vectors and two weight variables, a total number of 2r subproblems can be constructed. Taking the first reference solution s 1 for example, two subproblems can be constructed as follows: 
where λ 11 It is worth noting that there are two main differences between our proposed subproblem construction and the problem transformation in WOF. First, while the decision variables in WOF are divided using grouping techniques, the decision variables in the proposed LSMOF are controlled by the weight variables and optimized as a whole. On one hand, it can explicitly save the computational cost of variable analysis; on the other hand, it can implicitly take the variable interactions into consideration during the evolution. Second, while 
the direction vectors in WOF are unidirectional, those in the proposed LSMOF are bi-directional. In general, the coverage of the bi-directional search in LSMOF is larger than that of WOF, which enhances the exploration ability of the algorithm and maintains better population diversity.
3) Objective Space Reduction: Once the subproblems are reconstructed, the optimization of the decision vector x in the original decision space is transformed to the optimization of the weight vector in the reconstructed decision space. Correspondingly, the objective space can be reduced and the new optimization problem can be reformulated as
where H is a function to assess the quality of the 2r multiobjective solutions, and H can be any performance indicator, e.g., the hypervolume (HV) indicator [37] as adopted in this paper. By using such a reformulation scheme, the scale of the original problem can be substantially reduced. For example, an LSMOP with 1000 decision variables can be reformulated into an SOP with only ten decision variables as in our experiments.
To assess the quality of the reconstructed decision vectors, we propose a fitness assignment strategy for the evaluation of (7) in a reduced objective space. As given by the pseudo code of the fitness assignment procedure in Algorithm 2, the fitness of a weight vector (i.e., a sequence formed by the weight variables) in the reconstructed decision space can be calculated by two main stages. At the first stage, the objective vectors in accordance with the weight vector are calculated (steps 1-6). Then, at the second stage, the fitness value of the objective vector is calculated using the HV indicator. In this way, the algorithm is able to return a scalar value as the fitness of the reconstructed decision vector .
Once the original LSMOP is reformulated, the proposed LSMOF is expected to perform single-objective optimization of the weight variables in the reconstructed decision space and the reduced objective space. For simplicity, we adopt the widely used DE [38] as the single-objective optimizers t ← t + |A| /*|A| denotes the element size of A*/ 6: for r ← 1 : g do 7: for i ← 1 : NI do 
for j ← 1 : | 1 | do 11: if rand j [0, 1) ≤ CR or j = j rand then 12: b i ← Choose the jth element of i 13: else 14: b i ← Choose the jth element of a 15: end if 16: end for 17: fit(b)← Calculate b's fitness using Algorithm 2 18: A ← Collet the generated candidate solutions during the fitness assignment 19 :
i ← b 22: end if 23: A ← A ∪ A 24: end for 25 : end for in this paper. Note that we use DE in this framework due to its efficiency and simplicity, and any other single-objective optimization algorithm (e.g., the PSO [39] ) is also compatible with the proposed framework.
The details of the DE-based single-objective optimization are presented in Algorithm 3. In this algorithm, a set of weight vectors P are first initialized in range [0, 0.5] as defined in (5), and their fitness values are calculated using Algorithm 2. For each weight vector i in P , three different weight vectors are randomly selected from P to form a trail vector a for crossover, where a is associated with the weight vector to generate an offspring b according to a probability rate CR. If the fitness of offspring a is better than that of i , the weight vector i is replaced by b. The reproduction and replacement procedures repeat for a number of g iterations. All the candidate solutions generated during the evolution of the weight vectors are merged into the archive A, which will be used as the initial population of the embedded MOEA at step 10 of Algorithm 1.
It is worth noting that during the optimization of the weight variables, a number of candidate solutions of the original MOP are generated by Algorithm 2 (e.g., p 1 and p 2 ) . In other words, the optimization of the weight variables naturally leads to the optimization of the original problem.
To empirically investigate the performance of the proposed LSMOF framework, four representative MOEAs, namely NSGA-II [6] , MOEA/D-DE [10] , SMS-EMOA [12] , and CMOPSO [27] , are embedded into LSMOF and compared with their original versions on nine test problems from the LSMOP test suite [13] . Here we adopt these four algorithms as they represent different types of MOEAs as discussed in Section I, and the embedding of these algorithms could reveal the potential advantages of our proposed LSMOF. Then, two state-of-the-art large-scale MOEAs, namely WOF [26] 1 and MOEA/DVA [18] , are also compared with our proposed LSMOF. 2 In the remainder of this section, we first present a brief introduction to the adopted performance indicator, and then we give the parameter settings of the compared algorithms and our proposed LSMOF. Afterwards, each algorithm is run for 20 times on each test problem independently, and the Wilcoxon rank sum test [42] is used to compare the results obtained by the proposed LSMOF and the compared algorithms at a significance level of 0.05. Symbols "+," "−," and "≈" indicate the compared algorithm is significantly better than, significantly worse than, and statistically tied by LSMOF.
In the experiments, a widely used performance indicator, the inverted generational distance (IGD) [43] , is adopted for evaluating the performance of the compared algorithms.
Suppose that P * is a set of evenly distributed reference points on the PF and is the set of obtained nondominated solutions, IGD is defined as follows:
where dis(x, ) is the minimum Euclidean distance between x and points in and |P * | the number of elements in P * . A smaller value of IGD will indicate a better performance of the algorithm. In this paper, the size of P * is set to 10 000 (or a close number) for the IGD calculations.
Note that the HV [37] values of the obtained nondominated solutions are presented in the supplementary materials. In this paper, we use the IGD indicator instead of the HV indicator since the PFs of LSMOP test problems are relatively simple and regular. Meanwhile, the reference solution sets in PlatEMO [44] , [45] are evenly sampled, which enables the IGD indicator to well assess the qualities of the obtained solution sets.
For fair comparisons, we adopt the recommended parameter settings for the compared algorithms that have achieved the best performance as reported in the literature. All the compared algorithms are implemented in PlatEMO [44] .
In this paper, the simulated binary crossover [4] and the polynomial mutation (PM) [46] are adopted in the compared algorithms for offspring generation in NSGA-II and SMS-EMOA. The distribution index of crossover is set to n c =20 and the distribution index of mutation is set to n m =20, as recommended in [4] . The crossover probability p c is set to 1.0 and the mutation probability p m is set to 1/D, where D is the number of decision variables. In MOEA/D-DE and MOEA/DVA, DE operator [36] and PM are used for offspring generation, where the control parameters are set to CR =1, F =0.5, p m = 1/d, and η=20 as recommended in [10] . As for CMOPSO, the particle swarm operator [47] and PM are used, where parameters R 1 and R 2 are randomly selected from [0, 1] with γ set to 10 as recommended in [27] .
2) Population Size: The population size is set to 100 for test instances with two objectives and 105 for test instances with three objectives.
In MOEA/D-DE, the neighborhood size T is set to 20, the probability of choosing parents locally δ is set to 0.9, and the maximum number of solutions replaced by each offspring n r is set to 2. In WOF, the number of FEs for the optimization of each original problem t 1 is set to 500, and for the transferred problem, t 2 is set to 250, parameter q is set to 3, the number of groups γ is set to 4, and the ordered grouping is adopted as the grouping method [26] . Meanwhile, NSGA-II is embedded in both WOF and LSMOF to be compared on LSMOP problems. 3 In MOEA/DVA, the number of sampling solutions to recognize the control properties of the decision variables is set to 20, and the maximum number of trails required to judge the interaction between two variables is set to 6. In the proposed LSMOF, the number of reference solutions r is set to 10, the population size for the single-objective optimization is set to 30, and the mutation factor F m in DE is set to 0.8.
A total number of 50 000 FEs is adopted as the termination condition for all the test instances. The number of FEs is relatively small for existing MOEAs, but it is practical for real-world applications. It is attributed to the fact that the number of FEs is always limited by the economic and/or computational cost, especially for the largescale optimization problems.
To investigate the effect of LSMOF on different MOEAs, four representative algorithms, i.e., NSGA-II, MOEA/D-DE, SMS-EMOA, and CMOPSO, are embedded into the proposed framework. Pairwise comparisons are conducted between the heuristic algorithm and its LSMOF version in terms of both solution quality and algorithm runtime. The experimental results obtained by these compared algorithms are displayed in Table I , where LS-Alg denotes the LSMOF with algorithm Alg embedded. In Table I, 
Since one important motivation of this paper is to accelerate large-scale multiobjective optimization, we will further investigate the convergence speed and computation time of the compared algorithms.
The convergence profiles of the eight compared algorithms on LSMOP3 and LSMOP5 with 1000 decision variables are displayed in Fig. 4 . As can be observed, the original algorithms converge slowly and thus have failed to achieve an acceptable accuracy level by the end of the evolution. By contrast, the LSMOF-based algorithms have already converged to a promising accuracy level at a very early stage of the evolution (i.e., before 10 000 FEs).
2) Computation Time: In order to investigate the computation time of the proposed LSMOF, we display the average computation time of the eight compared algorithms on LSMOP3, LSMOP6, and LSMOP9.
As shown in Figs. 5 and 6, our proposed LSMOF has accelerated the computation time of MOEA/D-DE and SMS-EMOA on all the test instances. As for NSGA-II and CMOPSO, LSMOF has saved almost 1/3 computation time on problems with 500 and 1000 decision variables.
In conclusion, our proposed LSMOF is capable of reducing the computation time of MOEAs in large-scale multiobjective optimization, and the acceleration improvement is more significant on LSMOPs with a larger number of decision variables, e.g., LSMOPs with more than 500 decision variables. There are two main reasons that our proposed LSMOF performs efficiently in terms of both convergence rate and computation time. First of all, since the original LSMOP is reformulated into a low-dimensional SOP, the search complexity is significantly reduced. Second, since the proposed framework guides the early search toward the PS by the reference directions, the population exhibits more stable and efficient convergence behaviors.
In this section, we compare our proposed LSMOF with another two state-of-the-art large-scale MOEAs, namely MOEA/DVA and WOF, in terms of both optimization performance and computational efficiency. In both WOF and LSMOF, NSGA-II is embedded for fair comparisons. The statistics of IGD results achieved by MOEA/DVA, WOF-NSGA-II, and LS-NSGA-II are displayed in Table II. As can be observed, LS-NSGA-II has achieved 32 out of 54 best results, WOF-NSGA-II has achieved six best results, and MOEA/DVA has achieved five best results. To be specific, LS-NSGA-II has achieved the best results mainly on LSMOP1, LSMOP3, LSMOP5-LSMOP8, and bi-objective LSMOP2 and LSMOP9; WOF-NSGA-II has achieved the best results mainly on LSMOP5 and tri-objective LSMOP9; meanwhile, MOEA/DVA has achieved the best results on tri-objective LSMOP2 and tri-objective LSMOP4.
It should be noted that MOEA/DVA has achieved some results far from the PFs on LSMOP6 and bi-objective LSMOP7. This may be attributed to the failure of decision variables analysis which has caused the significant performance degeneration. After comparing the performance of these three algorithms on LSMOPs, their convergence rates on bi-objective LSMOP1 and tri-objective LSMOP6 with 1000 decision variables are presented in Fig. 7 , and the average computation time on LSMOP1 is displayed in Fig. 8 . It can be observed from these two figures that LS-NSGA-II has the fastest convergence rate on those two test instances, while its computation time is similar to that of MOEA/DVA and WOF-NSGA-II. Besides, it can be observed that LS-NSGA-II shows fast convergence at the early stage of the evolution (within 10 000 FEs) and stops converging before the second stage starts. This is attributed to the fact that LSMOF obtains the quasi-optimal solutions at the first stage, and then the embedded MOEA spreads the obtained solutions over the entire PS. This can be confirmed by the continuous improvement of the IGD values after the first stage. In conclusion, the proposed LSMOF shows a competitive performance and similar computational efficiencies in comparison with MOEA/DVA and WOF on these LSMOPs. The competitiveness of LSMOF in comparison with the state-ofthe-arts is verified.
In our proposed LSMOF, a threshold tr is used to control the number of evaluations used by the first stage. To analyze the effect of the threshold value on the performance of LSMOF, we conduct experiments on a set of LSMOP problems with 1000 decision variables. The threshold is set to 0.2, 0.4, 0.6, Table III . It can be observed that different settings of tr do not significantly affect the performance of the proposed algorithm. This can be attributed to the fact that the algorithm converges so fast during the first stage that it does not need too many FEs. Therefore, the performance of the proposed LSMOF is not sensitive to the setting of the threshold. For simplicity, we set the threshold value to 0.5 in all the other experiments in this paper.
In this paper, we have proposed a general framework for large-scale multiobjective optimization, termed LSMOF. The proposed LSMOF adopts a two-stage strategy, where the first stage conducts the problem reformulation for obtaining a set of quash-optimal solutions near the PS, and the second stage spreads these solutions over the approximate PS uniformly by an embedded MOEA.
At the first stage of the proposed LSMOF, the decision space is first reconstructed by associating a set of reference solutions with a set of weight variables in the decision space. Then, a series of subproblems are constructed by taking the weight variables as the input, where each weight variable is aimed at tracking a specific point on the PS. Meanwhile, a performance indicator is adopted to assess the quality of the reconstructed decision vector for objective space reduction, and the original LSMOP is thus reformulated into a lowdimensional SOP. The DE algorithm is adopted to optimize the SOP by using an indicator-based fitness assignment strategy, and the candidate solutions obtained therein are used as the initial population of the embedded MOEA at the second stage.
To assess the performance of the proposed LSMOF, a variety of empirical comparisons have been conducted on a set of LSMOPs. The general performance of our proposed LSMOF is tested by embedding four MOEAs, namely NSGA-II, MOEA/D-DE, SMS-EMOA, and CMOPSO into it. The statistical results indicate that LSMOF has accelerated the convergence speed and saved computation time of the embedded algorithms on most of the test instances. More importantly, the performance of the MOEAs has also been significantly improved. The second experiment assesses the performance of the proposed LSMOF in comparison with two state-of-the-art large-scale MOEAs, namely MOEA/DVA and WOF. The superiority of the proposed LSMOF over the other two algorithms is also verified by the experimental results.
The proposed LSMOF has shown good potential in largescale multiobjective optimization. Future work on developing more efficient problem reformulation method is highly desirable. It is also interesting to adapt our proposed LSMOF to real-world LSMOPs with more decision variables by parallel (e.g., GPU-based) computing.
The lifetime of a wireless sensor network is usually battery dependent requiring replacement or recharging while the former is either very difficult or infeasible. Recently, radio frequency (RF) energy harvesting arises as the most suitable technology to provide perpetual energy eliminating the need to replace batteries due to design of highly efficient RF energy harvesting hardware [1] . In wireless powered communication networks (WPCN), wireless users with RF energy harvesting capability; i.e., sensors and machine type communication (MTC) devices, communicate to a hybrid access point (HAP) in the uplink using the energy transferred by the HAP in the downlink [2] .
Minimum length scheduling (MLS) and sum throughput maximization (STM) problems have been studied for WPCNs under various models and assumptions. Several studies [3] - [6] have considered half-duplex model in which users transmit information and harvest energy in non-overlapping time intervals. On the other hand, WPCN studies have recently incorporated full duplex technology allowing the access point and the users achieve simultaneous energy transfer and data communication due to recent advances in self-interference cancellation techniques and their practical implementations under the development of 5G and beyond networks. The authors in [7] - [9] have considered the full duplex models for MLS and STM. Due to full duplex, a wireless user can harvest This work is supported by Scientific and Technological Research Council of Turkey Grant #117E241. energy during both its own and other users' transmission, making scheduling important which is missing in these works. Whereas, only few studies [10] , [11] have paid attention to scheduling but in either a limited context or employing a computationally-inefficient technique. In [10] , the scheduling frame is divided into a fixed number of equal length time slots resulting in underutilization of the resources. In [11] , authors have used Hungarian algorithm to find the schedule which is computationally very complex for such sequence dependent transmissions. Moreover, these studies have considered simplistic models compared to the system model discussed in this paper. Due to low processing cost and use of simple and cheap power amplifiers, on-off transmission scheme can be very useful for inexpensive sensor networks leading to affordable and widespread deployments of IoT applications. However, in the context of WPCN, no previous study have considered this scheme except [12] , [13] where the authors have analysed the average error rate and outage probability for a single user system. In this paper, we incorporate on-off transmission scheme in which the users either transmit with a constant power or remain silent if the user can not afford transmission at this power.
The goal of this paper is to revisit MLS and STM problems for determining the optimal time allocation and scheduling considering on-off transmission scheme and a realistic energy harvesting model in a full-duplex WPCN. The main contributions are listed as follows:
• We characterize the Minimum Length Scheduling Problem (MLSP) and Sum Throughput Maximization Problem (STMP) and mathematically formulate each as a mixed integer linear programming (MILP) problem. • For MLSP, we propose an optimal polynomial-time algorithm incorporating optimal time allocation and scheduling policies. • For STMP, upon analyzing the optimality conditions on the optimization variables, we propose a polynomial-time heuristic algorithm and illustrate that it performs nearly optimal for various simulation scenarios.
We describe the WPCN architecture and the assumptions used throughout the paper as follows: 1) The WPCN architecture, as depicted in Fig. 1 , consists of a HAP and N users; i.e., machine type communications devices and sensors. The HAP and the users are equipped with one full-duplex antenna for simultaneous wireless energy transfer and data transmission on downlink and uplink channels, respectively. The uplink channel gain from user i to the HAP and the downlink channel gain from the HAP to user i are denoted by g i and h i , respectively. 2) The HAP has a stable energy supply and continuously broadcasts wireless energy with a constant power P h . Each user i harvests the radiated energy from the HAP and stores in a rechargeable battery of certain capacity which is assumed to be large enough so that no overflow will occur. Each user has an initial energy B i stored in its battery at the beginning of the scheduling frame which includes the harvested and unused energy in the previous scheduling frames.
3) The energy harvesting rate of user i from the HAP, denoted by C i , is characterized as follows:
where η i is the antenna efficiency of user i.
access control for the uplink data transmissions from the users to the HAP. The time is partitioned into scheduling frames which are further divided into variable-length time slots each of which is allocated to a particular user. 5) We use constant power model in which all users have a constant transmit power P max during their data transmissions which is imposed to limit the interference to nearby systems. 6) We use constant rate transmission model, in which Shannon capacity formulation for an AWGN channel is used in the calculation of transmission rate r i of user i as
where W is the channel bandwidth and k i is defined as
The term βP h is the self interference at the HAP and N o is the noise power density.
In this section, we introduce the minimum length scheduling problem, denoted by MLSP. The joint optimization of the time allocation and scheduling with the objective of minimizing the schedule length is formulated as follows:
The variables of the problem are τ i , the transmission time of user i, and a ij , a binary variable that takes value 1 if user i is scheduled before user j and 0 otherwise. In addition, τ 0 denotes an initial unallocated time in which all users harvest energy without transmitting data. The objective of the problem is to minimize the schedule length which is equal to the completion time of the transmissions of all users, as given by Eq. (3a). Eq. (3b) represents the traffic requirements of the users where D i is the amount of data that should be transmitted by user i. Energy causality constraint is given by Eq. (3c): The energy consumed during data transmission cannot exceed the total amount of available energy including both the initial battery level and the harvested energy until and during the transmission of a user. Eq. (3d) represents the scheduling order constraint.
In the following, we investigate the characteristics of an optimal solution for MLSP. Lemma 1. There exists an optimal solution of MLSP in which the traffic requirement constraint (3b) is satisfied with equality; i.e., each user i transmits exactly D i bits in the scheduling frame.
is the optimal transmission time for a set of users. Then, the optimal schedule length is given by N i=0 τ * i . Further suppose that, for a user j,
such that it transmits more than its traffic requirement D j , where τ min j denotes the minimum transmission time required by user j to fulfill its traffic requirement. Let ∆τ j = τ * j −τ min j . The optimal schedule can be updated as τ * 0 = τ * 0 + ∆τ j and τ * j = τ * j − ∆τ j . Then, the schedule length does not change while the energy causality requirement of the users are not violated since the completion time of users i < j increase and the completion time of users i ≥ j remain same. Therefore, there exists an optimal solution in which the traffic requirement constraint (3b) is satisfied with equality for all users.
Based on Lemma 1, the required energy for the transmission of a user i is given as
Let s min i denote the minimum starting time for a user i such that it can harvest enough energy to complete its transmission. Then, considering the energy causality constraint (3c), s min i is given by
In an optimal solution of MLSP, users are allocated in increasing order of minimum starting time values. The optimal schedule can be updated by interchanging the transmission order of users j and j + 1 such that user j + 1 is scheduled at starting time s j+1 = s * j and user j is scheduled just after user j + 1 completes its transmission at s * j + τ * j+1 ; i.e., s j = s * j + τ * j+1 . Since s j+1 = s * j > s min j+1 and s j > s * j ≥ s min j , both users satisfy their energy causality requirements.
The foregoing lemma suggests that at any particular time instant t, it is an optimal policy to schedule any user i with s min i − t is nonpositive and minimum among all i ∈ [1, N ]. Then, the optimal schedule should start with an initial unallocated time τ 0 = min i∈ [1,N ] F ← F -{m}, 8: τ m ← D m /(W log 2 (1 + k m P max )), 9: τ waiting m ← max 0, s min m − t(S) , 10: τ 0 ← τ 0 + τ waiting i , 11: t(S) ← t(S) + τ m + τ waiting m , 12: end while
In this section, we introduce the sum throughput maximization problem, denoted by STMP. The joint optimization of the time allocation and scheduling is formulated as follows:
The objective of the problem is to maximize the sum of the throughput of the users, as given by Eq. (7a). Similar to MLSP formulation, STMP formulation includes the energy causality and scheduling order constraints given by Eqs. (7c) and (7d), respectively. Besides, the problem formulation employs a normalized schedule length of 1, as given by Eq. (7b), without loss of generality.
STMP formulation is a mixed integer linear programming (MILP) problem thus difficult to solve for the global optimum [14] . On the other hand, for a predetermined transmission order of the users, i.e., a ij values are given, STMP problem is a convex problem for which there exists polynomialtime solution algorithms. Hence, a straightforward solution to STMP would be to enumerate all possible transmission orders, solve each of them and determine the one yielding maximum throughput. However, since there are N ! possible transmission orders, such an optimal solution method is intractable. In the following, we present a polynomial-time heuristic algorithm by investigating the characteristics of an optimal solution.
In the following lemma, we present an optimality condition on scheduling suggesting a prioritization among users based on their transmission rates. Lemma 3. In the optimal solution of STMP, for any two users i and j such that r i > r j , if τ i = 0, then τ j = 0.
Proof. Suppose that τ * = [τ * 1 , τ * 2 , ..., τ * N ] is the optimal transmission time for a set of users such that τ * i = 0 and τ * j > 0 for some i and j such that r i > r j . For some τ > 0 which will not violate the energy causality requirement of user i, transmission time of user j can be divided into two slots of lengths τ * j − τ and τ , each allocated to users j and i, respectively. Then, sum throughput is increased by τ (r i − r j ) which is strictly positive. This is a contradiction.
While Lemma 3 indicates that high rate users should be prioritized for sum throughput maximization, an optimal schedule does not necessarily contain all users as long as maximum throughput is achieved using a subset of users. However, the following corollary of Lemma 3 states that the maximum rate user should be given a nonzero transmission time. Moreover, if the maximum rate user has enough initial battery level to transmit with P max during the entire scheduling frame, then, it needs to be allocated to the entire scheduling frame in the optimal schedule. Next, based on the foregoing analysis, we propose the Max-Rate First Scheduling Algorithm (MRSA), given in Algorithm 2. Input of MRSA algorithm is a set of users, denoted by F, sorted in decreasing order of transmission rates. It starts by initializing the unallocated time duration to 1. At each step, MRSA picks the user with maximum rate and determines the maximum feasible transmission time it can allocate to that user. MRSA performs allocation starting from the end of the scheduling frame to allow higher rate users to harvest more energy. Then, it updates the unallocated time duration accordingly and continues with the next user. If the unallocated time duration is 0 at any step, MRSA terminates by not scheduling the remaining users. Otherwise it schedules all users and the remaining unallocated time is specified as τ 0 . Upon termination, MLSA outputs the schedule S consisting of the allocated users and the corresponding sum throughput R(S). The computational complexity of MRSA is O(N ).
The goal of this section is to evaluate the performance of the proposed algorithms. Simulation results are obtained by averaging 1000 independent random network realizations. The users are uniformly distributed within a circle with radius of 10m. The attenuation of the links considering large-scale statistics are determined using the path loss model given by 8: if t u = 0 then [15] . The self interference coefficient β is taken as −70 dBm.
In Fig. 2 , we illustrate the performance of the proposed optimal algorithm MLSA in comparison to a predetermined scheduling order based algorithm, denoted by PDO, for different scenarios. PDO simply allocates the users in a given arbitrary order and thus does not exploit the benefit of optimal scheduling to decrease the length of the scheduling frame. We first illustrate the scheduling performance for different P h values. Schedule length decreases with the increasing P h since higher HAP power indicates that users can start and thus complete their transmissions earlier in the scheduling frame since any user will be able to afford P max transmit power earlier via harvesting more energy from the HAP. While for large values, optimal scheduling loses its importance on the performance, for relatively small and practical values of P h , MLSA outperforms PDO significantly. A similar superiority of MLSA can be observed from the figure for increasing P max values. As P max increases, performance of both algorithms initially improve since users continue to afford P max transmit power using their initial battery levels at the very beginning of the scheduling frame. However, above a critical value of P max , increasing transmit power leads to increasing initial unallocated time τ 0 in the scheduling frame. This results in a performance degradation for PDO while MLSA accomodates this effect by optimally determining the scheduling order. Finally, MLSA outperforms PDO for increasing number of users in the WPCN. While the schedule length almost increases 
In Fig. 3 , we illustrate the throughput performance of the proposed algorithm MRSA in comparison to the optimal solution, denoted by OPT. Optimal solution is obtained by enumerating all possible transmission orders and picking the one yielding the highest throughput via solving a convex optimization problem for each transmission order. One clear observation is that MRSA performs nearly optimal on average while achieving exact optimal solutions in most network realizations. As HAP power P h increases, sum throughput yielded by MRSA increases while it saturates for large values of P h since the energy that can be used by the users in a scheduling frame is limited. For increasing P max , sum throughput first increases since users can have higher transmission rates. Then, above certain P max values, users cannot afford P max in the beginning of the scheduling frame resulting an increase in the initial unallocated time and thus decrease in the total allocated time by the users. As the number of users increases, sum throughput achieved by the users almost increases linearly as expected ideally.
In this paper, we have investigated minimum length scheduling and sum throughput maximization problems for a full duplex WPCN considering on-off transmission scheme. For both problems, we have derived the characteristics of the optimal solution and proposed polynomial-time solution schemes. As future work, we plan extending this study for discrete rate based transmission rate model in which users can select a transmission rate from a finite set based on their SNR levels. Moreover, the WPCN architecture for multiple hybrid access points will also be investigated.
The class prediction problem with p-dimensional input x = (x 1 , . . . , x p ) and output variable y ∈ K, where K = {1, . . . , K}, is considered. For each class k ∈ K, x is a p-multivariate random vector generated from the multivariate probability distributions P k . The family of component-wise distance-based discriminant rules is defined by,
where x j ∈ is a test input, P k,j is the j-th marginal distribution of P k , and d(x j , P k,j ) is the distance between x j and P k,j (Hennig and Viroli, 2016a; Hall et al., 2009; Tibshirani et al., 2003) . The optimal prediction isŷ = argmin k∈K D k .
For example, the centroid classifier may be defined by d(x j , P k,j ) = (x j − µ k,j ) 2 where µ k,j is the mean of P k,j . This classifier is a special case of the naive Bayes classifier (Hastie et al., 2009) , also known as the diagonal linear discriminant classifier. It provides an effective classifier for large p and many types of high dimensional data inputs (Dudoit et al., 2002; Bickel and Levina, 2004; Fan and Fan, 2008) . When the input x includes symmetric random variables with fat tails, the median classifier (MC), d(x j , P k,j ) = |x j − m k,j |, where m k,j = median(P k,j ), k ∈ K and j = 1, . . . , p, often has better performance. Hall et al. (2009) proved that under suitable regularity conditions MC produced asymptotically correct predictions. In practice a training data set, where variables may be rescaled if necessary (Hennig and Viroli, 2016a, Section 4.1) , is used to estimate the parameters µ k,j or m k,j for k ∈ K and j = 1, . . . , p. It sometimes happens that the distribution of two variables is similar near the center but differs in the tails due to skewness or other characteristics. Quantile regression makes use of this phenomenon (Koenker and Bassett, 1978) . The Tukey mean difference plot (Cleveland, 1993, p.21 ) was invented to compare data from such distributions. Hennig and Viroli (2016a) extended MC to the quantile-based classifier (QC) defined by d(x j , P k,j ) = ρ θ j (x j − q k,j (θ j )), where q k,j (θ j ) is the θ j -quantile of P k,j for 0 < θ j < 1 and
is the quantile distance function (Koenker and Bassett, 1978; Koenker, 2005) . When θ = 0.5, QC reduces to MC. Hennig and Viroli (2016a) showed that the QC can provide the Bayes optimal prediction with skewed input distributions. The usefulness of QC was demonstrated by simulation as well as an application (Hennig and Viroli, 2016a ). An R package which implements the centroid, median and quantile classifiers is available (Hennig and Viroli, 2016b) . Although QC is effective for discriminating high-dimensional data with heavy-tailed or skewed inputs, it suffers from the restriction of assigning each variable the same importance, which limits its effectiveness when there are irrelevant extraneous inputs. Another limitation for QC and the median centroid classifier with high dimensional data may be noise accumulation. Fan and Fan (2008, Theorem 1a) proved that the centroid classifier may perform no better than random guessing due to noise accumulation with high dimensional data.
Our proposed ensemble quantile classifier (EQC), presented in Section 2, is a flexible regularized classifier that aims to overcome these two limitations and provides better performance with high-dimensional data, asymmetric data or when there are many irrelevant extraneous inputs. We introduce the binary EQC for discriminating observations into one of two classes and then extend it to situations with more than two classes. In Theorem 2 of Section 3, it is shown that sample loss function of EQC converges to the population value when the sample size increases. In Section 4 and Section 5, the improved performance of EQC is demonstrated by a simulation study and an application to text categorization.
Ensemble predictors were derived from the idea popularly known as Wisdom of the Crowd (Hastie et al., 2009; Silver, 2012) . Newbold and Granger (1974) showed that economic time series forecasts could be improved by using a weighted average of forecasts from a heterogeneous variety of time series models. Many advanced ensemble prediction methods for supervised learning problems have been developed such as random forests (Breiman, 2001) and various boosting algorithms (Freund and Schapire, 1997; Schapire and Freund, 2012) . The ensemble stacking method introduced by Wolpert (1992) and Breiman (1996) has also been widely used. Comprehensive surveys of ensemble learning algorithms are given by Hastie et al. (2009); Dietterich (2000) ; Zhou (2012) and Lior (2019) . Ensemble stacking uses a metalearner to combine base learners. In Section 2.1 and Section 2.2 a method for using regularized logistic regression to combine quantile classifiers is developed and is generalized to the multiclass case in Section 2.3.
For the classification problem with K classes and p inputs, let q k,j (θ j ) be the θ j -quantile of P k,j where 0 < θ j < 1 for k ∈ K and j = 1, . . . , p. The derived inputs to the metalearner are obtained from the quantile-difference transformation of x = (x 1 , . . . , x p ) defined by,
where, Q
and ρ θ (u) = u(θ − 1 {u<0} ) is the quantile-distance function. The superscript (k 1 , k 2 ) is omitted if k 1 = 1 and k 2 = 2. As shown in Figure 1 the quantile-difference transformation has constant tails so the derived inputs are insensitive to outliers.
Figure 1: Quantile-difference transformation for classes 1 and 2 when q 1 (θ) < q 2 (θ).
In the binary case, the QC discriminant function is given by s(x | θ) = j Q θ j (x j ) where x = (x 1 , . . . , x p ) is a test input and θ = (θ 1 , . . . , θ p ). The classifier predicts class 1 or 2 according as s(x | θ) 0 or > 0. Hennig and Viroli (2016a) estimated the parameter θ by minimizing the misclassification rate on the training data using a grid search. In most cases they found that using the same value of θ j = θ, j = 1, . . . , p for all input variables worked well for the QC, which means a restriction θ = {θ, . . . , θ} ∈ Θ ⊆ (0, 1) p . For simplicity and computational expediency, this restriction was imposed in the simulation study and the application to text categorization.
The discriminant function for QC is simply an additive sum Q θ j (x j ) for j = 1, . . . , p but in practice it is often the case that several of the variables are more important and should be given more weights. EQC is proposed to extend QC by providing an effective classifier that takes this into account. The discriminate function for the EQC binary case may be written,
where Q θ (x) is defined in Equation (4) and C(z | β 0 , β) is the metalearner with the intercept term β 0 ∈ R and the weight vector β ∈ R p . Then (β 0 , β) along with the p quantile parameters θ ∈ (0, 1) p may be estimated by minimizing a suitable regularized loss function with a regularization parameter α using cross-validation. The metalearner C(z | β 0 , β) can be substituted by the discriminant function of most regularized classifiers such as the penalized logistic regression (Park and Hastie, 2007) or the support vector machine (SVM) (Cortes and Vapnik, 1995) . For the penalized logistic regression α = λ, where λ is the penalty defined in Equation (7) while for the SVM model with the linear kernel defined in Equation (8), α = c, where c is the cost penalty. Ridge logistic regression is recommended as a default choice for C since it often performs well. For high-dimensional data where p > n, it is preferable to treat θ as a tuning parameter and estimate it together with α using cross-validation to avoid overfitting.
When the quantiles are substituted by their estimates, the estimated discriminant function is denoted byĜ(x | θ, β 0 , β) and the estimated quantile-difference transformation is denoted bŷ Q θ (x i ).
Using the penalized logistic regression for C,
Let α = λ be the penalty parameter in the regularized binomial loss function (Friedman et al., 2010) . So given λ and the input Q θ (x), (β 0 , β) may be estimated by minimizing,
where β 1 = p j=1 |β j | for LASSO and β 2 = p j=1 β 2 j for ridge regression. Using the SVM with the linear kernel (LSVM) has the same linear discriminant function as Equation (6), but (β 0 , β) is estimated by minimizing the regularized hinge loss (Hastie et al., 2009, Equation 12.25) ,
where [x] + indicates the positive part of x and c is the cost tuning parameter. If β 0 = 0 and β j = 1 for j = 1, . . . , p, then EQC has the same decision boundary as QC. In A, it is shown that EQC with C defined in Equation (6) has the same form as the Bayes decision boundary when P 1 and P 2 consist of independent asymmetric Laplace distributions. This motivates further exploration and development of the EQC. The estimation of (β 0 , β) by ridge/LASSO penalized logistic regression and LSVM are all capable of dealing with high dimensional data. The associated ensemble classifiers used in this paper are denoted respectively by EQC/RIDGE, EQC/LASSO and EQC/LSVM. A non-negative constraint of β was also investigated but we did not find an experimentally significant accuracy improvement, which agrees with a previous study of stacking classifiers (Ting and Witten, 1999) .
Algorithm 1 shows the entire process of tuning and training EQC. Here the misclassification rate is used as a criterion to choose the tuning parameters but in some cases other criteria such as the AUC may be appropriate.
The time complexity of this algorithm is determined by the time complexities for the quantile estimation, the quantile-difference transformation and the coordinate descent algorithm which are respectively O pn log(n) , O(npH), O n(p + 1)IH) , where H is the size of the tuning set Θ, and I is the number of iterations required for minimization of the loss function. In total Algorithm 1 has complexity O (T + 1)(pn log(n) + n(p + 1)IH) , where T is the number of cross-validation folds. The computational burden for cross-validation may be reduced by using parallel computation (Kuhn and Johnson, 2013) .
A practical method to extend the binary classifier to multiclass (K > 2) is to build a set of one-versus-all classifiers or a set of one-versus-one classifiers (Hastie et al., 2009, p. 658) . A less heuristic approach, similar to the multinomial logistic regression, is to use the K − 1 log-odd-ratios to implement maximum likelihood estimation (MLE). The multinomial logistic regression requires estimation of (K − 1)(p + 1) coefficients but here the multiclass EQC only requires p + K − 1 coefficients including p weights β j , j = 1, . . . , p, and K − 1 intercept terms β 0,k , k = 1, . . . , K − 1.
Data: S = {(x 1 , y 1 ), . . . , (x n , y n )}, p = the number of variables within x. Input: Θ = tuning set of θ ∈ (0, 1) p , A = tuning set of α, T = the number of cross-validation folds. begin tuning parameters:
Randomly partition S into T non-overlap folds, S 1 , . . . , S T . for t = 1, . . . , T do fit C on S \ S t , foreach θ in Θ do Estimateq k,j (θ j ) for k = 1, 2 and j = 1, . . . , p from S \ S t . ComputeQ θ (x i ) for i = 1, . . . , n. foreach α in A do Estimate (β 0 , β) by minimizing the loss function such as Equation (7) on Q θ (x i ) for (x i , y i ) ∈ S \ S t with the tuning parameter α. Apply the estimatedĈ onQ θ (x i ) for (x i , y i ) ∈ S t and return the misclassification rates. end end end Average the misclassification rate over folds for each combination of θ and α. Denote (θ,α) as the combination with the minimum average misclassification rate. end Output: Refit C with (θ,α) on the full data S.
Let β = (β 1 , . . . , β p ). Assume for an input x,
The negative sign prior to C is used because class K is used in the denominator of the log-odd-ratios and it is the alternative class in Q (k,K) θ (x), which implies that the smaller C Q (k,K) θ (x) | β 0,k , β is, the closer x is to class k compared to class K and hence the larger the log-odd-ratios between class k and class K is. Let β 0,K = 0 and thus,
Given the tuning parameter λ, the L2 regularized log-likelihood function may be written,
It can be shown that˜ (β, {β 0,k } K−1 k=1 | θ, λ) is a concave function so it is amenable to optimization based on gradients or Newton's method. For further details see B and the software implementation (Lai and McLeod, 2018) .
In this section, the theoretical result is derived in a slightly modified setup of the method in Algorithm 1. It is assumed that p is fixed while n increases, so θ and (β 0 , β) may be estimated by maximum likelihood. In addition, α is neglected as the asymptotic properties of the selection of the tuning parameter are not discussed.
Let (θ,β 0 ,β) be the parameters that minimize the population binomial loss function,
where π 1 and π 2 are prior probabilities of the two classes. Let (θ n ,β n0 ,β n ) be the parameters that minimize the empirical binomial loss function,
It is shown that under suitable assumptions, (θ n ,β n0 ,β n ) is a consistent estimator of (θ,β 0 ,β). The proofs are available in C. These results have been proved by Hennig and Viroli (2016a) for the quantile-based classifier with the 0-1 loss function. The proof given by them has been adapted to take into account the additional parameters (β 0 , β) and the change of the loss function from the 0-1 loss function to the binomial loss function. Assumption 2 is added in addition to Assumption 1 made by Hennig and Viroli (2016a) . The linear discriminant function or metalearner C(z | β 0 , β) in Equation (6), the discriminant function with multiplicative interactions, and the polynomial discriminant function used in polynomial kernel SVM all satisfy Assumption 2.
These assumptions ensure the convergence can still hold with (β 0 , β). The use of the binomial loss function simplifies the proof and the computation. Since the 0-1 loss function is not a convex or a continuous function, its minimization is NP-hard and hence the binomial loss function or the hinge loss function are used instead. Assumption 2 of Hennig and Viroli (2016a) is not needed because the binomial loss function is used.
Assumption 2. C(z | β 0 , β) is required to be differentiable with respect to z, β 0 and β. In addition,β 0 andβ are required to be bounded. That is, ∃C > 0 such that |β j | C, for j = 0, 1, . . . , p.
Assumption 2 is needed to ensure that the estimation of (β 0 , β) converges. Theorem 1 shows that the estimated parameters are consistent in achieving the minimal population loss. Beside, Theorem 2 states that the empirical minimal loss will converge to the population minimal loss asymptotically as n → ∞ with p fixed.
Theorem 2. Under Assumptions 1 and 2, ∀ > 0,
Based on Theorem 1 and Theorem 2, when n is large relative to p, Algorithm 1 can be modified to estimate θ by minimizing the training loss function instead of using cross-validation approach.
Simulation experiments are presented to demonstrate the improved performance of EQC over QC with high-dimensional skewed inputs as well as other classifiers. The following thirteen classifiers were compared:
QC quantile-based classifier (Hennig and Viroli, 2016a) ; MC median-based classifier (Hall et al., 2009 ); EMC EQC with θ = 0.5 with ridge logistic regression;
EQC/RIDGE EQC with ridge logistic regression;
EQC/LSVM EQC with linear SVM; NB naive Bayes classifier;
LASSO LASSO logistic regression (Friedman et al., 2010) ; RIDGE ridge logistic regression (Friedman et al., 2010) ; LSVM SVM with linear kernel (Cortes and Vapnik, 1995) ; RSVM SVM with radial basis kernel (Cortes and Vapnik, 1995) .
Tuning parameters were selected by minimizing the 5-fold cross validation errors. QC, MC and EQC were fit using the R implementation (Lai and McLeod, 2018) while NB, LSVM and RSVM used the algorithms in Meyer et al. (2018) . The LDA from (Venables and Ripley, 2002) was used. RIDGE and LASSO used the package glmnet (Friedman et al., 2010) . EQC/LOGISTIC used the base R function stats::glm.
Three location-shift input distributions, corresponding to heavy-tails, highly skewed and a heterogeneous skewed, were examined as discussed by Hennig and Viroli (2016a) :
LOGNORMAL log-normal distribution;
HETEROGENEOUS equal number of W , exp(W ), log(|W |), W 2 and |W | 0.5 in order, where W ∼ N(0, 1).
All generated variables were statistically independent and the distributions were adjusted to have mean zero and variance 1. The classification error rates were estimated using 100 simulations with independent test samples of size 10 4 . For each of the three distributions a location-shift vector δ was used to produce the second class where δ = (0.32, . . . , 0.32) for T3, δ = (0.06, . . . , 0.06) for LOGNORMAL and δ = (0.14, . . . , 0.14) for HETEROGENEOUS. The additive shifts were chosen to make the test error rate of the QC close to 10% for samples of size n = 100.
Simulation experiments to demonstrate the effectiveness of prediction algorithms with highdimensional data typically use a large number of non-informative features or noise variables. For example, the models of Hastie et al. (2009, Equation 18 .37) and Fan and Fan (2008, Section 5 .1) used 95% and 98% of the variables to represent informationless random noise. We considered the influence of these irreverent variables by including Gaussian predictors independent of the classes.
For each simulation scenario, the following settings were used, 1. Training sample size n: 100, 200;
2. Number of all variables p: 50, 100, 200;
3. Standard Gaussian noises with the percentage of noise variables within the p variables set to 0%, 50%, 90%, which corresponds to 0, p/2 and 0.9 × p variables being noninformative. The corresponding simulation parameter setting will be denoted as NOISE = 0%, 50%, 90%. For example, when NOISE = 90% there are respectively 5, 10 and 20 informative variables when p = 50, 100, 200.
In addition to the case where the input variables were statistical independent, the correlated variables case was also investigated. Correlation was imposed by using the Gaussian copula with the correlation matrix uniformly sampled from the space of positive-definite correlation matrices (Joe, 2006) with equal correlations distributed as beta(0.5, 0.5). The implementation is available in the R package clusterGeneration (Qiu and Joe., 2015) .
The mean test error rates for each of the 100 simulations are tabulated in D.
The boxplots of the test error rates for the independent variables in the low dimensional, n = 200, p = 50, and the high dimensional, n = 100, p = 200, scenarios are displayed in Figure 2 and Figure 3 respectively. The scenario with extraneous noise present is shown in the bottom two rows of Figure 2 and Figure 3 and here it is seen that in both the LOGNORMAL and HETEROGENEOUS cases, the EQC methods outperform all the other methods.
Focusing on Figure 2 , in the symmetric thick-tailed case, T3, MC is best but QC, EMC and EQC/RIDGE closely approximate the MC performance as might be expected. While in the skewed cases, LOGNORMAL and HETEROGENEOUS, the four regularized EQC methods outperform all others. It is also interesting that EQC/LOGISITC has a lower error rate than QC in the HETEROGENEOUS case as shown in the panels in the right-most column. This implies that the addition of weights using the ensemble method can help improve performance when the importance of variables varies. However, even in this case the regularized EQC methods are still best and the relative performance of the regularized methods over QC improves as the proportion of noise variables increases. Next in the LOGNORMAL case shown in the middle panels in Figure 2 , EQC/RIDGE has overall the best performance though when there is no extraneous noise, QC is about the same. But as extraneous noise is added, all EQC methods improve relative to QC where EQC/LOGISTIC's performance is slightly worse than the EQC regularized logistic methods. In the high-dimensional case in Figure 3 , the conclusions are broadly similar to the lowdimensional case in Figure 2 but with two notable differences. First, QC is much worse than the EQC/RIDGE in the LOGNORMAL scenario even when all variables are informative. Since QC lacks regularization, it becomes a victim of the accumulated noise phenomenon (Fan and Fan, 2008) . Second, EQC/LASSO is much worse than EQC/RIDGE and EQC/LSVM with the low (0%) and medium (50%) level of noises since the assumption of sparse predictors made by LASSO (Hastie et al., 2009, Section 16 Figure 4 shows that the difference in classifier performance between the independent case and the dependent case is negligible in the skewed scenarios LOGNORMAL and HETERO-GENEOUS. In the T3 scenario, The performance of LDA is best and is greatly improved over the case with independent variables. This improvement is not surprising since the correlations induce heterogeneous weights on variables for the LDA (Hastie et al., 2009, Equation 4 .9). Figure 5 shows the mean test rate of QC and EQC/RIDGE trained on a sample of size n = 100 and evaluated over a grid for θ ∈ (0, 1), where 200 simulations for 10 4 test samples for each parameter setting and grid point were used. The confidence limits are too narrow to show. Looking along the first row of panels corresponding to the T3 case, the performance of EQC/RIDGE and QC is about the same for all θ. Since θ = 0.5 corresponds to the Bayes optimal median centroid (Hall et al., 2009) , both QC and EQC/RIDGE provide optimal performance in the T3 case when θ = 0.5. For the LOGNORMAL and HETEROGENOUS scenarios EQC/RIDGE outperforms QC. This figure demonstrates that the estimation of a suitable θ is important in achieving a low test error rate. Figure 5 : Mean test error rates of the QC and the EQC/RIDGE against θ for fixed n = 100, the three distributional scenarios, the number of variables p = 100, 200 and NOISE = 0%, 50%.

As in Hall et al. (2009) we used a subset of the Reuters-21578 text categorization test collection (Lewis, 1997; Sebastiani, 2002) to demonstrate the usefulness of EQC and its improved performance over MC and QC. The improved performance may be expected since this data set is high-dimensional, sparse and the variables are highly skewed. The subset contains two topics, "acq" and "crude", which can be found from the R package tm (Feinerer and Hornik, 2017) . The subset has 70 observations (documents), where n 1 = 50 are of the topic "acq" and n 2 = 20 are of the topic "crude". The raw data set was preprocessed to first remove digits, punctuation marks, extra white spaces, then convert to lower case, and remove stop words and reduce to their stem. It ended up with a 70 × 1517 document-term matrix, where a row represents a document and a column represents a term, recording the frequency of a term. A summary of the processed data set is shown in Table 1 .
The performance of a classifier was assessed by the mean classification error rate estimated by 5 repetitions of 10-fold cross-validations with each fold containing 5 documents of the topic "acq" and 2 documents of the topic "crude".
Since the performances of some classifiers such as the naive Bayes classifier and the LDA could be much improved by using external feature selection strategies, three external strategies for variable selection were investigated. The first strategy was to use a subset of the data by removing low frequency terms that appear in only one document, denoted by removeLowFreq. This produced a 70 × 766 document-term matrix. The second and third strategies used Fisher's exact test to select L = 50 or L = 1000 terms with the smallest p-values within each fold of the cross-validation. Table 2 shows the estimated error rates and their estimated standard errors for each classifier. The second column indicates the situation where no external feature selection was used. The four EQC methods, including the EMC, performed the best even without any external feature selections, followed by the QC and the MC. It was found that most of the quantile-difference transformed variables were constants, which can be removed. This sparsity may explain the improved performance of the EQC family. 
To see how the EQC performs on the multiclass problem, a larger subset of Reuters-21578, denoted by R8 (Cardoso-Cachopo, 2007) was tried. This data set contains a training set and a test set that were obtained by applying the modApte train/test split on the raw data (Lewis, 1997) .
This resulted in retaining 8 classes with the highest number of positive training examples. In order to classify those 8 classes, the same preprocessing procedure as in Section 5.1 on the R8 data set was applied. The terms were preprocessed to first remove digits, punctuation marks, extra white spaces, then convert to lower case, and remove stop words and reduce to their stem. Terms that appeared in less than 0.5% of documents were also removed, resulting in a 5485 × 1367 document-term matrix for training and a 2189 × 1367 document-term matrix for testing. The number of samples for each class is summarized in Table 3 . Classifiers in the binary case were used but with the ridge logistic regression and the LASSO logistic regression extended to the multinomial regressions, and SVM extended to multi-class SVM by the one-against-one method (Hastie et al., 2009) . Table 4 shows the mean test error and the sensitivities of different classes. The EQC still outperformed the other methods on the larger subset while the EMC, the QC and the MC performed poorly this time. With a much larger sample size, the LSVM and the ridge multinomial regression were competitive with EQC. 
The aim of the ensemble quantile classifier is to derive a regularized weighted quantile-based classifier that can best retain the advantage of QC on skewed inputs and overcome the limitation of the QC with high-dimensional data that includes noisy inputs. The improvement using EQC has been demonstrated in simulation experiments as well as with an application to text categorization. We implemented the EQC methods in Lai and McLeod (2018) , where a vignette is available for reproducing the simulations and the Reuters text categorization application in the paper.
A random variable x is said to follow the asymmetric Laplace distribution, denoted as x ∼ AL(m, κ, λ), if its probability density function has the form,
where m ∈ R, λ > 0 and κ > 0 respectively are the location, the scale and the skewness parameters.
Let π 1 and π 2 be the prior probabilities of P 1 and P 2 . If P 1 and P 2 consist of independent asymmetric Laplace distribution with parameters (m 1 , κ, λ) and (m 2 , κ, λ), then the Bayes decision boundary becomes { x : s AL (x) = 0 } with,
where for m 1 < m 2 ,
Since m is also the [κ 2 /(1 + κ 2 )]-quantile of an asymmetric Laplace distribution, if we let θ j = κ 2 j /(1 + κ 2 j ), Equation (14) will become the C Q θ (x) | β 0 , β of EQC in Equation (6) with β 0 = log(π 2 /π 1 ) and β j = λ j [θ j (1 − θ j )] −1 for j = 1, . . . , p. If x j 's are rescaled by its standard deviation √ 1 + κ 4 /(λκ) first, then the β will become
Therefore, we can see that the decision boundary given by the EQC is the Bayes decision boundary in this special case while QC cannot be if κ is not homogeneous.
In this section, we formulate the log-likelihood function of for the multiclass EQC in a matrix form as well as its gradient vector and Hessian function. This is useful for further investigation of theoretical properties and the ease of computation. At the end, we will show that the Hessian matrix is semi-negative-definite so the log-likelihood function has a single, unique maximum.
Without loss of generality, we let β 0,k = 0 for all k = 1, . . . , K − 1 and disregard them. Define 1 K = (1) 1×K , 1 n = (1) 1×n , Y = (y k,i ) K×n , where y k,i = 1 if y i = k and 0 otherwise. We also define,
where exp[·] is the entrywise exponential operation and B 1 = diag n (1 K , . . . , 1 K ) is a block diagonal matrix consisting of n repetitions of 1 K . Then the log-likelihood function of EQC given θ can be expressed,
where log[·] is the entrywise natural logarithm operation and vec[·] is a matrix vectorization operation which creates a column vector by appending all columns of the matrix. The gradient vector can be expressed,
where
stands for the entrywise multiplication or the Hadamard product and stands for the entrywise division, E 1 is an nK × p matrix with p repeated columns of exp [Qβ] , which is,
and E 2 is an n × p matrix with p repeated columns of C = B 1 exp [Qβ] , which is,
In particular, the j-th element of ∇ (β) is, for j = 1, . . . , p,
e j is a unit column vector of length p where the j-th element is 1 and the other elements are 0's. The j-th row of the Hessian matrix can be expressed as, for j = 1, . . . , p,
where
Then the Hessian matrix can be expressed,
where B 2 = diag p [1 n C ] B 1 , . . . , [1 n C ] B 1 is a block diagonal matrix consists of p repetitions of [1 n C ] B 1 .
If we let W 1 = diag nK exp(Qβ) , W 2 = diag n (1 n C ), and
then A can be expressed,
and Equation (17) and Equation (19) can also be expressed,
In particular, B 1 W 2 2 B 1 − W −1 1 W 3 is a negative semi-definite diagonal matrix as its eigenvalues are all non-positive. We can then conclude that the Hessian of (β) is a negative semi-definite matrix and so is its L2 regularized version.
Without loss of generality, we set β 0 = 0 and disregard it in the following discussions.
Proof of Theorem 1. For abbreviation, denote η = (θ, β). From the continuity implied by Lemma 3 later on, we only need to show the following converges to zero,
By Lemma 4 later on, under Assumptions 1 and 2, ∀ > 0,
where S ⊂ (0, 1) p × R p . So Equation (21) forces the first and the third term of the right hand side of Equation (20) to converge to 0 in probability.
Consider the second term now. By definitions ofη andη,
Using Equation (21) again, then both |Ψ n (η n ) − Ψ(η n )| and |Ψ(η) − Ψ n (η)| will converge to zero in probability. This makes |Ψ n (η) − Ψ n (η n )| converge to zero in probability. Therefore, Equation (20) converges to zero in probability. That is,
Proof of Theorem 2. For abbreviation, denote η = (θ, β). We will investigate
By Theorem 1, the first term of the right hand side above converges to zero in probability. By Lemma 4, the second term of the right hand side above also converges to zero in probability. Therefore, |Ψ(η) − Ψ n (η n )| converges to zero in probability and we complete the proof.
Lemma 3. Under the assumption that θ 1 θ 2 and q 1 q 2 , or θ 1 θ 2 and q 1 q 2 , the following inequality holds, |ρ θ 1 (x j − q 1 (θ 1 )) − ρ θ 2 (x j − q 2 (θ 2 ))| |x j ||θ 2 − θ 1 | + 4|q 1 − q 2 |, j = 1, . . . , p, which further implies the continuity of C(Q θ (x) | β) under Assumptions 1 and 2, and hence the continuity of Ψ(θ, β).
Proof. The inequality follows directly from the Lemma 3 in the supplementary material of Hennig and Viroli (2016a) .
It implies that the quantile-based transformation Q θ (x) is a continuous function of θ. Furthermore, since empirical quantiles are strongly consistent, β is bounded and C(z | β) is required to be differentiable with respect to z and β by Assumption 2, then C(Q θ (x) | β) is bounded and a continuous function of θ and β. So the dominated convergence theorem still makes the integrals of the differentiable transformation of C(Q θ (x) | β) continuous with respect to θ and β. where η = (θ, β) and S ⊂ (0, 1) p × R p , Proof. Assuming that the conclusion does not hold, since η is bounded according to Assumption 2, then ∃ > 0, δ > 0, there is a convergent subsequence {η * m } ∞ m=1 with limit η * = lim m→∞ η * m such that for m = 1, . . . ,
Consider
(24) Firstly, continuity of Ψ(η) implies that the third term of the right side of Equation (24) converges to 0 as m → ∞.
Consider the second term, we define a new Ψ n with the true quantiles below, where the empirical decision rule C(Q θ (x) | β) in Equation (12) is replaced by the population decision rule C(Q θ (x) | β). → 0 as m → ∞. Now consider the first term of the right hand side of Equation (24). Firstly, for j = 1, . . . , p, |q k,j,m (θ * j,m )−q k,j,m (θ * j )| |q k,j,m (θ * j,m )−q k,j (θ * j,m )|+|q k,j (θ * j,m )−q k,j (θ * j )|+|q k,j (θ * j )−q k,j,m (θ * j )|. From Theorem 3 in Mason (1982) and Assumption 1, all terms on the right side of the above inequality converge to zero almost surely, and hence |q k,j,m (θ * j,m ) −q k,j,m (θ * j )| a.s.
→ 0 as m → ∞, where || · || represents L2 norm. Furthermore, since C(z | β) is required to be differentiable with respect to z and β by Assumption 2, then |C Q
→ 0 as m → ∞. Thus the first term of the right hand side of Equation (24) → 0, which is contradictory to Equation (23) and hence we conclude that, under Assumptions 1 and 2, ∀ > 0,
where S ⊂ (0, 1) p × R p .
In Section 4.2, we presented boxplots of the test misclassification error rates for each classifier. Their averages and standard deviations are tabulated in tables 5 to 10, for the T3, LOGNORMAL and HETEROGENEOUS distribution cases with independent or dependent variables. Table 5 : Simulation study: the mean test classification error rates, and their standard errors in parentheses, of each method for the independent T3 scenario. All numbers are in percentages and rounded to one digit. The third line indicates the percentage of irrelevant variables within p variables. n = 100 p = 50 p = 100 p = 200 0% 50% 90% 0% 50% 90% 0% 50% 90% Table 6 : Simulation study: the mean test classification error rates, and their standard errors in parentheses, of each method for the dependent T3 scenario. All numbers are in percentages and rounded to one digit. The third line indicates the percentage of irrelevant variables within p variables. n = 100 p = 50 p = 100 p = 200 0% 50% 90% 0% 50% 90% 0% 50% 90% QC
28 Table 10 : Simulation study: the mean test classification error rates, and their standard errors in parentheses, of each method for the dependent HETEROGENEOUS scenario. All numbers are in percentages and rounded to one digit. The third line indicates the percentage of irrelevant variables within p variables. n = 100 p = 50 p = 100 p = 200 0% 50% 90% 0% 50% 90% 0% 50% 90% 
This article is included in the gateway. RPackage Given a target (response or dependent) variable Y of n measurements and a set X of p features (predictor or independent variables) the problem of feature (or variable) selection (FS) is to identify the minimal set of features with the highest predictability a on the target variable (outcome) of interest. The natural question that arises, is why should researchers and practitioners perform FS. The answer to this is for a variety of reasons 1 , such as: a) many features may be expensive (and/or unnecessary) to measure, especially in the clinical and medical domains; b) FS may result in more accurate models (of higher predictability) by removing noise while treating the curse-of-dimensionality; c) the final produced parsimonious models are computationally cheaper and often easier to understand and interpret; d) future experiments can benefit from prior feature selection tasks and provide more insight into the problem of interest, its characteristics and structure. e) FS is indissolubly connected with causal inference that tries to identify the system's causal mechanism that generated the data.
R contains thousands of packages, but only a small portion of them are dedicated to the task of FS, yet offering limited or narrow capabilities. For example, some packages accept few or specific types of target variables (e.g. binary and multi-class only). This leaves many types of target variables, e.g. percentages, left censored, positive valued, matched case-control data, etc., untreated. The availability of regression models for some types of data is rather small. Count data is such an example, for which Poisson regression is the only model considered in nearly all R packages. Most algorithms including statistical tests offer limited statistical tests, e.g. likelihood ratio test only. Almost all available FS algorithms are devised for large sample sized data, thus they cannot be used in many biological settings where the number of observations rarely (or never in some cases) exceeds 100, but the number of features is in the order of tens of thousands. Finally, some packages are designed for high volume data b only.
In this paper we present MXM c 2 ; an R package that overcomes the above shortcomings. It contains many FS algorithms d , which can handle numerous and diverse types of target variables, while offering a pool of regression models to choose from and feed the FS algorithms. There is a plethora of statistical tests (likelihood-ratio, Wald, permutation based) and information criteria (BIC and eBIC) to plug into the FS algorithms. Algorithms that work with small and large sample sized data, algorithms that have been customized for high volume data, and an algorithm that returns multiple sets of statistically equivalent features are some of the key characteristics of MXM.
Over the next sections, a brief qualitative comparison of MXM with other packages available on CRAN and Bioconductor is presented, its (dis)advantages are discussed, its FS algorithms and related functions are mentioned. Finally a demonstration takes place, applying some FS algorithms available in MXM on real high dimensional data. a Predictive performance metrics include AUC, accuracy, mean squared error, mean absolute error, concordance index, F score, proportion of variance explained, etc. b In statistics and in the R packages the term "big data" is used to refer to such data. In the computer science terminology, big data are of much higher volume and require specific technology. For this reason we chose to use the term "high volume" instead of "big data".
When searching for FS packages on CRAN and Bioconductor repositories using the keywords "feature selection", "variable selection", "selection", "screening" and "LASSO" e , we detected 184 R packages until the 7th of May 2018 f . Table 1 shows the frequency of the target variable types those packages accept g , while Figure 1 shows the frequency of R packages whose FS algorithms can treat at least k types of target variables, for k = 1, 2, . . . , 8, of those presented in Table 1 . Table 2 presents the frequency of pairwise types of target variables offered in R packages and Table 3 contains information on packages allowing for less frequent regression models. Most packages offer FS algorithms that are oriented towards specific types of target variables, methodology and regression models, offering at most 3-4 options. Out of these 184 packages, 65 (35.32%) offer LASSO type FS algorithms, while 19 (10.32%) address the problem of FS from the Bayesian perspective. Only 2 (1.08%) R packages treat the case of FS with multiple datasets h while only 4 (2.17%) packages are devised for high volume data. We highlight the fact that especially on hrefhttps://cran.r-project.org/CRAN, packages are uploaded at a super-linear rate. Bioconductor is more strict with the addition of new packages. The phenomenon of abandoned or not maintained packages for a long time is not at all unusual. Such an example is "biospear", removed from CRAN (archived) in the 30th of April 2018. On the other hand we manualy added in our list a package that performs FS without mentioning it in its title. g We manually examined each package to identify the types of target variables it accepts and regression models it offers.
h Instead of having a target variable and a set of features, one can have two or more sets of target variables and features. The algorithm we have devised for this case uses simultaneously all target variables and sets of features Table 4 summarizes the types of target variables treated by MXM' FS algorithms along with the appropriate regression models that can be employed. The list is not exhaustive, as in some cases the type of the predictor variables (continuous or categorical) affects the decision of using a regression model or a test (Pearson and Spearman for continuous and G 2 test of independence for categorical). With percentages for example, MXM offers numerous regression models to plug into its FS algorithms: beta regression, quasi binomial regression or any linear regression model (robust or not) after transforming the percentages using the logistic transformation. For repeated measurements (correlated data), there are two options offered, the GeneralisedGeneralised Linear Mixed Models (GLMM) and Generalised Estimating Equations (GEE) which can also be used with various types of target variables, not mentioned here. We emphasize that MXM is the only package that covers all types of response variables mentioned on Table 1 , many types of which are not available in any other FS package, such as left censored data for example. MXM also covers 3 out 4 cases that appear on Table 3 .
Most of the currently available FS algorithms in the MXM package have been developed by the creators and authors of the package (see the last column of positive rates 10 . MMPC formed the basis of Max Min Hill Climbing (MMHC) 11 , a prototypical algorithm for learning the structure of a Bayesian network which outperformed all other Bayesian network learning algorithms with categorical data. For time-to-event and nominal categorical target variable, MMPC 12 , and 13 respectively, outperformed or was on par with LASSO and other FS algorithms. SES 2 was contrasted against LASSO with continuous, binary and survival target variables, resulting in similar conclusions as before. With temporal and time-course data, SES 14 outperformed the LASSO algorithm 15 both in predictive performance and computational efficiency. FBED 5 was compared to LASSO for the task of binary classification with sparse data exhibiting performance similar to that of LASSO. As for gOMP, our experiments have showed very promising results, achieving similar or better performance, while enjoying higher computational efficiency than LASSO 6 .
The main advantage of MXM is that all FS algorithms accept numerous and diverse types of target variables. MMPC, SES and FBED treat all types of target variables presented in Table 4 , while gOMP handles fewer types i .
MXM is the only R package that offers many different regression models to be employed by the FS algorithms, even for the same type of response variable, such as Poisson, quasi Poisson, negative binomial and zero inflated Poisson regression for count data. For repeated measurements, the user has the option of using GLMM or the GEE methodology (the latter with more options in the correlation structure) and for time-toevent data, Cox, Weibull and exponential regression models are the available options.
A range of statistical tests and methodologies to select the features is offered. Instead of the usual log-likelihood ratio test, the user has the option to use the Wald test or produce a p-value based on permutations. The latter is useful and advised when the sample size is small, emphasizing the need for use of MMPC and SES, both of which are designed for small sample sized datasets. FBED on the other hand, appart from the log-likelihood ratio test offers the possibility of using information criteria, such as BIC 16 and eBIC 17 .
No p-values correction (e.g. Benjamini and Hochberg 18 ,) is applied. Specifically MMPC (and SES essentially) has been proved to control the False Discovery Rate 19 . However, we allow for permutation-based p-values when performing MMPC and SES. FBED addresses this issue either by removing the non-significant variables or by using information criteria such as the extended BIC 17 . Borboudakis The Wald and log-likelihood ratio tests are asymptotic tests. In addition, the fitted regression models require large sample size. With small sample sizes, the fitted regression models must not contain many predictor variables in order not to estimate many parameters.
To address these issues both MMPC and SES perform conditional independence tests using subsets of selected variables, thus reducing the number of estimated parameters.
Not all MXM's FS algorithms and all regression models used are computationally efficient. The (algorithmic) order of complexity of the FS algortihms is comparable to state-of-art FS algorithms, but the nature of the other algorithms is such that many regression models must be fit increasing the computational burden. In addition, R itself does not allow for further speed improvement. For example, MMPC can be slow for many regression models, such as negative binomial or ordinal regression for which we rely on implementations in other R packages. gOMP, on the other hand, is the most efficient algorithm available in MXM k gOMP superseded the LASSO implementation in the package glmnet 24 in both time and performance., because it is residual based and few regression models are fit. With clustered/longitudinal data, SES (and MMPC) were shown to scale to tens of thousands and be dramatically faster than LASSO 14 . Computational efficiency is also programming language-dependent. Most of the algorithms are currently written in R and we are constantly working towards transferring them to C++ so as to decrease the computational cost significantly.
It is impossible to cover all cases of target variables; we have no algorithms for time series, and do not treat multi-state time-to-event target variables for example, yet we regularly search for R packages that treat other types of target variables and link them to MXM l . All algorithms are limited to linear or generalised linear relationships, and we plan to address this issue in the future. The gOMP algorithm does not accept all types of target variables and works only with continuous predictor variables. This is a limitation of the algorithm, but we plan to address this in the future as well.
Cross-validation functions currently exist only for MMPC, SES and gOMP, but performance metrics are not available for all target variables. When the target variable is binary AUC, accuracy or the F score can be utilised, when the target takes continuous values, the mean squared error, the mean absolute error or the proportion of variance explained can be used, whereas with survival target variables the concordance index can be computed. Left censored data, is an example of target variable whose predictive performance estimation is not offered. A last drawback is that currently MXM does not offer graphical visualization of the algorithms and of the final produced models.
Which FS algorithm from MXM to use and when In terms of sample size, FBED and gOMP are generally advised for large-sample-sized datasets, whereas MMPC and SES are designed mainly for small-sample-sized datasets m . In the case of a large sample size and few features, FSR or BSR are also suggested. In terms of number of features, gOMP is the only algorithm that scales up when the number of features is in the order of the hundreds of thousands. gOMP is also suitable for high volume data that contain a high number of features, really large sample sizes or both. FBED has been customized to handle high volume data as well, but with large sample sizes and only a few thousand features. If the user is interested in discovering more than one set of features, SES is suitable for returning multiple solutions, which are statistically equivalent. With multiple datasets n , both MMPC and SES are currently the only two algorithms that can handle some cases (both the target variable and the set of features are continuous). As for the availability of the target variable, MMPC, SES and FBED handle all types of target variables available in MXM, listed in Table 4 , while gOMP accepts fewer types of target variables. Regarding the type of features, gOMP currently works with continuous features only, whereas all other algorithms accept both continuous and categorical features. All this information is presented in Table 5 .

MXM is an R package that makes use of (depends or imports) other packages that offer regression models or utility functions.
• bigmemory: for large volume data.
• doParallel: for parallel computations.
k Based on our experiments 6 . l Currently, with little effort, one should be able to plug-in their own regression model into some of the algorithms. We plan to expand this possibility for all algorithms. m To the best of our knowledge there are not many FS algorithms dealing with small sample sized data.
n Instead of having one dataset only to analyze one might have multiple datasets from different sources. We do not combine all datasets into a larger one, neither perform FS for each dataset separately. Each step of the MMPC and SES algorithms is performed simultaneously to all datasets.
• coxme: for frailty models.
• geepack: for GEE models.
• lme4: for mixed models.
• MASS: for negative binomial regression, ordinal regression and robust (MM type) regression.
• nnet: for multinomial regression.
• ordinal: for ordinal regression.
• quantreg: for quantile regression.
• stats (built-in package): for generalised linear models.
• survival: for survival regression and Tobit regression.
• Rfast: for computational efficiency.
MXM contains functions for returning the selected features for a range of hyper-parameters for each algorithm. For example, mmpc.path runs MMPC for multiple combinations of threshold and max k , and gomp.path runs gOMP for a range of stopping values. The exception is with FBED, for which the user can give a vector of values of K in fbed.reg instead of a single value. Unfortunately, the path of significance levels cannot be determined at a single run o .
MMPC and SES have been implemented in such a way that the user has the option to store the results (p-values and test statistic values) from a single run in a hash object. In subsequent runs, with different hyperparameters this can lead to significant amounts of computational savings because it avoids performing tests that have been already been performed. These two algorithms give the user an extra advantage. They can search for the subset of feature(s) that rendered one more specific feature(s) independent of the target variable by using the function certificate.of.exclusion.
FBED, SES and MMPC are three algorithms that share common grounds. The list with the results of the univariate associations (test statistic and logged p-value) can be calculated from either algorithm and be passed onto any of them. When one is interested in running many algorithms, this can reduce the computational cost significantly. Note also that the univariate associations in MMPC and SES can be calculated in parallel, with multi-core machines. More FS related functions can be found in MXM's reference manual and vignettes section available on CRAN. it is ready to be used without internet connection. The system requirements are documented on MXM's webpage on CRAN.

We will now demonstrate some FS algorithms available in MXM, using real datasets. Specifically we will show the relevant commands and describe part of their output. With user-friendliness taken into consideration, extra attention has been put in keeping the functions within the MXM package as consistent as the nature of the algorithms allows for, in terms of syntax, required input objects and parameter arguments. Table 6 contains a list of the current FS algorithms, but we will demonstrate some of them here. In all cases, the arguments "target", "dataset" and "test" refer to the target variable, set of features and type of regression model to be used. o We plan to implement this more efficiently in the future 
The first dataset we used concerns breast cancer, with 295 women selected from the fresh-frozen-tissue bank of the Netherlands Cancer Institute 25 . The dataset contains 70 features and the target variable is time to event, with 63 censored values p . We need this information, to be passed as a numerical variable indicating the status (0 = censored, 1 = not censored), for example (1, 1, 0, 1, 1, 1, . . . ). We will make use of the R package survival 26 for running the appropriate models (Cox and Weibull regression) and show the FBED algorithm with the default arguments. Part of the output is presented below. Information on the selected features, their test statistic and their associated logarithmically transformed p-value q , along with some information on the number of regression models fitted is displayed. The first column of $res denotes the selected features, i.e. the 28th and the 6th feature were selected. The second and third columns refer to the feature(s)'s associated test statistic and p-value. The $info informs the user on the value of K used, the number of selected features and the number of tests (or regression models) performed. p Censoring occurs when partial information about some observations is available. It might be the case that some individuals will experience the event after completion of the study. Or when an individual is not part of the study for anymore, for a reason other than the occurrence of the event of interest. In a study about cancer, for example, some patients may die of another cause, e.g. another disease or car accident for example. The survival times of those patients has been recorded, but offer limited information.
q The logarithm of the p-values is computed and return in order to avoid small p-values (less than the machine epsilon 10 −16 ) being rounded to 0. This is a crucial and key element of the algorithms because they rely on the correct ordering of the p-values.
The above output was produced using Cox regression. If we used Weibull regression instead (test = "testIndWR"), the output would be slightly different. Only one feature (the 28th) was selected, and FBED performed 75 tests (based upon 75 fitted regression models). The element res presented below is one of the elements of the returned output. The first column shows the selected variables in order of inclusion and the second column is the deviance of each regression model. The first line refers to the regression model with 0 predictor variables (constant term only). 

The next dataset we will use is NCBI Gene Expression Omnibus accession number GSE9105 28 , which contains 22,283 features about skeletal muscles from 12 normal, healthy glucose-tolerant individuals exposed to acute physiological hyperinsulinemia, measured at 3 distinct time points. Following 14 , we will also use SES and not FBED because the sample size is small. The grouping variable that identifies the subject along with the time points is necessary in our case. If the data were clustered data, i.e. families, where no time is involved, the argument "reps" would not be provided. The user has the option to use GLMM 29 or GEE 30 . The output of SES (and of MMPC) is long and verbose, and thus we present the first 10 set of equivalent signatures. The first row is the set of selected features, and every other row is an equivalent set. In this example, the last four columns are the same and only the first changes. This means, that the feature 2683 has 9 statistically equivalent features, (2, 7, 10, ...). 
The next dataset we consider is from Human cerebral organoids recapitulate gene expression programs of fetal neocortex development 31 . The data are pre-processed RNA-seq, thus continuous data, with 729 samples and 58, 037 features. We selected the first feature as the target variable and all the rest were considered to be the features. In this case we used FBED and gOMP, employing the Pearson correlation coefficient because all measurements are continuous. gOMP on the other hand was more parsimonious, selecting only 8 features. At this point we must highlight the fact that the selection of a feature was based on the adjusted R 2 value. If the increase in the adjusted R 2 due to the candidate feature was more than 0.01 or (1/%), the feature was selected. 
The final example is on discrete valued target variable (count data) for which Poisson and quasi-Poisson regression models will be employed by the gOMP algorithm. The dataset with GEO accession number GSE47774 32 contains RNA-seq data with 256 samples and 43,919 features. We selected the first feature to be the target variable and all the rest are the features.
We ran gOMP using Poisson (test="testIndPois") and quasi Poisson (test="testIndQPois") regression models, but we changed the stopping value to tol=12. Due to over-dispersion (variance > mean), quasi Poisson is more appropriate r than Poisson regression that assumes that the mean and the variance are equal. When Poisson was used, 107 features were selected; since the wrong model was used, many false positive features were included, while with the quasi Poisson regression only 10 features were selected. r Negative binomial regression, test="testIndNB" is another alternative option. More recently 34 , applied MMPC in order to identify the features that provide novel information about steady-state plasma glucose (SSPG, a measure of peripheral insulin resistance) and are thus most useful for prediction 35 . used gOMP and identified the viscoelastic properties of the arterial tree as an important contributor to the circulating bubble production after a dive. Finally 36 , applied SES and gOMP were applied in the field of fisheries for identifying the genetic SNP loci that are associated with certain phenotypes of the gilthead seabream (Sparus aurata). Measurements from multiple cultured seabream families were taken, and since the data were correlated and GLMM was applied. The study led to a catalogue of genetic markers that set the ground for understanding growth and other traits of interest in Gilthead seabream, in order to maximize the aquaculture yield.
We presented the R package MXM and some of its feature selection algorithms. We discussed its advantages and disadvantages and compared it, at a high level, with other competing R packages. We then demonstrated, using real high-dimensional data with a diversity of types of target variables, four FS algorithms, including different regression models in some cases.
The package is constantly being updated with new functions and improvements being added and algorithms being transferred to C++ to decrease the computational cost. Computational efficiency was mentioned as one of MXM' disadvantage which we are trying to address. However, computational efficiency is one aspect, and flexibility another. Towards flexibility we plan to add of more regression models, more functionalities, options and graphical visualizations.
• The first dataset we used (survival target variable) is available from Computational Cancer Biology.
• The second dataset we used (unmatched case control target variable) is available from GEO.
• The third dataset we used (longitudinal data) is available from GEO.
• The fourth dataset we used (continuous target variable) is available from GEO.
• The fifth dataset we used (count data) is available from GEO.
MXM is available from: https://cran.r-project.org/web/packages/MXM/index.html.
Source code is available from: https://github.com/mensxmachina/MXM-R-Package
Archived source code at time of publication: https://doi.org/10.5281/zenodo.3458013 37
License: GPL-2.
expertise to confirm that it is of an acceptable scientific standard. 
Department of Biostatistics, Johns Hopkins University, Baltimore, MD, USA The manuscript introduces a new R package, MXM, that offers a variety of feature selection algorithms in regression models. The new package presents relevant contribution to the toolbox of feature selection algorithms by covering more types of target variables than most existing packages, accommodating data with small or big sample sizes, and providing additional functionalities and utility features. The manuscript also demonstrates the usage of several functions in the package by analyzing real data.
Regarding the presentation of the manuscript, I share the same concern with reviewer Thodoris that the acronyms in the paper come with poor explanation. Although several of the acronyms are explained in Table 4 and Table 6 , most of appearances of these acronyms have no reference to these tables or explanations. This makes the paper unnecessarily hard to read.
Secondly, I think the demonstration of the usage of the package could be more informative with more interpretations. For example, how can we interpret the p-values, adjusted R-squares, and deviance in the results of the model fittings? High level descriptions of the algorithms behind the demonstrated functions could also help the reader better understand the results.
Below are comments to specific places in the manuscript:
The 3rd paragraph in Introduction mentions that statistical tests (likelihood ratio, Wald, permutation based) can be plugged into the feature selection algorithms that work with small sample sized data. The second last paragraph on Page 6 is a bit confusing to me. The paragraph starts with pointing out computational efficiency as a disadvantage of MXM. However, this is followed by explaining why gOMP is efficient, and pointing out that SES and MMPC scales better and run faster than LASSO package. These seem like advantages in computational efficiency. In Paragraph 2 of "FS-related functions" section: for MMPC and SES, storing the results from one run and passing it to subsequent runs can lead to significant computational savings. Are there savings because the trained models serve as good starting points in the subsequent runs? On Page 9 in the output of MXM::fbed.reg, there are p-values associated with each selected variable. How can we interpret these p-values? It seems that these p-values are not adjusted for the extra degrees of freedom from feature selection. Below are some minor suggestions/typo fixes:
In Abstract -"The R package MXM is such an example, which offers ...": It's unclear to me what "such an example" refers to. It's clearer to state "The R package MXM offers ..." directly. Second paragraph in Introduction: "For example, packages that accept few or specific types of target variables." → "For example, some packages accept few or specific types of target variables." (make it a complete sentence.)
In "MXM versus other R packages" section: "... can treat at least one type of target variable, ..." → "... can treat at least k types of target variables, for k = 1, 2, ..., 8, ..." In the last paragraph on Page 5, does "BN" refer to Bayesian network (which also appears in the same paragraph)? In Paragraph 5 on Page 6 -"... making it one one of the few FS algorithms ...": remove the extra "one". In Paragraph 6 on Page6 -"MXM is using an efficient memory handling R package.": please cite the package. Last Paragraph on Page 7: "generalised" → "generalized" to be consistent with other places in the paper. First paragraph in Section "Use cases": the argument "test" refers to the type of regression model to be used. Why not name the argument something like "model type"? Some of the citations are not easily distinguishable from footnotes. e.g., citation 21 for Rfast on Page 8 and footnote 7 are both superscripts. Paragraph 2 on Page 11: "gOMP on the other has ..." → "gOMP on the other hand ...". First paragraph on Page 12 -"YouTube video streaming applications applications": duplicate "applications".


Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?
Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes
No competing interests were disclosed.
Reviewer Expertise: statistical machine learning, feature selection, high dimensional data, graphical models, time series analysis, clinical trial design I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.
Author Response 12 Jul 2019 , University of Crete, Greece
We are grateful to the reviewers for their on-the-spot comments which we have addressed.
The manuscript introduces a new R package, MXM, that offers a variety of feature selection algorithms in regression models. The new package presents relevant contribution to the toolbox of feature selection algorithms by covering more types of target variables than most existing packages, accommodating data with small or big sample sizes, and providing additional functionalities and utility features. The manuscript also demonstrates the usage of several functions in the package by analyzing real data.
Comment: Regarding the presentation of the manuscript, I share the same concern with reviewer Thodoris that the acronyms in the paper come with poor explanation. Although several of the acronyms are explained in Table 4 and Table 6 , most of appearances of these acronyms have no reference to these tables or explanations. This makes the paper unnecessarily hard to read. Reply: We have added the interpretation of the acronyms when they first appear, at various places within the text. Comment: Secondly, I think the demonstration of the usage of the package could be more informative with more interpretations. For example, how can we interpret the p-values, adjusted R-squares, and deviance in the results of the model fittings? High level descriptions of the algorithms behind the demonstrated functions could also help the reader better understand the results. Reply: We thank the reviewer for this comment. We have added a small description of each algorithm does in Section "The MXM's FS algorithms and comparison with other FS algorithms". This was also highlighted by Prof Kypraios. Also, in Section "Advantages and disadvantages of MXM's FS algorithms" we have added a small paragraph regarding the p-values produced by the algorithms. Below are comments to specific places in the manuscript:
Comment: The 3rd paragraph in Introduction mentions that statistical tests(likelihood ratio, Wald, permutation based) can be plugged into the feature selection algorithms that work with small sample sized data. The aforementioned tests traditionally rely on large sample theory. I wonder what adjustments are needed to accommodate small sample sizes?
Reply: We have added a foot note in page 5 regarding this. There as on we added this I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.
In the title compound, C 18 H 13 ClFNO, the dihedral angle between the mean planes of the chloro-and fluoro-substituted benzene ring and the naphthalene ring system is 60.5 (8) . In the crystal, molecules are linked by N-HÁ Á ÁO hydrogen bonds, forming a zigzag chain along [101] .
For the structural similarity of N-substituted 2-arylacetamides to the lateral chain of natural benzylpenicillin, see: Mijin & Marinkovic (2006) ; Mijin et al. (2008) . For the coordination abilities of amides, see: Wu et al. (2008 Wu et al. ( , 2010 Table 1 Hydrogen-bond geometry (Å , ). 
A. S. Praveen, J. P. Jasinski, J. A. Golen, B. Narayana and H. S. Yathirajan Comment N-Substituted 2-arylacetamides are very interesting compounds because of their structural similarity to the lateral chain of natural benzylpenicillin (Mijin et al., 2006 (Mijin et al., , 2008 . Amides are also used as ligands due to their excellent coordination abilities (Wu et al., 2008 (Wu et al., , 2010 . Crystal structures of some acetamide derivatives, viz., 2-(4-bromophenyl)-N-(2-methoxyphenyl)acetamide (Xiao et al., 2010) , N-benzyl-2-(3-chloro-4-hydroxyphenyl)acetamide (Davis & Healy, 2010), 2-(2,2-dimethyl-2,3-dihydro-1-benzofuran-7-yloxy)-N-(o-tolyl)acetamide (Li et al., 2010) , N-benzyl-2-(2-bromophenyl)-2-(2-nitrophenoxy) acetamide (Li & Wu, 2010) and N-(4-chlorophenyl)-2-(8-quinolyloxy)acetamide monohydrate (Wang et al., 2010) have been reported. In view of the importance of amides, we report herein the crystal structure of the title compound, (I), C 18 H 13 ClFNO.
In the title compound, C 18 H 13 ClFNO, the dihedral angle between the mean planes of the chloro, fluoro substituted benzene ring and the naphthalene-1-yl ring is 60.5 (8)° (Fig. 2) . Bond distances are in normal ranges (Allen et al., 1987) .
Crystal packing is stabilized by N-H···O hydrogen bonds ( Fig. 3 and Table 1 ).
Naphthalen-1-ylacetyl chloride (0.204 g, 1 mmol) and 3-chloro-4-fluoroaniline (0.145 g, 1 mmol) were dissolved in dichloromethane (20 mL). The mixture was stirred in presence of triethylamine at 273 K for about 3 h (Fig. 1) . The contents were poured into 100 ml of ice-cold aqueous hydrochloric acid with stirring, which was extracted thrice with dichloromethane. Organic layer was washed with saturated NaHCO 3 solution and brine solution, dried and concentrated under reduced pressure to give the title compound (I). Single crystals were grown from toluene by the slow evaporation method (M.P.:

The N-bound H atom was located in a difference Fourier map and refined isotropically with a distance restraint of N-H = 0.86 (2) Å. All of the remaining H atoms were placed in their calculated positions and then refined using the riding model, with C-H lengths of 0.95 Å (CH) or 0.99 Å (CH 2 ). Isotropic displacement parameters for these atoms were set to 1.19-1.21 (CH) or 1.20 (CH 2 ) times U eq of the parent atom. Figures   Fig. 1 . Reaction scheme of the title compound, (I). 
Crystal data 
Geometry. All esds (except the esd in the dihedral angle between two l.s. planes) are estimated using the full covariance matrix. The cell esds are taken into account individually in the estimation of esds in distances, angles and torsion angles; correlations between esds in cell parameters are only used when they are defined by crystal symmetry. An approximate (isotropic) treatment of cell esds is used for estimating esds involving l.s. planes. 
The stochastic block model (SBM), or planted partition model, is a celebrated model that captures the clustering or community structure in large networks. Fundamental phase transition phenomena and limitations for efficient algorithms have been established for the "vanilla" SBM, with equal-size communities [9, 10, 27, 30, 31, 24, 1, 16, 2, 11] . However, when applying the algorithms to real network datasets, one needs to carefully examine the validity of the vanilla SBM model. First, real networks are heterogeneous and imbalanced; they are often characterized by unequal community size, degree heterogeneity, and distinct connectivity strengths across communities. Second, in real networks, additional side information is often available. This additional information may come, for instance, in the form of a small portion of revealed community memberships, or in the form of node features, or both. In this paper, we aim to address the above concerns by answering the following questions:
Algorithm For a general stochastic block model that allows for heterogeneity and contains noisy or partial side information, how to utilize this information to achieve better classification performance?
Theory What is the transition boundary on the signal-to-noise ratio for a general heterogeneous stochastic block model? Is there a physical explanation for the optimal misclassification error one can achieve?
We define the general SBM with parameter bundle (n, k, N ∈ R k ,Q ∈ R k×k ) as follows. Let n denote the number of nodes and k the number of communities. The vector N = [n 1 , n 2 , . . . , n k ] T denotes the number of nodes in each community. The symmetric matrix Q = [Q i j ] represents the connection probability: Q i j is the probability of a connection between a node in community i to a node in community j . Specifically, one observes a graph G(V, E ) with |V | = n, generated from SBM as follows. There is a latent disjoint partition that divides V = k l =1
V l into k communities. Define (·) : V → [k] to be the label (or, community) of a node v. For any two nodes v, u ∈ V , there is an edge between (u ↔ v) ∈ E with probability Q (u), (v) . The goal is to recover the latent label (v) for each node v. Here we consider the following kinds of heterogeneity: unequal size communities (represented by [n i ]), different connection probabilities across communities (as given by [Q i j ]), and degree heterogeneity (due to both [n i ] and [Q i j ]).
We study the problem when either noisy or partial label information is available in addition to the graph structure and show how to "optimally" improve the classification result (in terms of misclassification error). We argue that this is common for many practical problems. First, in real network datasets, a small portion of labels (or, community memberships) is often available. Second, a practitioner often has certain initial guess of the membership, either through training regression models using node features and partially revealed labels as side information, or running certain clustering algorithms (for example, spectral clustering using non-backtracking matrix, semi-definite programs or modularity method) on a subset or the whole network. We will show that as long as these initial guesses are better than random assignments, one can "optimally weigh" the initial guess according to the network structure to achieve small misclassification error.
Formally, the noisy (or partial) information is defined as a labeling˜ prior on the nodes of the graph with the following stochastic description. The parameter δ quantifies either (a) the portion of randomly revealed true labels (with the rest of entries in˜ prior missing), or (b) the accuracy of noisy labeling˜ prior , meaning
and when˜ prior (v) = (v), each label occurs with equal probability.
In the literature on vanilla SBM (equal size communities, symmetric case), there are two major criteriaweak and strong consistency. Weak consistency asks for recovery better than random guessing in a sparse random graph regime (p, q 1/n), and strong consistency requires exact recovery for each node above the connectedness theshold (p, q log n/n). Interesting phase transition phenomena in weak consistency for SBM have been discovered in [10] via the insightful cavity method from statistical physics. Sharp phase transitions for weak consistency have been thoroughly investigated in [9, 30, 31, 32, 27] .
In particular for k = 2, spectral algorithms on the non-backtracking matrix have been studied in [27] and the non-backtracking walk in [32] . In these two fundamental papers, the authors resolved the conjecture on the transition boundary for weak consistency posed in [10] . Spectral algorithms as initialization and belief propagation as further refinement to achieve better recovery was established in [31] . Recent work of [3] establishes the positive detectability result down to the Kesten-Stigum bound for all k via a detailed analysis of a modified version of belief propagation. For strong consistency, [1, 16, 17] established the phase transition using information-theoretic tools and semi-definite programming (SDP) techniques. In the statistics literature, [40, 14] investigated the misclassification rate of the standard SBM.
For the general SBM with connectivity matrix Q, [15, 5, 7] provided sharp non-asymptotic upper bound analysis on the performance of a certain semi-definite program. They investigated the conditions on Q for a targeted recovery accuracy, quantified as the loss (as a matrix norm) between the SDP solution and the ground truth. The results are more practical for heterogeneous real networks. However, for the analysis of SDP to work, these results all assume certain density gap conditions, i.e., max 1≤i < j ≤k Q i j < min 1≤i ≤r Q i i , which could be restrictive in real settings. Our technical approach is different, and does not require the density gap conditions. Moreover, we can quantify more detailed recovery guarantees, for example, when one can distinguish communities i , j from l , but not able to tell i , j apart. In addition, our approach can be implemented in a decentralized fashion, while SDP approaches typically do not scale well for large networks.
For SBM with side information, [20, 6, 38] considered SBM in the semi-supervised setting, where the side information comes as partial labels. [20] considered the setting when the labels for a vanishing fraction of the nodes are revealed, and showed that pushing below the Kesten-Stigum bound [22, 21] is possible in this setting, drawing a connection to a similar phenomenon in k-label broadcasting processes [29] . In addition, [6, 38] studied linearized belief propagation and misclassification error on the partially labeled SBM.
The focus of this paper is on local algorithms, which are naturally suited for distributed computing [25] and provide efficient solutions to certain computationally hard combinatorial optimization problems on graphs. For some of these problems, they are good approximations to global algorithms [23, 13, 36, 35] . The fundamental limits of local algorithms have been investigated, in particular, in [28] in the context of a sparse planted clique model. We also want to point out that [34] studied the local belief propagation and characterized the expected fraction of correctly labeled vertices using fixed point analysis of the density evolution, in the case of vanilla SBM with side information.
Finally, we briefly review broadcasting processes on trees. Consider a Markov chain on an infinite tree rooted at ρ with branching number b. Given the label of the root (ρ), each vertex chooses its label by applying the Markov rule M to its parent's label, recursively and independently. The process is called broadcasting process on trees. One is interested in reconstructing the root label (ρ) given all the nth level leaf labels. Sharp reconstruction thresholds for the broadcasting process on general trees for the symmetric Ising model setting (each node's label is {+, −}) have been studied in [12] . [33] studied a general Markov channel on trees that subsumes k-state Potts model and symmetric Ising model as special cases, and established non-census-solvability below the Kesten-Stigum bound. [18] extended the sharp threshold to robust reconstruction, where the vertex' labels are contaminated with noise. The transition thresholds proved in the above literature correspond to the Kesten-Stigum bound b|λ 2 (M )| 2 = 1 [22, 21] .
The main results of the present paper are summarized as follows.
We propose a new local algorithm -Weighted Message Passing (WMP) -that can be viewed as linearized belief propagation with a novel weighted initialization. The optimal weights are jointly determined by the minimum energy flow that captures the imbalance of local treelike neighborhood of SBM, and by the second eigenvectors of the Markov transition matrix for the label broadcasting process. As we will show, these initializations are crucial for the analysis of general SBM that is heterogeneous and asymmetric.
For the technical contribution, we provide non-asymptotic analysis on the evolution of WMP messages. For general number of communities, it is challenging to track the densities of WMP messages during evolution. We overcome the difficulty through introducing carefully chosen weights and then prove concentration-of-measure phenomenon on messages.
We establish a close connection between the misclassification error and a notion called minimum energy through the optimally weighted message passing algorithm. In fact, we show that asymptotically almost surely, the misclassification error of WMP Err(ˆ wmp ) satisfies
, where E * (θ −2 ) is defined as the minimum energy based on the local tree-like neighborhood, with θ 2 chosen as the conductance level on the edges of the tree. Intuitively, the smaller the energy is, the better the misclassification error one can achieve. This result provides a physical interpretation for the misclassification error. In return, the above upper bound provides a principled way of choosing the optimal weights as to minimize the energy determined by the Thomson's principal [26] . This approach is key to dealing with asymmetric and imbalanced local neighborhoods.
Transition Boundary We show that the Kesten-Stigum bound is the sharp boundary for local algorithms on the signal-to-noise ratio for the general heterogeneous SBM. Define the following quantities
and SNR := λθ 2 ,
where N ,Q are defined in Section 1.1, and λ i (·) denotes the i -th eigenvalue. Then the Kesten-Stigum bound SNR = 1 is the threshold for local algorithms. Above it, the minimum energy E * (θ −2 ) is finite, which asserts a valid upper bound on the misclassification error. Below it, the minimum energy diverges and WMP fails. In fact, we show that below the threshold, no local algorithm can perform significantly better than random guessing.
Set Identification When the number of communities k ≥ 3, we define a notion of set identification to describe, for two disjoint sets (of communities) S, T ⊂ [k], whether one can distinguish S from T . This notion subsumes as a special case the classic identification when S, T are singletons. However, it describes more general cases when one cannot distinguish the communities inside S and T , but is able to distinguish S and T . We provide a mathematical description of this fact using the structure of eigenvectors for the Markov transition matrix K defined in (1) . Further, we show that one can weigh the labels in the "most informative direction" by initializing WMP according to the second eigenvectors.
The paper is organized as follows. Section 2 reviews the background, definitions, and theoretical tools that will be employed to solve the general SBM. To illustrate the main idea behind the theoretical analysis better, we split the main result into two sections. Section 3 resolves the k = 2 case, where we emphasize the derivation of WMP as a linearized belief propagation, and, more importantly, detail the initialization of WMP according to minimum energy flow. Then we establish the connection between misclassification and energy. In Section 4, we focus on the general k ≥ 3 case, where we incorporate an additional layer of weights on the labels introduced by the eigenvectors of the Markov transition matrix. We then describe the mathematical treatment of set identification. Discussions on the gap between local and global algorithms for growing k, and on how WMP utilizes the asymmetry follow in the end. Section 5 considers the numerical performance of the proposed algorithm. The proofs of the main results are given in Section 6.

Let T t (o) denote the tree up to depth t with root o. For a node v, the set of children is denoted by C (v), children at depth d denoted by C d (v) , and the parent of v is denoted by P (v). We use |v| to denote the depth of v relative to o. If we view a tree as an electrical network, one can define the current flow and energy on the tree [26] . Later in the paper we will show the close connection between these notions and the misclassification error. 
)i( v) = u∈C (v) i( u).
The energy E(r, i) of a unit flow i at resistance level r > 0 is defined as
The minimum energy E * (r) is
where the infimum is over all valid unit flows. Denote the minimum energy flow as i * .
When assigning resistance r d to edges that are d -depth away from the root, the energy enjoys the natural physical interpretation. We also remark that for a given resistance level, one can calculate the minimum energy flow i * on the tree using Thomson's principal. We identify the reciprocal of resistance level with the conductance level. Now we are ready to define the branching number of a tree T through minimum energy.
Definition 3 (Branching Number). The branching number br(T ) can be defined as br(T ) := sup{r : E(r) < ∞} = sup{r : inf
It is well known that the branching number not only captures the growth rate of the tree, but also the more detailed structure, such as imbalance [26] .
When viewed locally, stochastic block models in the sparse regime share similarities with a label broadcasting process on a Galton-Watson tree. In fact, the local neighborhood of SBM can be coupled with a broadcasting tree with high probability as n → ∞. This phenomenon has been investigated in studying the detectability and reconstruction threshold for vanilla SBM (equal-size communities, symmetric case), as in [30] .
Let us formally define the label broadcasting process conditioned on a tree T (o).
. Given a tree T (o), the k-broadcasting process on T with the Markov transition matrix K ∈ R k×k describes the following process of label evolution. Conditioning on a node v and its label (v) ∈ [k], the labels of children u ∈ C (v) are sampled independently from
where the first equality is the Markov property.
Let us review the definition of the multi-type Galton-Watson tree. We shall only consider the Poisson branching process. 
The moment generating function (MGF) for a random variable X is denoted by Recall that the hyperbolic tangent is tanh x = e x −e −x e x +e −x . The message-passing algorithm in the following sections involves a non-linear update rule defined through a function
In this Section we will illustrate the main results for the case of two, possibly imbalanced, communities. We motivate the weighted message passing algorithm, and its relation to minimum energy flow. We investigate the connection between misclassification and minimum energy, as well as the corresponding transition threshold for general SBM.
This section serves as an informal summary of the results for k = 2. As a start, we introduce the following weighted message passing (WMP) Algorithm 1.
Data: Graph G(V, E ) with noisy label information˜ prior . Parameters: neighborhood radiust and conductance levelθ 2 .
The labeling for each node o ∈ V . for each node o ∈ V , do Open the tree neighborhood Tt (o) induced by the graph G(V, E ) ;
Layert : for every node u ∈ Ct (o) with distancet to the root on Tt (o), initialize its message
where i * ( u) is the minimum energy flow to u calculated via Thomson's principal on Tt (o) with conductance levelθ 2 ;
We remark that WMP can run in parallel for all nodes due to its decentralized nature. For fixed depth t and sparse SBM (when n max i , j Q i j log n), the algorithm runs in O * (n) time.
The following theorem is a simplified version of Theorems 2 and 3 below:
. Consider the general stochastic block model G(V, E ) with parameter bundle (n, k = 2, N ,Q), with either partial or noisy label information˜ prior with parameter 0 < δ < 1.
Defineθ := 1 4
Let E * (θ −2 ) be the minimum energy on T t (o) with conductance levelθ 2 as t → ∞. 
for any fixed δ > 0. On the other hand, if λθ 2 < 1, for any local estimator σ t that uses only label information on depth t leaves, the minimax misclassification error is lower bounded by
Remark 1. We remark that Algorithm 1 is stated for the case when noisy label information is known for all nodes in layert . For the case of partial label information, there are two options to modify the initialization of the algorithm: (1) view the partial label information with parameter δ as the noisy label information on layert only, with P(
2 -with probability δ, the label is revealed exactly, and with probability 1 − δ, the label is decided using coin-flip -then proceed with the algorithm; (2) view the partial information as on each layer there is a δ portion of nodes whose label is shown exactly. Call the set of these nodes V l (Tt (o)). Then we need to initialize the message M (u)
It can be shown that these two treatments enjoy similar asymptotic performance in terms of misclassification error, above the SNR threshold. However, the latter performs better numerically for fixed depth tree as it utilizes more information.
We decompose the proof of Theorem 1 into several building steps: (1) conditioned on the local tree structure, prove concentration-of-measure on WMP messages when label propagates according to a Markov transition matrix K ; (2) for a typical tree instance generated from multi-type Galton-Watson process, establish connection among the misclassification rate, transition boundary and minimum energy through the concentration result; (3) show that in the sparse graph regime of interest, the local neighborhood of general SBM can be coupled with a multi-type Galton-Watson with Markov transition matrix
for label broadcasting (the explicit expression based on Eq. (1)). We remark that (3) follows similar proof strategy as in [30] , where the coupling for vanilla SBM has been established. The lower bound follows from Le Cam's testing argument, and the difficulty lies in analyzing the distance between measures recursively on the local tree.
Remark 2. When the local tree is regular and symmetric and λθ 2 > 1, the minimum energy can be evaluated exactly as
which implies that misclassification error takes the exponentially decaying form exp − SNR−1 2
. Hence, the result provides a detailed understanding of the strength of the SNR and its effect on misclassification,
i.e., the inference guarantee. More concretely, for the vanilla SBM in the regime p = a/n, q = b/n, the
for weak consistency in [32, 27] . In addition, one observes that SNR > 1 + 2 log n implies Err(ˆ ) < 1/n → 0, which asserts strong consistency. This condition on SNR is satisfied, for instance, by taking p = a log n/n, q = b log n/n in vanilla SBM and computing the relationship between a, b to ensure SNR =
The above agrees with the threshold for strong recovery in [1, 16] .
In this section, we will motivate our proposed weighted message passing (WMP) from the well-known belief propagation (BP) on trees. There are two interesting components in the WMP Algorithm 1: the linearization part, and the initialization part. We will discuss each one in details in this section.
Recall the Definition 4 of the label broadcasting process on tree T (o) with k = 2. For convenience, let us denote the Markov transition matrix K to be
The BP algorithm is the Bayes optimal algorithm on trees given the labels of leaves. Define for a node u ∈ V the BP message as
which is the posterior logit of u's label given the observed labels obs (T t (u))). Using Bayes rule and conditional independence, one can write out the explicit evolution for BP message through f θ 1 ,θ 2 in (3)
with θ 1 , θ 2 as in Markov transition matrix K . While the method is Bayes optimal, the density of the messages B (u, t ) is difficult to analyze, due to the blended effect of the dependence on revealed labels and the non-linearity of f θ 1 ,θ 2 . However, the WMP Algorithm 1 -a linearized BP -shares the same transition threshold with BP, and is easier to analyze. Above a certain threshold, the WMP succeeds, which implies that the optimal BP will also work. Below the same threshold, even the optimal BP will fail, and so does the WMP. The updating rule for WMP messages M (u, t ) is simply a replacement of Eq. (10) by its linearized version,
The initialization of the WMP messages on the leaves M (u, 0) whose labels have been observed is crucial to the control of the misclassification error of the root node, especially for general SBM with heterogeneous degrees. For general SBM, one should expect to initialize the messages according to the detailed local tree structure, where the degree for each node could be very different. It turns out that the optimal misclassification for WMP is related to a notion called the minimum energy E * . Moreover, the optimal initialization for leaf message u is proportional to the minimum energy flow i * ( u) on the local tree, with conductance levelθ 2 . In plain language, i * ( u) provides a quantitative statement of the importance of the vote u has for the root. Note that for imbalanced trees, i * could vary significantly from node to node, and can be computed efficiently given the tree structure T t (o) for a specified conductance level.
We now prove the concentration-of-measure phenomenon on WMP messages. Through the concentration, we will show the close connection between misclassification and energy. We will first state the result conditioned on the tree structure T t (o).
for any λ, with parameter
Define the following updating rules for a node v
Then the following concentration-of-measure holds for the root message M (o,t ):
both with probability 1 − exp(− x   2 2 ).
as the cut-off to provide classificationˆ wmp , then the misclassification error is upper bounded by
The above Lemma provides an expression on the classification error. The next Theorem will show that with the "optimal" initialization for WMP, the misclassification error is connected to the minimum energy. we have E * (θ −2 ) < ∞ and
i( v) =θ 2|v| [µ t −|v| (v, +) − µ t −|v| (v, −)] [µ t (o, +) − µ t (o, −)] .
Remark 3. The above Theorem 2 and Lemma 1 together state the fact that if br[
finite, and the optimal initialization of WMP enjoys the asymptotic misclassification error bound of
Qualitatively, the smaller the minimum energy is, the smaller the misclassification error is, and it decays exponentially. On the contrary, if the minimum energy is infinite (br[T (o)]θ 2 < 1), the misclassification error bound for WMP becomes vacuous. Another remark is that when the tree is regular, the minimum energy takes the simple form E * (θ −2 ) = ) on asymptotic misclassification error.
In this section, we will show that the SNR threshold (for WMP algorithm) is indeed sharp for the local algorithm class. The argument is based on Le Cam's method. Let us prove a generic lower bound for any fixed tree T t (o), and for the k = 2 label broadcasting process with transition matrix K (as in Eq. (9)).
as distributions on leaf labels given (o) = +, − respectively. Under the condition
, the following equality on total variation holds
where σ t (o) :˜ prior (C t (o)) → {+, −} is any estimator mapping the prior labels in the local tree to a decision.
The above theorem is stated under the case when the noisy label information is known and only known for all nodes in layer t . One can interpret the result as, below the threshold br[T (o)]θ 2 < 1, one cannot do better than random guess for the root's label based on noisy leaf labels at depth t as t → ∞.
The proof relies on a technical lemma on branching number and cutset as in [37] . We would like to remark that the condition log(1 + 
In this section, we will extend the algorithmic and theoretical results to the general SBM for any fixed k or growing k with a slow rate (with respect to n). There are several differences between the general k case and the k = 2 case. First, algorithmically, the procedure for general k requires another layer of weighted aggregation besides the weights introduced by minimum energy flow (according to the detailed tree irregularity). The proposed procedure introduces the weights on the types of labels (k types) revealed, and then aggregates the information in the most "informative direction" to distinguish the root's label. Second, the theoretical tools we employ enable us to formally describe the intuition that in some cases for general SBM, one can distinguish the communities i , j from k, but not being able to tell i and j apart. We will call this the set identification.
We summarize in this section the main results for general SBM with k unequal size communities, and introduce the corresponding weighted message passing algorithm (WMP). We need one additional notation before stating the main result. 
where S, T ⊂ [k] are two disjoint subsets. Define
Let 
for any fixed δ > 0, where 
The proof for general k case requires several new ideas compared to the k = 2 case. Let us first explain the intuition behind some quantities here. Again we focus on the case when the network is sparse, i.e. n max i , j Q i j n o (1) . According to the coupling Proposition 1, one can focus on the coupled multi-type Galton-Watson tree, for a shallow local neighborhood of a node o. K ∈ R k×k then denotes the transition kernel for the label broadcasting process on the tree, and λ denotes the branching number of the multitype Galton-Watson tree. The transition threshold λθ 2 = 1, also called Kesten-Stigum bound, has been well-studied for reconstruction on trees [21, 22, 29, 18] . Our contribution lies in establishing the connection between the set misclassification error, minimum energy flow, as well as the second eigenvectors of K . This is done through analyzing Algorithm 2 (to be introduced next) with a novel initialization of the messages, using both minimum energy flow and the eigenvectors of K .
Remark 4. One distinct difference between the general k case and the k = 2 case is the notion of set misclassification error, or set identification. This formalizes the intuition that for general SBM that is asymmetric and imbalanced, it may be possible to distinguish communities i , j from community l , yet not possible to tell i and j apart. The above Theorem provides a mathematical description of the phenomenon, for any initialization using vectors in the eigen-space corresponding to the second eigenvalue.
The key new ingredient compared to the Algorithm 1 is the introduction of additional weights w ∈ R k on the labels. The choice of w will become clear in a moment.
Data: Same as in Algorithm 1 and an additional weight vector w ∈ R k .
The labeling for each node o ∈ V . for each node o ∈ V , do Open the tree neighborhood Tt (o) ;
Layert : for every node u ∈ Ct (o), initialize its message
where w˜ prior (u) denotes the˜ prior (u)-th coordinate of the weight vector w, i * ( u) is the minimum energy flow ;
As in the k = 2 case, we establish the recursion formula for the parameter updates. However, unlike the k = 2 case, for a general initialization µ 0 , it is much harder to characterize µ t (u), σ 2 t (u) analytically, and thus relate the misclassification error to the minimum energy. We will show that this goal can be achieved by a judicious choice of µ 0 . We will start with the following Lemma that describes the vector evolution and concentration-of-measure.
for any λ, with parameter
Define the following updating rules for a node v
with probability 1 − 2 exp(− x   2 2 ). In addition, if we we classify the root's label aŝ
Remark 5. Unlike the k = 2 case, in general it is hard to quantitatively analyze this evolution system for µ t (u), σ 2 t (u). The main difficulty stems from the fact that the coordinates that attain the maximum of max i , j ∈[k] |µ t −1 (u, i )−µ t −1 (u, j )| vary with u, t . Hence, it is challenging to provide sharp bounds on σ 2 t (u). In some sense, the difficulty is introduced by the instability of the relative ordering of the coordinates of the vector µ t (u) for an arbitrary initialization.
As will be shown in the next section, one can resolve this problem by initializing µ 0 (u, l ), l ∈ [k] in a "most informative" way. This initialization represents the additional weights on label's types beyond the weights given by the minimum energy flow.
We show in this section that the vector evolution system with noisy initialization is indeed tractable if we weigh the label's type according to the second right eigenvector of K ∈ R k×k .
Denote the minimum energy flow on tree T (o) with conductance level θ 2 by i * . In the case of noisy label information with parameter δ, if we initialize
and
Remark 6. Observe that the upper bound becomes trivial when min i , j |w i − w j | = 0. In this case, one can easily modify in the proof of Theorem 5 so that the following non-trivial guarantee for set misclassification error holds. Assume w has m distinct values, and denote the set S i , 1 ≤ i ≤ m to be the distinct value sets associated with w. Then one has the following upper bound on the set misclassification error
In this section we provide a new lower bound analysis through bounding the χ 2 distance to the "average measure". The lower bound shows that the transition boundary λθ 2 = 1 achieved by WMP is sharp for any k. To the best of our knowledge, the first lower bound for general k case is achieved in [18] using a notion of weighted χ 2 distance. For completeness of the presentation, we provide here a different proof using the usual χ 2 distance. In addition, our approach admits a clear connection to the upper bound analysis through matrix power iterations.
and kδ 2 ( 1 δ+
where
is any estimator mapping the prior labels on leaves in the local tree to a decision. The above inequality also implies
The above result shows that even belief propagation suffers the error at least 1 2k in distinguishing i , j , which is within a factor of 2 from random guess. We remark in addition that the condition kδ 2 ( 1 δ+
can be satisfied when δ is small.
Local versus Global Algorithms In the balanced case with k equal size communities, and p, q denoting the within-and between-community connection probabilities, the Kesten-Stigum threshold for local algorithm class takes the following expression
However, it is known that the limitation for global algorithm class for growing number of communities is 
We apply the message passing Algorithm 1 to the political blog dataset [4] (with a total of 1222 nodes) in the partial label information setting with δ portion randomly revealed labels. In the literature, the state-of-the-art result for a global algorithm appears in [19] , where the misclassification rate is 58/1222 = 4.75%. Here we run a weaker version of our WMP algorithm as it is much easier to implement and does not require parameter tuning. Specifically, we initialize the message with a uniform flow on leaves (minimum energy flow that corresponds to a regular tree). We will call this algorithm approximate message passing (AMP) within this section.
We run AMP with three different settings δ = 0.1, 0.05, 0.025, repeating each experiment 50 times. As a benchmark, we compare the results to the spectral algorithm on the (1−δ)n sub-network. We focus on the local tree with depth 1 to 5, and output the error for message passing with each depth. The results are summarized as box-plots in Figure 1 . The left figure illustrates the comparison of AMP with depth 1 to 5 and the spectral algorithm, with red, green, blue boxes corresponding to δ = 0.025, 0.05, 0.1, respectively. The right figure zooms in on the left plot with only AMP depth 2 to 4 and spectral, to better emphasize the difference. Remark that if we only look at depth 1, some of the nodes may have no revealed neighbors. In this setting, we classify this node as wrong (this explains why depth-1 error can be larger than 1/2).
We present in this paragraph some of the statistics of the experiments, extracted from the above Figure 1 . In the case δ = 0.1, from depth 2-4, the AMP algorithm produces the mis-classification error rate (we took the median over the experiments for robustness) of 6.31%, 5.22%, 5.01%, while the spectral algorithm produces the error rate 6.68%. When δ = 0.05, i.e. about 60 node labels revealed, the error rates are 7.71%, 5.44%, 5.08% with depth 2 to 4, contrasted to the spectral algorithm error 6.66%. In a more extreme case δ = 0.025 when there are only ∼ 30 node labels revealed, AMP depth 2-4 has error 10.20%, 5.71%, 5.66%, while spectral is 6.63%. In general, the AMP algorithm with depth 3-4 uniformly beats the vanilla spectral algorithm. Note that our AMP algorithm is a distributed decentralized algorithm that can be run in parallel. We acknowledge that the error ∼ 5% (when δ is very small) is still slightly worse than the state-of-the-art degree-corrected SCORE algorithm in [19] , which is 4.75%. 
We will start with two useful results. The first one is a coupling proposition. The proof follows exactly the same idea as in Proposition 4.2 in [30] . The intuition is that when the depth of the tree is shallow, the SBM in the sparse regime can be coupled to a Galton-Watson tree with Poisson branching (as there are many nodes outside the radius R for the Poisson-Multinomial coupling, when R small). We want to prove a more general version for SBM with unequal size communities. The proof is delayed to Appendix 7. 
with high probability as n → ∞. Here the tree equivalence is up to a label preserving homomorphism.
Lemma 3 (Hoeffding's Inequality). Let X be any real-valued random variable with expected value EX = 0 and such that a ≤ X ≤ b almost surely. Then, for all λ > 0,
Proof of Lemma 1. Recall the linearized message passing rule that "approximates" the Bayes optimal algorithm:
Let us analyze the behavior of the linearized messages M (u, t ) for a particular node u. The proof follows by induction on t . The case t = 0 follows from the assumption about µ 0 (u), σ 2 0 (u) and Chernoff bound. Now, assume that the induction premise is true for t − 1. Note that
, where the last step uses the Hoeffding's Lemma. Rearranging the terms,
where K 1· denotes the first row of transition matrix K . Clearly, same derivation holds with (u) = −. Proof of Theorem 2. Using the result of Lemma 1, the proof analyzes evolution of
First, let us derive the expression for
T , it is easy to verify that
Using the above equation recursively, one can easily see that for any
Now for σ
which can be written, in turn, as
Using the above equation one can bound
where the remainder
Recall the definition of
It is clear from Eq. (22) that i is a valid unit flow, in the sense of Definition 1. Continuing with Eq. (23), one has
Let us now estimate R:
The last step is because for noisy label information with parameter δ,
In the case when lim t →∞ E t (i
. Going back to Eq. (24), to minimize the LHS (ratio between noise and signal), one needs to make sure that i = i * , the minimum energy flow. Therefore, the optimal strategy is to initialize µ 0 (u) according to i * ( u). Thus, if we choose
From Definition 3,
Proof of Theorem 5. Note that by Perron-Frobenius Theorem, we have |θ| = |λ 2 (K )| < 1. Thanks to the choice of w,
Let us first derive the formula for µ t (o) ∈ R k under the chosen initialization µ 0 (u). We claim that
Proof is via induction. The base case |u| = t is exactly the choice of the initialization. Let us assume for |u| > |v| the claim is true, and prove for v: 
Plugging in the definition R =
we have E(i * , θ −2 ) < ∞, and
Proof of Theorem 6. Recall that π( ∂T t (o)∩T t −|u| (u) | (u) = i ) denotes the probability measure on the leaf labels on depth t , given (u) = i . For a node u, when there is no confusion, we abbreviate the measure π( ∂T t (o)∩T t −|u| (u) | (u) = i ) as π u (i ). According to Perron-Frobenius Theorem, there is a unique left eigenvector for K with eigenvalue 1, denote this by w ∈ R k . Under the assumption K being symmetric, we
by deriving a recursive bound:
By definition, the above expression is
Recall the following fact that for any z 1 , z 2 , . . . z k ≥ 0,
Using this fact the lower bound the LHS, we reach
where the last two lines use the fact that
We will need the the following Lemma that describes the branching number through the cutset. 
and for all v such that |v| ≤ max x∈C |x|,
Here the notation |v| denotes the depth of v.
Let us use the cutset argument to prove
. For any small, the above Lemma claims the existence of cutset C such that Eq. (25) and (26) hold. Let us prove through induction on max x∈C |x| − |v| that for any v such that |v| ≤ max x∈C |x|, we have
with the choice η = kδ 2 ( 1 δ+
). First for the base case, the claim is true because of the choice of η.
Preceding with the induction, assume for v such that max x∈C |x|−|v| = t −1 equation (29) is satisfied, and let us prove for v : max x∈C |x| − |u| = t . We recall the linearized recursion
Using the assumption θ 2 λ < 1 1+η , the above can be upper bounded by
where the last inequality uses the fact that x∈C ∩T (u) 1 λ |x|−|u| < 1. Now we know that
By monotonicity of x/(1 + x) we have proved the induction claim holds as
. Define t := min{|x|, x ∈ C }, it is also easy to see from equation (25) that
Putting things together, under the condition
we have
Finally, we invoke the multiple testing argument Theorem 2.6 in [39] ).
, Proposition 2.4, Theorem 2.6). Let P 0 , P 1 , . . . , P k be probability measures on (X , A ) satisfying 1
Plugging in the result with P 0 =π o and P i = π o (i ), we conclude that
Proof of Theorem 1. Given Proposition 1, Theorem 2 and Theorem 3, the proof of Theorem 1 is simple. By Proposition 1, one can couple the local neighborhood of SBM with multi-type Galton Watson process asymptotically almost surely as n → ∞, where the label transition matrix is
For the upper bound, Theorem 2 shows that the misclassification error is upper bounded by exp −
as the depth of the tree goes to infinity. Note if we first send n → ∞, due to Proposition 1, the coupling is valid even when R → ∞ with a slow rate log n/ log log n. Therefore, the upper bound on misclassification error holds. One can establish the lower bound using the same argument together with Theorem 3. Finally, for the expression on transition boundary, we know that condition on non-extinction, the branching number for this coupled multi-type Galton Watson tree is λ 1 (Qdiag(N )) almost surely. Proof is completed.
Proof of Lemma 2. The proof logic here is similar to the k = 2 case. Again, we analyze the message M (u, t ) for a particular node u. Use induction on t for the claim
The case for t = 0 follows from the assumption about µ 0 (u), σ 2 0 (u) and Chernoff bound. Assume that the induction is true for t − 1, and prove the case for t . Note that
where the last step uses the Hoeffding's Lemma. Rearrange the terms, one can see that the above equation implies
where K l · denotes the l −row of transition matrix K . Apply the Chernoff bound to optimize over λ, one can arrive the exponential concentration bound. Induction completes.
To upper bound the misclassification error, simply plug in
Proof of Theorem 3. We will gave the proof of Theorem 3 (for the δ noisy label information case) here. Define the measure π
on the revealed labels, for a depth t tree rooted from o with label (o) = + (and similarly define π
). We have the following recursion formula
Recall that the χ 2 distance between two absolute continuous measures We will again need the Lemma 4 that describes the branching number through the cutset. Fix any λ such thatθ −2 > λ > br[T (o)]. For any small, Lemma 4 claims the existence of cutset C such that Eq. (25) and (26) holds. Let's prove through induction on max x∈C |x|−|v| that for any v such that |v| ≤ max x∈C |x|, we have Hence if we plug in t = 2 3 log n + 2np 0 log n, we know
≤ X ≤ np 0 + 2 3 log n + 2np 0 log n ≤ 2np 0 + 2 log n with probability at least 1 − n −1 . Now, through union bound, we can prove that P ∀r ≤ R, |∂G r | ≤ (2np 0 + 2 log n) r ≥ 1 −C · (2np 0 + 2 log n) R n −1 ≥ 1 − O(n −3/4 ).
And we know that on the same event, |∂G r | ≤ n 1/4 , ∀r ≤ R.
It is clear that bad events that G R is not a tree (with cycles) for each layer is bounded above by p Now we need to recursively use the Poisson-Binomial coupling (to achieve Poisson-Multinomial coupling). The following Lemma is taken from [30] (Lemma 4.6). Therefore if np 0 = n o (1) and k log n, we have the bad event (when we cannot couple) happens with probability going to 0 as n → ∞. And if p 0 = n o(1) , we can allow R to grow to infinity at a slow rate as R log n log[n o (1) +log n]
.
Back in the frosts of time, Alfred Bray Kempe introduced the notion of changing colorings by switching maximal two-color chains of vertices (for vertex colorings) [4] or edges (for edge colorings). The maximal two-color chains are now called Kempe chains and edge-Kempe chains respectively; switching the colors along such a chain is called a Kempe switch or edgeKempe switch as appropriate. This process is of interest across the study of colorings. It is also of interest in statistical mechanics, where certain dynamics in the antiferromagnetic q-state Potts model correspond to Kempe switches on vertex colorings [8] , [9] . In some cases, these dynamics also correspond to edge-Kempe switches [7] .
In the present work we are concerned with understanding when two edgecolorings are equivalent under a sequence of edge-Kempe switches and when not. We allow multiple edges on our (labeled) graphs; loops are prohibited (and will mostly be excluded by other constraints such as 3-edge colorability).
A single edge-Kempe switch is denoted by −. That is, if coloring c i becomes coloring c j after a single edge-Kempe switch, then c i −c j . If coloring c j can be converted to coloring c k by a sequence of edge-Kempe switches, then c j and c k are equivalent; we denote this by c j ∼ c k . Because ∼ is an equivalence relation, we may consider the equivalence classes on the set of colorings of a graph G edge-colored with n colors. In this paper we focus on the number of edge-Kempe equivalence classes and denote this quantity by K ′ (G, n). (In other work this has been denoted Ke(L(G), n) [6] and κ E (G, n) [5] .)
Note that any global permutation of colors can be achieved by edgeKempe switches because the symmetric group S n is generated by transpositions. Thus two colorings that differ only by a permutation of colors are edge-Kempe equivalent.
Recall that ∆(G) is the largest vertex degree in G and that χ ′ (G) is the smallest number of colors needed to properly edge-color G. When more colors are used than possibly needed to edge-color the graph, then there is but a single edge-Kempe equivalence class, i.e., when n > χ ′ (G) + 1 then K ′ (G, n) = 1 [6, Thm. 3.1] . More is known if ∆(G) is restricted; when ∆(G) ≤ 4, K ′ (G, ∆(G) + 2) = 1 [5, Thm. 2] and when ∆(G) ≤ 3, K ′ (G, ∆(G) + 1) = 1 [5, Thm. 3] . For bipartite graphs there is a stronger result: when n > ∆(G), K ′ (G, n) = 1 [6, Thm. 3.3] . Little is known about K ′ (G, ∆(G)). This paper focuses on cubic graphs, particularly those that are 3-edge colorable. Mohar suggested classifying cubic bipartite graphs with K ′ (G, 3) = 1 [6] ; we provide a partial answer here. Mohar also points out in [6] that it follows from a result of Fisk in [1] that every planar 3-connected cubic bipartite graph G has K ′ (G, 3) = 1. We show (in Section 4) that for G planar, bipartite, and cubic, G has K ′ (G, 3) = 1. The remainder of the paper proceeds as follows. Section 2 introduces decompositions of cubic graphs along 2-or 3-edge cuts that preserve planarity and bipartiteness. The theorems in Section 3 use the edge-cut decompositions to combine and decompose 3-edge colorings. We also show that any edgeKempe equivalence can avoid color changes at a particular vertex. Then, in Section 4 we compute K ′ (G, 3) in terms of the edge-cut decomposition of G, and exhibit infinite families of simple nonplanar bipartite cubic graphs with a range of numbers of edge-Kempe equivalence classes.
Any 3-edge cut of a cubic graph may be used to decompose a cubic graph G into two cubic graphs G 1 , G 2 as follows. For 3-edge cut E C = {(s 11 s 21 ), (s 12 s 22 ), (s 13 s 23 )} where vertices s 1j are on one side of the cut and s 2j on the other, let the induced subgraphs of
, as is shown in Figure 1 . This decomposition will be written as G = G 1 G 2 . A similar decomposition is defined analogously for a 2-edge cut of a cubic graph. Here G has 2-edge cut E C = {(s 11 s 21 ), (s 12 s 22 )} and for i = 1, 2 we define
This decomposition will be written as
For both of these decompositions, we say the edge cut is nontrivial if both G 1 and G 2 have fewer vertices than G. Using nontrivial edge cuts, we may decompose a cubic graph G into a set of smaller graphs {G i } where each G i has no nontrivial edge cuts (but may have additional multiple edges).
Notice that these decompositions are reversible, though not uniquely so. Consider two cubic graphs G 1 , G 2 . Form G 1 G 2 by distinguishing a vertex on each (v 1 , v 2 respectively) and identifying the edges incident to v 1 with the edges incident to v 2 . A priori, there are many ways to choose v 1 , v 2 and many ways to identify their incident edges. We will abuse the notation G 1 G 2 by using it to denote a particular one of these many choices. Similarly, G 1 G 2 can be formed by choosing an edge e i = (s i1 s i2 ) from each G i , deleting e i , and then adding the edges {(s 11 s 21 ), (s 12 s 22 )}. Note that constructing G 1 G 2 is equivalent to cutting an edge of G 2 and inserting it into a single edge of G 1 . Proof. Suppose that G has a cellular embedding on the sphere. Then the removal of an edge cut E C separates G into two subgraphs, G Note that the resulting degree-1 and degree-2 vertices of each subgraph are on its outer face (relative to D i ) as in Figure 2 . If E C was a 2-edge cut, edges
may be added on the outside face that join these vertices to create planar G i . If E C was a 3-edge cut, add vertices v 1 , v 2 on the outside faces of discs D 1 , D 2 respectively, and join v i to the degree-1 and degree-2 vertices in D i to create planar G i . Conversely, spherical embeddings of G 1 and G 2 may be converted to planar drawings with distinguished vertices v 1 , v 2 or edges e 1 , e 2 on the outside faces of discs D 1 , D 2 respectively. Removing v 1 , v 2 (resp. e 1 , e 2 ) produces G with three edges (resp. two edges) of a cut missing. Any desired pairing of the vertices may be completed on a sphere without edges crossing by using judicious placement of D i (and perhaps flipping one over). This will result in
G is bipartite if and only if G 1 and G 2 are bipartite.
Proof. If G is a cubic bipartite graph with nontrivial 2-edge cut, then let there be m j vertices from part j on side 1; if both cut edges emanate from part 1 then 3m 1 − 2 = 3m 2 which is impossible. Thus each cut edge must emanate from a different part on side i of the cut, so both removing the edge cut and placing edges on each side maintains bipartition.
Suppose G is a bipartite cubic graph with nontrivial 3-edge cut E C and G Conversely, if G 1 , G 2 are bipartite, with distinguished e 1 = s 11 s 12 , e 2 = s 21 s 22 for the purpose of forming G 1 G 2 , then the bipartition of G 1 extends to G 1 G 2 by assigning s 12 (resp. s 22 ) to the opposite part as s 11 (resp. s 21 ). Similarly, if G 1 , G 2 are bipartite, with distinguished v 1 , v 2 for the purpose of forming G 1 G 2 , then use the bipartition of G 1 and assign v 2 to the opposite part as v 1 to induce a bipartition of G 1 G 2 . Theorem 2.3. A cubic graph H that is 2-connected but not 3-connected may be decomposed via into a set of cubic loopless graphs {H i } where each H i is 3-connected.
Proof. The proof is inductive on the number of vertices of H. Because H is 2-connected but not 3-connected, there exists a 2-vertex separating set. Figure  3 shows the three possible edge configurations for a 2-vertex separating set of a cubic graph, along with (at top) associated 2-edge cuts. Each 2-edge cut can be used to form H = H 1 H 2 , and |H j | < |H| so the inductive hypothesis holds for H j .
It is worth noting that while the decomposition can create multiple edges, any multiple edge in a cubic graph will be associated with a 2-edge cut. Thus Proof. This follows from Lemmas 2.1 and 2.2.
An alternative decomposition using the product can also be found. This is because every 2 vertex separating set is also associated with a 3-edge cut as seen in Figure 3 (bottom). This decomposition also preserves planarity and bipartiteness.
We begin by showing that we can fix the colors on the edges incident to a given vertex, and accomplish any sequence of edge-Kempe switches without changing the fixed colors. As a result, representatives of all edge-Kempe equivalence classes will be present in the set of colorings with fixed colors at a vertex. The following theorem holds for all base graphs G, not just cubic graphs, and all n ≥ χ ′ (G). Recall that o i −o i+1 is the notation for two colorings that differ by exactly one edge-Kempe switch. It will be useful to have a further notation for the switch itself. Let s i = ({p i 1 , p i 2 }, t i ) where {p i 1 , p i 2 } is the pair of colors to be switched on the chain t i of G. Then write o i − s i o i+1 , if o i+1 is obtained from o i by switching colors {p i 1 , p i 2 } on chain t i . Considering S n as acting on the set of colors {1, . . . , n}, let π i ∈ S n be the transposition
The idea of the proof is as follows. Each time a switch s i = ({p i 1 , p i 2 }, t i ) affects an edge incident to v, replace it by making all other {p i 1 , p i 2 } switches in the graph. This results in a coloring of the graph that is equivalent to the original, at the same stage, via a global color permutation. Therefore we need to track the colors to be switched on t k , for k > i. Each switch s k that does not affect an edge incident to vertex v will be replaced by a switch, on the same chain t k , of the colors that are currently on that chain. Our proof gives this precisely as an algorithm.
and there is at least one i such that v ∈ t i . Let σ 0 be the identity permutation. For 0 ≤ i ≤ m−1, replace s i with a set of edge-Kempe switchesŝ i as follows.
Defineô i+1 to be the result of performing the sets of switchesŝ 1 , . . . ,ŝ i to c. We show thatô i+1 and o i are equivalent up to a global color permutation by σ i . Recall that o i (e) is the color assigned to edge e by o i . We must show that on each edge e,ô i+1 (e) = σ i+1 o i+1 (e). We proceed by induction and so assume that for
There are 5 cases. First suppose v ∈ t i . Case 1a. If e ∈ t i thenô i+1 (e) =π iôi (e) becauseπ i is the action of switchŝ i . By definition ofπ i and using the inductive hypothesis forô i , π iôi (e) = (σ i π i σ −1 i )(σ i o i (e)). Simplifying, we have σ i π i o i (e) = σ i o i+1 (e) (by action of s i on o i ), which, by definition of σ i+1 in this case, equals σ i+1 o i+1 (e) as desired. Similar reasoning justifies the remaining cases so we present them in an abbreviated fashion.
Case This result shows when counting the number of edge-Kempe equivalence classes it is sufficient to consider only colorings of G that are different up to global color permutation. To make this observation precise requires careful definition of an edge-Kempe-equivalence graph of a graph. This will be done in [2] .
Returning to cubic graphs, we next consider how combining graphs affects K ′ (G, n). Let G 1 , G 2 be two 3-edge-colorable cubic graphs and distinguish a vertex on each (v 1 , v 2 ) for the purpose of forming G 1 G 2 . Recall that in addition to the choice of v 1 , v 2 , there are multiple ways their incident edges may be identified; by G 1 G 2 we mean some particular set of these choices. Let {x 1 , x 2 , x 3 } and {y 1 , y 2 , y 3 } be the ordered sets of edges in G 1 and G 2 that will be identified in G 1 G 2 . Similarly, choose a distinguished edge in each graph (x ∈ G 1 , y ∈ G 2 ) for the purpose of forming G 1 G 2 . The following several results relate 3-edge colorings of G 1 and G 2 to those of G 1 G 2 and G 1 G 2 . 
if e ∈ G 2 c(e) =d(e) if e is one of the edges added after deleting x and y.
Lemma 3.3. Let E C be an edge cut of a of a 3-edge-colorable cubic graph G and c be any proper 3-edge coloring of G. Then (a) if E C is a 2-edge cut, then c(E C ) uses exactly one color, and (b) if E C is a 3-edge cut, then c(E C ) uses all three colors.
can be written as c 1 d 1 (resp. c 1 d 1 ) where c 1 is some 3-edge coloring of G 1 and d 1 is some 3-edge coloring of G 2 .
Proof. Consider a 3-edge coloring f of G = G 1 G 2 . There is a 3-edge cut E C corresponding to the decomposition G 1 G 2 . By Lemma 3.3(b), each e i ∈ E C must be a different color in c. Therefore considering f on the edges of G 1 (and particularly at v 1 ), it is still a proper coloring c 1 , and likewise f considered on G 2 is a proper coloring d 1 . The result for is similarly an immediate corollary of Lemma 3.3.
Implicit in the preceding results is the following.
, then G is 3-edge colorable if and only if G 1 and G 2 are 3-edge colorable.
Next we note how edge-Kempe equivalences on the colorings of G 1 and G 2 transfer to edge-Kempe equivalences in combinations of these graphs. 
Proof. Using the notation from Definition 3.2, let c . By Lemma 3.3, the two edges created after deleting x, y will be assigned the same color in any proper 3-coloring of G 1 G 2 , so fixing the color on one will also fix the color on the other. Hence, (
Lemma 3.7. Let G 1 , G 2 be 3-edge colorable cubic graphs with G 1 G 2 and
Proof. It is sufficient to show this when (
, where s = (p, t) with p a pair of colors and t an edge-Kempe chain. If t ⊂ G 1 or t ⊂ G 2 , then the lemma holds. Otherwise, t ∩ E C = ∅, and t must use exactly 2 edges of E C because every edge-Kempe chain of a proper 3-edge coloring of a cubic graph is a cycle. The decomposition G 1 G 2 (resp. G 1 G 2 ) over E C will decompose t into an edge-Kempe chain t 1 of G 1 and 
Theorem 3.8 can be extended to compose several graphs, or alternatively to decompose a graph into many smaller pieces. We will use the theorem below in both contexts to get results about possible numbers of edge-Kempe equivalence classes for cubic graphs.
where {G i } is a decomposition of G along nontrivial 2-edge cuts or 3-edge cuts.
Proof. This follows from multiple applications of Theorem 3.8.
The following theorem answers a question from [6, Section 3]. Recall that if G is cubic and bipartite then it must be bridgeless. Thus we get the following result. 
Matters are quite different for nonplanar bipartite cubic graphs. It is well known that K 3,3 has two different edge-colorings (shown in Figure 4 ). In each of these colorings, each color-pair forms a Hamilton cycle. Therefore, any edge-Kempe switch results in a permutation of the colors and neither coloring of Figure 4 can be obtained from the other. Thus, there are two edge-Kempe equivalence classes, i.e. K ′ (K 3,3 , 3) = 2. Proof. Every simple bipartite nonplanar cubic graph is a subdivision of K 3,3 .
To maintain the bipartition and avoid multiple edges, K 3,3 must be subdivided with at least 4 vertices, two on each of two edges. These edges may be independent or may be incident. Any coloring of the original graph extends to either one or two new (edgeKempe equivalent) colorings, as is shown in Figure 5 . If a coloring had three Hamilton cycles before subdivision (as is true for both colorings of K 3,3 ), at most it gains an isolated edge-Kempe cycle after subdivision of this sort. Thus when subdividing K 3,3 with a single 4-vertex subdivision, there still exist two colorings that are not edge-Kempe-equivalent.
Further examples of nonplanar cubic bipartite graphs with K ′ (G, 3) > 1 will be given in Section 4.3. In contrast, Figure 6 shows a bipartite nonplanar cubic graph U with 12 vertices and K ′ (U, 3) = 1. K ′ (U, 3) was computed manually and verified using custom Mathematica code. We can use U to produce an interesting infinite class of graphs. Notice that similar results can be obtained for graphs that are only 2-connected as well by using the composition.
We can form K 3,3 G with any 3-connected cubic graph G to obtain a 3-connected nonplanar cubic graph. By Theorem 3.8,
Theorem 4.7. For every even n ≥ 8, there exists a 3-connected nonplanar cubic graph G with n vertices and exactly 2 edge-Kempe equivalence classes.
Proof. Form K 3,3 G with any 3-connected planar cubic graph G on n − 4 vertices to obtain a 3-connected nonplanar cubic graph with n vertices and
Corollary 4.8. For every even n ≥ 12, there exists a 3-connected nonplanar bipartite cubic graph G with n vertices and exactly 2 edge-Kempe equivalence classes.
Proof. Form K 3,3 G with any 3-connected planar cubic bipartite graph G on n − 4 vertices. The smallest 3-connected planar cubic bipartite graph has 8 vertices.
More generally, once we have one example with k edge-Kempe equivalence classes then there will be an infinite family of them with the same number of classes. Theorem 4.9. IfĜ is a cubic graph onn vertices with k edge-Kempe equivalence classes then for every even n ≥n + 6, there exists a cubic graph on n vertices with exactly k edge-Kempe equivalence classes. Further, ifĜ is planar then a planar family exists, ifĜ is bipartite then a bipartite family exists and ifĜ is 3-connected then a 3-connected family exists.
Proof. ComposeĜ with any cubic planar bipartite graph on n+2−n vertices using the operation. The result follows from Theorem 3.8.
We can make graphs with increasingly large numbers of edge-Kempe equivalence classes this way as well. Proof. For k ≥ 1, take K 3,3 · · · (k copies) . . . K 3,3 , which has 2 + 4k vertices. By Theorem 3.8, it has 2 k edge-Kempe equivalence classes. This produces the desired graph.
Theorem 4.11. For every simple nonplanar (bipartite) cubic graph G with n vertices, there exists an infinite family of nonplanar (bipartite) cubic graphs G k such that G k has 6k + n vertices and 2 k K ′ (G, 3) edge-Kempe equivalence classes.
Proof. Take G K 3,3 . . . K 3,3 .
Computing K ′ (G, 3) for particular G, or for families of graphs, is surprisingly difficult. A single computation can be done by brute force by computer, but constructing a proof is another matter. As examples of the kinds of arguments needed to determine K ′ (G, 3) , we analyze Möbius ladder graphs, prism graphs, and crossed prism graphs.
Theorem 5.1. Let ML k be the Möbius ladder graph on 2k vertices, let Pr k be the prism graph on 2k vertices, and let CPr k be the crossed prism graph on 4k vertices.
Note that Pr k is planar, and bipartite exactly when k is even; ML k is toroidal.
Proof. Our arguments are inductive.
First, consider the edge coloring of ML k given at left in Figure 8 , and note that it only exists for k odd. Every edge-Kempe chain in this coloring is a Hamilton circuit, so this coloring represents a edge-Kempe-equivalence class of of ML k . Now consider any other 3-edge coloring of ML k . If it has a square colored as shown at right in Figure 8 , then the square may be removed (and the remaining half-edges glued together) to produce a 3-edge coloring of ML k−2 . If there is no such square in the coloring, then every square must be colored as one of the options shown in Figure 9 . In either case, we can do a single edge-Kempe switch to produce an edge-Kempe-equivalent coloring that contains a removable square. Therefore K ′ (ML k , 3) = K ′ (ML k−2 , 3). To complete the proof, it suffices to show (which direct computation does) that K ′ (ML 4 , 3) = 1 and K ′ (ML 3 , 3) = 2. Next consider any 3-edge coloring of Pr k . The same argument as for ML k applies, so by removing a square we see that K ′ (Pr k colorings of Figure 10 , the crossed square may be removed (and the remaining half-edges glued together) to produce a 3-edge coloring of CPr k−1 . If there are only crossed squares with coloring type of the rightmost coloring in Figure 10 , we can do a single edge-Kempe switch to produce an edgeKempe-equivalent coloring that contains a removable crossed square. (A parity argument shows that there must be at least two edge-Kempe chains in a relevant color pair.) Because K ′ (CPr 2 , 3) = 1 by direct computation, it then follows that K ′ (CPr k , 3) = 1.
6 Areas for future work Beyond just examining the number of edge-Kempe connected components, what is the structure of the edge-Kempe-equivalence Graph of G, whose vertices represent colorings of G and whose edges represent single edge-Kempe switches? This is the topic of [2] .
Language can be approached through three different, complementary perspectives. Ultimately, it exists in the mind of language users, so that it is a cognitive entity, rooted in a neuropsychological basis. But language exists only because people interact with each other: It corresponds to a convention among a community of speakers, and answers to their communicative needs. Thirdly, language can be seen as something in itself: An autonomous, emergent entity, obeying its own inner logic. If it was not for this third Dasein of language, it would be less obvious to speak of language change as such.
The social and cognitive nature of language informs and constrains this inner consistency. Zipf's Law, for instance, may be seen as resulting from a trade-off between the ease of producing the utterance, and the ease of processing it [1] . It relies thus both on the cognitive grounding of the language, and on its communicative nature. Those two external facets of language, cognitive and sociological, are similarly expected to channel the regularities of linguistic change. Modelling attempts (see [2] for an overview) have explored both how sociolinguistic factors can shape the process of this change [3, 4] and how this change arises through language learning by new generations of users [5, 6] . Some models also consider mutations of language itself, without providing further details on the social or cognitive mechanisms of change [7] . In this paper, we adopt the view that language change is initiated by language use, which is the repeated call to one's linguistic resources in order to express oneself or to make sense of the linguistic productions of others. This approach is in line with exemplar models [8] and related works, such as the Utterance Selection Model [9] or the model proposed by Victorri [10] , which describes an out-ofequilibrium shaping of semantic structure through repeated events of communication.
Leaving aside sociolinguistic factors, we focus on a cognitive approach of linguistic change, more precisely of semantic expansion. Semantic expansion occurs when a new meaning is gained by a word or a construction (we will henceforth refer more vaguely to a linguistic 'form', so as to remain as general as possible). For instance, way, in the construction way too, has come to serve as an intensifier (e.g. 'The only other newspaper in the history of Neopia is the Ugga Ugg Times, which, of course, is way too prehistoric to read.' [11] ). The fact that polysemy is pervasive in any language [12] suggests that semantic expansion is a common process of language change and happens constantly throughout the history of a language. Grammaticalization [13] -a process by which forms acquire a (more) grammatical status, like the example of way too above-and other interesting phenomena of language change [14, 15] fall within the scope of semantic expansion.
Semantic change is known to be associated with an increase in frequency of use of the form whose meaning expands. This increase is expected indeed: As the form comes to carry more meanings, it is used in a broader number of contexts, hence more often. This implies that any instance of semantic change should have its empirical counterpart in the frequency rise of the use of the form. This rise is furthermore believed to follow an S-curve. The main reference on this phenomenon remains undisputedly the work of Kroch [16] , which unfortunately grounds his claim on a handful of examples only. It has nonetheless become an established fact in the literature of language change [17] . The origin of this pattern largely remained undiscussed, until recently: Blythe & Croft [18] , in addition to an up-to-date aggregate survey of attested S-curve patterns in the literature (totalizing about 40 cases of language change), proposed a modelling account of the S-curve. However, they show that, in their framework, the novelty can rise only if it is deemed better than the old variant, a claim which clearly does not hold in all instances of language change. Their attempt also suffers, as most modelling works on the S-curve, from what is known as the Threshold Problem, the fact that a novelty will fail to take over an entire community of speakers, because of the isolated status of an exceptional deviation [19] , unless a significant fraction of spontaneous adopters supports it initially.
On the other hand, the S-curve is not a universal pattern of frequency change in language. From a recent survey of the frequency evolution of 14 words relating to climate science [20] , it appears that the S-curve could not account for most of the frequency changes, and that a more general Bass curve would be appropriate instead. Along the same line, Ghanbarnejad et al. [21] investigated 30 instances of language change: 10 regarding the regularization of tense in English verbs (e.g. cleave, clove, cloven > cleave, cleaved, cleaved), 12 relating to the transliteration of Russian names in English (e.g. Stroganoff > Stroganov) and eight to spelling changes in German words (ss > ß > ss) following two different ortographic reforms (in 1901 and 1996) . They showed that the S-curve is not universal and that, in some cases, the trajectory of change rather obeys an exponential. This would be due to the preponderance of an external driving impetus over the other mechanisms of change, among which social imitation. The non-universality of the S-curve contrasts with the survey in [18] , and is probably due to the specific nature of the investigated changes (which, for the spelling ones, relates mostly to academic conventions and affects very little the language system). This hypothesis would tend to be confirmed by the observation that, for the regularization of tense marking, an S-curve is observed most of the time (7 out of 10). It must also be stressed that none of these changes are semantic changes. In this paper, we provide a broad corpus-based investigation of the frequency patterns associated with about 400 semantic expansions (about 10-fold the aggregate survey of Blythe & Croft [18] ). It turns out that the S-curve pattern is corroborated, but must be completed by a preceding latency part, in which the frequency of the form does not significantly increase, even if the new meaning is already present in the language. This statistical survey also allows to obtain statistical distributions for the relevant quantities describing the S-curve pattern (the rate, width and length of the preceding latency part).
Apart from this data foraging, we provide a usage-based model of the process of semantic expansion, implementing basic cognitive hypotheses regarding language use. By means of our model, we relate the microprocess of language use at the individual scale, to the observed macro-phenomenon of a recurring frequency pattern occurring in semantic expansion. The merit of this model is to provide a unified theoretical picture of both the latency and the S-curve, which are understood in relation with Cognitive Linguistics notions such as inference and semantic organization. It also predicts that the statistical distributions for the latency time and for the growth time should be of the same family as the inverse Gaussian distribution, a claim which is in line with our data survey.
We worked on the French textual database Frantext [22] , to our knowledge the only textual database allowing for a reliable study covering several centuries (see Material and methods and electronic supplementary material, SIII). We studied changes in frequency of use for 408 forms which have undergone one or several semantic expansions, on a time range going from 1321 up to the present day. We chose forms so as to focus on semantic expansions leading to a functional meaning-such as discursive, prepositional or procedural meanings. Semantic expansions whose outcome remains in the lexical realm (as the one undergone by sentence, whose meaning evolved from 'verdict, judgment' to 'meaningful string of words') have been left out. Functional meanings indeed present several advantages: They are often accompanied by a change of syntagmatic context, allowing to track the semantic expansion more accurately (e.g. way in way too + adj.); they are also less sensitive to sociocultural and historical influences; finally, they are less dependent on the specific content of a text, be it literary or academic.
The profiles of frequency of use extracted from the database are illustrated on figure 1 for nine forms. We find that 295 cases (which makes up more than 70% of the total) display at least one sigmoidal increase of frequency in the course of their evolution, with a p-value significance of 0.05 compared to a random growth. We provide a small selection of the observed frequency patterns (figure 2), whose associated logit transforms (figure 3) follows a linear behaviour, indicative of the sigmoidal nature of the growth (see Material and methods). We thus find a robust statistical validation of the sigmoidal pattern, confirming the general claim made in the literature.
Furthermore, we find two major phenomena besides this sigmoidal pattern. The first one is that, in most cases, the final plateau towards which the frequency is expected to stabilize after its sigmoidal rise is not to be found: The frequency immediately starts to decrease after having reached a maximum (figure 1). However, such a decrease process is not symmetrical with the increase, in contrast with other cases of fashion-driven evolution in language, e.g. first names distribution [23] . Though this decrease may be, in a few handfuls of cases, imputable to the disappearance of a form (e.g. après ce, replaced in Modern French by après quoi), in most cases it is more likely to be the sign of a narrowing of its uses (equivalent, then, to a semantic depletion).
The second feature is that the fast growth is most often (in 69% of cases) preceded by a long latency up to several centuries, during which the new form is used, but with a comparatively low and rather stable frequency (figure 2). How the latency time is extracted from data is explained in Material and methods. One should note that the latency times may be underestimated: If the average frequency is very low during the latency part, the word may not show up at all in the corpus, especially in decades for which the available texts are sparse. The pattern of frequency increase is thus better conceived of as a latency followed by a growth, as exemplified by de toute façon (figure 4)-best translated by anyway in English, because the present meanings of these two terms are very close, and remarkably, despite quite different origins, the two have followed parallel paths of change.
To our knowledge, this latency feature has not been documented before, even though a number of specific cases of sporadic use of the novelty before the fast growth has been noticed. For instance, it has been remarked in the case of just because that the fast increase is only one stage in the evolution [24] . Other examples have been mentioned [25] , but it was described there as the slow start of the sigmoid. On the other hand, the absence of a stable plateau has been observed and theorized as a 'reversible 1  8  8  0  1  9  2  1  -1  9  3  0  1  9  7  1  -1  9  8  0   1  3  2  1  -1  3  3  0  1  3  7  1  -1  3  8  0  1  4  2  1  -1  4  3  0  1  4  7  1  -1  4  8  0  1  5  2  1  -1  5  3  0  1  5  7  1  -1  5  8  0  1  6  2  1  -1  6  3  0  1  6  7  1  -1  6  8  0  1  7  2  1  -1  7  3  0  1  7  7  1  -1  7  8  0  1  8  2  1  -1  8  3  0  1  8  7  1  -1  8  8  0  1  9  2  1  -1  9  3  0  1  9  7  1  -1  9  8  0   1  3  2  1  -1  3  3  0  1  3  7  1  -1  3  8  0  1  4  2  1  -1  4  3  0  1  4  7  1  -1  4  8  0  1  5  2  1  -1  5  3  0  1  5  7  1  -1  5  8  0  1  6  2  1  -1  6  3  0  1  6  7  1  -1  6  8  0  1  7 1  7  3  0  1  7  7  1  -1  7  8  0  1  8  2  1  -1  8  3  0  1  8  7  1  -1  8  8  0  1  9  2  1  -1  9  3  0  1  9  7  1  -1  9  8  0   1  3  2  1  -1  3  3  0  1  3  7  1  -1  3  8  0  1  4  2  1  -1  4  3  0  1  4  7  1  -1  4  8  0  1  5  2  1  -1  5  3  0  1  5  7  1  -1  5  8  0  1  6  2  1  -1  6  3  0  1  6  7  1  -1  6  8  0  1  7  2  1  -1  7  3  0  1  7  7  1  -1  7  8  0  1  8  2  1  -1  8  3  0  1  8  7  1  -1  8  8  0  1  9  2  1  -1  9  3  0  1  9  7  1  -1  9  8  0   1  3  2  1  -1  3  3  0  1  3  7  1  -1  3  8  0  1  4  2  1  -1  4  3  0  1  4  7  1  -1  4  8  0  1  5  2  1  -1  5  3  0  1  5  7  1  -1  5  8  0  1  6  2  1  -1  6  3  0  1  6  7  1  -1  6  8  0  1  7 change' [26] or a 'change reversal' [27] , and was seen as an occasional deviation from the usual Scurve, not as a pervasive phenomenal feature of the evolution. We rather interpret it as an effect of the constant interplay of forms in language, resulting in ever-changing boundaries for most of their respective semantic dominions. In the following, we propose a model describing both the latency and the S-growth periods. The study of this decrease of frequency following the S-growth is left for future work.

To account for the specific frequency pattern evidenced by our data analysis, we propose a scenario focusing on cognitive aspects of language use, leaving all sociolinguistic effects backgrounded by making use of a representative agent, mean-field type, approach. We limit ourselves to the case of a competition between two linguistic variants, given that most cases of semantic expansion can be understood as such, even if the two competing variants cannot always be explicitly identified. Indeed, the variants need not be individual forms, and can be schematic constructions, paradigms of forms or abstract patterns. Furthermore, the competition is more likely to be local, and to involve a specific and limited region of the semantic territory. If the invaded form occupies a large semantic dominion, then losing a competition on its border will only affect its meaning marginally, so that the competition can fail to be perceptible from the point of view of the established form. The idealized picture is therefore as such: Initially, in some concept or context of use C 1 , one of the two variants, henceforth noted as Y, is systematically chosen, so that it conventionally expresses this concept. The question we address is thus how a new variant, say X, can be used in this context and eventually evict the old variant Y?
The main hypothesis we propose is that the new variant almost never is a brand new merging of phonemes whose meaning would pop out of nowhere. As Haspelmath highlights [28] , a new variant is almost always a periphrastic construction, i.e. actual parts of language, put together in a new, meaningful way. Furthermore, such a construction, though it may be exapted to a new use, may have shown up from time to time in the time course of the language history, in an entirely compositional way; this is the case for par ailleurs, which incidentally appears as early as the fourteenth century in our corpus, but arises as a construction in its own right during the first part of the nineteenth century only. In other words, the use of a linguistic form X in a context C 1 may be entirely new, but the form X was most probably already there in another context of use C 0 , or equivalently, with another meaning.
We make use of the well-grounded idea [29] that there exist links between concepts due to the intrinsic polysemy of language: There are no isolated meanings, as each concept is interwoven with many others, in a complicated tapestry. These links between concepts are asymmetrical, and they can express both universal mappings between concepts [30, 31] and cultural ones (e.g. entrenched metaphors [32] ). As the conceptual texture of language is a complex network of living relations rather than a collection of isolated and self-sufficient monads, semantic change is expected to happen as the natural course of language evolution and to occur repetitively throughout its history, so that at any point of time, there are always several parts of language which are undergoing changes. The simplest layout accounting for this network structure in a competitive situation consists then in two sites, such that one is influencing the other through a cognitive connexion of some sort.
We now provide details on the modelling of a competition between two variants X and Y for a given context of use, or concept, C 1 , also considering the effect exerted by the related context or concept C 0 on this evolution.
-Each concept C i , i = 0, 1, is represented by a set of exemplars of the different linguistic forms. We note that N i μ (t) is the number at time t of encoded exemplars (or occurrences) of form μ ∈ {X, Y}, in context C i , in the memory of the representative agent.
-The memory capacity of an individual being finite, the population of exemplars attached to each concept C i has a finite size M i . For simplicity we assume that all memory sizes are equal (M 0 = M 1 = M). As we consider only two forms X and Y, for each i the relation N i X (t) + N i Y (t) = M always holds: We can focus on one of the two forms, here X, and drop out the form subscript, granted that all quantities refer to X. -The absolute frequency x i t of form X at time t in context C i -the fraction of 'balls' of type X in the bag attached to C i -is thus given by the ratio N i (t)/M. In the initial situation, X and Y are assumed to be established conventions for the expression of C 0 and C 1 , respectively, so that we start with N 0 (t = 0) = M and N 1 (t = 0) = 0.
-Finally, C 0 exerts an influence on context C 1 , but this influence is assumed to be unilateral.
Consequently, the content of C 0 will not change in the course of the evolution and we can focus on C 1 . An absence of explicit indication of context is thus to be understood as referring to C 1 .
The dynamics of the system runs as follows. At each time t, one of the two linguistic forms is chosen to express concept C 1 . The form X is uttered with some probability P(t), to be specified below, and Y with probability 1 − P(t). To keep constant the memory size of the population of occurrences in C 1 , a past occurrence is randomly chosen (with a uniform distribution) and the new occurrence takes its place. This dynamics is then repeated a large number of times. Note that this model focuses on a speaker perspective (for alternative variants, see electronic supplementary material, SIIA).
We want to make explicit the way P(t) depends on x(t), the absolute frequency of X in this context at time t. The simplest choice would be P(t) = x(t). However, we wish to take into account several facts. As context C 0 exerts an influence on context C 1 , denoting by γ the strength of this influence (see electronic supplementary material, SIIB for an extended discussion on this parameter), we assume the probability Absolute frequency x is given by the ratio of X occurrences encoded in C 1 . Effective frequency f also takes into account the M occurrences contained in the influential context C 0 , with a weight γ standing for the strength of this influence. (b) Schematic view of the process. At each iteration, either X or Y is chosen to be produced and thus encoded in memory, with respective probability P γ (x) and 1 − P γ (x); the produced occurrence is represented here in the purple capsule. Another occurrence, already encoded in the memory, is uniformly chosen to be erased (red circle) so as to keep the population size constant. Hence the number of X occurrences, N X , either increases by 1 if X is produced and Y is erased, decreases by 1 if Y is produced and X is erased, or remains constant if the erased occurrence is the same as the one produced.
P to rather depend on an effective frequency f (t) (figure 5a),
We now specify the probability P(f ) to select X at time t as a function of f = f (t). First, P(f ) must be nonlinear. Otherwise, the change would occur with certainty as soon as the effective frequency f of the novelty is non-zero: That is, insofar as two meanings are related, the form expressing the former will also be recruited to express the latter. This change would also start quite abruptly, while sudden, instantaneous takeovers are not known to happen in language change. Second, one should preserve the symmetry between the two forms, that is, P(f ) = 1 − P(1 − f ), as well as verify P(0) = 0 and P(1) = 1. Note that this symmetry is stated in terms of the effective frequency f instead of the actual frequency x, as production in one context always accounts for the contents of neighbouring ones. For the numerical simulations, we made the following specific choice which satisfies these constraints:
where β is a parameter governing the nonlinearity of the curve. Replacing f in terms of x, the probability to choose X is thus a function P γ (x) of the current absolute frequency x:
The dynamics outlined above (figure 5b) is equivalent to a random walk on the segment [0; 1] with a reflecting boundary at 0 and an absorbing one at 1, and with steps of size 1/M. The probability of going forwards at site x is equal to (1 − x)P γ (x), and the probability of going backwards to x(1 − P γ (x)). For large M, a continuous, deterministic approximation of this random walk leads, after a rescaling of the time Mt → t, to a first-order differential equation for x(t):
This dynamics admits either one or three fixed points ( figure 6a ), x = 1 always being one. Below a threshold value γ c , which depends on the nonlinearity parameter β, a saddle-node bifurcation occurs and two other fixed points appear close to a critical frequency x c . The system, starting from x = 0, is then stuck at the smallest stable fixed point. The transmission time, i.e. the time required for the system to go from 0 to 1, becomes therefore infinite ( figure 6b ). Above the threshold value γ c , only the fixed point x = 1 remains, so that the new variant eventually takes over the context for which it is competing. Our model thus describes how the strengthening of a cognitive link can trigger a semantic expansion process. Slightly above the transition, a stranglehold region appears where the speed almost vanishes. Accordingly, the time spent in this region diverges. The frequency of the new variant will stick to low values for a long time, in a way similar to the latent behaviour evidenced by our dataset. This latency time in the process of change can thus be understood as a near-critical slowing down of the underlying dynamics.
Past this deterministic approximation, there is no more clear-cut transition (figure 6b) and the above explanation needs to be refined. The deterministic speed can be understood as a drift velocity of the Brownian motion on the [0; 1] segment, so that in the region where the speed vanishes, the system does not move in average. In this region of vanishing drift, the frequency fluctuates over a small set of values and does not evolve significantly over time. Once it escapes this region, the drift velocity drives the process again, and the replacement process takes off. Latency time can thus be understood as a firstpassage time out of a trapping region.

We ran 10 000 numerical simulations of the process described above ( figure 5b) , with the following choice of parameters: β = 0.808, δ = 0.0 and M = 5000, where δ = (γ − γ c )/γ c is the distance to the threshold. The specific value of β has been chosen to maximize x c . As x c is the frequency at which the system gets stuck if γ is slightly below the threshold, it corresponds to the assumption that, even if the convention is not replaced, there is room for synonymic variation and the new variant can be used marginally. We chose δ = 0.0 for the system to be purely diffusive in the vicinity of x c . The choice of M is arbitrary.
Even if this set of parameters remains the same throughout the different simulation runs, the quantities describing each of the 10 000 S-curves generated that way, especially the rate and the width, will change. It then becomes possible to obtain the statistical distributions of these quantities. Thus, while there is no one-to-one comparison between a single outcome of the numerical process and a given instance of change, we can discuss whether their statistical properties are the same.
From the model simulations, data are extracted and analysed in two parallel ways. On one side, simulations provide surrogate data: We can mimic the corpus data analysis and count how many tokens of the new variant are produced in a given time span (set equal to M), to be compared with the total number of tokens produced in this time span. We then extract 'empirical' latency and growth times (figure 7a), applying the same procedure as for the corpus data. On the other side, for each run we track down the position of the walker, which is the frequency x(t) achieved by the new variant at time t. This allows to compute first-passage times. We then alternatively compute analytical latency and growth times ('analytical' to distinguish them from the former 'empirical' times) as follows. Latency time is here defined as the difference between the first-passage times at the exit and the entrance of a 'trap' region (see electronic supplementary material, SIB for additional details). Analytical growth time is defined as the remaining time of the process once this exit has been reached. Their distribution over 10 000 runs of the process are fitted with an inverse Gaussian distribution, which would be the expected distribution if the jump probabilities were homogeneous over the corresponding regions (an approximation then better suited for latency time than for growth time). Figure 7b,d shows the remarkable agreement between the 'empirical' and 'analytical' approaches, together with their fits by an inverse Gaussian distribution.
Crucially, those two macroscopic phenomena, latency and growth, are thus to be understood as of the same nature, which explains why their statistical distribution must be of the same kind. Furthermore, the boundaries of the trap region leading to the best correspondence between first-passage times and empirically determined latency and growth times are meaningful, as they correspond to the region where the uncertainty on the transmission time significantly decreases (figure 7c). distribution of growth times (b) (a) Figure 8 . Inverse Gaussian fit of the latency times T L (a) and the growth times w (b), extracted from corpus data. Data points are shown by blue dots, the inverse Gaussian fit being represented as a full red curve with star-shaped marks. The dashed red lines represent the standard deviation from the model. We detail in Materials and methods how we extracted these growth times and latency times from corpus data.
Our model predicts that both latency and growth times should be governed by the same kind of statistics, inverse Gaussian being a suited approximation of those. Inverse Gaussian distribution is governed by two parameters, its mean μ and a parameter λ given by the ratio μ 3 /σ 2 , σ 2 being the variance. We thus try to fit the corpus data with an inverse Gaussian distribution (figure 8). In both cases, the Kullback-Leibler divergence between the data distribution and the inverse Gaussian fit is equal to 0.10. The rate h (slope of the logit) also follows a non-trivial distribution, as shown in electronic supplementary material, SIC.
Although there are short growth times in the frequency patterns of the forms we studied, below six decades they are not described by enough data points to assess reliably the specificity of the sigmoid fit. On figure 8 there are therefore no data for these growth times. The inverse Gaussian fit is not perfect, and is not expected to be: The model only predicts the distribution to be of the same family as the inverse Gaussian. Satisfyingly, among a set of usual distributions (exponential, Poisson, Gaussian, Maxwellian), the inverse Gaussian proves to be the most adequate for both the growth and the latency (see electronic supplementary material, SIC for additional details).
The main quantitative features extracted from the dataset are thus correctly mirrored by the behaviour of our model. We confronted the model with the data on other quantities, such as the correlation between growth time and latency time, two quantities which our model predicts to be independent. There again, the model proves to match appropriately these quantitative aspects of semantic expansion processes (see electronic supplementary material, SID).
Based on a corpus-based analysis of frequency of use, we have established two robust stylized facts of semantic change: an S-curve of frequency growth, already evidenced in the literature, and a preceding latency period during which the frequency remains more or less constant, typically at a low value. We have proposed a model predicting that these two features, albeit qualitatively quite different, are two aspects of one and the same phenomenon.
Our analysis is based on the a priori assumption that a frequency rise is caused by a semantic expansion. An alternative would be the reverse mechanism, that semantic expansion is induced by an increase in the frequency of use. Actually, it is not infrequent to find unambiguous traces of the semantic expansion throughout and even before the latency phase. Also, we often looked for forms in a syntactic context compatible only with the new meaning-e.g. for j'imagine we searched specific intransitive patterns, like 'il y a de quoi, j'imagine, les faire étrangler' (1783) (There's good reason to have them strangled, I suppose)-so that, in such cases, it leaves no doubt that the latency phase and the The detailed hypotheses on which our model lies are well grounded on claims from Cognitive Linguistics: Language is resilient to change (nonlinearity of the P function); language users have cognitive limitations; the semantic territory is organized as a network whose neighbouring sites are asymmetrically influencing each other. The overall agreement with empirical data tends to suggest that language change may indeed be cognitively driven by semantic bridges of different kinds between the concepts of the mind, and constrained by the mnemonic limitations of this very same mind. According to our model, the onset of change depends on the strength of the conceptual link between the source context and the target context: If the link is strong enough, that is, above a given threshold, it serves as a channel so that a form can 'invade' the target context and then oust the previously established form. In a sense, the sole existence of this cognitive mapping is already a semantic expansion of some sort, yet not necessarily translated into linguistic use. Latency is specifically understood as resulting from a near-critical behaviour: If the link is barely strong enough for the change to take off, then the channel becomes extremely tight and the invasion process slows down drastically. These narrow channels are likely to be found between lexical and grammatical meanings [33, 34] . This would explain why the latency-growth pattern is much more prominent in the processes of grammaticalization, positing latency as a phenomenological hint of this latter category.
As acknowledged by a few authors [35, 36] , it is interesting to note that, in the literature, the Sgrowth is given two very different interpretations. According to the first one, an S-curve describes the spread of the novelty in a community of speakers [4, [37] [38] [39] ; as for the second one, it reflects the spread in language itself, the new variant being used in an increasing number of contexts [17, [40] [41] [42] . According to the interpretation we give to our model, the diffusion chiefly happens over the linguistic memory of the whole speech community. It does not involve some binary conversion of individuals towards the new variant; it is a spread within the individuals rather than a spread among them. On the other hand, the S-curve arises in the taking over of a single context, and does not rely on a further diffusion over additional contexts to appear. Though the latter spread need thus not be responsible for the S-shape, it may nonetheless influence the evolution in other ways (e.g. the total duration). The interplay between the specific features of an S-curve and the structure of the conceptual network remains to be investigated.
We note, however, that our model may be given a different, purely sociolinguistic interpretation, as discussed in electronic supplementary material, SIIC. Nevertheless, several arguments argue against this interpretation. First, the semantic evolution involves very long timescales, up to several centuries [41] ; second, societal diffusion, of a new technological device, for instance, is associated with a specific scaling law between the steep and duration of the S-curve of − 2 3 [43] , which is very different from the behaviour of the forms in our dataset, where no scaling law is to be found (the two parameters are related by a trivial −1.0 exponent; see electronic supplementary material, SID).
Recently, the nature of linguistic change has been investigated through different case studies, separating internal (imitation between members of a community) and external (e.g. linguistic reforms from language academies) factors of change [21] . While internal factors give rise to an S-curve, external factors lead to an exponential growth of frequency; hence, the S-curve is not the only dynamics by which language change can occur. However, in this work, agents choose between the two variants on a binary basis, and language-based mechanisms, such as the network asymmetric links at the core of our own model, would count as an external mechanism. These strong differences make it difficult to quantitatively compare their approach and ours, although it is to be agreed that S-curves contain crucial information on language change and need to be investigated and quantified further on. Moreover, as semantic change is seldom driven by external forces such as linguistic reforms, the exponential pattern is not to be expected in this case, and indeed we have not found it in our dataset.
Finally, we argue that our results, though grounded on instances of semantic expansion in French, apply to semantic expansion in general. The time period covered is long enough (700 years) to exclude the possibility that our results be ascribable to a specific historical, sociological or cultural context. The French language itself has evolved, so that Middle French and contemporary French could be considered as two different languages, yet our analysis applies to both indistinctly. Besides, the latency-growth pattern is to be found in other languages; for instance, although Google Ngram cannot be used here for a systematic quantitative study, specific queries for constructions such as way too, save for, no matter what, yield qualitative frequency profiles consistent with our claims. Our model also tends to confirm the genericity of this pattern, as it relies on cognitive mechanisms whose universality has been well evidenced [44, 45] . 

We worked on the Frantext corpus [22] , which in 2016 contained 4674 texts and 232 millions of words for the chosen time range. More details are given in electronic supplementary material, SIIIB. It would have been tempting to make use of the large database Google Ngram, yet it was not deemed appropriate for our study, as we explain in electronic supplementary material, SIIIC.
We studied changes in frequency of use for about 400 instances of semantic expansion processes in French, on a time range going from 1321 up to the present day. See electronic supplementary material, SIIID for a complete list of the studied forms.
We divided our corpus into 70 decades. Then, for each form, we recorded the number of occurrences per decade, dividing this number by the total number of occurrences in the database for that decade. The output number is called here the frequency of the form for the decade, and is noted x i for decade i. To smooth the obtained data, we replaced x i by a moving average, that is, for i ≥ i 0 + 4, i 0 being the first decade of our corpus: x i ← 1 5 i k=i−4 x k .
We looked for major increases in frequency. When such a major shift is encountered, we automatically (see below) identify frequencies x min and x max , respectively, at the beginning and the end of the increasing period. If we, respectively, note i start and i end the decades for which x min and x max are reached, then we define the width (or growth time) w of the increasing period as w = i end − i start + 1. To quantify the sigmoidal nature of this growth pattern, we apply the logit transformation to the frequency points between x min and x max :
If the process follows a sigmoidx i of equatioñ
then the logit transform of this sigmoid satisfiesỹ i = hi + b. We thus fit the y i 's given by (6.1) with a linear function, which gives the slope (or rate) h associated with it and the residual r 2 quantifying the quality of the fit. The boundaries i start and i end have been chosen so as to maximize w, with the constraint that the r 2 of the linear fit should be at least equal to a value depending on the number of points, in order to ensure that the criterion has a p-value significance of less than 0.05 according to a null model of frequency growth. Further explanations are provided in electronic supplementary material, SIA.
In most cases (69% of sigmoidal growths), one observes that the fast increasing part is preceded by a phase during which the frequency remains constant or nearly constant. The duration of this part, denoted by T L (latency time) in this paper, is identified automatically as follows. Starting from the decade i start , previous decades j are included in the latency period as long as they verify |x j − x min | < 0.15 * (x max − x min ) and x j > 0, and cease to be included either as soon as the first condition is not verified, or if the second condition does not hold for a period longer than 5 decades. Then the start i lat of the latency point is defined as the lowest j verifying both conditions, so that T L is given by T L = i start − i lat . 
Large scale networked systems such as automated highway systems, air-traf£c control systems, and cooperative robotic systems present unique modeling and control challenges. Sensory, communication, or computational limitations often require that a desired global behavior is produced using only local control of each robot, node, or subsystem. Furthermore, these subsystems often exhibit complex interactions between their continuous modes, network topology, and logical modes.
In this paper, we introduce the notion of an embedded graph grammar which extends the graph grammar model [1] from a purely topological construct to one that includes geometry, continuous dynamics and local conditions on the evolution of those dynamics. We present this model through an example that also highlights a hierarchical and systemsoriented approach to using graph and embedded graph grammars. Methods such as the composition of grammars, structured labeling and lexicographic ordering of Lyapunov functions on grammars allow us to design grammars in a principled manner.
After brie¤y discussing related work, we present a motivating example called the load balanced multiple rendezvous problem. Section IV de£nes embedded graph grammars and discusses our use of temporal logic on graph transition systems. Sections V through VII present grammars and controllers designed to address each of the subtasks in the example. In Section VIII, we compose these elements to create an embedded graph grammar and prove it meets the desired speci£cation.
Researchers are currently uncovering rich connections between graphs, information ¤ow, and formation control [2] , [3] , [4] . Local restrictions on communication may induce arbitrary switching in the network. In light of this, Ji and Egerstedt [5] design graph-based controllers that maintain connectivity as robots rendezvous. Jadbabaie et al. [6] approach the issue differently, constructing controllers that are stable under arbitrary switching sequences.
Klavins [7] examines the notion of explicitly controlling the graph structure by describing the self-organization of robot formations as a graph process. Klavins, Ghrist, and Lipsky [1] introduce graph grammars to assemble any prespeci£ed graph from an initially disconnected graph. By restricting rewrites to small subgraphs, graph grammars provide a useful method to program the concurrent behavior of large decentralized systems of robots [8] , [9] . Olfati-Saber [10] investigates controlled switching in robot formation using a global hybrid automata.
However the interplay between all these elements (information ¤ow, hybrid switching, local restrictions on sensing, and continuous dynamics) in cooperative control remains largely uncharted territory. Our goal is to introduce embedded graph grammars as a single graph-centric model containing all these elements.
Consider a group of networked robots with range limited communication executing formation control on Mars in search of ice. Simultaneously, a number of robots sense ice. We call these robots bases. A reasonable goal would be to route other robots (commuters) in essentially equal numbers to the base locations while maintaining connectivity in the overall system. The problem is complex, containing elements of well known formation control problems such as multiple-leader rendezvous, connectivity maintenance, and load balancing.
As shown in Figure 1 , our solution to the problem is to compose three distributed algorithms that, when run in parallel, result in the desired global behavior. These are: Distributed Cycle Removal (DCR), which removes cycles from the graph; Load Balancing (LB), which distributes commuters equally among the bases; and Connectivity Maintenance and Rendezvous (CMR), which maintains connectivity while allowing commuter robots to rendezvous at the appropriate base. The solution combines controlled switching of a locally restricted network and continuous spatial dynamics. Labeled graphs and their realizations in continuous space are a natural model for such systems. So in Section IV we develop the formal de£nitions of embedded graph grammars and in Section IV-E we return to this example and formally specify the Load Balanced Multiple Rendezvous Problem (LBMR) . We then propose a embedded graph grammar to solve the problem and prove its correctness through composition.
For clarity, we introduce some notation. If Σ is a set of labels, a labeled graph is the quadruple (V, E, l, e) where V is a set of vertices, E is a set of edges, l is the vertex labeling function that maps vertices into Σ, and e is the edge labeling function that maps edges into Σ. We denote by G the space of labeled graphs and byḠ the space of unlabeled graphs. Often the label set Σ is a cartesian product of atomic label spaces we refer to as £elds. We often name £elds. For example the system introduced in Section IV-E has a node label space given by Σ = {base, unknown} × N × N × N where the £elds are named (mode, owner, dist, degree). We use dot notation to indicate the values of label £elds for speci£c robots. For instance i.owner = 10 indicates that robot i has the value 10 in its owner £eld.
If S is a set of vertices, G[S] denotes the subgraph of G induced by S. For any graph G the neighborhood of a vertex i ∈ V G is denoted N G (i) and the closed neighborhood is denoted
We denote the ¤oor function by x . De£nition 4.1: A function f is well de£ned with respect to an equivalence relation ∼ if x ∼ y implies f (x) = f (y).
Consider a system of N communicating robots, each with an identical state space X.
De£nition 4.2: An embedded graph γ is a pair γ = (G, x) where G is a labeled graph and x : V → X is a realization function. The space of all embedded graphs is denoted Γ.
Embedded graphs model the network topology, discrete and continuous states of an interconnected collection of robots, vehicles or particles in the following manner. A vertex i ∈ V indicates the index of the ith robot. The presence of an edge ij ∈ E corresponds to a physical and/or maintained communication link between robots i and j. The vertex label l(i) keeps track of local information and also indicates the operational mode of robot i. The edge label, e(ij), contains information maintained by two connected robots. The realization function x assigns to each robot a continuous state or realization in the state space X.
We write G γ , x γ , V γ , and E γ to denote the labeled graph, continuous state, vertices, and edges associated with an embedded graph γ. If S ⊆ V γ , then the embedded graph induced by S, γ [S] , is given by the pair (G[S], x |S ).
An embedded graph transition system is a triple (γ 0 , A, u) where γ 0 is an embedded graph describing the initial state, A ⊆ Γ × Γ is the embedded graph transition relation and u : V ×Γ → T X is the vector £eld describing the continuous ¤ow. In the systems we consider (γ 1 , γ 2 ) ∈ A implies x γ1 = x γ2 , thus an embedded graph transition system describes a hybrid system where the discrete (or jump) states are labeled graphs and the continuous states are realization functions.
A trajectory is a map σ : R ≥0 → Γ such that there exists a sequence τ 0 , τ 1 , τ 2 , ... where 1) x σ(t) is continuous.
2) τ k ≤ τ k+1 and if the sequence has any £nite length
We denote the set of nondeterministic trajectories of a system by T (γ 0 , A, u). When the dependence on σ is clear we will often write γ(t) to mean σ(t), x(t) instead of x σ(t) , and G k instead of G σ(τ k ) . Embedded graph transition systems allow the control of the evolution of the geometry, network connectivity and logic variables of a system. However, their direct implementation is undesirable because the global graph matching problem is computationally intense (especially from a distributed point of view), the model lacks local restrictions on sensing and communication, and the transition relation is not permutation invariant.
By explicitly including notions of locality in our model, we can address the issues above by ensuring 1) Graph matching involves only a small subset of vertices, 45th IEEE CDC, San Diego, USA, Dec. [13] [14] [15] 2006 FrIP4.10
2) The ¤ow and transition relations use only the information available to robots via local sensing and communication, and 3) The ¤ow and transition relation are permutation invariant. The £rst choice facing a system designer who will use embedded graph grammars is the construction of a proximity function, ψ, describing local restrictions on sensing and communication. A proximity function takes embedded graphs as inputs and returns a digraph ψ(γ) where an edge ij is in E ψ(γ) if robot i can sense and (if it chooses) communicate with j. A proximity graph is shown in Figure 2 . We call the out neighborhood of i, N + ψ(γ) (i) the proximity neighborhood. We call the set of vertices
In keeping with existing graph literature we write
We say an embedded graph γ is edge-consistent when every edge ij in E γ is also in E ψ(γ) . We denote by D the set of all edge-consistent embedded graphs. And denote by D G0 = {γ ∈ D | G γ = G} the subset of edge-consistent graphs with the topology of G. 
. We say a controller u : V × Γ → X is locally implementable if it is well de£ned with respect to the point of view equivalence relation ∼. As shown in Figure 3 when the vertex set contains only one element, A = {i}, the information available to compute the control u(i, γ) is restricted to the embedded graph induced by the friends of i, γ[F The formation of new links and the updating of the robots labels is executed on a local scale by the use of guarded rules. A guard g is a function g : P(V ) × Γ → {true, f alse}. A guard g is locally checkable if it is well de£ned with respect to point of view equivalence relation, 
, is a pair of labeled graphs over some small vertex set V L = V R and a locally checkable guard function g. Given a rule, a witness is a label preserving subgraph isomorphism mapping V L into G γ . If a witness is found and if g(h(V L )) = true, the rule is applicable via an action (r, h). Formally, the application of an action (r, h) on an embedded graph γ = (G, x) produces a new embedded graph γ = ((V, E , l , e ), x) de£ned by
That is, we replace h(L) (which is a copy of L) with h(R) in the graph G γ . We write γ r,h − − → γ to denote that we obtain G γ from G γ by applying action (r, h).
Consider the rule (g r , L, R), given by gr :
where the guard function g(A, γ) is true if the set of vertices A checking the rule only have neighbors in A, that is ∪ i∈A N ψ(γ) (i) = A. Figure 4 demonstrates the application of this rule to an embedded graph.
We often display rules in tables and use schema to represent multiple rules with a single diagram. Figure 5 shows four rules. In rule r 1 of Figure 5 , the pair of £eld names (owner, dist) appears above the transition arrow, indicating that the pairs labeling the vertices in the left and right graphs have values corresponding to those £elds. For rule r 1 to be applicable we must £nd a label preserving subgraph 45th IEEE CDC, San Diego, USA, Dec. [13] [14] [15] 2006 FrIP4.10 The mapping h 1 {1 → 1, 2 → 2, 3 → 3} is a subgraph isomorphism but does not preserve labels, so the rule is not applicable. The mapping h 2 {1 → 4, 2 → 6, 3 → 5} is a label preserving subgraph isomorphism, however the guard is false because ∪ i∈{4,5,6} N ψ(γ) (i) = {3, 4, 5, 6} = {4, 5, 6}. The mapping h 3 {1 → 8, 2 → 7, 3 → 9} shown in bold is a witness that satis£es the guard so the rule is applicable here. (b) Applying the action, (r,
isomorphism, h, mapping the left graph L into G such that if h(1).owner = m and h(2).owner = n, then the guard n > m is satis£ed. Label £elds not displayed in a rule are not changed if that rule is applied.
An embedded graph grammar system (EGG) is a quadruple (γ 0 , Φ, ψ, u) where γ 0 is an embedded graph representing the initial state, Φ is a set of locally checkable guarded rules, ψ is a proximity function and u is a locally implementable controller. We denote the trajectories of a local embedded graph grammar by T (γ 0 , Φ, ψ, u). An embedded graph transition ((G, x), (H, x)) is consistent with a rule r if there exists a witness h such that (r, h) is applicable to (G, x) and G r,h − − → H. We denote by A(r) the set of transitions consistent with rule r. If Φ is a set of rules, A(Φ) = ∪ r∈Φ A(r). The relationship between the trajectories of an embedded graph grammar and embedded graph transition system trajectories is given by
By a proposition, we simply mean a subset of embedded graphs, P ⊆ Γ. An embedded graph γ satis£es a proposition P , denoted γ |= P , if γ ∈ P . We often construct propositions that only refer to the topological elements of embedded graphs. Given a logical statement on graphs P , the indicator function I(P ) evaluates to 1 if the statement is true and 0 otherwise. The LTL speci£cation eventually always P , written FGP , means for every sequence in T (γ 0 , Φ, ψ, u) there exists some k (not necessarily the same) such that for all n ≥ k, G n ∈ P . When this is true we write (γ 0 , Φ, ψ, u) |= FGP . We often prove an eventually always speci£cation by constructing a discrete Lyapunov function. De£nition 4.5: Let be an ordering on R n with an unique zero element. A discrete Lyapunov function for the system (γ 0 , Φ, ψ, u) is a function V : G → R n such that a. V is a positive decreasing function over all trajectories, b. V 0 implies that at least one action (r, h) is eventually applicable, and c. V(G k ) = 0 implies that for all future graphs G n , V(G n ) = 0.
Theorem 4.1: Let (γ 0 , Φ, ψ, u) be any system where the set of reachable labeled graphs is £nite. Let P be a proposition on graphs and V be a discrete Lyapunov function for the system (γ 0 , Φ, ψ, u) such that V(G) = 0 implies G ∈ P . Then (γ 0 , Φ, ψ, u) |= FG P . Different literatures have different ¤avors of this theorem. For example, the theorem (and its proof) expressed in the temporal logic of actions can be found in [12] .
We now have the formal language to express the LBMR problem. Let X = R 2 . Let ij ∈ E ψ(γ) if and only if ||x γ (i)− x γ (j)
De£ne the proposition for distributed cycle removal by
Finally, let P CMR denote the set of embedded graphs where an edge between a base i and commuter j implies that x i = x j . We can now formally state the load balanced multiple rendezvous task. Task 4.1: Design an embedded graph grammar (γ 0 , Φ, ψ, u) such that for all trajectories σ, the graphs are connected, edge-consistent and
We introduce a locally implementable controller u in section V, and two grammars Φ DCR and Φ LB in sections VI and VII. The embedded graph grammar (γ 0 , Φ DCR ∪ Φ LB , ψ, u) de£nes an algorithm that removes cycles, converting highway robots to commuters. It then routes commuters to the bases along the highways. This method does not require maps or even a global coordinate system. For proof simplicity, we assume that |F | |B| evaluates to an integer, noting our grammar converges to an invariant set when this is not the case.
We introduce a locally implementable controller u similar to the one de£ned in [5] .
Suppose E 1 denotes the set of edges ij where either i.mode = base or j.mode = base but not both and j is in F (i). Let E 2 be the set where neither are bases and j is in F (i). De£ne the function
and de£ne the total potential function by
45th IEEE CDC, San Diego, USA, Dec. [13] [14] [15] 2006 FrIP4.10 Fig. 5 . Rule set for distributed cycle removal, Φ DCR . The boolean statement to the left of the colon is the guard. If a match of the graph on the left side of the transition arrow is found that also satis£es the guard, it can be replaced with the graph on the right side. The label £elds involved are (owner, dist) and the light dashed lines in the fourth rule indicate that an edge may or may not be present.
The controller u is de£ned by
where
Since the controller depends only on F (i) it is locally implementable.
We must show that if we £x a graph G and implement the control law u on any edge-consistent graph γ(t), then γ(t ) is edge-consistent for all t ≥ t. Additionally, we must show that x(t) converges to safe con£gurations in which if the rules introduced in section VII insert edges, the embedded graph remains edge-consistent.
Proposition 5.1: Suppose γ is any connected edgeconsistent graph and that G(t) = G for all t ≥ t. Then the continuous state x(t) of the system evolving under u converges to the set of limit points M = {x γ | γ ∈ D G anḋ U(x γ ) = 0}. Furthermore any point in M is a £xed point.
Proof: Without loss of generality, suppose one base i has coordinates x i = (0, 0). Let Ω 0 = {x γ |x γ ∈ D G0 U(x) ≤ U(x 0 )}. Ω 0 is closed and bounded by r = |G|∆. SinceU = − ∂U ∂x T ∂U ∂x ≤ 0, then by LaSalle's invariance principle [13] every solution converges to M . Furthermore since wheneverU = 0, thenẋ = 0, it follows that any x ∈ M is a £xed point.
Additionally, γ(t ) must be edge-consistent for all t ≥ t since U ij → ∞ as ∆ − ||x i − x j || → 0.
We call any vertex i where degree(i) = 1 a leaf vertex. Corollary 5.1: Suppose i, j ∈ V are any vertices such that degree(i) = 1 and ij ∈ E. If x * j denotes a £nal value of x j (t), then x i converges asymptotically to x * j . Proof: Suppose x i = x j . Because i is only connected to j,∇ xi U = xj −xi
T (∇ x U), it must be the case thatU < 0, implying that a con£guration with x i = x j is not in M .
We have shown that each leaf converges to its neighbor's position. We next construct a grammar in Section VI that forms trees (and thus leaves). Then we construct a grammar in Section VII that changes the connectivity of leaf robots so the controller u can direct their progress toward the bases.
We wish to construct a rule set, Φ DCR , such that eventually it is always the case that G γ is a connected tree (proposition P DCR ). We denote by |C G | the number of cycles in a graph G.
A. Grammar Figure 5 shows the rule set designed for distributed cycle removal. In the initial graph G γ0 , (owner, dist) = (i, 0) if node i is a base. Otherwise, (owner, dist) = (0, |G 0 |). The algorithm propagates a nondeterministic ordering based on path length from a base node and uses the ordering to determine where to cut a cycle. The guard for r 1 requires the owner £eld of the £rst node to be less than the owner £eld of the second node. As seen in Figure 6 , when this is the case, the £rst node assumes the owner value of the second and sets its distance to be one unit larger. This rule guarantees that eventually all vertices will have the same owner. When two connected vertices i and j share the same owner, rule r 2 updates their perceived distance to the owner based on lowest value in their dist £elds. This rule guarantees that the perceived distance, i.dist, converges to the true distance, d(i, i.owner). As shown in Figure 6 , rules r 3 and r 4 determine when two paths from the same base node meet (and thus a cycle is present) and delete a connecting edge.
In this section we show that (γ 0 , Φ DCR , ψ, u) |= FGP DCR . Let V : G → R × R × R where Proof: We show that V meets the three requirements in De£nition 4.5.
V is positive decreasing: We have constructed V 1 , V 2 , and V 3 such that applying rule r 1 to any graph decreases V 1 , applying rule r 2 decreases V 2 , and applying r 3 or r 4 decreases V 3 . Using the lexicographic ordering we must show that applying rules r 3 or r 4 does not increase V 2 or V 1 which is true because it does not change the owner or dist labels. An application of r 2 does not increase V 1 because it does not alter the owner label. Thus V is positive decreasing with respect to ≺. V 0 implies that at least one action (r, h) is applicable: Assume V 0 but that no action is applicable. Note that V 1 > 0 means r 1 is applicable, which contradicts our assumption. Thus, V 1 = 0 and all vertices are labeled with the same owner. However if V 2 > 0, then |i.dist − d(i.owner)| > 0. This implies that there exists a vertex j with an edge ij where either 1) j.dist < i.dist + 1 and r 2 is applicable or 2) j.dist = i.dist and r 3 is applicable. If V 2 = 0, then Proposition 6.1 implies that every node is labeled with its true distance from the owner node. Finally, if V 3 = |C G | > 0, then there must exist two nodes i and j such that i.dist = j.dist and either r 3 or r 4 is applicable.
If V(G k ) = 0, then V(G n ) = 0 for all n > k: When V 1 = V 2 = 0 the guards for rules r 1 and r 2 are false. Since no rule forms an edge, |C G | = 0 for all future graphs.
Theorem 6.1: Given the system (γ 0 , Φ DCR , ψ, u) it is eventually always the case that G γ is a connected tree.
Proof: The reachable set of graphs is bounded because the vertex set and label sets of the system are £nite. V 3 = 0 implies there are no cycles in the graph and Proposition 6.1 implies connectivity. Thus if V = 0, then γ ∈ P DCR . We then invoke the Lyapunov Theorem 4.1, to show it is eventually always the case that G is a connected tree.
VII. LOAD BALANCING Our goal is to route (in balanced proportions) all the commuters to the bases. By A ⊂ C, we refer to the strict subset of commuters without an edge to a highway robot and informally call these alley robots. For instance in the £rst panel of Figure 8 , robot k is an alley robot. Suppose G ω is a tree where every edge ij is labeled with four £elds: (b ij , s ij , b ji , s ji ). Suppose we cut the graph at any edge ij as pictured in Figure 8 . We denote the resulting tree containing vertex i (respectively j) by T i , (respectively T j ). As seen in Figure 8 , the strength of T i , s ij = | V Ti C|, is the number of commuter robots in the tree. The base mass, b ij , of tree T i is the number of bases in the tree. Since initially the structure of the graph and number of bases is unknown, we consider a set of edge-consistent embedded trees
We de£ne the function err to be the number of edge £elds labeled 0. We propose a grammar Φ LB such that (ω 0 , Φ LB , ψ, u) |= FGP LB . Our approach is to route commuters in the alleys to the highway where they can make locally optimal choices that distribute them evenly to the bases. Figure 7 displays the rule set, Φ LB , designed for this purpose. Rule r 5 collects and propagates the strength and base mass information through the graph. Rule r 6 moves alley robots toward the highways. We de£ne the pressure differential of an edge in the ij direction by
As seen in Figure 8 , if the pressure differential is greater than zero, rule r 7 moves the commuter into the subtree with the lowest pressure (where a subtree is chosen nondeterministically if all pressures differentials are equal and greater than zero.)
Let C i = 1 if the number of commuters attached to i is greater than zero, and zero otherwise. And [x] = x if x > 0 and [x] = 0 otherwise. As in the previous section, we associate a function with each rule, construct a lexicographic ordering among them, and propose Υ as a Lyapunov function for the grammar where
Rules r 6 and r 7 have guards of the form ||x n − x j || < ∆ that guarantee the system remains edge-consistent when the rules insert edges. In order to guarantee progress we must show that these guards are always satis£ed in £nite time.
45th IEEE CDC, San Diego, USA, Dec. [13] [14] [15] 2006 FrIP4.10
||x1 − xn|| < ∆ :
(1, w + 1) Fig. 7 . Rule set for load balancing, Φ LB . Note the inclusion of geometric conditions in the guards on r 6 and r 7 . β(i) I(i.mode = base) and µ(i) I(i.mode = base ∧ degree(i) = 1).
= base = commuter = highway Fig. 8 . Graph transition due to the concurrent application of rules r 6 and r 7 . The £gure shows the value of the pressure differential, ρ ij , between subtrees T i and T j (marked by dashed ovals). Rule r 7 moves a commuter from i to j, decreasing the pressure differential.
Proposition 7.1: Let γ be edge-consistent with a £xed topology G where ij, jk ∈ E G and i is a leaf. Then x i converges to a set of points where ||x i − x k || < ∆ in £nite time. The proposition follows directly from Corollary 5.1 and the fact thatU < 0.
is a Lyapunov function for the system (ω 0 , Φ LB , ψ, u).
Proof: We must again show the following. Υ is positive decreasing: Showing Υ 1 and Υ 2 decrease with respect to the lexicographical ordering ≺ is straightforward. Rule r 7 is applicable when [ If Υ 0 at least one action is applicable: Assume Υ 0 but no rule applies. The tree structure guarantees if err > 0, then rule r 5 is applicable, so it must be the case that Υ 1 = 0. By Proposition 7.1, eventually the guard ||x i − x j || < δ for rules r 6 and r 7 is satis£ed. Thus if A = ∅, then r 6 applies. It follows that Υ 2 = 0 and all the commuters are attached to the highway. However, if
, then r 7 applies because there is a commuter connected to a node with a pressure differential. This contradicts our original assumption, thus Υ 0 implies an action is applicable.
The guards on the rules r 6 and r 7 are false when Υ = 0. Applying rule r 5 does not change the graph since all £elds are correct.
Proof: We must show that if Υ = 0 then for all k ∈ B, 
The preceding sections introduce two grammars, Φ DCR and Φ LB and a continuous control law u such that the system (γ 0 , Φ DCR , ψ, u) |= AFP LB and (ω 0 , Φ LB , ψ, u) |= AFP LB . The correctness of each of the grammars is proved using discrete Lyapunov functions. In this section, we compose the grammars and show that for γ 0 with the edge labeling (0, |G γ0 |, 0, |G γ0 |)
Building on previous results, we form a new function
where R 6 is lexicographically ordered. We must show that applying any rule from Φ LB does not change the ordering property of Proposition 6.1 since this property is necessary to prove that V is a Lyapunov function. This is true since Φ LB repairs the dist £eld when it moves a commuter. And we note that the convergence property in Proposition 7.1 still holds for the composition.
45th IEEE CDC, San Diego, USA, Dec. [13] [14] [15] 2006 FrIP4.10 Lyapunov functions for a representative run. A) Prior to an application of r 3 . B) After an application of r 3 . C) The £nal state. Note the lexicographic ordering allows X to be positive decreasing even though Υ increases at B.
To show that X is positive decreasing, under the ordering, we only need to show that applying a rule from Φ LB does not increase V. This is the case since Φ LB does not alter the owner £eld, does not create cycles, and repairs the dist £eld. To show that X 0 implies an action is applicable we note that since the rules from Φ LB have not changed the information used in Φ DCR , V 0 implies that a rule is applicable. If V(G) = 0, then the graph G is a tree and thus the Φ LB grammar has a valid initialization point. Since Φ DCR does not alter edge labels, V = 0 ∧ Υ 0 implies an action is applicable. Invoking the Lyapunov theorem proves the temporal logic statement in Equation 8 . By the convergence properties proved in sectionV we have the desired result that for all trajectories σ ∈ T (γ 0 , Φ DCR ∪ Φ LB , ψ, u) lim t→∞ σ(t) ∈ P DCR ∩ P LB ∩ P CMR .
Note that Φ DCR and Φ LB are really compositions of single rules. Given a collection of smaller grammars, our design methodology is to: 1) Identify initialization conditions and invariant properties of those grammars such as the ordering property of Proposition 6.1. 2) Compose the grammars and show the composition satis£es the initialization and invariant properties. 3) Form a Lyapunov function for the new system under a lexicographic ordering that allows us to build upon previous results.
We simulated the system in Matlab using cyclic initial graphs ranging in size from 30 to 70 vertices. Every simulation terminated with a correct topology and trajectories where the commuters were converging on the bases. Figure 9 shows a representative run and the three Lyapunov functions V, Υ and X . Note the £gure presents qualitative information since we use a normalizing scheme to represent the lexicographic ordering. One of the more persuasive arguments for our design is that the system can execute in a concurrent fashion as shown in Figure 9 . The £gure displays the states just before and after an execution of rule r 3 where Υ increases because commuters are added to the alleys. However, by ordering the V elements of X before the Υ elements, we can guarantee that X decreases.
The embedded graph grammar formalism models networked hybrid systems with local restrictions on their interactions. We have shown a mapping from an embedded graph grammar to a non-deterministic hybrid automata. The automata may be large, unwieldy and may obscure information about the local processes controlling the evolution of the embedded graph grammar. Nonetheless using this mapping, we hope to £nd conditions on the structure of the embedded graph grammar that will allow us to build upon existing results in hybrid systems literature.
Additionally, we developed a notion of the composition of grammars in our proof. In future work, we will de£ne formally the semantics of graph grammar composition, examine which properties and behaviors are preserved in grammar composition, and develop formal design methods for composing grammars.
Five axis machine tools are well known in the industry for their high versatility and productivity by allowing the control of both position and orientation of the cutting tool relative to the workpiece. However, the higher number of axes results in more sources of errors, such as geometric link errors, which represent the relative location error between successive axes of a machine tool. Furthermore, the effects of those link errors are combined, resulting in potentially significant volumetric errors between the tool and the workpiece. Many studies have been carried out to estimate the link errors. Abbaszadeh-Mir et al. developed a linearised model relating the eight link errors of a five axis machine tool to the volumetric errors [1] which can be solved in a least square sense to estimate the link errors. Lei and Hsu proposed a measuring instrument to evaluate volumetric errors [2] with a method based on the closed chain principle [3] . Bringmann and Knapp developed a method called "Chase-the-ball" using a ball artefact mounted in the tool holder and linear probes to * Corresponding author.
Tel.:+33 1 47 40 27 57; Fax:+33 1 47 40 22 20
Email addresses: loic.andolfatto@lurpa.ens-cachan.fr (L. Andolfatto), rene.mayer@polymtl.ca (J.R.R. Mayer), sylvain.lavernhe@lurpa.ens-cachan.fr (S. Lavernhe) measure the Cartesian volumetric errors and identify link errors using a similar approach [4] . Zargarbashi and Mayer presented a non-contact measuring instrument called CapBall to measure volumetric errors and identify the eight link errors [5] . The knowledge of those eight link errors allows to enhance the machine accuracy by a compensation for their effect on volumetric errors at the tool center point.
In [6, 7] , Bringmann and Knapp showed how the machine tool performance influence identification tests, considering the contribution of motion errors on uncertainty.
The purpose of this paper is to investigate the uncertainty contributions on the identified errors when using a Cartesian closed chain calibration approach and the CapBall sensing head. The identification procedure is briefly described and the result of identification are presented and evaluated. The second part presents the multi-output Monte Carlo technique and the sources of uncertainty considered.
The machine drift uncertainty is included in the model: a statistical model is compared to one that takes into account the cyclic character of the drift observed in the measurement chain.
Finally, the estimated uncertainties are compared to experimental results, and the uncertainty due to each source are separately evaluated to pinpoint the most penalising source of uncertainty.

The tests were performed on a Huron KX8-Five five axis machining center, depicted in Fig. 1 . This machine has a WCAYFXZT structure, with a 45-degree-tilted A rotary axis. The strokes of the axis are 650 mm, 700 mm and 450 mm for X, Y and Z respectively. The numerical command controller is a Siemens Sinumerik 840D Powerline.
In [1] , Abbaszadeh-Mir et al. presented a linear relation between the eight link errors of the machine and the six positioning errors of the setup -gathered in an array δp -on one hand and the three components of the Cartesian volumetric errors 1 at the tool center point expressed in the tool frame -written in the vector δτ -on the other hand.
Following the methodology proposed in [1] , the relevant error parameters are the following:
where δα i , δβ i and δγ i denotes a small rotation of the joint i around the X-, Y-and Z-axis respectively with respect to its nominal orientation (e.g. δγ Y is the squareness between X and Y expressed as a small rotation of the Y axis around the Z-axis); and δy C denotes a translation of joint C in the direction y with respect to its nominal position relative to joint A. Subscripts W and T stand for workpiece and tool respectively. Explanations about the components of the vector δp are given in Table 1 .
The errors gathered in δp are modelled by small translational and rotational displacements. The Cartesian volumetric errors δτ results from the sum of the effect of those small displacements, each propagated at the tool center point. Joint motion errors, e.g. straightness, yaw, pitch, roll or positioning error of the axis are assumed negligible compared to the influence of link error in this model [5] .
The link errors are modelled with small displacement screw. Considering a pose k of the machine, the small displacement screws can be transported at the tool tip involving linear transport equations. This leads to a linear operator J k called Jacobian of the link and setup errors and built as an algebraic expression of the sum of the effect of the previously introduced small displacements, leading to eq.(2). 
More details about the construction of the Jacobian are given in [1] and [5] .

In order to measure the volumetric errors at the tool center point δτ , a non-contact measuring instrument, called CapBall, had been developed in house [5] . It consists in a master ball mounted on the table and a sensing head fitted with three capacitive sensors, mounted in a tool holder in the spindle.
The three sensors have their axes nominally orthogonal and intersecting in one point called P t . The machine is programmed to keep the tool center point P t coincident with the center of the master ball called P w (Fig. 1) . Due to the geometric errors of the machine and setup, P t and P w are not exactly coincident, and the resulting volumetric error is measured by the sensing head.
The capacitive sensors are pre-calibrated for the spherical target using a set of three high precision linear stages. The response non-linearity is kept under 0.5% as long as the eccentricity is kept within ±300 µm and the distance between the sensor and the ball is in a ±300 µm range centred on the position at which the sensor gives a 0 Volt signal.
Eq.(2) provides three scalar equations for each pose of the machine. Those three scalar equations correspond to the measurement of the three components of the Cartesian volumetric error δτ with the CapBall.
As the array δp has 14 components, it requires several poses to have enough scalar equations for its identification. With n poses, eq.(2) can be written n times relating n different values of the volumetric error vectors δτ 1 · · · δτ n to δp, as in eq.(3).
The array
is called the error vector set. Finally, eq. (3) is written in a condensed manner as follows:
The link and setup errors δp are identified using a Moore-Penrose pseudo inverse matrix:
A set of 807 poses has been defined as the identification trajectory. Those poses are chosen considering certain constraints:
• maximise the volume inscribed into the identification path to cover the largest part of the machine working envelop to identify more representative parameter estimates;
• provide a relatively good condition number for the identification Jacobian to reach a better data noise immunity;
• provide a large number of scalar equations to identify the parameters with a good outlier points robustness.
The trajectory shown in Fig. 2 describes the 807 successive positions of the points P t and P w in the machine coordinate system. The identification trajectory is performed with a clockwise A-axis and C-axis motion. The X-,Y-and Z-axis motions are calculated to keep the position of the tool tip relative to the workpiece nominally constant.
During the measurements, the sensing head orientation is maintained constant relative to the machine coordinate system by programming the regulation of the spindle orientation on the machine. However, the sensing head measurement frame is not parallel to the machine tool frame. The CapBall allows the measurement of the Cartesian volumetric errors in the sensing head frame. To be able to express δτ in the machine tool frame, the orientation of the sensors axes must be calibrated.
The procedure is achieved by executing a calibration trajectory within a cube of 0.2 mm × 0.2 mm × 0.2 mm centred on the nominal position of the master ball, with a regular mesh of 125 points. For each of the 125 points of the calibration trajectory, the vector δτ t defined from P w to P t , expressed in the machine coordinate system, is programmed at a known value. At the same time, the sensor readings are recorded in δτ s . The two following homogeneous coordinate matrices can be built to model this process:
Those two matrices can be theoretically related by a transform matrix M s→t to convert the sensors readings into master ball motion relative to the sensing head expressed in the machine frame:
Then, the best transform matrix from sensors frame to machine frame considering a least square criterion is:
where ∆ s + is the Moore-Penrose pseudo inverse of ∆ s .
With this method, none of the terms of M s→t are constrained. The sensors are ordered so that (e 1 , e 2 , e 3 ) is a right-handed reference frame. In an hypothetically perfect case, where the sensors directions would be exactly orthogonal, the gains perfectly known, the sensors response exactly linear and the machine movements perfect, then M s→t should be written as:
with the norm of each e i equal to 1 and all the e i forming together and orthogonal matrix. The experimental M s→t obtained with eq. (9) is quite similar to those theoretical values. The Details are given in Table 2 . The last row of the experimental M s→t matrix is equal to its theoretical value, with an approximation of 10 −15 , supposed to be numerical errors only.
Differences between theoretical and experimental norms of the e i vectors come mainly from differences in the sensors gains. As shown in Table 2 , it is less than 1%. Considering the projections e i · e j , the resulting values could be attributed to lack of orthogonality between the sensors axes and the machine's local out-of-squareness.
The vector d results from the sensors reading offset due to inaccurate axial position of the sensors in the sensing head. Each of its components were below 10 µm, which contributes to keeping the sensors in their linear domain.
The difference between the theoretical and the experimental M s→t is considered negligible. The experimental M s→t obtained during the calibration procedure is used for the identification of the link errors.
To measure the volumetric errors δτ k for the pose k, the trajectory is run with exact stop and dwell time at each pose with a tolerance of 1 µm. Compared to the method proposed by Zargarbashi and Mayer [5] , this requires a longer measurement time (about 10 minutes instead of 2), but considering the entire procedure, including preparation time, this is not a significant penalty.
The sensors signals are gathered continually trough a LABView application with a 1000 Hz sampling rate. The volumetric errors for the programmed poses are extracted using a steady value algorithm developed for this purpose.
The identification process has been performed using the identification trajectory illustrated in Fig. 2 . The identified error parameters are given in Table 1 . Fig. 3 shows the measured volumetric errors and the prediction using the identified model (top). The difference between measured errors and modelled errors is called residual errors (bottom). This portion of the errors unexplained by the model are under 10 µm, with root mean square value of 1.5 µm, 1.8 µm and 1.3 µm in the x, y and z directions respectively. 
The robustness of the identified model is evaluated by comparing its predictions against experimental measurements for a validation trajectory (Fig. 4) run with a counter-clockwise A-axis motion. The comparison between predictions and measurements is given in Fig. 5 . The RMS values of the residual errors are then equal to 2.7 µm, 3.6 µm and 2.5 µm in x, y and z direction respectively. Those values are twice as large as those for the identification trajectory.

A Monte Carlo approach has been implemented to evaluate the uncertainty of the identified link errors. For such problems of least square identification, a Monte Carlo approach is not necessarily needed if uncertainties on the input can be modelled by a simple distribution [9] . However, as will be shown, the uncertainty on the input (i.e. the measured volumetric errors δτ 1 · · · δτ n ) cannot be modelled by a simple distribution.
Instead of choosing an arbitrary number of trials, considered sufficient, the simulation is adaptive, as described in [8] . The main purpose of implementing an adaptive method is to provide control variables (i.e. expected values y, standard uncertainties u(y) and lower and higher coverage interval endpoints y low and y high ) with an expected numerical tolerance.
Briefly, the adaptive method consists in performing the Monte Carlo simulation divided into sequences of M trials until the maximum of each standard deviations sets associated with the average of the estimate for each sequence s y , the standard uncertainty s u(y) , and lower and higher coverage interval endpoints s y low and s y high are lower than the chosen numerical tolerance (see [8] and Fig. 6 ). The algorithm described in [8] has been implemented in MAT- In this case, M has been set to 10 4 for a coverage probability of 95%. The numerical tolerance δ has been set to 0.5 · 10 −4 mm. The model of uncertainty includes three sources: the drift of the closed kinematic chain during the measurement, the uncertainty on the sensors output values and the uncertainty due to the projection from the sensing head frame to the tool frame. Those uncertainty sources are part of the measurement and identification process.
Potential uncertainty due to the NC-unit action is removed using the exact stop option in the NC program. The influence of joint motion errors [7] was not considered.
Standard type A uncertainty on the sensors output has been evaluated in the useful measurement range, according to [10] . It is used to produce random values within a normal distribution in the Monte Carlo analysis for each point and sensor. The standard measurement uncertainties are given in Table 3 .
As described previously, the measurement result of each sensor is used to provide the measured volumetric error in the machine reference frame, using the transform matrix M s→t . Standard uncertainties on the terms of this matrix induce additional uncertainties on the estimated parameters. To evaluate this effect, the trajectory used for calibration was run four times to provide a total of 500 points where the small programmed X-, Y-, Z-axes motions can be compared to the measured ones. Distributions of the differences between programmed and measured errors are given in Fig. 7 .
The distributions observed correspond with normal distributions so the influence of the transformation from the sensors frame to the machine frame has been modelled by a normal distribution reflecting the evaluated standard deviations observed.
This model can be seen as slightly pessimistic, since this uncertainty model is based on data that are themselves suffering sensors uncertainties and thermal variations. The thermal variation effect can be neglected, considering the 4 tests have been performed within 3 minutes. No further hypothesis were made to avoid including the contribution of sensors uncertainties here. The modelled effect of the frame transformation is known to be over evaluated, leading to higher total uncertainties.
The thermal expansion of the spindle during machining is a well known problem among the machining community [11] . The dimensional variations -or drift -of the closed kinematic chain has been considered in the uncertainty model.
An example of the drift cycles is shown in Fig. 9 . The spindle temperature is regulated by a cooling device which generates thermal cycles. During those cycles, the spindle suffers small movements of approximately 7.5 µm peak to valley in the z direction and 4.5 µm peak to valley in the x and y directions. Such a high magnitude of the drift may result from the use of the machine not under normal condition met during machining. Similar were observed for three different poses, depicted in Fig. 8 . For those three poses, similar period, magnitude and pattern have been observed between channels. It suggests that the observed effect is not a variation of the mesurand (i.e. the 8 link errors) but a variation in the measurement chain, as the observed volumetric variations do not vary with the pose, influenced itself by the link errors. However, if the machine link errors -the mesurandare affected, it is still a contribution to uncertainty since it means that the desired conditions of the mesurand are not met. Such conditions are difficult to define for a machine tool since its metrology is not conducted during normal machining conditions.
The drift is included in the uncertainty model. Two different methods are compared: on one hand the drift is treated statistically and on the other hand, its cyclic character is preserved in the model.

-For i=1 to M y h =(δp1 … δpM)
The principle given for linear positioning measurement in ISO/TR 230-9 [12] is applied. It takes into account the magnitude of the drift -called E V E -to calculate the uncertainty due to environmental variation u EV E as described by eq.(11).
In this case, a value of u EV E is evaluated for each sensor to produce random values within a normal distribution subtracted for each point in the Monte Carlo analysis. The values of E V E summarised in Table 4 are evaluated according to the measurements shown Fig. 9 . In the following, this is called the statistical method.
In the cyclic method, the uncertainty due to environmental drift is evaluated considering the cyclic character of the variation. A typical variation period of these cycles (Fig. 9 ) has been chosen so that the drift is modelled as a periodical function of time D EV E from t = 0 to +∞.
The beginning of the first trial of each sequence is chosen randomly in the period which gives the starting time Sensor -Channel EV E uEV E Sensor 4 -Channel 1 6.95 µm 2.00 µm Sensor 5 -Channel 2 3.42 µm 0.99 µm Sensor 6 -Channel 3 6.63 µm 1.91 µm t 0 . The duration of a simulated trajectory is known as t m , and the time interval between the volumetric errors measurement at two following point is known as t i . Then, the measurement time t nk of the point number k (k varies from 1 to 807) in the trial number n (n varies from 1 to M ) is given by eq. (12) .
where:
The simulated drift at this point is given by D EV E (t nk ). The calculated variation is subtracted from the simulated measured value. In the following, this is called the cyclic method.

The reliability of the multi-output Monte Carlo simulation method was evaluated by confronting simulation results to experimental values: 15 identification trajectories were executed, leading to 15 error sets δχ. The first 5 sets were removed from the analysis to avoid including thermal variation of the joints, leading to potential variations of the mesurand. Then, eq.(5) provides an array of identified link and setup errors for each of the 10 tests considered as relevant. The execution of those 10 tests took approximately 3 hours, so several drift cycles occurred over this period (while the machine joints, assumed to be unaffected by the thermal drift, mainly attributed to the spindle, remains in a steady state after the 5 previous test warm up period).
The mean value of the 10 δχ sets is used as an input for the Monte Carlo simulation. Other inputs are either the u EV E value for each sensor or the periodical function of the time D EV E , standard uncertainties on the sensors output and standard uncertainties of the sensor to machine frame transformation. Outputs of the simulation are the expected distribution of the identified link errors (Fig. 10 ) and lower and higher coverage interval endpoints (Fig. 11 and Fig. 12 ). The 95% coverage intervals given by the Monte Carlo simulation are compared to the 10 relevant tests results in Fig. 11 for the statistical method and in Fig. 12 for the cyclic method. The mean values of Fig. 11 and Fig. 12 do not exactly match the results presented in the Table 1 because identification results of the Table 1 were obtained during the winter and the the experiments leading to Fig. 11 and Fig. 12 were performed during the summer in a laboratory where the global climate condition can fluctuate a lot. This statement shows that the machine geometry is also subject to larger alterations on a long period, but this phenomenon is not discussed further in this paper. The statistical method leads to uncertainty intervals smaller than observed variations of the identified errors, whereas the simulation with the cyclic method provides a more realistic size for the 95% coverage interval for each link error. This illustrates the importance of taking into account the cyclic character of the drift in the model. From this point onward, only the cyclic method is kept in the analysis.
As shown in Fig. 10 , the measurement chain variation cycles lead to non-normal distributions 2 , even if expected values correspond to identified values with the δχ input.
Sizes of the 95% coverage intervals for the cyclic method are given for each link error in Table 5 .
Ten simulations were executed with the cyclic method to characterise the numerical stability. All ten simulations converged to the same outputs in terms of expected values, standard uncertainties and lower and higher endpoints of the 95% coverage interval, with a ±δ tolerance previously defined as 10 −4 mm. From this point of view, the method can be considered numerically stable.
A typical convergence of the control variables is shown in Fig. 13 . The number of sequences performed to reach the convergence criteria for all the control variables is h = 20 in this case. The lower and higher endpoints of the 95% coverage interval proved to require more sequences to reach convergence. For the ten simulations, the number of sequences before convergence varied from 17 to 24. 
Three other simulations were performed this time, including only one source of uncertainty at a time. This allows to compare the contribution of each source of uncertainty. Table 5 shows the total contribution of all uncertainty sources ∆ 95% (for the cyclic method) compared to ∆ EV E for the drift variations only (cyclic method), ∆ sensors for the sensors output uncertainty only and ∆ trans for the frame transformation uncertainties only. Those simulated values show that the dimensional variations are responsible for most of the total uncertainties.
It is noticeable that for each error parameter, the quadratic sum of the 3 contributions ∆ 2 is lower than the predicted ∆ 95% probably a consequence of the cyclic character of the drift included in the model.
The paper presents a method to evaluate contributions to the uncertainties for identified link errors of a five axis machine tool. Sources of uncertainty in the identification procedure have been inventoried, considering certain hypothesis. Those standard uncertainties have been propagated with a multi-output adaptive Monte Carlo approach, using either a statistical model or a cyclic model for the drift.
The main source of uncertainty in the procedure, for the tested machine in the prevailing experimental conditions, was the drift of the closed kinematic chain. The cyclic nature of the drift proved to have a significant impact on the simulation results.
The numerical stability of the implemented method was evaluated, and the obtained uncertainties compared to results from ten experimental identification trials.
The method was also used to evaluate the contribution of each of the three uncertainty source in order to pinpoint the dominant source, in order to improve the link errors identification method by decreasing or removing its impact. The effect of environmental variation errors, generally attributed to thermal variations of the spindle, proved to be predominant under the conditions of the tests. The thermal stability control during the identification must now be improved to decrease the total uncertainty of the procedure.
In [1] , we have given a representation of the set of intervals in terms of associative algebra. More precisely, we define on the set IR of intervals of R a R-vector space structure. Next we embed IR in a 4-dimensional associative algebra. This embedding permits to describe a unique distributive multiplication which contains all the possible results of the usual product of intervals and the monotony property is always conserved. Moreover, this new product is minimal with respect the distributivity and the monotony properties. In this section, we present briefly this construction (for more details, see [1] ).
Let IR be the set of intervals of R, that is
This set is provided with a semi-group structure that we can complete as follow: we consider the equivalence relation on IR × IR:
for all X, Y, Z, T ∈ IR. The quotient set is denoted by IR. The addition of intervals is compatible with this equivalence relation :
(X, Y ) + (z, t) = (x + z, y + t)
where (x, y) is the equivalence class of (X, Y ). The unit is 0 = {(X, X), X ∈ IR} and each element has an inverse (X, Y ) = (Y, X).
Then (IR, +) is a commutative group. We prove also in [1] , that any equivalence class admits a canonical representant of type (K, 0) or (0, K) or (a, a), with K ∈ IR and a = [a, a], a ∈ R. We provides the group (IR, +) with a real vector space structure, the external product being given by
The triplet (IR, +, ·) is a real vector space.
Remark. To simplify notations, we write (K, 0) or (0, K) in place of (K, 0) or (0, K).
Recall that by an algebra we mean a real vector space with an associative ring structure. Consider the 4-dimensional associative algebra whose product in a basis {e 1 , e 2 , e 3 , e 4 } is given by e 1 e 2 e 3 e 4 e 1 e 1 0 0 e 4 e 2 0 e 2 e 3 0 e 3 0 e 3 e 2 0 e 4 e 4 0 0
The unit is the vector e 1 + e 2 . This algebra is a direct sum of two ideals: A 4 = I 1 + I 2 where I 1 is generated by e 1 and e 4 and I 2 is generated by e 2 and e 3 . It is not an integral domain, that is, we have divisors of 0. For example e 1 · e 2 = 0. The ring A 4 is principal that is every ideal is generated by one element. The cartesian expression of this product is, for x = (x 1 , x 2 , x 3 , x 4 ) and y = (y 1 , y 2 , y 3 , y 4 ) in A 4 : x · y = (x 1 y 1 + x 4 y 4 , x 2 y 2 + x 3 y 3 , x 3 y 2 + x 2 y 3 , x 4 y 1 + x 1 y 4 ).
The multiplicative group A * 4 of invertible elements is the set of elements x = (x 1 , x 2 , x 3 , x 4 ) such that
If x ∈ A * 4 we have:
We define a correspondence between IR and A. Let ϕ be the map
given by
and
Thus the image of IR in A 4 is constituted of the elements
The map ϕ : IR−→A 4 is not linear. We introduce in A 4 the following equivalence relation R given by
and consider the map
where Π is a canonical projection. This map is surjective. In fact we have the correspondence
•
This correspondence defines a map ψ : A 4 −→ IR.
In the following, to simplify notation, we write ϕ instead of ϕ.
Definition 1 For any X , X ′ ∈ IR, we put
This multiplication is distributive with respect the the addition. In fact
Suppose that ϕ(X 1 + X 2 ) = ϕ(X 1 ) + ϕ(X 2 ). In this case this means that ϕ(X 1 ) + ϕ(X 2 ) / ∈ Imϕ. But by construction ϕ(X 1 + X 2 ) ∈ Imϕ and this coincides with ϕ(X 1 + X 2 ). For numerical application of this product, see ([?, GN-R].
Let gl(n, IR) be the set of square matrices of order n whose elements are in IR. A matrice of gl(n, IR) is denoted by
It is clear that gl(n, IR) is a real vector space. We define a product on it puting
This last product being the associative product on IR. Thus gl(n, IR) is an associative algebra.
Définition 1 A matrice A ∈ gl(n, IR) is called inversible if its determinant, computed by the Cramer rule, is an inversible element in IR.
Recall that the group IR of inversible elements contain
To compute the determinant, we use the classical formula of Cramer. The determination of A −1 can be computed using the classical rules.
Example. If we consider the invertible matrix B 2 , we obtain
Let us verify that B 2 B −1 2 = Id. Using the product on IR we obtain
The coefficient in place (1, 1) is
¿From the definition of the product (see section 1), this element is 

Let A be in gl(n, IR). An eigenvalue of A is an element X ∈ IR such that there exists a vector
Thus X is a root of the characteristical polynomial with coefficients in the ring IR C A (X ) = det(A − X I) = 0.
Example. Let
We have .
We obtain
], 0), 
We obtain six eigenvalues.
Remark. To compute the interval-eigenvalues of a matrix A, we have to find the roots of the characteristical polynomial of A. But this polynomial is with coefficients in IR (or A 4 ) and this set is not a field neither a factorial ring. Then it is natural to meet some special results (e.g if we consider the second degree polynomial X 2 − 1 with coefficients in Z 8Z which is not factorial, it admits four roots,1, 3, 5, 7.) In our example we finds 6 roots. Now if we consider the real matrix whose coefficients are the centers of interval-coefficients of B 3 , that is c B3 = 1.5 1.5 2 3.5 then the eigenvalues of c B3 are 4.5 and 0.5 which are closed to the center of X 1 and X 3 . We call these eigenvalues, the central eigenvalues.
Definition 3 Let A be a matrix in gl(n, IR). Lat A c be the real matrix whose elements are the center of the intervals of A. We say that an eigenvalue of A is a central eigenvalue if its center is (close to) an eigenvalue of A c .
Remark. The determination of negative eigenvalues that is of type (0, K) is similar. Nevertheless we have to consider only matrices with positive entries thus we studies only the positive eigenvalues. The negative eigenvalues do not correspond to physical entities.
Now we will look the problem of reduction of an interval matrix. Recall that the characteristical polynomial is with coefficient in a non factorial ring. This is the bigest change with respect the classical real linear algebra.
Définition 2 Let A a square matrix with coeffcients in IR. If X is an eigenvalue of A, then every vector V ∈ IR n satisfying A t V = X t V is an eigenvector associated with X .
Let E X be the set
Then E X is a R-subspace of IR n where n is the order of the matrix A. It is also a IR submodule of IR n .
Proposition 1 Let X 1 and X 2 be two distinguish eigenvalues of A. Then E X1 ∩ E X2 = {0}.
Proof . Let V be in E X1 ∩ E X2 . We have
This X 1 V X 2 V = (X 1 X 2 )V = 0. As IR is without zero divisor, we have X 1 X 2 = 0 or V = 0. We deduce E X1 ∩ E X2 = {0}.
Proposition 2 Let C A (X ) be the characteristical polynomial of A. If the real polynomial C CA (X ) associated with the central matrix of A is a product of factor of degree 1, then C A (X ) admits a factorization on IR
We have seen that C A (X ) can be have more than degree(C A (X ) roots. If X 1 , · · · , X n are the central roots, we have the decomposition
Example. If we consider the matrix
then C B3 (X ) admits X 1 , · · · , X 6 as positive roots. The central eigenvalues are X 1 and X 3 and we have det(B 3 X I) = (X X 1 )(X X 3 ).
If we consider the roots
], 0), and if we assume that C B3s (X ) = (X X 2 )(X Y ), we obtain
which does not correspond to a positive eigenvalue.
(X X i ), and if for any i = 1, · · · , n the dimension of E Xi coincides with the multiplicity of X i , then we have the vectorial decomposition IR n = ⊕ i∈I E Xi where the roots X i , i ∈ I are pairwise distinguish.
Example. Let us compute the eigenspaces of B 3 associated to the central eigenvalues.
•
This gives
If we choose V 1 = ([1, 1], 0) we have 
We define the exponential map Exp : gl(n, IR) −→ gl(n, IR) in a classical way by series expansions. If the matrix A is diagonalizable, then 
considerable number of difficulties learners face in listening comprehension are discussed in literature (DonaldsonEvans, 1981; Underwood, 1989; Ur, 1984) . All these facts lead to the idea that to make students successful listeners, the teacher must support the learners according to their needs, goals, and situation in which they perform. To reach some optimal degree of comprehension, pre-task activities providing background knowledge, linguistically and nonlinguistically, have been demonstrated to be helpful in performing different activities (Richards, 1990; Rost, 1990) .
Morley (1991) has explained that in developing listening materials and activities, the following three important features of listening need to be taken into account: (1) Listening is an act of information processing which involves the listeners in various communicative modes; (2) Broadly speaking, real-world spoken communication serves two linguistic functions: an interactional and transactional functions, and (3) The cognitive processing of spoken language involves simultaneously activation of both top-down and bottom-up processes to construct the intended meaning. Richards (1983) has classified the various types of listening activities from different aspects. He has discussed that the material can be in the form of a monologue or a dialogue. They can be delivered by a native speaker or non-native speaker.
Ur (1984) believes that in order for benefiting from listening tasks, it is necessary to develop this skill in a direct and systematic way. To reach this goal, teaching listening has been suggested to include pre-task period. The period prior to act on listening task, pre-listening phase, is associated to preparation stage in which learners are provided by some activities as a kind of support to help them act on task. For Rost (2001) , listening tasks "involve explicit "pre-listening" steps, some activities that the learner does prior to listening to the main input in order to increase readiness" (p. 20). Chastain (1988) has argued that pre-listening activities can be considered as the most crucial aspects in listening process because other activities depend on the extent to which the teacher has been successful in activating students" background and directing them to reach the goals of activity. Underwood (1989) has listed pre-task activities as: discussion about the topic, looking at pictures, list of items, guiding questions, reading a text, predicting, making list of possibilities. So, the aim for providing pre-listening activities is to activate pre-existing knowledge embedded in learner"s mind. Widdowson (1983) has highlighted three sources a listener utilizes in the process of comprehension. He has referred to them as (a) systemic and linguistic knowledge (knowledge of phonological, syntactical, and semantic components of the language system) (b) contextual knowledge (knowledge of situation and co-text) and (c) schematic or background knowledge (factual, socio-cultural and procedural knowledge).
Prior knowledge in listener"s mind entails the contribution of schematic knowledge when performing on listening tasks. Edwards and McDonald (1993) hold that "schema theory details how people store and use knowledge about a domain" (p.60). Yule (2006) has maintained that "a schema is a general term for a conventional knowledge structure that exists in memory" (p. 132). Generally, schematic knowledge refers to the socio-cultural background knowledge. Edwards and McDonald (1993) maintain, "Schema theory suggests that knowledge level is a much more important predicator of listening than are other variables" (p. 72). Taylor and Crocker (1981) have noted:
A schema is a cognitive structure that consists in part of the representation of some defined stimulus domain. The schema contains general knowledge about that domain, including a specification of the relationships among its attributes, as well as specific examples or instances of the stimulus domain. (p. 91) Schmidt-Rinehart (1994) carried out a research to find out whether there was an interaction between topic familiarity and listening comprehension. The results revealed that all of the students in different levels outscored in listening task of familiar topic. Also, Chiang and Dunkle (1992) investigated the effect of speech modification, prior knowledge, and listening proficiency on EFL listening comprehension. The Chinese EFL students" listening comprehension was measured over listening to a lecture. The students were required to answer a multiple-choice test which contained both passage-dependent and passage-independent items. The results indicated that the students outperformed on familiartopic lecture than on unfamiliar-topic lecture.
Weissenreider (1987) tested intermediate and advanced learner"s comprehension of Spanish-language newscast over the role of textual and content schema. News casting materials were highly specialized texts, in economic register. The materials were supposed to be familiar in topic to students. Knowledge of the organizational structure of the materials (textual schema) and knowledge about the content of materials (content schema), helped comprehension of new data, especially when combined with some listening strategies such as identifying key semantic elements, hypothesizing associations, anticipating related issues.
In addition to the effect of background knowledge prior to listening test, Widdowson (1983) has stated that vocabulary provision can compensate for the lack of linguistic knowledge. Among difficulties numerated above, lack of vocabulary knowledge can be considered one of the most important one. Lexico-grammatical knowledge is considered to allow L2 learners to derive literal meaning of the message which facilitates listening (Mecartty, 2000) . Some authors believe that lack of vocabulary is one of the primary causes which exacerbated listening difficulties (Goh, 2000; Kelly, 1991; Rost, 1990) . Vandergrift (2003) indicated that "less-skilled listeners tended to segment what they heard on a word-by-word basis, using almost exclusively a bottom-up approach" (p. 467). Osada (2001) investigated low-proficient learners to see whether they preferred using bottom-down procedure or top-down procedure. The result of the study showed that EFL low-proficient learners tended to rely on bottom-up processing. Also, Vandergrift (2003) studied less-skilled and moreskilled learners" way of using different strategies. He concluded that less-skilled learners used word-by-word method of translating a text paying little attention to connection of ideas between the text segments. Therefore, the less-skilled learners acted mainly on bottom-up procedure.
Despite the fact that lack of vocabulary knowledge seems to cause the most worry of EFL learners, there are few studies regarding the effect of vocabulary preparation on listening comprehension (Chang, 2006; Chang & Read, 2008; Lin & Chui, 2009 ). Looking at the other works done in examining the effect of prior information on listening comprehension, we see somewhat different results. While the findings of the studies highlighted the role of prior information in listening tasks, there are other studies whose findings delimit the effectiveness of such information (Chang & Read, 2007; Jensen & Hansen, 1995) . Chang and Read (2007) investigated the effect of different types of supports on language learners. They found that the provision of written general information providing general information on content of listening texts in learners" native language increased their listening comprehension in a limited degree. Jensen and Hansen (1995) looked at whether prior study of a lecture topic enhanced performance on the lecture subtests of a content-based listening with underlying thought on the efficacy of prior knowledge on high proficient learners" listening comprehension.
To reach a solid conclusion, this study attempted to shed more light on supporting listening skill and as a result help learners reach an optimal degree of listening comprehension. Building on the previous studies, this study aimed at discovering the extent to which two pre-task activities of glossary of unknown vocabulary items and content related support assisted EFL language learners with their performance on listening comprehension questions across two levels of low and high proficiency.

RQ1: Are there any differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL low proficient learners" listening comprehension?
H01: There are no significant differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL low proficient learners, listening comprehension.
H1: There are significant differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL low proficient learners, listening comprehension.
RQ2: Are there any differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL high proficient learners" listening comprehension?
H02: There are no significant differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL high proficient learners" listening comprehension.
H2: There are significant differences in the effects of two pre-task activities, glossary of unknown vocabulary items and content related support, on improvement of EFL high proficient learners" listening comprehension.
A total of 120 language learners with age range of 15-25 participated in this study. 100 female and 20 male participants constituted this population. Before the onset of the study, the participants were divided into two different levels of high and low proficiency by a TOEFL actual test. This led to the formation of three groups, 20 participants in each class, totalling to 60 L2 learners in each level. The three classes were randomly assigned into two experimental groups and one control group. The experimental groups consisted of vocabulary and content supported groups.
Two language tests were used in the present study to measure language learners" proficiency level and listening comprehension. The first testing material was a TOEFL actual test administered in the past by ETS in 2004. This test was used to divide participants into two levels of proficiency. The second test materials were listening tests following 5 listening comprehension multiple-choice questions and a cloze test with 5 blank spaces, resulting 10 listening comprehension questions totally. The materials used for this study prior to taking the tests were chosen from among recorded lectures appropriate for the levels of participants. The pre-task activity of vocabulary was offered one session before conducting listening tests, whereas written content related support was given only 10 minutes before the test.
Before the onset of the study, the participants were divided into two different levels of high and low proficiency by a TOEFL test. There remained 60 participants in each level making three classes, 20 participants in each class. The three classes were randomly assigned into two experimental groups and one control group. A pre-test of listening comprehension was conducted to guarantee the homogeneity of participants in their listening skill and measure their listening proficiency. One experimental group in each level was given a glossary of unknown vocabulary items with the pronunciations as a type of pre-task activity. To make them familiar with the pronunciations and relate the vocabularies to listening text, the glossary was given one session before taking the listening test. Similarly, the other group received written information about the content of forthcoming listening piece. This type of pre-task activity was given just 10 minutes before taking the tests. This pre-task activity was aimed to activate the listeners' pre-existing knowledge and offer a general view about the forthcoming data. It was taken care not to give detailed information in these summaries.
At the beginning, the participants were required to listen once before receiving the questions. The second time along with listening to the task, they were required to answer the questions. At the end, Due to their exposure two times to the listening task, they were asked to fill in the blanks in cloze test without listening to the lecture. To prevent the learners from being aware of the listening text, the students were required to answer the multiple questions at the beginning and then to answer the cloze test.

To represent comprehensive information about the quantitative analysis of obtained data, the means and standard deviations for the post-test of low proficient learners in each of the three groups are shown in Table 1 . The results indicated that there are significant differences between the three groups" post-test mean scores after providing the pretask activities. Especially, the difference between the mean scores of vocabulary group and the other groups, content and control, is higher than between content and control groups. In order to find out whether there are statistically significant differences in the effects of pre-task activities on the learners" performance in three groups, the post-test scores were submitted to a one-way ANOVA analysis with between-group factor. The results (p=.029, α=0.05, p<α) illustrated that the difference between the performance of three groups is statistically significant. In other words, the pre-task activities, especially vocabulary items, had a supportive role on LP learners" listening comprehension. So, the first null hypothesis is rejected and the alternative hypothesis is confirmed. To illuminate where the significant differences exist among the groups, Tukey"s post hoc test (with an alpha level of .05) was conducted. The results revealed that only vocabulary group outperformed the other two groups. It can be concluded that only the pre-task of vocabulary had a significant and meaningful effect on LP participants" listening comprehension. The results are shown in Table 2 . 
The mean scores and standard deviations for post-test of high proficient learners in each of the three groups are represented in Table 3 . The results indicated that there are significant differences between the performances of three groups. This means that post-test mean scores of content and vocabulary groups are higher than control group. It shows that provision of pre-task activities enhanced HP learners" listening comprehension in answering post-lecture questions. Especially, the group provided by content related support outscored in listening post-test. Similar to procedure of finding out the significant difference among the three groups for low proficiency level, a oneway ANOVA analysis with between-group factor was conducted for high proficient level. The results indicated that the computed p value (.000) is less than the level of significance set in this analysis (p=0.000, α=0.05, p<α). Therefore, this difference is statistically significant, and the second null hypothesis is also rejected and consequently the second alternative hypothesis is accepted. This means that offered pre-task activities for listening tasks improved HP learners" listening performance. To find out exactly which group is supported better by the pre-task activities, Tukey"s post hoc test (with an alpha level of .05) was computed. The results showed that both content and vocabulary groups performed better than control group. Despite the effects both pre-task activities had on this level, content related support outperformed vocabulary group. Table 4 illustrates the differences in performances among the groups of HP level. As it is clear from the performance of groups in two levels, it can be concluded that pre-task activities had different supportive roles on low and high proficient learners" performance. In other words, low proficient learners benefited from glossary of unknown vocabulary items in answering post-lecture listening con comprehension questions, whereas high proficient learners" used content related support to answer the listening tests. So, the lack of similarities between two proficiency levels in terms of the effect pre-task activities had on learners" answering post-lecture listening comprehension questions, justifies existence of differences in the effects of pre-task activities on two proficiency levels. This means, comparing the roles of pre-tasks brought out differences in the performance enhancement in two levels. The conclusions are shown in Figure 1 below. The first research question addressed the effects of two pre-task activities on low proficiency level. The results obtained by applying the two pre-task activities indicated that only vocabulary provision enhanced LP learners" listening performance. Related to factors influential in listening comprehension, lack of enough vocabulary knowledge is considered one of the most important ones (Chang, 2005; Chang & Read, 2006) . This has also been pointed out by Ur (1984) who has maintained that failure in relating the linguistic knowledge (vocabularies) to the context and failure in using strategies to summarize the text in macro-level and micro-level can be an important factor in listening comprehension. Also, Boyle (1984) identifies lexical and syntactical knowledge as the most crucial factors contributed to listening process.
According to what is mentioned about the effective role of vocabulary support in bottom-up processing, the finding of this study is in line with studies supporting the low proficient learners" vocabulary use (Lin & Chui, 2009; Osada, 2001; Tsui & Fullilove, 1998; Vandergrift, 2003) . Additionally, extra preparation time offered to the vocabulary group might be another factor affecting listeners" listening comprehension. Time factor had a crucial role in helping the learner process and internalize the lexical items during the offered time. The findings related to high proficient learners are discussed answering the next research question.
The second research questions concerned the effects of so-called pre-task activities on high proficiency level. The results of offering pre-task activities indicated that content related support enhanced the learners" listening comprehension more than vocabulary items. The factor influential in helping HP learners" use of content related support with background knowledge might be related to their use of specific source activation. In other words, high proficient learners could benefit from the provided content to infer meaning and guess what was in the forthcoming data. In relation to listener"s use of schematic knowledge in listening comprehension, conclusion the researches have reached confirms the crucial role it has on listening comprehension (Long, 1989; Markham & Latham, 1987; Mueller, 1980; Schmidt-Rinehart, 1994; Weissenreider, 1987) . Additionally, The finding of this study is sharp contrast with Jensen and Hansen (1995) who concluded that prior knowledge does not support effectively high proficient learners" listening comprehension. All these findings, confirm the significant effect of background knowledge on learners" listening comprehension performance. Here, the results of the study are in line with these findings. The findings of this study justified the existence of differences in the performances of learners in two proficiency levels. The glossary of unknown vocabularies in LP level and content related support in HP level, improved learners" listening comprehension. Therefore, differences have been found in supportive roles of pre-task activities across two different proficiency levels.
Language teachers and syllabus writers are supposed to incorporate a range of pre-listening activities and change the weight of listening lessons from testing listening into teaching listening so that they could support language learners to enhance their listening performance. Listening skill being one of the problematic areas of learning has been focal centre of attention for some researchers recently. To improve this skill the existence of pre-task activities has been emphasized. The most important implication of the current study for the language classes has to do with the type of pre-task activities employed in classrooms to support learners" performance.
The traffic scene based on virtual reality can imitate the real scene realistically, and the parameters in the scene can be modified repeatedly, which greatly saves the project cost, and is helpful to the study of driver dynamic vision [1] , as well as modify the parameters of the scene repeatedly. Through the pilot visual search behavior model of experimental research, Ma Yong studied the font and speed of the driver's visual characteristics and other factors [2] . Lin studied the variation of visible distance of traffic signs under adverse light conditions [3] . Masamarty of Israel studied the visual search of drivers based on digitally covered images of traffic scenes [4] . Jane Reverse-Halbrook of Canada studied the audiovisual load of drivers in different traffic environments and the effects of drivers' eyeball movement characteristics on different driving loads [5] . Many studies on visual recognition are only focused on a single factor such as road markings. They tend to focus on the driver's visual search, cognitive load, visual perception behavior and other aspects. And the research on visual recognition is mainly focused on the following aspects: visual search, cognitive load, visual perception behavior and so on. In the research of visual recognition, it is rarely involved in the analysis and weight determination of the various factors in the entire virtual scene, and there are few comprehensive evaluation models used to judge the visual recognition of virtual traffic. The above comprehensive factor analysis and evaluation model are very important for the study of visual identity.
In this paper, on the basis of setting up the virtual scene, the author modifies the parameters of the virtual scene and generates different scenes for the experimenter to evaluate. Finally, through the experiment, the author obtain the optimal scene and establish a comprehensive traffic visual perception evaluation model to verify the accuracy of the test.
The legibility of traffic scenes is generally used to describe the driver's perception of the scene information. There are two kinds of influencing factors on visual perception: subjective factors of drivers and objective factors of traffic scenes. In the aspect of the driver, the writer mainly analyzes the attention theory of the driver in the process of driving [6] , and studies the driver's attention characteristics from the point of view of the driver's attention traits. The main factors influencing drivers' attention are: age, driving age, personality, driving habits, mood, physical condition, fatigue and so on. In the aspect of traffic scene, the establishment of virtual traffic scene is realized by using corresponding software. The main influencing factors of visual fidelity of virtual traffic scene are road and line marking, signboard, signal lamp monitoring, building, pedestrian and bicycle, vehicle, greening, sky, color, shape, size, style, complex process, texture, lighting, shading effect, proportion, visual distance, LED level of detail, texture processing and so on.
To construct the index system of drivers and traffic visual perception factors, we should consider both driver factors and virtual visual factors. According to the principles of systematic, hierarchy and maneuverability, the index set of visual perception should be determined. After that, the ambiguous and repetitive indicators are deleted to retain the professional indicators generally accepted by the industry, so as to obtain a complete, high-quality and accurate visual recognition indicators. Finally, according to the expert experience and fuzzy comprehensive evaluation to determine the weight of the index, the final evaluation index system [7] . The indicator system of visual recognition is as follows: 
The comprehensive evaluation is mainly based on the following model [8] : R=E × P. R is the result vector of comprehensive evaluation, P is the weight vector of each index, E is the evaluation matrix of each index.

If it is a consistent matrix, a transitive matrix, and a transitive matrix. 
Then C is the optimal transfer matrix.
It will calculate the weight vector. This method effectively overcomes the shortcomings that the judgment matrix in the traditional AHP is modified repeatedly and the consistency is not easy to be satisfied, and improves the calculation efficiency.
Hypothesis: there are n indicators, m experts, experts assess the importance of indicators 1 2 , ,..., n B B B .
The resulting matrix is n m × .
Using the "minimum variance method" that is to say, to assume an optimal weight 
The relative proportion of the weight error of the first individual at the time of the assessment, the smaller the better, and therefore:
The weight of the composite index can be determined by 
is the correlation coefficient between the k th index and the k optimal index in the m th questionnaire.
The first step is to construct the judgment matrix D.
is the optimal value for the k-th indicator;
( 1, 2,..., )
is the original value of the k-th indicator of the i-th questionnaire.
The second step: the dimensionless process of "extreme difference" is carried out, and the matrix is transformed from D to C. Through R E P = × , Calculate the size of each party's case value R, and draw out the order of the advantages and disadvantages of each scheme [8] .

Based on the research of drivers' traffic visual attention, it is used to analyze the influence of traffic characteristics or indexes on drivers' visual perception with a set of virtual traffic scenes constructed by 3DS Max software. The main work of this paper is as follows: using 3DS Max software to construct a group of virtual traffic scenes to explore the influence weight of visual perception of drivers in virtual scene design; finding out the satisfaction degree of drivers to each factor of traffic visual recognition to explore which factors have significant influence on visual recognition; and modifying unsatisfactory factors. The specific optimal index value of each traffic feature is determined to make the visual recognition optimal [9] .
In order to study the influence of virtual scene recognition on drivers, a section of road in Zibo was selected, and 3DS Max was used to complete the modeling of virtual scene, and then the model was introduced into the driving simulator [10] . 
A total of 100 people, including 20 bus company professional drivers, 20 private car drivers, the other people are school staff and students.
The driver respectively numbered 1-100. After the virtual scene was introduced into the analog driver, according to the same set of traffic scenes, each driver was arranged to carry out experiments in turn to drive the vehicle along the road to the destination in the virtual scene. And fill out the questionnaire after driving.
According to the questionnaire, the relevant traffic characteristic parameters were modified and the traffic scenes were reconstructed. After 30 minutes, two or three experiments were carried out to repeat the previous process, and the optimal traffic scenes were constructed according to the driver's evaluation [11] . Finally, the 1-100 pilot experimental table is sorted out as the basis for the analysis of the later experimental data [12] .
By establishing the evaluation model of traffic visual perception, the optimal value of the evaluation index is determined, and the dimensionless treatment and the weight are determined to evaluate each index comprehensively [13] , and then the results of the three experimental questionnaires are input into the model. Finally, the evaluation results are analyzed [14] . The results of the weights for each indicator are as follows: The correlation coefficients for the indicators are as follows: So we get the judgment matrix:
0.0000 0.0000 0.3333 3 0.3333 0.4444 1.0000 ( . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , . , )
To sum up, according to the correlation degree in R, we can draw the conclusion that the second modification scheme is more satisfactory than the original data scheme and the first modification scheme in the above three schemes [15] .
Based on the subjective situation of driver's own factors and the objective situation of setting up relevant indexes of constructing scene, an index system that affects the evaluation of driver's virtual scene recognition is established, which is based on the analysis of the objective situation of the setting of relevant indexes of constructing scene.
The weight and influence of each factor of the visual recognition optimal scene are analyzed in the experiment. Single factor analysis: the traffic visual perception has a significant impact on the indicators which are: roads and signs, greening, motor vehicles, stickers and textures, driving age. Factor analysis results will be basically classified into eight categories of indicators, and AHP to determine the weight of a small amount of difference, in the accepted range. After the experiment is modified, the correlation analysis is done, and the mutual influence of each index with the modification is analyzed.
Control theory of networks and multiagent systems has gained enormous popularity in the last years, because it has numerous important applications (Olfati-Saber and Murray, 2004; Beard et al., 2006; Scardovi and Sepulchre, 2009) , and many unsolved mathematical questions. The dynamics of such networks are governed by the underlying topology (given by the edges of a graph) with predefined coupling rules and the dynamics of each agent at the nodes of the graph.
One important question in this context is, whether a topological change in the form of a removal or addition of an edge always has an effect on the dynamics. If not, this may lead to severe problems in applications because faults or attacks may stay unnoticed until much later, leading to performance losses or even to instability phenomena.
The problem of detecting topology variations in dynamical networks has therefore gained much attention in the last years (Rahimian et al., 2012; Rahimian and Preciado, 2014; Torres et al., 2015; Tesi, 2015, 2016) . In all the aforementioned works, however, the analysis is confined to networks whose dynamics can be described via ordinary differential equations (ODEs). On the other hand, there are no results dealing with networks of differential-algebraic equations (DAEs). Networks of DAEs arise in several applications of practical interest, examples being electrical and water distribution networks, where the algebraic equations describe laws of conservation of mass, energy and current.
In this note, inspired by the results in Tesi (2015, 2016) , we consider networks of DAEs with This work was supported by DFG-grant TR 1223/2-1 1 This work was carried out while the second author was at the University of Kaiserslautern.
diffusive coupling, and study under what conditions topological changes (a removal or addition of an edge) cannot be inferred from observations of the network dynamics, referring to this phenomenon as "indiscernibility". We provide necessary and sufficient conditions for indiscernibility that can be checked by only looking at the eigenvalues/eigenvectors of the nominal network configuration. This property is extremely appealing since it avoids the need to determine eigenvalues/eigenvectors of the various "faulty" topologies. Interestingly, the proposed analysis is general enough to include the case where each network node obeys different dynamics (and has possibly different state dimension).
The results presented here consider discernibility based on the whole state trajectory. This is just a first step, because once a topological change results in a change of the dynamics, the next question is, whether this change can actually be seen by a limited amount of sensors in the network. This problem has been widely studied in the general framework of switched systems; however, these results do not take into account the special structure of topological changes and it is a topic of future research to consider discernibility also for networks with a limited amount of measurements.
One should mention that the problem of detecting topological changes can also be cast as a topology identification problem; for example, see Materassi and Innocenti (2010) ; Sanandaji et al. (2011); Chowdhary et al. (2011) . In fact, detection and identification are certainly closely connected problems. However, identification does not assume prior knowledge of the nominal network configuration. This information is crucial in order to relate nominal and modified network configurations, and to provide conditions on discernibility that can be checked by only looking at the properties of the nominal configuration. This note is structured as follows: We first introduce a nominal network of DAEs and the resulting model of the overall dynamics (which by itself is a large DAE) together with the possible topological changes. Afterwards we formally define discernibility and state two main results highlighting an important connection between discernibility and certain eigenvalue-eigenvector pairs (Theorem 2) which leads to a very simple characterization for discernibility (Theorem 5) that can be checked solely in terms of the nominal network configuration. Afterwards, we consider the special case where all agents have the same dynamics, and we show that discernibility only depends on the graph topology (Theorem 8). In Section 5 we apply our results to a power grid model. Finally, we relate our results with mode-detection notions for switched DAEs in Section 6.
Due to space limitations, the proofs are omitted.
For a finite index set V = {1, 2, . . . , N }, N ∈ N, we consider a family of differential algebraic equations (DAEs),
where
These DAEs are connected via a network given by an undirected graph G = (V, E) with E ⊆ V × V and via diffusive coupling of the form
where w ij > 0 with w ji = w ij for i, j ∈ V. Let x = col{x 1 , . . . , x N } represent the overall state of the network. The collective dynamics can be written in compact form as
Note that L is the weighted Laplacian of G, in particular, L is symmetric and positive semidefinite. The simplest case for (3) is the case n i = 1,
This case was studied in (Battistelli and Tesi, 2015) and our goal is to expand the results therein to the more general case above. An interesting example for the general structure (3) is a model of a power grid, see Section 5 for details.
Our goal is to study the effect of topological variations, i.e., the removal (or addition) of an interconnection of the network. In terms of the (weighted) Laplacian the removal of an edge (i, j) ∈ E can be written as a rank-one change as follows:
where e i ∈ R n is the i-th unit vector. Connecting two previously unconnected nodes leads to a similar rank-one change: L = L + w ij (e i − e j )(e i − e j ) . We are now interested in determining the existence of nontrivial initial values such that the solution x of (3) is identical to a solution x of
If this is the case, we are not able to detect the topological change (e.g., induced by a fault or by an attack) even if we have full knowledge of the state of the system. Only if we can give a negative answer to the above question, it makes sense to further study the mode detection problem where we only have a subset of the state available for measurement (which is not considered in this note and is a topic of future research).

A topological change of (3) resulting in (6) is called possiblyindiscernible iff it has at least one indiscernible initial state x 0 ∈ R n \ {0}, i.e., there exists a solution x of (3) with x(0) = x 0 which is also a solution of (6). Otherwise, the topological change is called discernible.
2
Note that one could also introduce the stronger notion of indiscernible topological changes (and its weaker opposite: possibly-discernible), which means that the topological change cannot be detected no matter what the initial state is (all initial states are indiscernible); however this is not such a useful definition in the context considered here.
We first relate indiscernible initial states with common eigenvalue-eigenvector pairs of (E, A L ) and (E, A L ).
Theorem 2. Consider (3) and (6) and assume that (E, A L ) and (E, A L ) are regular matrix pairs, i.e., det(sE − A L ) and det(sE − A L ) are nonzero polynomials in s. Then there exists a nontrivial indiscernible initial state if, and only if, there exists a common eigenvalue-eigenvector pair
In particular, any common eigenvalue-eigenvector pair (λ, v) leads to an indiscernible initial state given by the real part of v. Remark 3. (Regularity of (3)). The restriction to regular DAEs (3) is needed to have existence and uniqueness of solutions and is also needed to have a well-defined notion of eigenvalues and eigenvectors. Note however, that we do not assume that each agent has dynamics defined by a regular DAE; in fact our two forthcoming examples show that it may be a common situation to have non-regular DAEs for the node dynamics. On the other hand, coupling regular DAEs does not necessarily lead to regularity of the overall DAE system. Altogether, the problem of characterizing regularity of (3) is extremely important; however, we are not aware of simple conditions which guarantee regularity of the coupled system. 2
We now highlight a simple characterization for the existence of a common eigenvalue-eigenvector pair.
This simple observation now leads to our main result.
Theorem 5. Consider a family of DAEs of the form (1) connected by a network graph G = (V, E) with weighted Laplacian L resulting in the overall system (3), which we assume to be regular. Any regularity-preserving removal/addition of the edge (i, j) is a possibly-indiscernible topological change if, and only if, either b i = 0 and
Note that indiscernibility can be solely checked in terms of the nominal system parameters (E, A L ), in particular, it is not necessary to calculate eigenvalues/eigenvectors of the various "faulty" topologies. Another interesting implication of Theorem 5 is that there is a vast literature devoted to the computation of the eigenspace of Laplaciandependent matrices (for example, the eigenspace of a Laplacian matrix is known analytically for many fundamental graphs). These results can be used to effectively assess the conditions dictated by Theorem 5 (cf. Section 4).
We now illustrate the result by a variant of the well known Wheatstone bridge.
Example 6. Consider an electrical RC circuit as shown in Figure 1 . Here vertices 1 and 2 are dynamic vertices whereas vertices 3 and 4 lead to algebraic equations. In the framework of (1) we have Node 1 :
Altogether, we get a system given by (3) with
. Then, (3) reduces to Ev(t) = Lv(t).
(7) Note that although the DAEs of node 3 and 4 are not regular the coupled system (7) is regular for all positive capacitor and resistor values. Assume now that all circuit parameters are equally one, then the matrix pair (E, L) has the two finite eigenvalues λ 1 = 0 and λ 2 = −2 with corresponding eigenvectors
Since there exists an eigenvector and a pair (i, j) such that the i-th and j-th entries are equal, the electrical circuits has initial values where a removal of an edge could not be detected. In fact, since all eigenvectors corresponding to the finite eigenvalues have matching 3rd and 4th entries, the removal of edge (3, 4) is undetectable for any consistent initial value.
In the homogeneous case, where all coupled systems are identical, i.e., E i = E, A i = A, b i = b, c i = c and n i = n for all i ∈ V, the overall dynamics can be written in compact form as
where
We will show that in this special case indiscernibility is in fact independent of the system parameters and only depends on the Laplacian of the connection graph, provided a certain observability condition holds.
We first highlight the properties of eigenvalue-eigenvectors pairs of (E, A L ) in the homogeneous case.
Lemma 7. Let α 1 , α 2 , . . . , α N ∈ R be the N real eigenvalues (counting multiples) of the symmetric Laplacian L. Then
Furthermore, let v be a (generalized) eigenvector of (E, A L ) for an eigenvalue λ of (E, A L ) with λ ∈ spec(E, A− αbc) for some eigenvalue α of L. Then v = z ⊗ w (10) where z is an eigenvector of L corresponding to α and w is a (generalized) eigenvector of (E, A − αbc) corresponding to λ. Moreover, if v and w are generalized eigenvectors, then they have the same rank.
Using these results, we can obtain a more specific conditions on the existence of non-null indiscernible states. Specifically, we have the following result.
Theorem 8. Consider a family of identical DAEs (1) of the form Eẋ = Ax + bu y = cx connected via the diffusive coupling (2) by a network with weighted Laplacian L resulting in the overall system (8), which we assume to be regular. Suppose furthermore that b = 0 and that (E, A, c) is observable in the behavioral sense, i.e, rank [ λE−A c ] = n for all λ ∈ C, see e.g. Berger et al. (2017) . Then, any regularity-preserving removal/addition of the edge (i, j) is a possibly-indiscernible topological change if, and only if, there exists an eigenvector z ∈ C n \ {0} of L such that z i = z j . In that case any such eigenvector leads to an indiscernible initial state v = z ⊗ w as in (10).
The Laplacian matrix L always has eigenvalue zero with corresponding eigenvector z = (1, 1, . . . , 1) . Consequently, any topological change of a homogeneous network is possibly-indiscernible. This special eigenvector corresponds to the situation where all subsystems start with the same initial value; as a consequence, the diffusive coupling is zero and a topological variation has no effect on the dynamics. However, Theorem 8 also shows which initial values may lead to indiscernibility and it is also reasonable to expect that eigenvectors with z i ≈ z j give rise to initial values where it is theoretically possible but difficult in practice to detect topological changes.
Furthermore, we would like to stress that b = 0 is an obvious assumption (otherwise there is no coupling) and that the observability assumption on (E, A, c) is quite natural, as otherwise there would be dynamics which would not effect the output of the individual systems and therefore are also not effected by the coupling structure.
We will show now how power grids can be modelled such that they fit into our framework. In particular, we are able to determine whether there are critical removal/additions of power lines which may go undetected for certain initial states even if we would have knowledge of the whole state of the network. This may have important application in the context of the security/vulnerability of cyberphysical systems, for example, we are able to answer the question whether an attacker can change the network topology without the grid operators being able to detect that change. Another application is determining the right moment when to disconnect a power line (e.g., for maintenance) such that the effect on the remaining network is negligible.
We use a power grid model described in (Groß et al., 2016) , which consists of p generators and q load buses. The i-th generator is modeled aṡ 
are the mass, damping and stiffness matrices, respectively. Let θ i (t) ∈ R be the voltage angle at the generator bus, then the electrical power acting on the generator, can be approximatly expressed as:
where z i > 0 is the transient reactance of the generator. Finally, the linearized power flow equations are
at the generator buses i = 1, . . . , p and
at the load buses i = p + 1, . . . , p + q, where b ij = b ji ≥ 0 is the susceptance between bus i and j and p i (t) is the external load at each bus. Assuming constant generator power and constant loads, the generator and bus models can be brought into the structure (1) via
for the generator buses i = 1, . . . , p and x i = (p i , θ i ) ,
for the load buses i = p + 1, . . . , p + q; the coupling equations for all buses are
where b ij = b ji ≥ 0 is the susceptance between buses i and j. In (Groß et al., 2016) it was shown that the overall DAE is regular if, and only if, each connected component of the network graph contains at least one generator bus, hence we can apply Theorem 5 to check whether there are possibly-indiscernible topological changes. Note that similar to the Wheatstone bridge example the DAEs corresponding the load buses are not regular.
Example 9. Consider a power grid system as in Section 5 with one generator which is connected to bus 1. Define Fig. 2 . Power grid of example 10
Then v is an eigenvector to the eigenvalue zero of the corresponding network DAE. It satisfies Cv = (1 1 · · · 1) , hence the system is possibly indiscernible by Theorem 5. This means it can happen in a power grid that there is an equilibrium with no flow on a particular line and thus switching of this line cannot be detected. However, as long as there is a power flow on a line, a switch of this line will affect the solution and thus be detected. 2
Example 9 is artificial in the sense that the power produced at the generator is consumed at the same node, hence no power is distributed trough the network. The following example shows that indiscernible topological changes can happen even when there is a power flow in the network.
Example 10. Consider the simple power network consisting of one generator and two loads, connected as shown in Figure 2 .
Then it is easily seen that there is an eigenvector v corresponding to the eigenvalue zero such the (Cv) 2 = (Cv) 3 , i.e., there are initial values for which a disconnection of buses 2 and 3 remains undetected. These initial values correspond to the situation where each load node gets exactly the amount of power from the generator that is consumed there, i.e., when there is no power flow between buses 2 and 3. 2
Example 10 points towards an obvious principle in these power grid models: A removal of a power line is undetectable if there is no power flow on this line. However, the latter situation is not an obvious necessary or sufficient condition in terms of the formalism in Theorem 5. Clarifying the connection between these conditions is a topic of future research.
The problem of detecting topological changes in dynamical networks can be seen as a special case of the modedetection problem in switched systems. It was recently highlighted in that the ability to deduce the active mode from the measured output of the system (σ-observability) in all situations reduces to the ability to distinguish the outputs from systems with constant switching signals. In particular, the effect of the actual switching event (here the topological change) is not taken into account. The weaker notion of switching time observability (t S -observability) is concerned with the problem of detecting the time of a switch and is actually exactly the problem we studied here (see the forthcoming Lemma 12). If a switch occurs it is also of interest from which mode to which mode the system switched (e.g., which power line was disconnected if a topological change is detected). This mode-detection problem for nonconstant switching signals (σ 1 -observability) is in general indeed weaker than σ-observability and stronger than t Sobservability.
In this section we want to investigate these three observability notions for the specially structured case considered here. It turns out (see the forthcoming Theorem 13) that all notion are in fact equivalent if the DAE-models have index one (i.e., switches do not produce Dirac impulses and the input is not differentiated).
Towards this end we first introduce the formal definitions of σ-, σ 1 -, and t S -observability of a switched DAE of the form E σẋ = A σ x (11) where σ : R → {0, 1, . . . , K}, K ∈ N, is a piecewiseconstant right-continuous switching signal, which is constant on (−∞, t 0 ) for some t 0 > 0. Note that a switched DAE may exhibit jumps and Dirac impulses in its solution, hence we have to consider the space of piecewise-smooth distributions (Trenn, 2009a (Trenn, , 2012 as the underlying solution space; in particular, x in (11) can only be evaluated at t ∈ R as left-/right-limit denoted by x(t − )/x(t + ) or as the impulsive part, denoted by x[t], see Trenn (2009b) for details. If each matrix pair (E k , A k ), k ∈ {0, 1, . . . , K} is regular, then the switched DAE (11) is uniquely solvable for any (possibly inconsistent) initial condition x(0 − ) = x 0 ∈ R n and we will denote this solution by x (x0,σ) . Note that we assume the initial condition x 0 to be consistent with the initial mode σ(0).
When studying discernibility or mode-detection for (11) it is necessary to exclude the zero initial state, because for an identically zero state the trivial dynamics are unaffected by the current value of the switching signal. While for switched ODEs (where E k = I for all k) it is sufficient to exclude x 0 = 0, the situation is a bit more complicated in the switched DAE case, as it is possible that the state jumps to zero later on. Therefore, we restrict our attention to the following intervals
If the state jumps to zero at time t S , we have s (x0,σ) = (−∞, t S ) if there is no impulse at the switch and else s (x0,σ) = (−∞, t S ]. The relevant switching times are then denoted by T (x0,σ) := { t | t is a discontinuity of σ } ∩ s (x0,σ) . We are now ready to formally define σ-, σ 1 -, and t Sobservability. Definition 11. The regular switched DAE (11) is called
• σ-observable :⇔ for all σ,σ and all x 0 ∈ R n it holds
where J := s (x0,σ) ∪ s (x0,σ) ; • σ 1 -observable :⇔ (12) holds for all x 0 ∈ R n and for all σ,σ with 1 ≤ min T (x0,σ) , T (x0,σ) ;
From these definitions, we can directly conclude σ-obs. ⇒ σ 1 -obs. ⇒ t S -obs.; (13) the converse is not true in general, cf. for the switched ODE case and for the switched DAE case.
We now relate t S -observability of the switched DAE (11) with possible-discernibility of each mode pair, where possible-discernibility of two modes (A i , E i ) and (A j , E j ) is defined analogously as in Definition 1. Lemma 12. Consider the switched DAE (11). It is t Sobservable if, and only if, the modes k ∈ {0, . . . , K} are pairwise possibly-discernible.
As the conditions for σ-and σ 1 -observability in are rather technical, we restrict the attention here to DAEs having index one, i.e. the nilpotent matrix in the quasi-Weierstrass form (see e.g. Berger et al. (2012) ) of (E, A) is zero or, equivalently, rank E = deg det(sE −A). Note that the power grid model considered here has this property. Theorem 13. Consider the switched DAE (11) and assume that each mode of the system (11) has index one. Then t S -, σ 1 -and σ-observability are all equivalent and coincide with pairwise possible-discernibility of the modes.
We will now embed the problem of detecting topological changes in a network of DAEs as discussed above in the framework of switched DAEs. Therefore consider a family of (possibly non-regular) DAEs (1) connected by a graph G = (V, E) with Laplacian L and corresponding matrix pair (E 0 , A 0 ) given by E 0 = E and A 0 = A − BLC (14) as in (3). Let L 1 , L 2 , . . ., L K , K ∈ N, denote the Laplacian matrices resulting from all relevant (in particular, regularity preserving) removal/addition of each of the edges (i 1 , j 1 ), . . . , (i K , j K ) in E and E k = E and A k = A − BL k C.
(15) Now we are interested in switching signals σ with σ(t) = 0 for t ≤ 0 and with at most one switch. In this case, t Sobservability is equivalent to mode 0 being discernible from any other mode k ∈ {1, . . . , K}. The condition for switching signal observability remains unaffected.
pose an adaptive sparsity-based DPD (ASDPD) algorithm to dynamically adjust both the overcomplete basis and the sparse solution so that the solution can better match the actual scenario. The method first performs supervised offline dictionary training by using the quadratic programming approach. During the online stage, the dictionary is continuously updated in an incremental fashion to adapt to time-varying factors. The notation used in this paper is according to the convention. Symbols for matrices (upper case) and vectors (lower case) are in boldface. The remainder of the paper is organized as follows. Section 2 briefly describes the system model assumed throughout this paper and formulates as a sparse recovery problem. In Section 3, we introduce a scheme calibrating the overcomplete basis dynamically and estimating the sparse solution adaptively. Simulation results are given in Section 4. Finally, Section 5 concludes the paper.
Consider N base stations (BS) intercepting the narrowband signals transmitted by L possible sources. Each BS which knows its coordinates is equipped with an antenna array consisting of M elements. Denote the lth unknown target position by the vector of coordinates l P . We use the far-field point-target model, which is commonly used for source localization due to its simplicity [3, 4, 9] . Based on this model, the received signal observed by the nth BS is given by
where ( ) l s t is the signal waveform considered known. ( ) n l a p is the array response at the nth BS from a signal transmitted position, and the propagation delay from the lth transmitter to the nth BS is given by ( )
represents noise terms, which is assumed as the independent and identically distributed (i.i.d.) complex Gaussian process, uncorrelated with the signals.
We divide the area of interest into K grids. In general,
. Then, we formulate the location problem as a following CS problem
where
is an overcomplete basis matrix at the nth BS, and 
, we can obtain
Note that H is known under the ideal channel condition, which means that we can estimate the actual coordinates of targets as long as we find the positions of nonzero values in θ . That is, the problem of localization is converted into one of sparse signal recovery from (3). Moreover, the number of these dominant nonzero values gives L.
However, the non-ideal factors are inevitable in a practical localization system. These factors include the channel attenuation, phase error, time-varying fluctuations of the radio channel and so forth. When these happen, the predefined dictionary cannot effectively express the actual signal, which will cause performance degradation in sparse recovery process.
For avoiding the difficulty of estimate all kinds of the time-varying factors, we assume the error dictionary matrix Γ which describe the difference between the predefined dictionary and the practical received signals. Note that the error matrix Γ is time-varying and cannot be known in advance. In this scenario, the sparse positioning model is correspondingly modified as:
where = D ΓH denotes the actual overcomplete basis with the time-varying interference. To prevent D from having arbitrarily large values (which would lead to arbitrarily small values of θ ), it is common to constrain its columns 1 
to have a 2 l norm less than or equal to one. Obviously, the mismatch exists between the columns of D and the corresponding columns of the predefined basis H , and thus the performance degradation is inevitable in the sparse recovery process. Focused on this problem, an adaptive sparse recovery algorithm is proposed in this paper, which dynamically calibrate the overcomplete basis so that the sparse solution can better fit the actual scenario.
tive adjustment of the overcomplete basis. This process generally learns the uncertainty of the dictionary, which is not available from the prior knowledge, but rather has to be estimated using a given set of training samples. Several different DL algorithms have been presented recently [10] . However, these methods generally cannot effectively handle very large training sets or dynamic training data changing over time. To overcome these shortcomings, we propose a two-stage DL approach that can adapt to the varied upcoming samples.
So far, the most DL methods are generally based on alternating minimization. In one step, a sparse recovery algorithm finds sparse representations of the training samples with a fixed dictionary. In the other step, the dictionary is updated to decrease the average approximation error while the sparse coefficients remain fixed. The proposed method in this paper also uses this formulation of alternating minimization.
The above problem of noisy sparse signal recovery can then be converted into a following optimization problem 
where λ is the regularization parameter. However, it should be emphasized that larger coefficients in θ are penalized more heavily in the 1 l norm than smaller coefficients, unlike the more democratic penalization of the 0 l norm [11] . In practice, large coefficients are usually the entries corresponding to the actual positions of targets, while small coefficients commonly represent the noise entries. The imbalance of the 1 l norm penalty will seriously influence the recovery accuracy, which may result in many false targets. Therefore, in this paper we choose the reweighted 1 l norm minimization algorithm in [11] as our sparse recovery method, which can overcome the mismatch between 0 l norm minimization and 1 l norm minimization while keeping the problem solvable with convex estimation tools.
In this paper, we propose a two-stage DL framework in which the offline DL method allows to train the dictionary in a supervised manner to integrate the large training sets, and the incremental DL method based on the results in the offline stage handles the unseen online variation to enhance its adaptability. 1) Offline dictionary learning In this stage, the ideal overcomplete basis H is optimized to better represent the data of the training sets. Since the sparse coefficients θ are fixed in the DL stage, the resulting optimization problem becomes: 
Let's introduce several new expressions for clarity of notation
Omitting the terms that do not depend on D, the objective function in (6) can be equivalent to
Note that (8) is a standard form of constrained quadratic programming problem which can be solved by any standard optimization method, such as the gradient projection algorithm in [12] . Moreover, the matrix G is obviously a positively definite matrix, and thus (8) is convex function and can be guaranteed to find a global optimum [13] in this DL phase.
2) Online dictionary learning Although the offline DL stage has adjust the overcomplete basis according the training data, it is impossible to be fit for all kinds of time-varying interference patterns. Moreover, its computation load is quite large for realtime localization. On the contrary, the online incremental learning is especially applicable when one seeks to find the variation in the sense that the time-varying channels pattern might not be specifically learned offline but can be distinguished from the past online observations. Based on the incremental learning pattern, the online learning algorithm in [14] can use the result of the offline DL stage as a warm restart for computing the next dictionary where the new samples will be fed into the online dictionary learning procedure, and thus a single iteration has empirically been found to be enough [14] .
For completeness, a full description of the algorithm is given in Algorithm 1. 1) update the pre-trained dictionary according to the latest observations using the online DL algorithm in [14] with off D as warm restart, and return the learned dictionary 
In order to examine the performance of the proposed ASDPD method, we compare it with the decoupled DPD approach in [4] and covariance-based sparse DPD (CDPD) method [9] . Consider four BSs placed at the corners of a 1 km × 1 km square. Assume the number of grids in the location area is 26 26 K = × , which means yielding a 40m resolution along both x and y axes. The carrier frequency of the simulated signal is assumed to be 900 MHz. Each BS is equipped with a uniform linear array of ten antenna elements with the adjacent elements spacing of half a wavelength. The locations of targets are selected at random, uniformly, within the square. All the simulation results are obtained based on 200 Monte Carlo realizations. In each simulation, we consider the following multipath channel model . b = 1/16 is the exponential power delay profile and i τ is the delay spread for the ith path [15] . As shown in Figure 1 , the improvement in location accuracy for the proposed method can be seen in terms of the root mean square error (RMSE), when the number of emitters is two and the SNR is set to 5 dB. We can observe that the location performance of DPD and CDPD algorithms decreases evidently as the number of multipath increases. On the contrary, the variation of RMSE in the ASDPD algorithm is very small due to its adaptive ability through DL technique. This result reveals that our method is very robust to multipath channels and effectively enhances location accuracy. Figure 2 illustrates the location error with respect to the number of emitters when the SNR is set to 5 dB. Here, real lines describe the case of single-path channel for three algorithms, while dashed lines represent the case of three paths. With the increase in the number of emitters, the RMSE of DPD algorithm increases quickly due to the high sensitivity to the estimated number of targets. Note that the CDPD method does not rely on a good estimate of the number of emitters in the single-case, but its performance decreases evidently as the number of multipath increases. On the contrary, the ASDPD algorithm is very robust to two scenarios. The importance of the low sensitivity of our algorithm to the number of targets is twofold: first, the number of sources is usually unknown, and second low sensitivity provides robustness against mistakes in estimating the number of targets. 
In this paper, we exploit the inherent spatial sparsity to present a novel direct location method by combining the offline training and online learning into a unified DL framework, thereby better matching time-varying scenarios. The effectiveness of the proposed scheme has been demonstrated by simulation results where substantial improvement for localization performance is achieved. Further research will emphasize on the off-grid error analysis and the theoretic bound on the location estimation precision.
Nowadays many managers or decision maker always face the challenging situation of selecting the right solution for a given decision making problem. In order to handle multiple, convicting values, a class of methodologies has developed over the last 60 years called problem structuring methods (PSM), or soft operational research (OR) methods. These methods are characterized as a family of approaches for supporting decisions by groups of a diverse composition within a complex environment to agree a problem focus and make commitments to a series of actions. They are usually applied to unstructured problems or ill-structured problems characterized by multiple actors, multiple perspectives, conflicting interests, and high levels of uncertainty and can often involve models as transitional objects to aid the decision-making process [1] .
A scientific decision making process can be recognized by Figure 1 [2] .
Almost all the decision making methods accept the process above. For example, ANP is a helping tool to make future plans or solve the complicated problems by using qualitative or quantitative data.
The paper uses ANP to help leaders of Wuhan Iron and Steel Corporation make decisions on sustainable development problems in low-carbon economy.
ANP is one of the most important methods of multicriteria decision aid; (also named MCDA) the multicriteria decision aid method could scientifically select the best decision or optimization under situations characterized for having more than one criterion. AHP and ANP are both the appropriate methods for solving the decision and evaluation problems, but there are something different. Saaty suggested the usage of AHP to solve the problem of independence on alternatives or criteria and the usage of ANP to solve the problem of dependence among alternatives or criteria [3] . ANP provides a general framework to deal with decisions without making assumptions about the independence of higher-level elements from lower level elements and about the independence of the elements within a level. In fact ANP uses a network without the need to specify levels as in a hierarchy [4] , which could deal with more complicated problems than AHP.
There are many factors for Wuhan Iron and Steel Corporation's managers to consider about that how to make the low-carbon in enterprise.
We choose 14 factors, such as market demand, coal usage, emissions, sustainable development capacity, etc. Then, using the ANP method to compute the result of the factors' influences.
The ANP is composed of four major steps [5] :
Step 1: Model construction and problem structuring: The problem should be stated clearly and be decomposed into a rational system, like a network. This network structure can be obtained by decisionmakers through brainstorming or other appropriate methods.
Step 2: Pairwise comparison matrices and priority vectors. In this step alternatives are determined.
Selecting the alternatives from the successful ones in their field of activity by using the preliminary elimination will increase the quality of the decision. And the elements are determined.Decision-makers are asked to respond to a series of pairwise comparisons of two elements or two clusters interactions between and within clusters and to be evaluated in terms of their contribution to their particular upper level criteria [6] .
In addition, interdependencies among elements of a cluster must also be examined pairwise; the influence of each element on other elements can be represented by an eigenvector. The relative importance values are determined with Saaty's 1-9 scale, where a score of 1 represents equal importance between the two elements and a score of 9 indicates the extreme importance of one element (row cluster in the matrix) compared to the other one (column cluster in the matrix) [7] .
A reciprocal value is assigned to the inverse comparison Like with AHP, pairwise comparison in ANP is performed in the framework of a matrix, and a local priority vector can be derived as an estimate of the relative importance associated with the elements (or clusters) being compared by solving the following equation:
Step 3: Supermatrix formation:
To obtain global priorities in a system with interdependent influences, the local priority vectors are entered in the appropriate columns of a matrix. As a result, a supermatrix is actually a partitioned matrix, where each matrix segment represents a relationship between clusters in a system. Generally in this step the supermatrix will be an unweighted one. Because in each column it consists of several eigenvectors which of them sums to one (in a column of a stochastic) and hence the entire column of the matrix may sum to an integer greater than one. The supermatrix needs to be stochastic to derive meaningful limiting priorities. So for this reason to get the weighted supermatrix, firstly the influence of the clusters on each cluster with respect to the control criterion is determined.
This yields an eigenvector of influence of the clusters on each cluster. Then the unweighted supermatrix is multiplied by the priority weights from the clusters, which yields the weighted supermatrix.
The limit supermatrix has the same form as the weighted supermatrix, but all the columns of the limit supermatrix are the same.
The final priorities of all elements in the matrix can be obtained by normalizing each cluster of this supermatrix. Additionally, the final priorities can be calculated using matrix operations, especially where the number of elements in the model is relatively few. Matrix operations are used in order to easily convey the steps of the methodology and how the dependencies are worked out.
Step 4: Selection of the best alternatives: If the supermatrix formed in Step 3 covers the whole network, the priority weights of the alternatives can be found in the column of alternatives in the normalized supermatrix. On the other hand, if a supermatrix only comprises clusters that are interrelated, additional calculations must be made to obtain the overall priorities of the alternatives. The alternative with the largest overall priority should be selected, as it is the best alternative as determined by the calculations made using matrix operations. Figure 3 The ANP network in our case 

The weight of the initial matrix is subjective. How to get the valid origin data is a difficult task. 

Training objectives National Open University Open Education Business Administration (college) students, that is the basic theory of culture in business administration and marketing, the application of basic skills, the basic method, with enterprise-level managers and market operators of basic literacy and capacity talents. In order to effectively achieve the above training objectives, Open University of China (Chengdu Branch) on the professional core curriculum put forward the theory and practice, classroom and on-site, online and offline integration courses and practice teaching mode, aimed at forging a comprehensive student the practical operation ability and practical training applied talents for the development of local economy and society.
Online and offline integration and Background Practice Teaching Model of Open Education is that the professional practice and integrated practice curriculum teaching status quo is not satisfactory, basically in a "mere formality" or "neglected" state, each open education professional status of teaching practice as set forth below.
Currently, the Open University of China most courses taught mainly in theory-based, ignoring the cultivation of students' practical abilities. Course more emphasis on curriculum theory to explain the contents of the course practice mainly in the form of coursework to complete, life lessons practice even less. Students practice concentrated mainly rely on practice teaching to carry out, and concentrate basically teaching sexual practices are on the third or fourth semester, students are required to learn and then be completely portion theoretical knowledge, theory and practice the time interval longer period, it is difficult to achieve the desired practical ability to enhance the effect.
Currently, the Open University of China most cultural and economic class professional focus is the completion of the practical teaching social studies and thesis, science and engineering professional focus of practice teaching is completed practical training reports and graduation design, whether it is social investigation report or practical training reports is required by means of practical teaching bases or simulation training lab to complete. From now, the majority of school units, and not established practice teaching base in the true sense, experimental training equipment and conditions are also wide disparities exist, a large concentration of teaching practice and has not been effectively carried out, concentration the effect of practical teaching is not optimistic.
Currently, the Open University of China all students off-campus practice teaching base are mostly companies, enterprises or other organizations operating in the market economy today, companies, enterprises or other business organization is an independent self-financing economic entities, no obligation or unwilling to provide students with internship service. The reason is that students practice short time, for the company's continuing existence of certain barriers to business development; economic profit organization does not intent to let students know their business secrets or exposure to non-standard business practices; In accordance with the principle of "old and new", internships students will occupy a lot of the old company employee working hours, pay close attention to economic benefits for the company's leadership, the general reluctance to accept student interns. Thus, the operating results campus practice teaching base is not ideal.
Currently, the professional practice teaching Open University of China in general by the guidance of teachers in graduate school or university as professional teachers, most professional teachers are from the "college to TV," they never engaged in real business practice to work on concrete enterprise business environment is not experienced, practical teaching guide for students only on their own perception of textbook knowledge and ability to guide and explain its lack of authenticity and authority. At the same time, the majority of professional teachers are "behind closed doors", contact the school practitioners, communication little, good and bad teachers practice teaching to enhance students' practice has seriously affected the ability.
Initiative is the primary feature of autonomous learning, active and passive student's specific learning activities are expressed as "I want to learn" and "want me to learn." Currently, the National Open University of the professional practice teaching curriculum, teachers teach students knowledge on the basis of professional rules, students active learners rarely, basically passive acceptance of teachers to impart knowledge. Most students still stuck in the "me 'stage, a small percentage of the students," I want to learn ", students active learning enthusiasm did not fully mobilized, so learning outcomes of learners by a certain extent.

We believe that the course content comes from real life, but also for real life service. Practical certain course of production or life, reveals the relationship between teaching content and actual production or life. Contact with a particular emphasis on the specific course of life, how to focus on instructional design students learning from life experience and existing knowledge and understanding of the course, students explore consciousness, so that students learn to use the preliminary expertise and methods learned to solve some simple the practical problems. Open University of China (Chengdu Branch) "marketing" course in instructional design, organization and implementation, teaching strategies useful exploration, achieved a certain course of education quality.
We believe that through the network can break our limitations of classroom space, so that the classroom extends to the broader community, to achieve communication and integration of curricular and extracurricular. Course Tutorial combination of theory and practice of teachers seeking entry point, combined with educational institutions, the actual situation of the students attempt from life, appropriate life, "theoretical classroom teaching -Field Practice Teaching -online exchange Deepening -Analysis of classroom communication," the online and offline, and live interactive classroom teaching mode.
The class moved into the farm building "Experience + Dialogue + Network + Planning" mode We rely on the schools helping object, "The most tide microblogging marketing farmersLixue You" organic farms, the practice of the" Bus theoretical teaching + Teaching farm experience + Dialogue Teaching +Network depth interaction + Planning report sharing "Practical Teaching strategy.
Immediate south of college students are mostly part-time students, have some work experience and life experience, Combined with one of my school help object "Lixue You" healthy organic vegetables microblogging marketing as a teaching of the living body, to farming as a starting point, lead students to practice on-site experience, feeling the farm cultivation process, summarize highlights areas for improvement and marketing, in improving students' research skills, analytical skills, decision-making ability, for the farm business brainstorming.
Tutors use take the bus went to the farm on the way time, participation in the teaching activities of 40 students carriage theoretical teaching, focus on explaining the marketing of the eyeball economy, exchange, advertising, personal selling, sales promotion, public relations, microblogging marketing, micro-channel marketing; Lixue You introduced organic vegetable sales price, sales model and planting elements (land area, soil, air, water, fertilizer, planting process, etc.). For students to actively participate in the follow-up dialogue teaching theoretical foundation.
Teachers and students arrived farms by farmers as a practical teaching Lixue You tutors, farmers Lixue You lead us to recognize the vegetables, pick vegetables, sweet potato digging, planting organic healthy vegetables explain the whole process of farming and environmental protection philosophy, soil cultivation, seed selection, fertilization catch the worm, field management, harvesting vegetables, were the focus of talks, teachers and students to feel healthy organic vegetable cultivation process and culture, environmental protection farming experience.
Farmer Li Xueyou teachers and 40 students (8 teams, 5 people / group) discussion and in-depth exchanges, detailing its microblogging marketing legend, and the question raised by the various team members one by one to answer. For example: organic vegetables connotation, pricing model, sales model, sales channels, logistics and information communication tools and so on.
Link four: depth interactive network 40 students grouped in the field and online to explore Lixue You "Healthy vegetables + Microblogging marketing" success points less points and their improvements. Site experience and dialogue, students participate in online discussions with unprecedented enthusiasm, students actively speak, mutual inspiration, wisdom sparks frequently burst, within one hour, the teachers and students to interact posts reached 400, a lot of posts have larger practicality and value.
Link five: planning report share Within a month's time, 40 students in the study group as a unit for the "Li Xueyou" plan "healthy vegetables + network marketing" creative marketing programs, and to study groups for the report creative solutions unit, invite Lixue You, marketing experts or actual teacher reviews Program.
To complete the dramatic experience of the process of, fun, entertaining, educational units also placed some game elements, such as when a farm field experience as a team to identify the vegetables Dishes activities to individual units to participate in the selection of about 1,000 words experience writing activities and can best embody the essence of the event photography activities, selected the outstanding works will show and give some incentives on the school website. In addition, students can personally pick vegetables from the farm, the rationale for him, personally cooking, and "school -breaks -do" integration.
The class moved into the supermarket, build a "Classroom + Store + Network + Classroom" model We rely on "Sichuan University campus Education supermarket", the practice of the "Theoretical classroom teaching + Teaching Marketplace Experience +Network depth interaction +Group share Classroom" course practical teaching strategies.
Sichuan University study points directly under the age of students is relatively small, no practitioners work experience but there are certain life experiences, teachers select a medium-sized supermarket nearby (Sichuan University campus Education supermarket) as a teaching of the living body to store layout, store and create an atmosphere merchandise display as a starting point to lead the students into groups to enter the store observations, summarized highlights and areas for improvement store operations, to provide reference of theory and practice for students future employment and entrepreneurship.
Link one: theoretical classroom teaching Tutors in the classroom teaching focus on the basic principles of marketing, consumer shopping psychology, found that demand, to meet demand, the theoretical knowledge of market research, marketing mix and so on.
Link two: store experience teaching Tutors lead students in-depth store site teaching, teachers lead students to visit the store, and guide students display merchandise from the store site, Duitou display, light background, music scene, promotional etiquette, promotional techniques, warehouse management and other stores of cultural elements summarizes store marketing successes, shortcomings and suggest improvements.
Tutors guide students to learn the team as a unit, arbitrarily selected stores in a commodity, combined with the marketing principle of classroom teaching to write creative marketing planning copy of the product, a statement in the course forum for discussion and exchange. Study group practice based on observations and written reports and after online communication on creative marketing planning copy optimization.
Link four: Panel classroom to share Tutors organize various groups reporting Marketplace Experience report in the classroom teaching practice, teachers review and given in recognition of outstanding programs and sent to store managers, to reach the perfect combination of theory and practice.
We believe that when students are familiar with the learning content and background of life closer degree students to consciously accept the higher knowledge. Life is an inexhaustible source of Teaching, current textbooks and there are a lot of practical problems of student life experiences and real life. Teaching teachers not only to provide real-life material, the creation of student life close to the actual situation, recognize the reality of life from the perspective of observation programs, focusing on curriculum knowledge and actual contact, let the students observe the operation, speculation, exchange, reflection, etc. activities gradually realize the production of knowledge, the formation and development of a positive emotional experience, to feel the power of knowledge to enable students to learn look a gift horse, learned to use, so that students learn the course content from the source of mining life, depicting the practice of the teaching process to enhance students' organizational skills, communication skills, leadership, innovation, learning ability, calling capability and adaptability, fully in line with the Chengdu forging economic, high-quality application-oriented social development required, complex talent.
We believe that the practice teaching evaluation should be a 360° assessment, evaluation subject should implement multidimensional, including student assessment teams, the school evaluation, evaluation of outside experts, industry enterprise evaluation, etc.
Students mainly evaluates the teaching practice guidance teachers' teaching attitude, teaching contents, teaching methods and teaching effects, etc.
Teaching evaluation teams to focus on the campus practice teaching counseling teachers' teaching efficiency and teaching quality, teaching attitude and teaching, etc.
Outside experts focus on evaluating the course design of practice teaching counseling teachers, curriculum design, curriculum design, content, teaching mode and teaching methods, etc.
Industry enterprise key evaluation practice teaching counseling teachers curriculum idea and train of thought, pertinence and applicability, course organization and arrangement, teaching mode, the application of information technology and course teaching ethics, ability and level, etc.
In summary, based on the implementation of the theory and practice of classroom and on-site, online and offline integration courses and practice teaching mode, designed to effectively alleviate the concentration of Practice Teaching shortcomings, the students theoretical knowledge and practical ability to merge culture, improve students 'knowledge structure, improve students' practical ability to train for local economic and social development required for application-oriented and practical talents were actively explored, is "everyone is learning, always able to learn, everywhere can learn" the country's open practice teaching philosophy deepened.
There are estimated to be 1.5 billion mobile phones in the world today and presently, Nigeria alone has over 107.4 million mobile phone subscribers (Prensky, 2004; NCC, 2012) . In fact, roughly half of the world's population already has some type of mobile phone, making it the most wide spread technology and most common electronic device in the world. This implies that mobile phones are more than three times the number of personal computers (PCs) and most of today's phones have the processing power of an average PC. These facts and the range of computer-like functionality offered by mobile phones and PDAs, are leading some observers to speculate that many people in the not so distant future will start to see the mobile phone as an alternative to a PC. It is to this background that mobile phones have been seen to become relevant in the world of learning.
It is interesting to note that mobile devices such as phones and PDAs are much more reasonably priced than the PCs and therefore represent a less expensive method of communicating in the learning environment. Many claims about the potential and benefits of M-learning to make learning possible anywhere, anytime, in anyway and anyhow have been reported (Salmon, 2000; Young, 2002; Adedoja & Oyekola, 2008; Adedoja, Omotunde & Adelore, 2010) .
The need to explore this new trend in pedagogical/andragogical shift is therefore crucial if progress in terms of ICT use is to be made. As noted by Green (2002); Campbell (2004; 2006) ; Hooper, Fitzpatric and Weal (2008) , the use of ICT and newer technologies in the form of PDAs and mobile phones can indeed help to increase communication and interaction and enhance the quality of learning, particularly in distance education. Hooper et al. (2008) argue that mobile technologies are increasingly being used to create innovative mobile learning experiences for learners; and a key benefit has been in learners' collaboration around the use of the PDAs and mobile phones.
The objectives of the study on the use of mobile phones at the Distance Learning Center (DLC) of the University of Ibadan are to: a) Determine the benefits of using mobile phones for learning; b) Reveal the attendant challenges of using mobile phones for learning among University students;
Open Praxis, vol. 5 issue 3, July-September 2013, pp. 249-254
The Distance Learning Centre, University of Ibadan
The Distance Learning Center (DLC) of the University of Ibadan was established in 2002 to cater for the needs of distance learners. The Center also seeks to achieve one of the major objectives of the Nigerian National Policy on Education, the provision of equal educational opportunities to all citizens at different levels of education-widening participation. National Universities Commission's policy guidelines for open and distance learning in Nigerian universities (NUC, 2009 ) encourage the use of technology in deploying distance education programs. In this regard, content delivery should be based on resource-based pedagogies and marking of assignments should be automated.
A number of Educational Technology Initiatives (ETIs) are currently being pursued. Amongst others, some key reasons for investing in educational technology at the University of Ibadan are to:
• Implement discipline-specific pedagogical strategies that require students' active engagement and develop problem-solving and problem-posing skills in the context of technology-assisted learning environment; • Create interactive learning that is technology driven; • Achieve learner-centered teaching and learning, using ICT tools that enable open and distance learning; and • Develop teachers who can, through a technology driven environment, make learning relevant, exciting and effective, at the same time achieving efficiencies that will give them time to embark on other activities expected of them, like research and community service.
In line with the National Universities Commission (NUC) (2009) policy guidelines, the current project explored the use of radio broadcasting and the mobile phone in supporting distance education students, although the report on radio instructional delivery is still underway. Data on this is still being collected; therefore this report is based on acceptance of mobile phones for tutorial delivery in distance education. Pedagogical underpinning for the integration of mobile phones in education emanates from 3 very strong reasons. First, distance learners are in diverse geographical locations and are thereby learning in isolation. In order for them to maintain connections with institution and other learners, mobile phone affords both academic and administrative support. Second, on-thego-learners have the ability to carry the device with them wherever they go. Third, mobile phone penetration in Africa is high and is relatively cheaper for users than the PC. Against this backdrop, visionary educators, designers and developers within the University are beginning to consider the implications of using mobile devices for the modern teaching and learning environment. In such an environment, contents and services can be relayed to a university student by personal wireless mobile devices. This will add another layer to the personal computer-based model of teaching and learning. This also means e-learning will take place in conditions that will be radically different from those educators and learners are familiar with. Providing university students with services, content instruction and information outside the traditional learning space is becoming more acceptable among education providers who predicate their services on the routine use of advanced information and communication technologies. This paper therefore presents the result of the study carried out showing the perceived benefits and perceived problems of using mobile technologies for learning as presented by the sampled students of the Distance Learning Centre, University of Ibadan.
The study's framework was based on Davis (1986) Technology Acceptance Model (TAM), which made use of the Theory of Reasoned Action (TRA). TRA postulates that an individual's attitude towards behaviour is influenced by his/her belief. Notably, the model deals with the acceptability of an information system/tool, how it can be used to predict acceptability of the system/tool, and modifications to be made for acceptability.
The model assumes that acceptability is majorly determined by two factors:
(a) Perceived Usefulness (PU); and (b) Perceived Ease of Use (PEU)
PU can be described as the degree to which an individual believes that the use of a system/tool will improve his/her performance while PEU refers to the degree to which an individual believes the use of a tool/system will be effortless or require minimum effort. The model postulates that the use of a system/tool is determined by behavioural intention, individual's attitude to its use and the perception of its utility (figure 1). Davis (1986) posits that the attitude of an individual is not the only factor that determines his/her use, but the impact the tool or system will have on his/her performance is also a significant factor. Many studies have been carried out using Davis' (1986) TAM. Most conclude that the model is incomplete because it fails to account for social influence in the acceptance, adoption and utilization of a new tool/system. It is important to take this into account because human beings are influenced by their social environment. However many studies have used the construct of PU, PEU and subjective norms to explain technology acceptance and usage for a variety of instructional systems including online learning. Mun and Yujong (2003) exposed students to Microsoft applications for a period of eight weeks. After a two-week trial period it was found that learners' self-efficacy, enjoyment and learning goalorientation determined the actual use and acceptance of the system. Shen, Laffey, Lin and Luang (2006) explored the extent to which subjective norms (influence of instructors, mentors and peer) influence and shape the perception of learners towards the use of course delivery modes. Results of the study show that instructors' influence had significant contribution to students' PU while mentors' influence is significant to PEU of the learning system. This shows the importance of instructors' role in shaping impressions of the value of using course delivery system. Miller, Rainer and Corley (2003) find that PEU and PU have a significant positive relationship with the amount of time students spend on a course. They also note that both are significant factors for predicting intention to use. Sumak, Hericko, Pusnik and Polancic (2011) show that the use of MOODLE by learners depends on behavioural intentions and attitude. PU is found to be the strongest and most important predictor of attitude. 
Basically, for this study, the responses of the respondents were measured along the line of their perceived usefulness, perceived ease of use, perceived benefits and perceived problems of mobile technology for learning which they were exposed to. The Focus Group Discussions (FGD) questions were channeled towards gathering information to determine the perceived use, perceived ease of use, perceived benefits and perceived problems of using the identified mobile technology-mobile phone, for learning. The study adopted a qualitative research method. An outline of questions was developed for the Focus Group Discussions to measure the perceived benefits and problems of using mobile phones for learning instruction among distance learners of University of Ibadan. The population of this study comprises students of Distance Learning Centre of University of Ibadan. A sample of 201 students was drawn from the faculties of Arts and Education. The sample was further stratified into 25 groups with the groups having a minimum of 5 and maximum of 10 members; all in the third year of the distance-learning program.
Two sessions of the FGD were held for the 25 groups within the space of two weeks in the Faculties of Arts and Education among students who receive the courses: Production of Speech (LIN 241), Primer Writing (ADE 205) and Introduction to Instructional Technology (TEE 353). Due to the peculiarity of the distance learning students, students were informed of the FGD through bulk SMS service informing them of the date, time and venue of the discussion. After an informal welcoming and a quick overview of the FGD by the facilitator, each group was then asked to choose a group leader and a recorder.
Questions used during the FGD sessions were the following:
1. What are the benefits of using mobile phones for learning? 2. What are the problems you are likely to encounter when using mobile phones for learning? 3. Can you imagine learning on mobile phones? 4. What forms of education do you consider feasible for mobile phone use? 5. Have you ever tried using your mobile phone for an assignment?
The analysis of students' responses to the above questions as asked during the FGD sessions reveals the following:
1. In response to question 1 about benefits of using mobile phones for learning, most students who have used this technology responded that mobile phones have actually reduced their learning stress and greatly eased up their learning activity. They equally agreed that mobile phone use for learning has made learning more interesting and attractive. 2. Respondents, in response to question 2 about problems likely to encounter when using mobile phones for learning, submit that network failure and poor supply of electricity greatly affected their participation in using these devices. Poor supply of electricity -which usually leads to low cell phone batteries and network failure-could make instant messaging and accessing content a serious setback to using mobile phones. Sometimes electricity supply is unavailable for several days at a go, thereby making charging of batteries impossible. This usually is taken care of by the use of generators that are used in many homes and institutions, irrespective of geographical locations. They also concluded that small screen sizes would lead to small text size, which can make the viewing of information from the mobile platform a tiring experience. This, they claim, may cause fatigue especially if they stared at the screen for too long.
3. About imagining learning in mobile phones, some 90% of the students in each group see mobile learning as a welcome innovation into their course and advancement in their learning process. 4. Regarding the question about forms of education feasible for mobile phone use, three out of the nine different focus groups agreed that quiz would be the most feasible form of activity on the mobile platform. Four groups agreed that reading short texts and lecture notes would be the most feasible form of activity. One group agreed that all reading texts and lecture notes, taking quiz and tests are all feasible forms of activities on the mobile platform. One group however did not respond to the question. 5. In response to question 5, five groups indicated that they had never used their mobile devices for assignments. Although, it was noted that a few of the students in these groups had attempted the use of their mobile device for assignment (searching for information in the web). Four other groups indicated that they have at one time or the other used their mobile phones for assignments.
Besides, in the course of the project, the following challenges were encountered with respect to students using the mobile platform:
• Login difficulties: Some students found it difficult to log in, as some names for login were not correctly written -e.g student's name: Joshua, registered name: Josua-; these login problems made students become confused and frustrated.
• Network problems: Some students complained about loss of Internet connectivity, e.g. a student complained of not being able to log in because of rain. This is usually the case with MTN Mobile network infrastructure anytime it rains; connectivity is lost.
• Special needs: Students with physical challenges were not catered for in the project as they were not able to interact with content, which is wholly text-based. Some of these students complained and requested that their physical challenge should be factored into the design and implementation of the mobile learning project.
• Computer skills: During quizzes, some students complained about not being able to initiate the quiz (this complaint was made about TEE -Introduction to Instructional Technologycourse quiz,) or to submit the quiz questions after answering them, due to low computer skills.
• Mobile platform: Students also complained about the way and manner in which the mobile platform works and their inability to navigate it. This also boils down to the problem of poor or low Internet skill. This complaint, among others, instructs that at the student orientation programme there is the need to intimate students with the way the mobile platform works.
The paper has considered the great benefits and the possible challenges of using mobile phones for learning. The study, among other things, has shown that the application of mobile technologyparticularly mobile phone-into learning has made learning become easier and more interesting. It has been able to bridge the divide of time and space that is the peculiarity of the erstwhile formal mode of learning. These benefits notwithstanding, mobile phone use for learning has its peculiar challenges or problems as experienced by learners in the Distance Learning Programme of the University of Ibadan. While some learners had issues with adequacy of IT skills, others had problem of power supply to sustain their phone batteries for use, among other raised issues.
In spite of the attendant challenges, the good that mobile phone use for learning has got to offer, as shown by the study, is of greater value. So, it is important to note that a study of this magnitude is of great importance to our educational system, particularly at such a time when the world is Open Praxis, vol. 5 issue 3, July-September 2013, pp. 249-254 craving for equal access to qualitative education. Institutions and educational providers should work with every sense of purpose to reduce the challenges raised by this study and further studies should be encouraged in the area of application of mobile technology to the teaching and learning situation towards the attainment of mutual access to qualitative education by all and sundry.
Meaning can be extracted from an artifact when we analyse it, determine why its parts are arranged in a particular way, and discover something about its intended or assumed purpose. When we create an artifact we instill meaning into it by organising its parts to express some sort of purpose. The parts which make up the whole are selected and configured to achieve some end. We here equate 'meaning' with 'intentionality'. The idea of capturing design knowledge in such a way that the generation of designs can be achieved mechanistically has fascinated artists and artisans for some time, and various methods have been devised for producing complex designs from the combinatorial possibilities presented by a set of simple rules. The utility of rules as a basis of human problem solving, and as a way of modeling human behaviour, has been widely recognised. Attention must be directed towards the selection and ordering of rules if a mechanistic model of the design process is to take account of the meaning of artifacts Gero et al, 1984) . Newell and Simon (1972) discuss human problem solving as information processing, whereby a set of operators act on patterns to produce solution states. We can regard a pattern as a set of characteristics belonging to some object which enable us to recognise similarities with other objects. Patterns also enable us to copy objects. They need describe not only dimensional attributes (as with a dressmaker's pattern), but also topological relationships between elements: colour, texture, orientation, temperature, etc. It is generally understood that humans are extraordinarily adept at recognising patterns. The term 'cognitive map' is often used to describe a network of patterns and associations. This has been discussed in the context of human perception of the environment (Downs and Stea, 1973; Lynch, 1960) , but it can be argued that it is generally applicable to all types of mental activity (Kaplan and Kaplan, 1982) . We have difficulty, however, in externalising patterns, that is, describing patterns that enable us to class similar objects together; and even greater difficulty in representing the knowledge which enables us to recognise patterns. The implementation of a mechanistic view of the design process as transformation rules acting on patterns will undoubtedly be simplistic, representing but a pale imitation of human cognitive activity, yet, we find that at a pragmatic level this model provides a basis for demonstrably workable systems. Empirical studies of designers at work have been discussed in the context of this model, notably by Eastman (1970) and Akin (1978) .
Design states, therefore, represent conglomerations of patterns. The designer is able to recognise patterns contained within the design state which match those contained within some mental repository of patterns. A strong association between patterns prompts the designer to replace one pattern with another. These associations can be regarded as transformation rules, the general form of which is
Transformation rules are important as a basis for describing language understanding and other cognitive activities (Chomsky, 1975) , and provide a powerful way of representing design knowledge. In this context we consider transformation rules as 'design rules'.
Design theorists, such as Alexander, have given formal expression to design rules in a practical context (Alexander et al, 1968) , although with little thought as to how such rules may be used entirely mechanistically. A 'shape grammar' is the name given to a collection of formal rules for manipulating patterns of lines and labeled points to produce designs resembling architectural plans after the style of a certain body of architectural work (Stiny and Mitchell, 1978) . This latter approach is of interest as it demonstrates the degree of precision with which rules have to be described to facilitate mechanistic design generation.
The model of design as transformation rules acting on patterns within design states requires an understanding of how goal states are achieved. Problem-solving activity is goal-directed. We are trying to find a state which contains a particular pattern, and the process of generation can cease when that pattern is achieved. There may also be 'end states' that are nongoal states, that is, there are no more rules that can be applied and yet the goal state is not achieved. The general mechanism for satisfying goals under such circumstances is backtracking. This involves a return to an earlier state, where a different rule can be tried. This results in the exploration of alternative solution paths. Design activity is therefore described as a goal-directed search through a space of states or potential design solutions.
The above model is complete in that it describes how, with a set of rules and a goal set, we are certain to generate designs. In practice, however, it is insufficient for producing solutions to real design problems. There are high costs, in terms of computational time, associated with forward search and backtracking. The process of transforming one state into another may be costly; the cost of 'undoing' partial designs (that is, backtracking to earlier states) may be expensive; and there may just be too many intermediate states requiring evaluation to be practicable (that is, the space of design states may be subject to a combinatorial explosion).
One mechanism for reducing the number of 'branches' in a design space (and therefore the amount of generating and backtracking) is to evaluate intermediate states for their potential to develop into the goal state and to investigate only those branches that are likely to lead to a goal state. This task is not as easy as it may seem at first. The criterion used to evaluate when a goal state has been reached is generally inappropriate as an evaluator of intermediate states. The design problem illustrated in figure 1 demonstrates this. The goal consists of two subgoals: create a structure which spans the canal and which connects A to B. Already, in the above example, we have utilised one of the major components of a planning system, that of abstraction. The design states were explored by considering only certain selected attributes of planks and pylons. For artifacts that are created in the 'ground' level of abstraction the cost of backtracking tends to be high (although it is less for sand castles than for marble sculptures). It is obviously cheaper to design buildings by considering only certain attributes of walls and spaces, on paper, before committing the design to bricks and concrete, but it may also be appropriate to consider higher levels of abstraction again. We discuss this below. The second mechanism is that of working backwards from goals. This is similar to spanning the canal with the planks and then working out how to support them. It will be noted that this type of exploration is one that can be carried out only in an abstraction of the real world. For most domains, working backwards from goals involves less searching and in some instances can reduce a complex problem to a trivial one. This is the process of planning: determining sequences of actions that enable goals to be satisfied. The actions, of course, are the design rules. The theory of planning, in artificial intelligence, stems mainly from an interest in determining robot actions, but also proves applicable to design theory. A comprehensive discussion of the general theory of planning is given by Nilsson (1982) . We develop this idea of the role of sequential planning in design by means of an example. Figure 2 shows a set of design rules for generating simple building forms from wall elements and columns. The elements form rooms, each of which has a name and an orientation. A dot indicates the top of a room. The design space is depicted in figure 3 as a tree structure, with the root node as the initial state (the fact 'start') and the ends of the branches as various end states, where no more rules can be applied. It will be noted that the application of certain rules precludes the achievement of certain states. So the end states are all different. The design space of this set of rules is atypical of most design domains in that it is small enough to be represented on one page.
One of the rules, rule(2), is depicted in program form as follows:
rule (2) The rule name is given first, followed by a list of preconditions. These preconditions must be satisfied before the rule can be applied or 'fired'. The second list is a set of actions or consequents. When the rule is fired, the objects matched in the precondition part of the rule are deleted from the current description of the world and replaced with the objects on the consequent side of the rule. The rule is interpreted by a general-purpose design system written in the logic programming language, PROLOG (Clocksin and Mellish, 1981) . The rule is also shown diagrammatically in figure 4. Here a room is defined by four columns. For this rule to be fired, it is necessary to find patterns in the current design state which match the descriptions of room_«(l) and short_wall(2). These objects are deleted from the description of the design state and replaced with the objects room_«(l), again, room_c(2), short_wall(4), short_wall(5), long_wall(5), and open_wall(2). The arguments denote different instances of the same object. The elements used in the rules have real coordinates in a two-dimensional world and it is possible to construct different rules from a vocabulary of such objects. The current state is depicted in terms of facts, in predicate calculus notation. Facts are therefore deleted and asserted with the firing of each rule. The representation of objects is essentially hierarchical: for example, rooma(l) is a composite object made up of the objects column(l), column(2), column(3), and column(4); these objects consist of line segments, which are defined by points expressed in homogeneous coordinate notation:
A composite object in a rule matches the current state if there exists a composite object described in the facts base which has the same predicate name, but different instance number, and both are made up of similar objects. The geometrical transformation which enables the rule object to be mapped onto the object in the state description is calculated from the points defining the ends of the line segments. This task is made simpler by restricting transformations to translations and rotations of 0°, 90°, 180°, and 270°. So here we do not consider mappings between objects that involve reflection or changes of scale. A representation of the knowledge required for this type of pattern matching is given in appendix 1. The process of matching nongeometrical descriptions is relatively simple as it is necessary to find only a one-to-one match between a fact within a rule and the corresponding fact in the state description.
The costs involved in determining whether rules are to be fired, and the cost of backtracking, are fairly high in terms of computational time. It is therefore not entirely practical to use this abstraction space when searching for goal states. To reduce the need for costly backtracking it is necessary to derive the sequence of rules which will achieve particular goal states, by searching within a higher level of abstraction before implementing the rules to produce geometrical designs.
Suppose we have the goal that both room b and room d are to be located. This is a simple goal, but one that cannot be achieved without some backtracking, as figure 5 illustrates. To see if any state satisfies this goal it is not necessary actually to generate geometric descriptions as long as we can find a suitable rule abstraction. For example, rule(2) can be represented by using simple facts in the following way: [located(rooma), located(roomc), clear(room_c, top), clear(room_c, right), clear(room_c, left) ].
This produces the search space shown in figure 6 . With rules in this form it is possible to generate abstracted design states and evaluate them for compliance with the goal, and to backtrack if necessary without actually generating geometrical descriptions.
Even when operating in an abstracted search space, however, forward reasoning, as described here, can be very inefficient, particularly when we consider that in a real design problem it may be necessary to search many long branches before arriving at a state that matches the goals. The branches marked * in figure 5 could have been very long, for example. We therefore turn to reasoning backwards from goals within an abstraction space as a more effective method of planning.
Backward reasoning, or regression (the backward application of rules or operators), involves knowing what rules must be applied to satisfy goals. This knowledge is contained in the rules themselves. If the goal is located(roomd)', we find that there are two rules that satisfy this condition, rule(4) and rule(6). Selecting the first rule, we look to see what conditions must be satisfied before that rule can be implemented. These are that room b is located and that the right-hand side of room b is clear. These conditions can be satisfied by rule(3); and so we form a backward chain through these conditions until we come to the initial state. Regression is a trivial process when there is only one goal to be achieved, but becomes more difficult when there are more, as satisfaction of one goal can produce a state which precludes the satisfaction of another. The planning system STRIPS represents a general paradigm for backward chaining to satisfy multiple goals (Fikes and Nilsson, 1971) . Kowalski (1979) has formulated the planning problem in such a way that it is able to be solved by a PROLOG-type theorem prover, and a planning system based on a different approach has been developed using PROLOG by Warren (1974) . We present here a simple STRJPS-like planner which maintains a current description of the design state as well as a list of appropriate actions by which states are achieved. The planning knowledge is shown in appendix 2.
In figure 7 we illustrate the mechanism by which the multiple goals: located(roomd) and located(roomJ>) are satisfied. The boxes contain goals and subgoals. A downward arrow points to the subgoals which must be satisfied before a particular goal can be met. Goals are considered one at a time in a recursive fashion. A goal is satisfied when it matches the current state description or can be achieved by means of some rule. The condition 'start' is the first to be satisfied, as this matches the initial condition. Once it has been found that rule(l) achieves the subgoal, located(room_«), this rule can be used to change the current state. The subgoal, clear(room_a, right), matches the current state of the world and is therefore true (indicated by T'). (roomji, bottom) clear (room_a, right) rule (2) located(roomja) clear (room_a, top) clear (room_a, bottom) located(room c) clear ( room_c~ top) clear (room_c, right) clear (room_c, left) rule{6) located (room_ clear(room_a, clear(room_a, located(room_ clear(room_c, clear(room_c, located(room_ clear(room_d, clear(room_d, clear(room_d, a) 
I mfe (8) located (room_ clear(room_a, clear(room_a, located(room_ clear(room_c, clear(room_c, located(room_ clear(room_d, clear(room_d, located(room_ clear(room_e y The sequence of rules, rule(l), rule(3), rule(4), therefore results in a state that satisfies the goal: located(roomd). The second goal, located(roomfe), also matches the current state. So both goals are satisfied. It will be noted that the goal, located(room_d), could have been achieved by another rule: rule(6). It should be possible to generate other plans by backtracking to the last goal that can be achieved by another rule. In all of these examples that goal or subgoal is indicated by the symbol *. Figure 8 shows an attempt to achieve a second plan. The rule sequence, rule(l), rule(2), rule(6), satisfies the goal: located(room_d). The only rule that can be invoked to satisfy the second goal, located(room_&), is rule(3), which requires as its precondition that room a is located and that the righthand side of room a is clear. As the second of these subgoals cannot be matched against the current state description, and as there is no rule which enables this goal to be satisfied, the plan as formulated so far fails. In a logic programming framework, a fail causes backtracking to the last goal which presents an alternative proof, that is, the last goal that is able to be achieved by means of another rule. This is the goal indicated *. Figure 9 shows how a new plan is constructed from that goal. In this case both goals: located(room_d) and located(room_b) are satisfied. order, that is, 'located(roomfc)' is before 'located(roomd)'. It should be noted that the representations of states in these diagrams are depicted as full geometrical entities, whereas the state descriptions maintained by the planner are actually similar to those abstractions shown in figure 6 . The diagrammatic representation of states has been illustrated to make the figures easier to follow. 
In most design domains it is unlikely that goals will be able to be matched against facts contained within a state description, that is, the goals are not expressed in the same terms as the facts describing states, so a one-to-one match is not possible. Under such circumstances it is necessary to use inference to determine whether a goal state has been achieved. We consider two approaches to this problem here, the use of knowledge about the design rules and the use of knowledge about design states. Both approaches involve augmenting the design knowledge (the design rules themselves) with some form of metaknowledge, that is, knowledge about that knowledge. A simple example of this is where we wish to achieve the goal that two rooms are next to each other, for example, next_to (room_c, room_d) . By looking at the rules we can determine something about them which will prove useful in achieving this goal. This can be expressed as the metarule:
IF the goal Is that two rooms are to be next to each other THEN the room in the precondition part of the rule is NEXT TO the room in the resultant part of the rule.
When our planning system is searching for a match with the goal: next_to(A, B), this metarule ensures that a 'nextto' fact appears on the resultant side of a design rule, by implication. The search tree which produces the first sequence of rules for satisfying this goal is illustrated in figure 13 .
The second approach is to employ knowledge about design states in determining whether goals have been achieved. For example, we can infer from the abstracted state description that a particular room is located on a corner of the building by noting if two walls from that room are clear and form a corner. This can be expressed in the same form as a design rule:
Corner_wall(y4, B) is derived by using the following knowledge, which states that A and B are corner walls if they are both found within a fact headed by the predicate 'corner':
corner (top, right), corner (top, left) . corner (bottom, right) . corner (bottom, left) .
Consider the goal: cornerroom(roomd) and located(roome). Figure 14 shows how the first rule that enables the goal, corner_room(room_d), to be satisfied is inferencerule(l). The preconditions that must be satisfied are the subgoals: and clear(room_<i, B) . Note that these subgoals contain variables. These variables cannot be instantiated until the subgoals are matched against the current state description. So their values will not be known until a partial sequential plan for achieving these subgoals has been discovered. The rule sequence, rule(l), rule(3), rule(4), produces a design state that satisfies these conditions and instantiates the variable A to 'top' and B to left'. The goal, cornerroom(roomd), is satisfied by this rule sequence, but the second part of the goal, located(room_e), fails. Figure 15 shows the rule sequence that results from backtracking. The sequence is: rule(l), rule(2), rule(6), rule(8). Design goals are rarely expressed in these terms however. The aspirations of a designer for the object being created are generally articulated at a higher level. It becomes necessary to explicate the knowledge which enables high-level goals to be interpreted in terms that can be understood by a mechanistic planning system. Figure 16 illustrates this. Suppose that the building under consideration is some kind of public building. One of the ways in which the goal 'high public utility' can be achieved is by providing an attractive restaurant facility. This is achieved if the restaurant has good views and is accessible to the public. For the restaurant to have good views it should be on the corner of the building. This last goal is a 'system' goal, that is, one that can be understood by the planning system. The selection of appropriate planning goals may be regarded as an expert task and one that could be handled by an 'expert system'. The utility of expert systems in architectural design has been discussed elsewhere (Gero and Coyne, 1984) . It should be possible to derive an appropriate goal set where goal conflicts are minimised and where, perhaps, goals are ordered in importance in some way, for use by a planning system. 
We have considered the model of the design process as a search through a space of design states. The operators which transform one state into another are transformation rules, which in this context we have called 'design rules'. Design rules capture design knowledge that facilitates the mechanistic generation of designs. If such designs are to be imbued with any meaning then they must express some kind of purpose. Purposeful design is synonymous with goal achievement in this context. In realistic design domains the process of generating and backtracking to achieve goals is expensive. We have discussed the importance of selecting and ordering design rules within an appropriate level of abstraction as a means of reducing this cost. Regression is the process of chaining backwards from goal states to the initial state and proves effective in reducing the amount of search necessary. This becomes increasingly difficult when more than one goal has to be satisfied. A STRIPS-like planner is demonstrated as a mechanism for satisfying goals which produce conflicts. Implicit goals can be satisfied if we supplement the design rules with a type of metaknowledge with which we explicate some of our knowledge about the design rules themselves. We can also use our knowledge about design states to infer whether or not they achieve goals. The mechanism of inference also proves important in interpreting the aspirations of designers in terms that a planner can understand.
Within realistic design domains the mechanisms we have discussed so far for achieving goals are insufficient. The domain-independent planning strategies of GPS (Newell and Simon, 1972) , STRIPS, and WARPLAN (Warren, 1974) , etc still present us with a search strategy that can involve the consideration of an unwieldy set of states, before reaching a solution. It is necessary to provide a mechanism for representing and using our knowledge about formulating plans. Systems which do this generally consider planning within a hierarchy of abstractions. ABSTRIPS (Sacerdoti, 1974 ) and the blackboard model of HEARSAY-II (Erman et al, 1980) are two such paradigms. The analogy is made of planning a car journey between two cities. We do not 'search' a map of every street and laneway, but rather we consider the problem by planning the journey at a coarse scale first, that is, we consider the major highways and stopping points, and then resolve the problem at successively finer levels of detail. This type of planning strategy requires some prior knowledge about the domain under consideration, and we may consider the procedures for selecting appropriate planning strategies as metaplanning. This will have to be considered if we are to satisfy goals within realistic domains.
In other words, Web 2.0 has enormous potential to bring user-generated content to the Internet. The idea is to free data from corporative control and allow anyone to assemble and locate content to meet their own needs or the needs of clients. Rather than having to conform to the paths laid out for us by content owners or their intermediaries, we create the content.
Is Web 2.0 something that we need to start thinking about? What does it mean for how we provide medical information services? Perhaps this is another bubble that will disappear if we just ignore it for a while. However, in thinking about this, I would like to share with you some ideas and tools that will set up the way for Web 2.0. I have written a series of short reviews of the major trends and surmised their application in health and medical information services.
Let's start with RSS feeds. In subsequent articles we'll move onto blogging and podcasting, continue with Wikis and folksonomies (tagging), and conclude with social networking tools.
RSS (RDF Site Summary, or Rich Site Summary, or Really Simple Syndication) is an easy-to-use XML format for distributing content on the Web. It has been around since the late 1990s but has received considerable attention very recently because of the expansion of blogging. In short, RSS is a simple XML syntax for describing recent additions of content to a Web site. These additions can include news items, blog updates, library acquisitions, or any other information elements. A Web site with one or more RSS feeds is said to be syndicated. Users subscribe to the feeds using an RSS aggregator or newsreader that crawls the sites on a regular basis, usually several times per hour. An aggregator displays feeds and enables users to organize them and to access related Web pages when these are available. RSS feeds can have the following applications for health librarians:
( 
There are a number of RSS aggregators available. The aggregators can be categorized as follows:
(1) Web-based readers -These Web sites collect RSS feeds online and can be accessed from any Internet-enabled computer. Bloglines (http://www.bloglines.com/) is my favorite free online RSS reader, allowing easy access to selected RSS feeds from both my home and work desktops. (2) Standalone clients -These software packages access selected RSS feeds and download results to your computer. SharpReader (www.sharpreader.net) is my favorite free standalone RSS reader. (3) Plugins -These programs are integrated into software packages installed on your desktop (e.g., Microsoft Outlook). For an easy introduction to RSS feeds, I recommend a simple Web-based aggregator such as Bloglines. Since Bloglines is Web-based, there is no software to download, and subscribed feeds can be accessed from any Internetconnected machine.
Making an RSS file is easy to do. If you understand basic HTML, you know enough to use someone else's RSS to make your own file. Don't know HTML? You might consider starting a blog, as the majority of today's blogging tools automatically generate RSS files. Danny Sullivan's SearchEngineWatch offers great advice for compiling a simple RSS feed (http://searchenginewatch.com/sereport/article.php/2175271). In addition, Syndic8's How To section also lists numerous tutorials that describe the building of RSS files (http://www. syndic8.com/documents.php?Section=HowTo).
In summary, my sense is that 2006 will be a year of increasingly pushed and user-created content on the Internet. RSS will not necessarily become the core of Web 2.0 services, as it does not include any transactional pulled component. In other words, you cannot use RSS to purchase a DVD or reserve an airline flight. However, for information professionals, particularly those in life and health sciences, and those that use the Web primarily to retrieve, provide, and update information, RSS will be increasingly more prominent. RSS is becoming an essential communication tool that allows us to provide the most up-to-date information to our clients. It is definitely a trend to watch.
In the sub 0.25 micron regime, IC feature sizes become smaller than the wavelength of light used for silicon exposure. Resultant light distortions create patterns on silicon that are substantially different from a GDSII layout. Although light distortions have traditionally not affected the design flow, the techniques used to control these distortions have a potential impact on the design flow that is as formidable as the recently addressed Deep Sub-Micron transition. This session will discuss the design implications arising from techniques used to control sub-wavelength lithography. It will begin with an embedded tutorial on subwavelength mask design techniques and their resultant effect on the IC design process. The panel will then debate the extent of the resulting impact on IC performance, design flow, and CAD tools.
In the subwavelength era below 0.25 micron, two critical technologies are being adopted for by IC manufacturers: phaseshifting and optical proximity correction. Combined, these techniques enable the reliable design and manufacture of IC's with feature sizes half the wavelength of light. They offer the potential for substantial speed, power and area benefits. Because they are implemented through software models, the cost and time to realize these benefits is substantially smaller than equipment upgrades.
Unlike past manufacturing innovations, the successful use of these two technologies is predicated on a changed relationship between the IC design and manufacturing flows. With these subwavelength technologies, the IC layout, the mask and the end silicon look substantially different. Current rule-based process models are inadequate to describe the effects. A new method for creating process models for design and verification is critical to the industry's use and adoption of subwavelength design techniques.
Subwavelength optical lithography changes the process of releasing a physical layout for mask creation and final manufacturing. Before 1996, mask release from design to manufacturing was a simple, raw design-data mapping.
In late 1996 and into 1997, features on leading edge designs began to approach subwavelength. Manufacturers began adopting Optical Proximity Correction (OPC) to compensate for near wavelength effects. OPC offers the first step to a new paradigm; design shapes are changed at the mask level, and no longer match layout.
In 1998, and for the foreseeable future, leading edge designs are in subwavelength. Phase-shifting technology is a necessity to produce the next generation of IC's. Phase-shifting adds a leap in performance but also new complexity to design manipulation during the release process. Silicon shapes no longer replicate physical design. In order to ensure that the desired physical design will correlate with both the mask and the end silicon, new checking and verification tools are required throughout the design to manufacturing process.
Continual reduction in IC feature size is driven by competitive requirements for cost reduction and product performance. However, fundamental DSM-era process effects limit the precision with which the ideal IC layout can be transferred to the silicon product. Critical circuit features are distorted due to process-dependent proximity errors. The effect of these errors is compounded by the extreme aspect ratios of interconnects which have not been scaled similarly due to requirements for performance and reliability.
These DSM fabrication factors are responsible for performance and yield reduction, signal integrity degradation, as well as reduced predictive capability of EDA design and verification tools. These factors challenge the implicit assumptions of EDA tools ---that the layout is precisely printed on the silicon product, and that it is a sufficient representation of the physical circuit adequate for predicting the eventual product performance.
Fortunately, process proximity effect distortion can be mitigated by advanced mask techniques; new advanced EDA physical design and verification tools will be used to achieve this productive and cost-effective end.
Lance Glasser, KLA-Tencor, San Jose, CA Subwavelength lithography will be a fact of life for the next decade. The wavefront engineering needed to make subwavelength lithography work guarantees that mask geometries will look little like either design geometries or wafer patterns. There is a many to one mapping between possible mask geometries and their corresponding ideal wafer pattern. Creating effective mask designs and then validating their correctness and manufacturability, including inspectability, will be a key challenge for the next ten years.
No advance statement submitted.
Proceedings of the 36th ACM/IEEE Conference on Design Automation (DAC'99) 1-58113-109-7/99 $ 10.00 ACM
Balancing the energy utilization of a large system is fundamental for their efficient behavior, especially in electrical systems and the electric grid [1] . Peak demand (or peak load), i.e., the largest simultaneous energy demand by all users in a system, might cause many severe issues such as low quality of service and service disruption, high cost of energy production and distribution, and so forth. For this reason, various technical and mostly economical methods have been used to control the peak demand, with the most widely adopted being peak-demand pricing [2] . In a peak-demand pricing policy, a large commercial electricity customer is charged not only for the amount of electricity it has consumed but also for its maximum demand over the billing cycle. The unit price of the peak demand charge is usually very high, up to 240 times in some cases [3] and even more, to discourage energy usage under peak load conditions. In other words, peaks in energy usage are inefficient and expensive for both suppliers and customers.
Building systems consist of many sub-systems such as heating, ventilating, air conditioning and refrigeration This material is based upon work supported by the Greater Philadelphia Innovation Cluster (GPIC) for Energy Efficient Buildings an energy innovation HUB sponsored by the Department of Energy under Award Number DE-EE0004261.
(HVAC&R) systems, boiler/chiller systems, and lighting systems. These components are often operated in an uncoordinated manner, i.e., independently of each other, which may result in temporally correlated power demand surges and in turn cause expensive electricity cost under the peak-demand pricing policy. As a result, demand control is important for the efficient operation of building control systems. Not only it helps reducing the electricity cost, it also has other benefits such as improving the quality of energy distribution and smoothing and flattening the curve of power usage.
While there exist several different approaches to load balance power consumption, e.g., by load shifting and load shedding [4] , [5] , they operate on coarse grained time scales and do not guarantee any thermal comfort. Another approach to energy efficient control for commercial buildings is model predictive control (MPC) ( [6] , [7] ). In [6] the authors investigated MPC for thermal energy storage in building cooling systems. Peak electricity demand reduction by MPC with real-time pricing was considered in [7] .
The problem of scheduling real-time computing tasks under resource constraints has been well studied over the past few decades in real-time systems [8] . In [9] and [10] the authors investigated the integration of control and real-time scheduling, in which control tasks were specified as periodic tasks with fixed execution time or CPU utilization and conventional real-time scheduling approaches were applied. From the resource allocation aspect, our control scheduling problem is similar to multiprocessor real-time scheduling with full migration [11] , however the tasks' periods and execution times are highly dependent on the system's dynamics, the safety specifications, and the current state instead of being given a priori. Recently, a control scheduling problem for peak power reduction has been considered in [12] , [13] , in which independent control systems with affine dynamics and no disturbances are scheduled so that at most 1 actuator can be activated at any time and each state variable is bounded in a given range. However, these works are limited to simple dynamics and do not consider any disturbances to the system.
In our recent paper [14] , we described an approach called green scheduling to reduce the peak demand of a large number of heating systems. We proposed a more general constraint that at most k ≥ 1 actuators can be activated at any time. The system model considered in the paper was affine dynamics with no interaction between the sub-systems and no disturbances. The main contribution of this paper is twofold.
1) The results in [14] are extended to system dynamics that are bounded between monotone affine dynamics. This class of dynamics is larger and more practical because it can capture certain nonlinear dynamics, inter-dependencies, and constrained disturbances. 2) We propose a scalable method for synthesizing periodic schedules for large-scale systems. This paper is structured as follows. First, we formulate the system model and the problem in Section II. Section III proves a sufficient schedulability condition. Based on this proof, a method for synthesizing periodic schedules is described in Section IV. It is followed by a large-scale simulation in Section V which demonstrates the scalability and effectiveness of the proposed method. Finally, we conclude the paper with a road map of our future work in Section VI.
We present in this paper a scheduling approach to reduce the peak power demand of a heating system of multiple zones. Consider n > 1 zones. Each zone is heated by a heater that can be turned on, when it provides a constant heat input rate to the zone, and turned off, when it consumes no energy and provides no heat input. Let x i ∈ R denote the air temperature ( 
in which C i > 0 is the thermal capacity of the zone (kJ/K), K i > 0 the thermal conductance between the ambient air and the zone (kW/K), K ij ≥ 0 the thermal conductance between zone i and zone j = i (kW/K). The control input to heater i is its on/off state, denoted by u i ∈ {0, 1} where u i = 0 corresponds to the off state and u i = 1 the on state. Then its instant heat input rate is
for some constant
is the value space of x. Since x i is the temperature of a zone, it cannot receive any real value but only those in a valid range, for example between 15
• C and 30
• C. Therefore, X is a bounded subset of R n . From equations (1) and (2), the dynamics of x i is governed by
We assume that the dynamics of x i is monotone: x i always grows when u i = 1 and always decays when u i = 0. Hence, a zone's temperature always increases when its heater is on and always decreases when off.
At any time t, the aggregate demand Q of the entire system is the sum of the power demands of individual heaters:
As mentioned in Section I, under a demand-based tariff, the high charge for peak demand is an incentive for reducing the peak demand over a given time horizon [0, t f ], to save energy and to reduce cost. However, we must also maintain the thermal comfort in each zone i, which requires that x i ∈ [l i , h i ]. Therefore, the peak demand reduction problem can be stated as follows. Peak demand reduction problem: Compute control inputs u i (t) for the heaters to minimize the peak demand max 0≤t≤t f Q(t) while maintaining thermal comfort in each zone.
Conventional demand management strategies such as load shifting and load shedding ( [4] , [5] ) can be used for this problem but they operate on coarse grained time scales and do not guarantee thermal comfort. Model predictive control ( [6] , [7] ) is a powerful control framework for this problem, however its high computational requirement prevents it from being used for large-scale systems (with hundreds of zones and heaters).
In [14] , we proposed green scheduling as an approach to reduce the peak demand by coordinating the heaters so that at any time, at most k of them, for 1 ≤ k ≤ n, can be on simultaneously while maintaining thermal comfort. The results were obtained for systems with no interaction between zones (K ij = 0), no internal heat gain (d i = 0), and a constant global ambient air temperature (T a,i = T a = const). In this paper, we remove those restrictions and derive a schedulability condition for the system as well as a scalable method for synthesizing periodic schedules.
Buildings in practice are typically well thermally insulated between zones and between the interior and the ambient air, i.e., K i and K ij are small. Furthermore, the disturbance w i of a zone is usually constrained in some small bounded subset
. Therefore, it is reasonable to assume that the disturbances in each zone and the interactions between zones are small enough so that the temperature dynamics of each zone is bounded between two affine dynamics, independently of the other zones and the disturbances. This assumption is stated formally as follows.
Assumption 1 (Affinely bounded monotone dynamics):
T and the disturbance for each zone i, the trajectory of the system is the function x : R + → X that satisfies the differential equation (3) 
, thermal comfort is maintained in all zones. The system is said to be kschedulable, for 1 ≤ k ≤ n, if there exists a safe schedule u(t) such that u(t) 1 ≤ k for all t ≥ 0, i.e., at most k heaters can be on simultaneously.
We present a sufficient k-schedulability condition in the next section and a scalable method for synthesizing periodic schedules in Section IV.
Remark 1: Assumption 1 captures a larger class of systems than the heating dynamics in equation (3), including certain nonlinear dynamics. Because the results in the rest of this paper only use the parameters of the bound dynamics, they also hold for all systems belonging to this class.
The following theorem states a sufficient condition for the system to be k-schedulable.
Theorem 1: If for each i, l i < x i (0) < h i and η i < η i where
and
then the system is k-schedulable for all
Because −a off,i l i + b off,i < 0 and −a on,i l i + b on,i > 0 (Assumption 1), η i is well-defined and satisfies 0 < η i < 1. Similarly, η i is well-defined and 0 < η i < 1.
The rest of this section will present the proof of Theorem 1, which forms the basis of the scheduling synthesis in Section IV. We first define the following bound systems.
Definition 1 (Lower-bound system): For each i, define the lower-bound system S i with state x i ∈ R, control input u i ∈ {0, 1}, and dynamics given by
1 In this paper, we use the terms control input and schedule interchangeably for u(t). Definition 2 (Upper-bound system): For each i, define the upper-bound system S i with state x i ∈ R, control input u i ∈ {0, 1}, and dynamics given by
The following result is straightforward, which bounds the system's trajectory between those of the lower-bound and upper-bound systems.
Lemma 2: Given any schedule u(t) and disturbances
. For each i, the trajectory x i (t) is bounded by
where x i (·) and x i (·) are respectively the trajectories of the lower-bound system S i and the upper-bound system S i with the same initial condition x i (0) = x i (0) = x i (0) and the same control u i ≡ u i ≡ u i . It follows that if the trajectories of S i and S i are bounded in [l i , h i ] then so is x i (t). The next lemma states the existence of safe periodic schedules for each heater i. Lemma 3: Suppose l i < x i (0) < h i . For any η i such that η i < η i < η i and any r i ≥ 0, there exists δ i > 0 such that if u i (t) is a periodic schedule
for any δ satisfying 0 < δ < δ i , then
Proof: For the sake of clarity, we drop the subscript i in this proof. First, note that η always exists since 0 < η < η < 1. Let x(t) and x(t) be the corresponding trajectories of the bound systems as defined in Lemma 2. Define y(j) = x ((j + r)δ) and z(j) = x ((j + r + η)δ) for j ∈ N. Figure 1 illustrates these bound trajectories and the sequences {y(j)} j∈N and {z(j)} j∈N . It follows from the monotonicity of the dynamics (Assumption 1) that
sup t≥0 x(t) = max x(0), sup j∈N z(j)
By Lemma 2, inf t≥0 x(t) ≤ x(t) ≤ sup t≥0 x(t), ∀t ≥ 0.
Infimum of x(t): From (6), the sequence {y(j)} j∈N is given by
where
Because δ > 0, a off > 0, a on > 0 and 0 < η < 1, 0 < A y (δ) < 1. From linear system theory [15] , the sequence {y(j)} j∈N is monotonic and asymptotically converges to y ∞ (δ) = B y (δ)/(1 − A y (δ)). Therefore inf t≥0 x(t) = inf j∈N y(j) = min {y(0), y ∞ (δ)}
Using straightforward calculus and algebra calculations, we can show that y(0) ≥ l for all 0 < δ < , and that lim δ→0 + y ∞ (δ) > l. Therefore, there exists δ y > 0 such that 0 < δ < δ y implies inf t≥0 x(t) ≥ l.
Supremum of x(t): Similarly, the sequence {z(j)} j∈N is monotonic and asymptotically converges to z ∞ (δ) = C z (δ)/(1 − A z (δ)) where
Again, it can be shown that there exists δ z > 0 such that 0 < δ < δ z implies sup t≥0 x(t) ≤ h. Let δ = min{δ y , δ z } > 0. Then 0 < δ < δ implies inf t≥0 x(t) ≥ l and sup t≥0 x(t) ≤ h, hence l ≤ x(t) ≤ h for all t ≥ 0.
We now prove Theorem 1. To prove k-schedulability, we will construct a safe δ-periodic schedule u(t) for the system so that at any time t, u(t) 1 ≤ k. The time period δ > 0 is chosen so that for every i, x i (t) ∈ [l i , h i ] for all t ≥ 0. a) Constructing periodic schedules: We can always find η i > 0 for each i such that η i < η i < η i and
We then distribute n non-overlapping right-open intervals, each of length η i respectively, into the interval [0, k] on the real line (Fig. 2a) .
a distribution is always possible. Given a time period δ, we construct the periodic schedule u i as in (8) where r i = s i − s i ≥ 0 and s i denotes the largest integer that is no greater than s i . Figure 2b illustrates this schedule construction for n = 3 and k = 2. It is straightforward to show that with these schedules, u(t) 1 ≤ k for all t.
b) Choosing δ: For each i, by Lemma 3, there exists δ i > 0 such that schedule u i (t) is safe with any period δ satisfying 0 < δ < δ i . Choose 0 < δ < min {δ i : i = 1, . . . , n}. Then with this δ, the schedule u(t) is safe, i.e., x i (t) ∈ [l i , h i ] for all t ≥ 0 and all i.
Therefore, the system is k-schedulable.
Suppose that the conditions in Theorem 1 are satisfied, thus the system is k-schedulable. The proof in Section III suggests that periodic schedules of the form (8) can be 0 1 2 used for the system. However, to make these schedules safe, their time periods might need to be chosen very small (Lemma 3). In practice, this is usually undesirable, even impossible, because of the physical constraints of the actuators or the performance degradation of the actuators caused by high-frequency switching. Therefore, to consider practical periodic scheduling, we need to relax the safety requirements:
where τ i ≥ 0 is finite. In other words, x i is allowed to be out of the comfort range for a finite time horizon [0, τ i ), and after that it must be bounded in the range. Each periodic schedule u i (t) in (8) has three parameters: time period δ i , utilization η i , and offset r i . Since η i is the fraction of the time period that u i = 1, we borrow the term utilization from real-time scheduling [8] . We denote the set of parameters of schedule u i by (δ i , η i , r i ). For each i, x i (t) is bounded between x i (t) and x i (t) (Lemma 2), whose bounds are given by (9) , (10), (15) , and (18). Since the safety requirements have been relaxed, we only need to consider the limits y ∞,i (δ i , η i ) and z ∞,i (δ i , η i ) calculated as
where A y,i , B y,i , A z,i and C z,i are defined in (13), (14), (16), and (17). Observe that these limits depend on the values of δ i and η i , but not r i . Therefore, we can construct periodic schedules u i in two steps: (1) for each i, compute δ i and η i to make the limits bounded in [l i , h i ]; (2) given parameters (δ i , η i ) for all i, find r i so that at any time t, u(t) 1 ≤ k.
A.
Step 1: Compute (δ i , η i ) for each i
In this step we compute δ i and η i for each i so that y ∞,i (δ i , η i ) ≥ l i and z ∞,i (δ i , η i ) ≤ h i . In practice, the time period δ i is determined by the characteristics of the physical equipments and the hardware platform. Thus, we assume that δ i > 0 is provided and we need to compute η i .
By taking the derivative of y ∞,i with respect to η i , it is straightforward to verify that dy∞,i dη i > 0 for all 0 < η i < 1 and δ i > 0, i.e., the function y ∞,i is strictly increasing with respect to η i . It follows that y ∞,i (η i ) ≥ l i is equivalent to η i ≥ η i where η i is the root of the equation y ∞,i (η i ) = l i . Though we do not have a closed-form expression for η i , the equation can be numerically solved efficiently using Newton's method since y ∞,i (η i ) is strictly monotonic. Similarly, the constraint z ∞,i (η i ) ≤ h i is equivalent to η i ≤ η i where z ∞,i (η i ) = h i , which can also be solved numerically.
If η i ≤ η i then we can choose any value η i in the range 
so that the periodic schedule defined in (8) satisfies u(t) 1 ≤ k, ∀t ≥ 0. In general, this problem is similar to multiprocessor real-time scheduling of periodic tasks with full migration [11] , in which δ i is the task's period, η i is the task's utilization, and k is the number of identical processors. Conventional multiprocessor scheduling algorithms can be used to derive a schedule for the system. However, if the time periods δ i are uniform (i.e., δ i = δ for all i), there exists a simple algorithm for obtaining the values r i as shown in Section III. This algorithm is simple and scalable for a large value of n, however it requires that all schedules u i have the same time period.
Let X 0,i be the set of initial states of x i . Using the periodic schedule (8) with parameters {(δ i , η i , r i )} n i=1 , we can find a finite time horizon τ i for each i such that x i (t) is guaranteed to be in the range [l i , h i ] for all t ≥ τ i . Indeed, since the sequences {y i (j)} j∈N and {z i (j)} j∈N (Section III) that bound the trajectory x i (t) are monotonic and converge to y ∞,i and z ∞,i respectively, once they are in [l i , h i ] they will stay in that range indefinitely. Therefore, τ i corresponds to the smallest time step j such that y i (j) ≥ l i and z i (j) ≤ h i for all initial state x i (0) ∈ X 0,i . It can be easily calculated from the expressions of y i (j) and z i (j). 
In this section we present simulation results and compare the proposed approach to uncoordinated On-Off control for large scale systems. The periodic schedule described in Section IV was implemented in MATLAB.
We considered 500 zones whose parameters were randomly generated with zone's thermal capacity C i ∈ [2000, 3000] (kJ/K) and thermal conductance K i chosen proportionately from [0.4, 0.6](kW/K). The thermal capacity of a zone is an indicator of the size of the zone, so a greater value of C i corresponds to a larger zone. The zones are heated by heaters with different heat input rates q i also chosen based on the size of the zone q i ∈ {7, 10}(kW). Since the number of zones was large, we randomly assigned zones which can thermally interact with each other and the value of their inter-zonal thermal conductance K i,j was chosen from [0, 0.06] (kW/K), with the value 0 implying that the zones do not interact.
Zone temperatures were required to be kept between l = 20
• C and h = 24 The simulation time was 6 hours. We ran two simulations with different values of the time period δ of the schedule and compared the peak demand and total energy consumption with the uncoordinated On-Off control. MPC was not implemented for comparison since it could not be scaled to a large (500) number of zones. The peak demands and energy consumptions are reported in Table I .
Compared to the uncoordinated On-Off controller, the periodic scheduler significantly reduced the peak demand by about 38%. When the time period δ was 20 min, the value of k (k-schedulable) was 316 for 500 zones which resulted in a 37.0% reduction in the peak power demand and a 2.49% decrease in the total energy consumption as compared to the uncoordinated case. The energy consumption for this case is shown in Figure 4 . Obviously, the curve of energy usage of the periodic schedule was much more smooth and flat compared to that of the uncoordinated On-Off control. The periodic schedule computation ran very fast and took about 3.3 seconds to complete. When the time period δ was decreased by half from 20 min to 10 min we observed a greater reduction in the peak demand (38.58%). The value of k decreased from 316 (for δ = 20 min) to 308 for this case. The savings in the total energy consumption also increased to 4.97%. Since the time period was smaller, tasks switched more frequently but resulted in better savings. This can be seen in Figure 5 which shows the temperature profile for a single zone for the On-Off (shown in dashed) and the periodic case (shown in solid). Although the periodic scheduler switches more frequently than On-Off control, it maintains the temperature of the zone within a smaller range (around the mean temperature of 22
• C), which is better with respect to both thermal comfort and the power consumption of the zone. The peak demand and energy consumption for the uncoordinated On-Off case remained the same in both cases as it did not depend on the value of the time period.
We presented an approach to energy efficient control of building systems by scheduling them within a constrained peak demand envelope while maintaining the required environmental specifications. The class of system dynamics considered in this paper can include inter-dependencies between sub-systems, constrained internal and external disturbances, and certain nonlinear dynamics. A sufficient schedulability condition was derived and a method for synthesizing periodic schedules for the system was proposed. Through a large-scale simulation, the method was shown to be scalable as well as effective in reducing peak demand.
In this paper, we assumed the existence of independent bound dynamics that capture the inter-dependencies between sub-systems. Although this assumption is reasonable in practical applications, it might not hold when the interdependencies are large. In the future, we aim to remove this drawback by directly working with the inter-dependent dynamics of the system. We are also investigating statefeedback dynamic scheduling algorithms, dynamic pricing models, operational efficiency and task-specific cost functions for system-wide optimization.
This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.
Categorial Grammars (CGs) consist of two components: (i) a lexicon, which assigns syntactic types (plus an associated meaning) to words, (ii) a calculus which determines the set of admitted type combinations. The set of types (T) is defined recursivety in terms of a set of basic types (To) and a set of operators ({\,/} for standard bidirectional CG), as the smallest set such that (i) To C T, (ii) /f x,y E T, then x\y, x/y E T. 2 Intuitively, lexical types specify subcategorisation requirements of words, and requirements on constituent order. We here address a particular flexible CG, the (product-free) Lambek calculus (L: Lambek, 1958) . The rules below provide a natural deduction formulation of L Barry et al. 1991) , where dots above a type represent a proof of that type. Proofs proceed from a number of initial assumptions, consisting of individual types, some of which may be "discharged" as the proof is constructed. Eact~ type in a proof is associated with a lambda expression, corresponding to its meaning. The elimination rule/E states that proofs of A/B and B may be combined to construct a proof of A. The introduction rule /l indicates that we may discharge an assumption B within a proof of A to construct a proof of A/B (square brackets indicating the assumption's discharge). There is a side condition on the introduction rules, reflecting the ordering significance of the directional sloshes. For /1 (resp. \1), the assumpt I am grateful to Esther KSnig for discussion of the paper. The work was done under a grant to the Cambridge Univer-";ty Computer Laboratory, "Unification.based models of lexical •ccvu and incremental interpretation", SPG 893168. ~ln this notation, x/y and x\y are both functions from y into x. A convention of left association is used, Bo that, e.g.
((t~np)/pp)/np may be written s\np/pp/np. tion discharged must be the rightmost (resp. leftmost) undischarged assumption in the proof. Elimination and introduction inferences correspond semantically to steps of functional application and abstraction, respectively.
Hypothesis rule:
Elimination rules: Following Prawitz (1965) , a normal form (NF) for proofs can be defined using the following meaning preserving contraction rule and its mirror image dual with \ in place of/, which, under a version of the Curry-Howard correspondence between proofs and lambda terms, are analogous to tile/~-contraction rule ((,~x.P)Q t> P[Q/z]) for lambda expressions. 

A/B B A --./E A Under this system, every L proof has an equivalent 'fl-NF' proof. Such fl-NF proofs have a straightforward structural characterisation, that their main branch (the unique path from the proof's end-type to an assumption, that includes no types forming the argument for an elimination step) consists of a sequence of (> 0) eliminations followed by a sequence of (> 0) introductions.
The main approach for parsing L ha:~ been sequeu~ calculus theorem proving. 3 Used naiwdy, this approach is inefficient due to 'spurious ambiguity', i.e. the existence of multiple equivalent proofs tor combinations. K6nig (1989) and llepple (1990u) develop a solution to this problem baaed on detining a NF for sequent proofs. These NF systems as yet cover only the basic calculus, and do not extend to various additions proposed to overcome the basic system's shortcomings ms a grammatical framework.
Some importance has been attached to the properties of flexible CGs in respect of incremental process.
ing. These grammars typically allow sentences to be given analyses which are either fully or prinmrily left> branching, in which many sentenceinitiat ~ubstrings are interpretable constituents, providing for processing in which the interl)retation of a sentence is generated 'on-line' as the sentence ix presented, lncrementality is characteristic of hmnan sentence processing, and might also allow more efficient machine processing of language, by allowing early filtering of semantically implausible analyses. It ix notable, however, that us methods have yet been proposed tbr incremental parsing of Lambek grammars. In what tbllows, 1 d~seribe a chart method for L and then show how it may bc modified to allow both inclusion of an operator [3, used for handling locality constraints, and also to allow incremental parsing of L.
Standard chart *nethods are inadequate lot L because proving that some combination of types is possible may involve 'hypothetical reasoning', i.e. using additional assumptions over and above just the types tttat are being combined. For example, the above proof of a/b,b/c ::~ a/c requires an additional assumption c, subsequently discharged. Standard chart parsing involves ordering the edges tbr lexical categories along a single dimension, and then adding edges for con. stituents that span wider substretches of this dimension as constituents are combined. Tim problem fi3r L is that there is no place in this set up for additional hypothetical eleme*lts. Placing edges tot them anywhere on the single dimension of a normal chart would simply be incorrect. K6nig (1990, 1991) , in the only previous chart method for L, handles this probhun by placing hyt,o thetical elements on separate, independently ordered 'minicharts', which are created ('emitted') in rc~aponue to the presence of edges ttmt bear 'higher order' func tor types (i.e. seeking argunlents having fimctional types), which may require 'hylmtheticul reasoning' in the derivation of their argmnent. Minicharts may '~tt-taeh' themselves into other charts (including other minicharts) at points where combinations are po~i. ble, so that 'chains of attachment' may arise. Somc 3 Spa¢:e limits preclude diac~msion of I~cei*t proof *~t work. fairly complicated I)ook-keeping is required to keep track of ~:'hat has combined with what as a basin for ensuring correct 'discharge' of hypothetical elements. This information ix encoded into edgc~4 by replacing the simple indices (or vertices) of standard charts with 'complex in(liccs'. Unfortunately, the complexity of thin method precludes a proper exposition here. However, somc (lifl~rcnc~ between K6nig's method and the method to be proposed will hc mentioned at the end of the next section.
A New Chart Approach I next present a new chart parsing method for L. Its most striking diflbrence to tim standard approach is that there ix typically more than one ordering governing the association of edges in a chart. These orderings intersect and overlap, making a chart a 'nmltidimensioual object'. A uecond difl~rence in that the basic unit we adopt ior ~q,ecifying the orderings of the chart is pr*ntilivc iuteroals, rather than point-like vertices, wi~ere the relative order of the primitive intervals that make up an ordering must be explicitly defined. The span of edg?'~ is specilicd cxtcnsionally tm tim concatenated sum of some number of primitive intervals. The method i, pcfliaps most easily explained by example.
'lb parse the coml)ina~ion x/y, y/z, z =~-x, we require a three clement ordering orde~cing (a.b. e) (a b and c being primitive interwdu). Tim three types give three edg(.~, each having thrc~: fields: (i) the edge's spun (her,, a primitive mtervat), (ii) its type (iii) the typc~8 'meaning' (here ~ m,ique constant).
[~t, :~/y, £1] [b, y/z, ~s2Zl [c, z, t3J Edg~ are combined uodvr the lollowing chart rules, corre~po,diag to our elimination ru]c~:
The ruh~4 a[luw two edges with appropriate types to combine provided tilat the concatenation of their spans is n gubstrhlg of uome defined ordering (a test made by the predicate iua ~ubord), Given these rul~, our chart will expt~nd to include the following two edges. The l)reaeuce of atL edge with type x that ~l)anu the full width ol-the ningle defined ordering show~ that x/y, y/z, z -~ x can be derived.
[ 
Figure 1: The EMIT procedure
Our next example x/(y\p/q), y/z\p, z/q =~ x requires 'hypothetical reasoning' in its derivation, which is made possible by the pr~ence of the higher-order functor x/(y\p/q). In the natural deduction approach, deriving the functor's argument y\p/q might involve introduction inference steps which discharge additional assumptions p and q occurring peripherally within the relevant subproof. To chart parse the same example, we require firstly the following three edges and require a three element ordering:
As in KSnig's approach, an 'emit' step is performed on the edge which bears a higher-order type, giving various additions to the chart needed to allow for hypothetical reasoning. Firstly, this gives two new edges, which are assigned new primitive intervals:
Some new orderings must be defined to allow these edges to combine. Since the higher-order functor is forward directional, possible arguments for it must occupy non-empty intervals It such that isa_subord(a.H). Hypothetical reasoning with the two new edges is useful only in so far as it contributes to deriving edges that occupy these spans. Hence, the required new orderings are (d.H.e) such that isa_subord(a.H). Such new orderings are most conveniently created by including the following condition on ordcrings. In general, such conditions may fire after tile emit step that creates the condition, when other new orderings are created that include the emitting edge's span. The above condition causes new orderings (d.b.e) and (d. b. c. e) to be defined, allowing combinations that yield the following edges:
It.e, z, (t3 v2)]
The final thing created by the emit process is the following condition on edges (where I@B represents tambda abstraction over I in B):
if Cd.O.e, y, S] and isa_subord(a.O) then [0, y\p/q, v2@vl@S] This condition has the effect that whenever an edge of a certain form is added to tile chart, another edge of a related form is also added. The condition completes the process of hypothetical reasoning, by syntactically and semantically abstracting over the hypothetical elements ('discharging' them) to derive the function required as argument by the higher order functor. Note that, since the intervals d and a arc unique to the two edges created in the emit process, any edge spanning an interval (d. 13. e) must have involved these two edges in its derivation. The condition 'fires' on the above edge spanning (d.b.c.e) to give the first of tile following edges, which by combination gives the second. This final edge demonstrates the derivability of original goal combination.
We have now seen all the basic ingredients required for handling hypothetical reasoning in the new approach. Figure 1 . shows a general (if still somewhat informal) statement of the emit procedure which is called on every edge added to the chart, but which only has an effect when an edge bears a higher-order type. The specific consequences depend on the functor's directionality. , and a condition on edges, which fires to produce an edge for the result of hypothetical reasoning, should it succeed. Note that edges produced by such condi~ tions are there only to be argument to some higher order functor, and allowing them combine with other edges as funclov wmdd be unnecessary work. I assume that such edges are marked, and that some mechanism operates to block such combinations, s A slightly modified emit procedure is required to allow for deriving overall combinations that have a functional result type. I will not give a full statement of this procedure, but merely illustrate it. For example, in proving a combination F :¢, y\p/q, where an ordering Q had been defined for the edges of the types F, emitting the result type y\p/q would give only a single new ordering (not a condition on orderings), a condition on edges, and two new edges for the hypothetical elements as follows:
That completes description of the new chart metimd for L. A few final comments. Although the method has been described for proving type combinations, it can also be used for parsing word strings, since lexical ambiguity presents no problems. Note that defining a new ordering may enable certain combinations of edges already present in the chart that were not previously allowed, tlowever, simply checking for all edge combinations that the new ordering allows will result in many previous combinations being redone, since new orderings always share some suborderings with previously defined orderings. One way to avoid this problem is to only check for corr~binations allowed by substrings of the new ordering that were not previously suborderings.
Concerning the soundness of this method, note that chart derivations can be easily translated into (correct) natural deduction proofs, given a knowledge of 4This notsttion il rather clumsy in that it anpears to mugKent the prettence of at lemU otie forward and one backward directional arguinent and also a relative ordering of these argu~ meats, wiles tteither of these impllcationa is intended. A similar point can be made about abstraction8 in the schematic semalltics m~lvna .... wtlvllS, whose order and number will iu fact mitTor that of the corresponding syntactic arguments. A more satisfactory statement of the emit procedure could be made recursively, but tiffs would take up too nmch space. 5An alternative would be not entering 6uclt edgea at all, but instead have a condition on edges that creates tat edge for the restllt of combining the emitting higher-order functor with its implicitly derived argtulteltt, directly.
which edges gave rise to which others, i.e. with binary edge combinations corresponding to elimination inferences, and with the creation of an edge by a condition on edges corresponding to some sequence of introduction inferences. In fact, chart derivations all translate to fl-NF proofs, i.e. with introductions always made after any eliminations on tile main branch of any subproof. This observation provides at least art informal indication of the completeness of the method, since ttle mechanisms described should allow for chart derivations corresponding to all possible/~-NF proofs of a given combination, which (as we noted earlier) are fully representative. Another issue is whether the method is minintal in tile sense of allowing only tt single chaxt derivation for each reading of a combination. This is not Bo, given that distinct but equivalent fl-NF proofs of a combination axe possible, due to a second source of equivalence for proofs analogous to y-equivalence of lambda expressions (i.e. that f "= Ax.f~:). For exanipie, the combination a/(b/c), b/c ==~ a has two fl-NF proofs, one involving 'unnece~axy' hypothetical rea~ soning. However, having equivalent edges represented on the chart, and the undesirable consequences for subsequence derivations, can be avoided by a simple identity check on edge addition, provided that the meaning terms of edges produced by conditions on edges are subject to rt-normalisation.
I will finish with some comparisons of the method to that of kSnig (1990, 1991) . The importance of KSnig's method tm precursor for the new method cannot be overstated, tlowever, the new approach is, I believe, conceptually much simpler than KSnig's. This is largely due to the use of 'conditions on edges' in the new approach to handle discharge of hypothetical elements, which allows edges to be much simpler objects than in KSnig's approach, where edges instead have to encode the potentially complex information required to allow proper discharge in their 'complex indices'. The complex nature of KSnig's edges considerably obscures the nature of parsing as being simply reasoning about sequences of types, and also makes it difficult to see how the method might be adapted to allow for extensions of L involving additional operators, even ones that have straightforward sequent rules.
A second difference of the new method is that orderings that govern the association of edges are explicitly defined. There is a sense m which the multiple intersecting orderings of the new approach can be seen to express the dimensions of the search space addressed in sequent calculus theorem proving, although collapsing together the parts of that search space that have common structure, ht K6nig's method, although the elements that belong together in a miniehart are relatively ordered, tt~e attachment of one minichaxt to another is allowed wherever relevant edges can combine (although subject to some constraints preventing infinite looping). This means that elements may be combined that would not be in sequent calculus theorem proving or in the new chart method. The consequences of this difference for the relative complexity of the two chart methods is at present unknown.
Various extensions of the basic Lambek calculus have been proposed to overcome some of its limitations as n grammatical approach. Morrill (1989 Morrill ( , 1990 ) . In a standard flexible CG treatment, extraction is handled by functional abstraction over the position of the missing (i.e. extracted) element. The type of the abstracted element is determined by the type of the higher order lexical type that requires this abstraction, e.g. the relative pronoun type rel/(s/np) abstracts over np. Note that this relative pronoun type cannot extract out of an embedded modal domain, because it abstracts over a bare (i.e. non-modal) np, whose presence would block O1 rule's use in deriving the overall modal constituent. However, a relative pronoun rel/(s/[]np), which abstracts over a modal type Drip, can extract out of an embedded modal domain.
Including this operator presents considerable problems for efficient processing. Firstly, it excludes the use of the NF systems devised for the calculus (KSnig, 1989; Hepple, 1990a) . As noted above~ spurious ambiguity makes ordinary (i.e. non-normal form) sequent theorem proving of L inefficient. This problem is greatly increased by inclusion of O, largely due to nondeterminism for use of the OE rule. 6 6 Conaldrr a sequent S = Ox ! ,l:]x:h..., nXn ::~ x0, where the r~l~ted amquent S s ~ Xl,X2,... ,xn ~ x0 is a theorem. Nondeterminism for use of [[]L] means that there are n{ different paths of inference from S to S t, so that there are at least n! proofs of S for each proof of S I. In fact, interaction of [OLl with other inference ruler means that there is typically many more proofs than this.
The new chart method is fairly easily adapted to allow for o, avoiding the non-determinism problem of the sequent system, so that parsing examples with [] is typically only slightly slower than parsing related examples without any Os. Firstly, it is crucial that we can always identify tbe parent edge(s) for some edge (i.e. the immediate edge(s) from which it is derived), and thereby an edge's more distant ancestors. I ignore here the precise details of how this is done. The following chart rule serves in place of the bE rule: The procedure check_modal_history used by this condition checks the edge's 'history' to see if it has appropriate ancestors to license the O-introduction step. Recall that the D! rule requires that the undischarged assumptions of the proof to which it applies are all Otypes. The corresponding requirement for the chart system is that the edge must have ancestors with ~-types that together span the full width of the edge's span H (i.e. there must be a subset of the edge's ancestor edges that have [:l-types, and whose spans concatenate to give H). The edge [(b.c) , y, (12 13)] satisfies this requirement, and so the condition will fire, allowing the parse to proceed to successful completion, as follows:
More complicated cases arise when an emitted functor seeks an argument type that is both functional and modal. As suggested above, a satisfactory statement of the emit process is best given recursively, but there is not sufficient space here. Hopefully, an example will adequately illustrate the method. Consider what is required in emitting an edge [a, w/([] These three conditions 'chain' together to create edges with the type required by tbe emitted functor. Of course in practice, the.three conditions could be collapsed into a single condition, and such a move seems sensible from the viewlmint of efficiency.
Despite considerable interest in the theoretical possibility of incremental processing using the Lambek calculus, no incremental parsing methods have as yet been proposed. Indeed, most Lambek parsing work has been based around sequent theorem proving, which might be viewed as antithetical to incremental processing since it fundamentally involves reasoning about complete sequences of types. In fact it is fairly easy to modify the chart method to a|low some extent of incremental processing, i.e. so that scanning left-toright through an input string (or type sequence), the chart will contain edges assigning types to substrings that would not otherwise receive types (luring parsing, including some for initial substrings of the input.
The modification of the chart method involves allowing an additional extent of hypothetical reasoning over that so far allowed, so that edges for hypothetical types are added not only for higher-order functors, but also for first-order functors. This is allowe by a new procedure emit*, described below, emit* is called on every edge added to the chart, but only has an effect if the edge's type is functional, creating a new edge for a hypothetical type corresponding to the function's first argmnent, as well as a condition on orderings and one on edges. The condition on orderings creates new orderings allowing the hypothetical edge to cmubine with its 'emitter', and tile result of that combination to he combined with filrther edges. (The requirement J \= i.K prevents the condition incorrectly reapplying to its own output.) Note that the new edge's interval is peripheral ill the new orderings that are defined since it is only in peripheral position that the new hypothesis cau be discharged (hence, we have (G. H. i) in the condition of tile first case rather than (0. H. i. J)). Such discharge is made by the new condition on edges. Let us look at some examples (where we limit our attention to just edges relevant to the discussion). Consider parsing the type sequence (x/y, y/z, z). Since the method should not depend on the parser knowing the length of the input sequence in advauce, an ordering will be defined with each scanning step that covers just tile material so far scanned, and which extends tile ordering of the previous scanning step by one. After scanning the first two types of the input, the chart will include at least tile following two edges and ordering:
Applying emit* to the second edge (ignoring the first edge here) yields the following edge and conditions:
The condition on orderings will fire ou the ordering (a.b) to produce a new ordering (a.b. i), which permits the first two of the following edges to be built, the third being generated from the second by the condition on edges. The type x/z this edge assigns to the initial substring (x/y, y/z) of the input (corresponding to the composition of the two functions) would not have been created during parsing with other Lambek parsing methods.
Acll~s OE COL1NG-92, NAN-I£S. 23-28 Aot~'r 1992
As a second example, consider the stage of having scanned the first two types of the input sequence (y, x\y/z, z). Scanning yields the following ordering and the first two edges. Applying emit* to the second edge yields the third edge, and two conditions: The ordering condition gives the following new ordering, allowing creation of the subsequent new edges. As before, the last edge assigns a type to the combination of the first two input types which would not otherwise be expected during parsing. Although the method allows for a considerable degree of inerementality, some conceivable incremental constituents will not be created that would be in parsing with alternative categorial frameworks. For example, rules of type raising and composition in Combinatory Catcgorial Grammar (Steedman, 1987; Szabolcsi, 1987) would allow incremental combination of types vp/s, np ::~ vp/(s\np), not allowed by the present approach. The modified chart method instead allows for the construction of incremental constituents in a manner that most closely relates to tim notion of dependency constituency argued for by Barry & Pickering (1990) (see also Hepple, 1991) , although since the modified parser is still a complete parser for L it cannot be viewed as implementing a notion of dependency constituency. 7 Finally, it should be noted that the additional hypothetical reasoning allowed by emit* and combinations involving additional 'incremental constituents' result in many 'spurious' analyses, so that the incremental chart method is in general slower than the non-incremental chart method.
I have presented a chart parsing method for the Lambek calculus, which I would argue has several advantages over that of KSnig (1990 KSnig ( , 1991 . Firstly, I believe that it is considerably conceptually clearer than KSnig's method, and more straightforwardly reflects intuitions about the nature of hypothetical reasoning :'However, some version of a chart parser that used only the kind of hypothetical reasoning allowed by the emit* procedure, and not that of the emit procedure, might well implement a notion of dependency constituency. in proving L combinations. Secondly, the relatively straightforward nature of the system with respect to reasoning about sequences of types should, I believe, make it easier to adapt the method to allow for additional type-forming operators over those already provided in the (product-free) Lambek calculus, particularly where operators have fairly straightforward sequent rules. We have seen how the method can be extended to allow for Morrill's Eloperator. We have also seen how the method may be modified to allow incremental parsing of Lambek grammars.
Constraint-based mining has become a popular framework for supporting pattern discovery tasks. First, it enables to provide more interesting patterns when the analyst can specify a priori relevancy by means of constraints. Next, this has been identified as a key issue to achieve the tractability of many data mining tasks: useful constraints can be deeply pushed into the extraction process such that it is possible to get complete (every pattern which satisfies the user-defined constraint is computed) though efficient algorithms.
In this paper, we focus on patterns that hold in 0/1 data sets. In a popular setting, such data sets generally correspond to relations between two attributes only, e.g., transactions × items or objects × f eatures.
Frequent itemset mining or formal concept mining are typical data mining tasks in such binary relations. Frequent itemset mining has been introduced by Agrawal et al. [2, 3] . To tackle difficult cases, one major breakthrough has been the study of (frequent) closed set mining, formal concepts being the mapping between closed sets of items (or features) and their supporting sets of transactions (or objects) [4, 7, 9, 17, 18, 19] .
We address here the more general problem of closed pattern mining in n-ary discrete-valued relations. Hereafter, such patterns are called closed n-sets. When n = 2, this task turns to be the classical closed set mining from a binary relation. Mining n-ary relations with n > 2 is clearly useful across multiple application domains. For example, in the context of sale data analysis, we can easily have relations crossing items, customers, dates, and regions. We may want to extract maximal associations between such attributes for business decision making. Another typical (generic) application domain concerns the numerous situations where object properties can be recorded as features for a collection of objects over time. This typically provides ternary relations.
A formal concept associates a closed set of transactions with a closed set of items. This mapping is bijective. In this perspective, a formal concept is a maximal pattern w.r.t. the two sets. This definition, used in FCA (Formal Concept Analysis), is "generalizable" to n-ary relations. However, the bijection is not between two sets anymore. Each (n − 1)-sets can be associated with the remaining one.
Ideas aiming at directly reusing FCA on preprocessed n-ary relations do not seem to work. The issue is to find a bijection between n-ary relations and binary ones on which the extraction of formal concepts provides the closed n-sets of the original relation. Such a transformation would certainly lead to a combinatorial explosion of the number of attributes. Indeed the attributes of the binary relation should combine several elements of the different sets to encompass the n-ary relation.
The main challenge of constraint-based closed n-set mining in n-ary relations relies on the ability to push constraints during the extraction and to handle an im-portant amount of data. This is especially difficult when no assumption is made on the arity of the relation and their attribute domain sizes. The pattern enumeration strategy becomes even more important than for itemset and/or formal concept extraction. Indeed, one cannot enumerate anymore one attribute domain (usually items) and compute the rest of the pattern thanks to a Galois connection. In the general case, n − 1 attributes are needed to determine the remaining attribute.
Furthermore, the enumeration strategy has a major impact on the class of constraints which can be efficiently pushed. To achieve tractability, this is especially crucial to perform an efficient closeness constraint checking. Algorithms Trias [11] and CubeMiner [12] have been recently proposed to compute closed 3-sets in ternary relations. They have different enumeration strategies. Trias basically relies on formal concept mining (i.e., closed 2-set mining) from two different binary relations that are projections of the original ternary relation. It works well if we assume that at least one attribute has a small domain size. CubeMiner uses a ternary enumeration that recursively splits the data set into smaller pieces. Unfortunately, several additional checks must be performed to ensure the unicity of the extracted patterns.
We propose a new algorithm called Data-Peeler. It is inspired by D-Miner [4] which computes complete collections of closed 2-sets that satisfy minimal size constraints in binary relations. Data-Peeler extracts closed n-sets in n-ary relations when n ≥ 2. It is based on an original enumeration process which considers any attribute domain on a same basis (i.e., no selection of a particular one is done a priori). It enables to push a large class of constraints called piecewise (anti)-monotonic constraints. In particular, this class includes the classes of monotonic and antimonotonic constraints. Furthermore, Data-Peeler efficiently checks the closeness constraint in such a way that there is no need to store previously computed patterns. Data-Peeler can exploit new constraints like the promising isolated constraint: it enables to compute only patterns containing elements which are "significantly different from the elements "outside" it".
The rest of the paper is organized as follows. We formalize the mining task in Section 2 and we discuss the type of constraints our algorithm handles. In Section 3, we present the Data-Peeler algorithm which extracts closed n-sets under constraints in nary relations. Implementation issues are discussed in Section 4. Section 5 studies space complexity. Experimental results are provided in Section 6. Finally, related work is discussed in Section 7 and Section 8 briefly concludes. 
R is a n-ary relation on these attributes, i.e., R ⊆ D 1 × · · · × D n . n-sets are elements of 2
We now provide the formal definitions of closed n-sets and the new class of piecewise (anti)-monotonic constraints. We use the ♯ operator to denote set cardinality.
Closed n-sets are a generalization of formal concepts or closed itemsets to n-ary relations. Intuitively, a n-set
is a closed n-set iff (a) all elements of each set X i are in relation with all the other elements of the other sets in R, and (b) X i sets cannot be enlarged without violating (a). Formally, H is a closed n-set iff it satisfies both the constraints C connected and C closed :
In binary relations, bi-sets X 1 , X 2 satisfying C connected ∧ C closed are formal concepts (i.e., X 1 and X 2 are closed sets).
2.2 Piecewise (anti)-monotonic constraints Enabling user-defined constraints is extremely useful to support subjective interestingness and thus the relevancy of the extracted collections. It is also well-known that the active use of constraints (i.e., "pushing" them into the extraction phase) is a key issue to achieve extraction tractability (i.e., working on large domain sizes and/or a high density of related elements). For example, we may ask for patterns with a minimal number of elements in some domains (i.e., a counterpart of the classical minimal frequency constraint on itemsets) and/or patterns covering at least a given number of elements of R (i.e., some kind of minimal area or volume constraint). We now define the monotonicity property of constraints in the context of n-set mining.
Definition 3. (Monotonicity) Let us consider a constraint C taking m sets P 1 , · · · , P m as arguments. Each argument is a subset of an attribute domain. C is monotonic on its i th argument iff ∀P 1 , · · · , P m and
When we have C(
This definition is a straightforward extension of the monotonicity as defined on binary relations. If a constraint is (anti)-monotonic on each of its arguments, then it is (anti)-monotonic following the classical terminology in the itemset mining framework.
It is however possible to define a new and larger class of constraints which can be efficiently exploited. These constraints may have an argument occurring several times in their definitions. For instance, assume we want to compute each n-set having a mean above a threshold α on a criterion V al + :
The argument X i appears twice in the expression of C 1 . To introduce the notion of piecewise monotonic constraint, we have to rewrite such a constraint by using a different argument for each occurrence of the same argument. Our example constraint can be rewritten as the new constraint P C1 :
A second example would be a constraint which specifies that each n-set has to contain a proportion of a given 2-set E, F larger than a threshold α:
Such a constraint can be rewritten as:
We can now define the class of piecewise (anti)-monotonic constraints.
(Piecewise (anti)-monotonic constraint) A constraint C is piecewise (anti)-monotonic if its associated constraint P C is either monotonic or anti-monotonic on each of its arguments.
Both C connected and C closed constraints are piecewise (anti)-monotonic. Some other examples of piecewise (anti)-monotonic constraints are: Among the piecewise (anti)-monotonic constraints, let us discuss a promising one when considering pattern relevancy. For a given closed n-set, nothing enforces the outside elements to be different enough from the inside elements. In other terms, an element outside of the nset may be in relation with almost all the elements of the n-set. To avoid the extraction of such n-sets, we propose a new constraint, namely C δ−isolated , defined as follows:
In other words, if this C δ−isolated constraint is satisfied, each x ∈ D i that is outside the n-set must have a density (in terms of relative number of elements in R) on the elements inside the n-set lower than 1 − δ. When δ = 1, any element of D i outside the n-set must not be in relation with elements from the (D j ) j =i contained in the n-set. When δ = 0, the C 0−isolated constraint is equivalent to the C closed constraint on dimension i.
is not because of the element 2 or the element 4.
To summarize, the data mining task considered in this paper is the extraction of closed n-sets that satisfy piecewise (anti)-monotonic constraints and, in particular, the C δ−isolated constraint. 
The enumeration of all the patterns by materializing and traversing all possible n-sets is in practice not feasible. Therefore, we look for a decomposition of the original search space into smaller pieces such that each portion can be independently studied in main memory and such that the union of the closed n-sets extracted from each portion is the whole collection of closed n-sets. Thus, n-sets are explored in a depth-first search manner.
Data-Peeler uses a binary enumeration. Each node N in the enumeration tree is a pair (U, V ) where U and V are two n-sets. N represents all the nsets containing all the elements of U and a subset of the elements of V . In other words, this is the search space of n-sets
represents all possible n-sets. In the contrary, nodes such that ∀i ∈ 1 . . . n,
At a node N = (U, V ), Data-Peeler recursively selects an element p from V (see below for the selection criterion) and generates two new nodes
. N L (respectively N R ) represents the nsets of N which contain (resp. do not contain) p.
Example 4. Considering the node E from Example 3, the selection of element 3 of V leads to the two nodes
3.2 Checking C connected It is possible to exploit constraint C connected to reduce the size of V L and then cut down the number of candidates to be considered. Indeed elements of V that cannot be added to U L without violating C connected can be safely removed from V L .
More formally, let
, are such that:
•
j is simply removed from V R and then N R does not contain n-sets with p j anymore. Thanks to this enumeration strategy, all the C connected n-sets are extracted once and only once.
Example 5. In our running example, when enforcing C connected , we eventually obtain
Until now, we discussed how to extract all n-sets satisfying C connected in n-ary relations. We now need to enforce the closeness property.
3.3 Checking C closed We want to exploit the closeness constraint during the enumeration process (i.e., giving rise to safe pruning) rather than in a postprocessing phase. Basically, if there exists an element
is satisfied, then all the nsets represented by N can be extended with p j to form a larger n-set satisfying C connected . In other terms, they are not closed. In that case, the n-sets are closed in the data set formed by elements of U and V but not in the whole data set. Therefore, such a node can be safely pruned.
Thanks to our enumeration strategy, we do not have to check every element of
. Indeed, elements which have been removed from V when applying C connected do not need to be checked. By definition, they have been removed because they cannot be used to form any connected n-set with the elements of U . Only the elements selected and removed during the enumeration, that is to say when N R is built, have to be checked. We use a stack denoted S in which we store such elements.
More formally, the closeness constraint is defined in the node space as follows:
Example 6. Referring to the running example, assuming that γ ∈ S at the node E, then E L satisfies C closed , whereas E R does not (3 can extend it).
Let us now study how Data-Peeler can take advantage of piecewise (anti)-monotonic constraints, i.e., how it can prune a node as early as possible without missing any closed n-set. The idea is to define a new constraint Mod C in the node space such that, for all H represented by N, ¬Mod C (N ) ⇒ ¬C(H). In other words, if a node does not satisfy Mod C then no n-set it represents satisfies C. When V i = ∅, for all i, we have ¬Mod C (N ) ⇔ ¬C(H) Definition 6. Let C a piecewise (anti)-monotonic constraint. Let us denote by M(P C ) (respectively A(P C )) the set of parameters of P C (see Definition 3) which are monotonic (resp. anti-monotonic). By definition, A(P C ) ∪ M(P C ) contains all the parameters of P C .
We can now define Mod(C):
where ∀j = 1 . . . m, P j = U i if parameter P j belongs to M(P C ) and is related to attribute domain i or P j = U i ∪ V i if parameter P j belongs to A(P C ) and is related to attribute domain i.
be two piecewise (anti)-monotonic constraints. The related constraints in the node space are:
In Example 3, Mod C1 (( (A, B), (1, 2) , (C), (4) )) ≡ ♯{A, B, C} × ♯{1, 2, 4} ≥ 10 is false. This node can be safely pruned.
Figure 2 depicts a part of the enumeration tree of Data-Peeler on R E . It illustrates Examples 4 to 6. Every closed 3-set X 1 , X 2 , X 3 satisfying C 5−volume is to be extracted. The bold circled leaf is a closed n-set. The crossed nodes are pruned.
4 Implementation 4.1 Algorithm Data-Peeler is a depth-first search algorithm. It takes two arguments: the current node N and its related stack S. It starts with the root node
and an empty stack S = ∅. Its major steps are presented in Algorithm 1. First of all, the closeness property is checked (see Section 3.3) as well as a user-defined piecewise (anti)-monotonic constraint C (see Section 3.4). If they are both satisfied, either the n-set U is output if there is no element to be enumerated anymore, or the enumeration process keeps going by splitting the current node N into two new nodes. In that case, an element p of V to be enumerated is selected (see below Section 4.2). Then, the two new nodes are built with the function Children(N, p) described in Section 3.2. Finally, DataPeeler(N L , S) and Data-Peeler(N R , S ∪ {p}) are recursively called. Notice here that the stack S of N R now contains p. Indeed, p has been removed from N R by the enumeration and not by the enforcement of C connected .
Input: A node N = (U, V ) and a stack S Output: Closed n-sets satisfying
4.2 Selecting the element to be enumerated As we saw in Section 3.1, an element p of V is selected to proceed with the enumeration, i.e., to generate two new nodes partitioning the original one. Our selection strategy is to select p to maximize the number of elements that C connected enforcement can potentially remove from V . First, Data-Peeler sorts elements of every attribute domain in increasing order w.r.t. their density in R. We use the fact that the less elements are connected in R, the more likely C connected will help to remove elements from the current node during the enumeration phase.
So far, the choice of the attribute domain, on which the element is to be enumerated, remains open. Elements of an attribute may be removed only when (a) they are in V and (b) elements from the n − 1 other attributes are in U . The following formula gives the maximum number of elements of R which are browsed when enforcing C connected after an element from V d is enumerated:
We choose to enumerate an element on the attribute domain d maximizing this formula. The experiment in Section 6.2 empirically shows that the proposed selection criteria outperforms other sensible criteria. 
The size (in bits) of an element ID is denoted a and the size (in bits) of a pointer is denoted b.
5.1 Storing the data set Unlike for binary relation mining algorithms, it is not possible to speed up the extraction by storing for each element e of D 1 , . . . , D n the projection of the input data set R on e (usually called "tidset"). The use of sophisticated data structures like FP-Trees [10] remains an open problem because of the multiple attributes to be considered and the need to use each one during the enumeration.
The whole data set must be stored in main memory to check both C connected and C closed . Two classes of data structures were investigated, namely a bitset-based structure, and a list-based structure.
In both cases, the data set is stored in a complete prefix tree of height n−1 corresponding to the n−1 first attributes. The nodes at depth i ∈ 0 . . . n − 2 always have ♯D i+1 children, one for every element of D n+1 . From depth 0 to n − 2, the edges binding a node to its children are pointers. Each leaf stands for a prefix of size n − 1 of every element of
The difference between the two studied structures resides in how the last attribute elements are stored.
In such a structure, every leaf of the prefix tree points to a bitset representing the last attribute elements. A "0" (respectively "1") in the bitset stands for the absence (respectively the presence) of the related element of R. The presence of such an element is tested in constant time. The space occupied by the data set is: The presence of such an element is tested in O(log ♯D n ). Choosing D n as the smaller attribute domain minimizes the access time.
If d = ♯R Q n i=1 ♯D i denotes the density of the data set, the space requirement is: (size of an integer on modern hardware), the density of the data set must be under 1.56% for the list-based structure to present a space advantage over the bitsetbased structure. Thus, the bitset-based structure is always better in data access time and, in most cases, in space requirement too. Therefore, we choose this structure for our implementation.
Notice that other sparser structures were theoretically investigated. They consist in using an incomplete prefix tree. Of course, the time access cost increases (O( n i=1 log ♯D i ) for a totally sparse tree). Furthermore, the space requirement can be greater since we need to add an element ID to each node. Indeed the child node addressed by a pointer cannot be identified from the position of the child in the list of children (some are "missing"). It can be shown that a space gain occurs only when, in average, a node at depth i has less than b a+b ♯D i+1 children. Unless the data set is very sparse and/or non-homogeneous, even depth n − 2 does not satisfy such a property. This justifies the fact that we focused on the list-based structure where only the deepest level is sparse.
Both U and S can be statically stored. At every recursive call, one single element is pushed in either U (when constructing N L ) or S (when constructing N R ) and popped once this recursive call is completed.
Any element of V can be removed when C connected is enforced. As a result, V cannot be statically stored. The construction of the enumeration tree being depthfirst, the worst case is bound to reaching the deepest node. At worst, the depth of the enumeration tree is n i=1 ♯D i where each recursive call removes only one element from V . In this case, the required space to store V is:
Combining the results from Section 5.1 and Section 5.2, the space complexity of Data-Peeler is:
2 ) if n = 2 (the space requirement for the V set predominates)
6 Experimental Results Every experiment described here has been performed on a GNU/Linux system equipped with a AMD 2600+ processor and 512 Mo of RAM. Data-Peeler is implemented in C++ and compiled with GCC 4.1.2. Every plotted curve uses a logarithmic scale for its time axis.
6.1 Presentation of the data sets 6.1.1 Synthetic data sets To study the behavior of Data-Peeler and to compare it to competitors in different situations, we have used the IBM Quest data generator [3] . Various "basket data"-like data sets with predefined attributes and densities have been generated. Three attributes are considered, namely the customers, the bought items, and the time periods (in months).
To assess the added-value of Data-Peeler both in terms of the relevancy of the extracted n-sets and its performance w.r.t. competitors, we have been working on logs from the DistroWatch.com website. This popular website gathers comprehensive information about GNU/Linux and BSD distributions. Every distribution being described on a separate page, a visitor loading such a page is considered "interested" in the distribution. Every IP address is analyzed to identify the country it comes from. Timestamps allow to study the evolution of the interest granted to the different distributions along time. The whole data set gathers 36 months, 243 countries and 538 distributions. Two different data sets have been derived from it. In both cases, data have been normalized so that every country and every time period has the same importance. They have been transformed in 0/1 data in the following way: for each distribution, we kept the elements of R containing this distribution and such that its normalized interest exceeds a threshold equals to one quarter of the maximal normalized interest for this distribution in R.
6.2 Impact of the enumeration strategy Let us first empirically compare the enumeration strategy presented in Section 4.2 with two other sensible strategies. For each node (U, V ),
• the enumerated attribute j is chosen such that it has the smallest non-empty ♯V j . Among all the elements of V j , the element with the smallest density in R is selected.
• the enumerated element
The first strategy enumerates every element of the n − 2 attributes with the smallest cardinalities. Then, when enumerating elements from the two remaining attributes, C connected may finally succeed in reducing the V set.
The second strategy globally sorts the elements of the attributes altogether. If every attribute domain has the same cardinality, this order follows a growing density. Otherwise, an element p j from a small attribute domain size is usually preferred since
n is larger. Tests have been performed on the data sets generated by the Quest data generator. Whereas the chosen enumeration strategy of Data-Peeler scales very well, the other strategies force us to choose small size attributes to be able to plot results: 36 customers buying in average 6 items out of 18 (density of about 33%) per month. The number of months has been varying from 6 to 36 and we enforced the constraint that every closed 3-set had to contain at least 3 customers, 2 items and 3 months.
Results are represented in Figure 3 . The enumeration strategy of Data-Peeler largely outperforms the two other strategies. The performances of Enumeration 1 mainly depend on the size of the smallest attribute domain (above 18 months, the smallest attribute domain becomes the set of items which is constant). This behavior, as mentioned earlier, is due to the complete enumeration of the smallest domain. The performance of Enumeration 2 emphasizes the need, when selecting the element to be enumerated, to take into account the characteristics of the current node.
DataPeeler is compared with both CubeMiner [12] and Trias [11] on 3-ary relations. We have been using the implementations provided by their respective authors.
Comparisons with CubeMiner and Trias are achieved on synthetic data sets provided by the Quest data generator. 144 customers buying in average 6 items out of 72 (density of about 8.3%) per month have been generated. We make the number of months vary from 6 to 66 and we constrain every closed 3-set to involve at least 2 customers, 2 items and 2 months.
The results are represented in Figure 4 . DataPeeler outperforms its competitors by several orders of magnitude. The growing number of months (the smallest domain) significantly alters the performance of Trias, whereas it has less effect on CubeMiner.
For example, considering data along 48 months, to extract all the 5801 closed n-sets, CubeMiner takes 1 hour and 50 minutes, Trias 3 hours and 14 minutes, whereas Data-Peeler only needs 2.5 seconds. Unlike its competitors, even with 600 months, Data-Peeler is still able to extract all closed n-sets in a reasonable time, i.e., 1 minute and 21 seconds for 431892 closed n-sets. 
A ternary relation has been derived from the logs of DistroWatch.com. It indicates, month after month, whether visitors from a country look interested in a distribution. All data (36 months, 243 countries and 538 distributions) have been kept. Compared to the synthetic data sets considered above, this one is relatively large even if it has one small attribute domain. It is also much sparser since its density is 0.55%. We have constrained every closed 3-set to involve at least 2 countries and 2 distributions. The minimal number of months a closed 3-set must contain has been varying from 0 to 36. Results are represented in Figure 5 . Data-Peeler outperforms its competitors by several orders of magnitude. Thanks to the small number of months in the data set, Trias succeeds in extracting the closed 3-sets even without any constraint on the time attribute. CubeMiner suffers a lot from the global size of the data set. It is unable to perform the extraction under a size constraint of 33/36 months.
With a requirement of at least 6 months out of 36, Data-Peeler needs 2.6 seconds and Trias 69.3 seconds to extract all the 87 patterns.
Without any minimal size constraint on the number of months, 10658 closed n-sets are computed in 4.4 seconds by Data-Peeler and in 1531 seconds by Trias. In both cases, CubeMiner can not perform the task. 6). This relation covers 1.7% of the possible associations between attributes. We aim at extracting all the maximal sets of distributions which are simultaneously interesting for people from a maximal set countries during a maximal set of semesters. To obtain them, we need to extract the closed 4-sets
This constraint is anti-monotonic. However, handling it by a modification of the enumeration strategy is much more efficient: whenever a distribution is chosen to be enumerated (either moved from V to U or from V to S), the same distribution in the other domain is moved in the same manner. In the following, all the extracted n-sets will satisfy this constraint.
Data-Peeler extracts every closed 4-set from the data set in 229 seconds. Since it is impossible to inspect all the 602290 closed 4-sets, minimal sizes are enforced on every set: we constrain every closed 4-set to involve at least 2 semesters, 2 countries and 3 distributions. After 20 seconds, Data-Peeler provides 17196 closed 4-sets. Again, it prevents from a systematic interpretation of each closed 4-set.
Therefore, we have enforced a volume constraint to keep only the largest patterns. We considered the new constraint
. It is a generalization of a minimal volume constraint where every attribute is weighted w.r.t. its cardinality. The function α is such that elements from an attribute domain which is twice smaller will "count" twice more in the weighted volume. C v−W eighted V olume is anti-monotonic. It reduces the computation to 14 seconds and the number of extracted closed 4-sets to 352.
Given such a collection of 352 patterns, it has been possible to manually inspect them and assess their relevancy.
• 94 closed 4-sets have on the distribution domains a subset of {Fedora, FreeBSD, Debian, Ubuntu, Gentoo, MEPIS, Slackware, Yellow Dog, Mandriva, openSUSE}. Considering all of them, every semester is mentioned. All these distributions are mainstream general-purpose distributions. These closed 4-sets involve many countries all over the world. However Great Britain shows off by being present in all these n-sets but one.
• 64 closed 4-sets have on the distribution domains a subset of {Astaro, ClarkConnect, IPCop, m0n0wall, Devil, SmoothWall, CensorNet}. Every semester is involved. These seven distributions are meant to serve a common interest: they are all specifically designed to act as a firewall. These closed 4-sets involve countries from every continent. Australia (closely followed by Belgium) is the most present country.
• 80 closed 4-sets have on the distribution domains a subset of {dyne:bolic, ArtistX, AGNULA, MoviX, GeeXboX}. Every semester is involved. These five distributions are meant to serve a common interest: they are all specifically designed to manipulate movies and music. These 4-sets mainly contain occidental countries but India is very present too. Switzerland belongs to all these 4-sets. GNU/Linux is obviously a popular choice among Swiss artists.
• 83 closed 4-sets have on the distribution domains a subset of {dyne:bolic, ArtistX, AGNULA, MoviX, GeeXboX} ∪ {Ubuntu, Damn Small, KNOPPIX, MEPIS, PCLinuxOS, Xandros}. It could be seen as a "collision" between two separate interests. Nevertheless, the distributions from the second set being primarily designed for desktop use, they are also suited to play movies and music. Furthermore, every distribution from these two sets uses the APT package management system (if any).
The few remaining closed 4-sets are interesting too. Among the distributions which appear once in the returned closed 4-sets, Gentoox and GentooTH form with Gentoo a closed 4-set running along the last four semesters (GentooTH did not exist before) in 11 countries. Their common point is obvious from their names: they are all based on Gentoo.
