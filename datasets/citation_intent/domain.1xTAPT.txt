. Illustration of the confidence sets.
optimizing variable and ξ is the unknown parameter, distributionally robust optimization solves max x∈X [inf μ∈C E ξ∼μ u(x, ξ)], where C is an a priori known set of distributions.
We highlight our contributions by comparing with [1] . In [1] the state-wise ambiguity set is restricted to the following form:C s = {μ s |μ s (O i s ) ≥ α i s ∀ i = 1, . . . , n s }, where α i s ≤ α j s and O i s is a proper set of uncertain parameters with a "nested-set" structure, i.e., satisfying O i s ⊆ O j s , for all i < j [see Fig. 1(a) ]. This setup can effectively model distributions with a single mode (such as a Gaussian distribution), but less so when modeling multi-mode distributions such as a mixture Gaussian distribution. Moreover, other probabilistic information such as mean, variance etc. cannot be incorporated. Thus, in this technical note, we extend the distributionally robust MDP approach to handle ambiguity sets with more general structures. In particular, we consider a class of ambiguity sets, first proposed in [18] as a unifying framework for modeling and solving distributionally robust single-stage optimization problems, and embed them into the distributionally robust MDPs setup. These ambiguity sets are considerably more general: they are characterized by a class of O i s which can either be nested or disjoint [as shown in Fig. 1(b) ], and moreover, additional linear constraints are allowed to define the ambiguity set, which can be used to incorporate probabilistic information such as mean, covariance or other variation measures. We show that, under this more general class of ambiguity sets, the resulting distributionally robust MDPs remain tractable under mild technical conditions, and often outperform previous methods thanks to the fact that it can model uncertainty in a more flexible way.
Throughout the technical note, we use capital letters to denote matrices, and bold face letters to denote column vectors. We use e i (m) to denote the ith elementary vector of length m, and use R n + to denote the nonnegative orthant of R n . If C is the set of joint probability distributions of three random vectors a, b, and c, then (a,b) C denotes the set of marginal distributions of (a, b). We use ⊕ to represent mixture distribution: given two probability distributions F 1 , F 2 and a Bernoulli random variable x which takes value 1 w.p. p, xF 1 ⊕ (1 − x)F 2 is a random variable such that it follows distribution F 1 w.p. p, and follows F 2 w.p. 1 − p. We use N (m, σ 2 ) to represent a Gaussian distribution with mean m and variance σ 2 .
A (finite) Markov Decision Process (MDP) is defined as a 6-tuple T, γ, S, A, p, r . Here, T is the (possibly infinite) decision horizon; 0018-9286 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
γ ∈ (0, 1] is the discount factor; S is the state set and A s is the action set of state s ∈ S, both assumed to be finite. The parameter p and r are the transition probability and the expected reward, respectively. That is, for s ∈ S and a ∈ A s , r(s, a) is the expected reward and p(s |s, a) is the probability that the next state is s . Following [2] , we denote the set of all history-dependent randomized strategies by Π HR . We use subscript s to denote the value associated with the state s: e.g., r s denotes the vector form of the rewards associated with the state s, and π s is the (randomized) action chosen at state s for strategy π.
The elements in the vector p s are listed in the following way: the transition probabilities of the same action are arranged in the same block, and inside each block they are listed according to the order of the next state. We use s to denote the (random) state following s, and Δ(s) to denote the probability simplex on A s . We use to represent Cartesian product, e.g., p = s∈S p s . For a given strategy π ∈ Π HR , we denote the expected (discounted) total-reward under parameters pair (p, r) as u(π, p, r)
A Distributionally Ambiguous MDP (DAMDP) is defined as a tuple T, γ, S, A,C S , where the transition probability p and the expected reward r are unknown. Instead, they are assumed to obey a joint distribution μ 0 (also unknown) that belongs to a known ambiguity set
While the DAMDP framework can be very general, mostC S result in formulations that are computationally intractable (e.g., [1] , [19] ). Hence, we make the following requirement ofC S such that the parameters among different states are independent.
Assumption 1: The ambiguity setC S has the following property:
where "state-wise ambiguity set"C s is a set of distributions of parameters of state s. By the definition ofC S , the state-wise property applies to C S as well. This property is the same as the concept of "s-rectangularity" in [16] , and is essential for reducing DAMDP to robust MDP in Lemma 1. In addition, [20] showed that the robust MDP with coupled uncertainty sets is computationally challenging, which implies solving DAMDP with nonrectangular ambiguity sets is even harder.
We now discuss the admissible state-wise ambiguity set. Our formulation of the state-wise ambiguity set follows the unifying framework of [18] . In specific, given s ∈ S, the state-wise ambiguity set is representable with the following standard form:
are the lower and upper bounds of the probability that parameters belong to a confidence set. Thus, each confidence set O i s provides an estimation of the uncertain parameters pair (p s , r s ,ũ s ) subject to a different confidence level. Ambiguity setsC s contain prescribed conic representable confidence sets and mean values residing on an affine manifold, which is rich enough to encompass and extend several ambiguity sets considered in recent literature (e.g., [1] , [19] , [21] ). The set of joint distribution of (p s , r s ) is hence C s Δ = (ps ,rs)C s . Notice that a classical technique called "lifting" is used here: We introduce an auxiliary random vectorũ, so that some non-linear relationship can be modeled linearly. For example, a constraint on the variance can be modeled using this standard form (see [22, Example 2] ), which is otherwise impossible without the auxiliary variable. This lifting technique thus allows us to model a rich variety of structural information about the marginal distribution of (p, r) in a unified manner. Note when the ambiguity set only contains the support of random variables, i.e.,
where the a-priori information of unknown parameters is that they belong to an uncertainty set.
Assumptions 2 to 4 are standard requirements for the confidence sets, proposed in [18] . The first one asserts the relationship between different confidence sets.
The nesting condition is illustrated in Fig. 1(b) . Next, for any s ∈ S we require thatC s satisfies the following regularity condition.
1) The confidence set O ns s is bounded and has probability one, that is, 
s are proper cones (i.e., a closed, convex and pointed cone with nonempty interior).
This section focuses on DAMDP with a finite number of decision stages. We show that a strategy defined through backward induction, which we call S-robust strategy, is distributionally robust. We further show such a strategy is solvable in polynomial time under mild technical conditions. This generalizes results in [1] to a significantly more general class of ambiguity sets.
Similar to [10] , we assume that when a state is visited multiple times, each time it can take a different parameter realization (nonstationary model). This assumption is justified mainly because the stationary model is generally intractable and a lower-bound of it is given by the non-stationary model. Therefore, multiple visits to a state can be treated as visiting different states. By introducing dummy states as in [1, Assumption 2.2], for finite horizon DAMDP we make the following assumption without loss of generality. This will simplify our exposition.
Assumption 5: 1) Each state belongs to only one stage.
2) The terminal reward equals zero.
3) The first stage only contains one state s ini .
Using the condition 1 of Assumption 5, we partition S according to the stage each state belongs to. That is, we let S t be the set of states belong to tth stage.
For π ∈ Π HR and μ ∈ C S , we denote the expected performance of a DAMDP as w π, μ, (s ini ) Δ = E (p,r)∼μ {u(π, p, r)} = u(π, p, r)dμ(p, r).
In words, each strategy is evaluated by its expected performance under the (respective) most adversarial distribution of the uncertain parameters, and a distributionally robust strategy is the optimal strategy according to this metric. The main focus of this section is deriving approaches to solve the distributionally robust strategy. To this end, we need the following definition.
Definition 2: Given a DAMDP T, γ, S, A,C S , we define the Srobust strategy as follows
3) A strategyπ * is a Srobust strategy if ∀ s ∈ S, and every history h that ends at s, we haveπ * s , conditioned on history h, is a S-robust action.
The definition requires that the strategy must be robust w.r.t. each sub-problem, and hence the name "S-robust." The following theorem shows any S-robust strategy π * is distributionally robust, and is the main result of this technical note.
Theorem 1: Let T < ∞. Under Assumptions 1, 2, 4, and 5, if π * is a S-robust strategy, then 1) π * is a distributionally robust strategy with respect to C S . 2) There exists μ * ∈ C s such that (π * , μ * ) is a saddle point. That is
Proof: We first state a Lemma from [1, Lemma 3.2] without proof.
Lemma 1: Under Assumption 1, fix π ∈ Π HR and μ ∈ C S , denote p = E μ (p) and r = E μ (r). We have w(π, μ, (s ini )) = u(π, p, r).
Lemma 1 means for any strategy, the expected performance under an admissible distribution μ only depends on the expected value of parameters under μ. Thus, the distributionally robust MDPs reduce to robust MDPs. Next we characterize the set of expected value of the parameters.
Lemma 2: For s ∈ S and π s ∈ Δ(s), we define the set Z s = {E μs (p s , r s )|μ s ∈ C s }. Then set Z s is convex and compact.
Proof: First, we show that, for s ∈ S and π s ∈ Δ(s), the set defined asZ s = {E μs (p s , r s ,ũ s )|μ s ∈C s } is convex and compact. The convexity can be easily shown, which is omitted due to space constraints (see [22] for details). To show the compactness, notice thatC s is weakly closed (i.e., closed w.r.t. to the weak topology) since the feasible set of each of constraint is weakly closed which implies their intersection is also weakly closed. Thus,Z s is closed since it is the image ofC s under expectation (which is a continuous function). This impliesZ s is compact since O ns s is bounded and henceZ s is bounded. Finally, since Z s is the projection onto the first two coordinates of set Z s , its convexity and compactness thus follow.
Lemma 2 implies that, for s ∈ S and π s ∈ Δ(s), there exists (p * s , r * s ) ∈ Z s that satisfies inf (ps,rs)∈Zs u(π s , p s , r s ) = u(π s , p * s , r * s ). Since saddle point of the minimax objective exists for robust MDPs (e.g., [10] , [11] ), we can complete the proof of part 2) following a similar procedure as the last portion of proof for [1, Theorem 3.1]. We omit the details due to space constraint (see [22] for details). Part 1) then follows part 2) immediately.
We now investigate the computational aspect of finding the S-robust action.
Theorem 2: Under Assumption 2, 3, 4, and 5, for s ∈ S t where t < T , the S-robust action is the optimal solution of the following optimization problem (termed Srobust problem hereafter): Proof: The proof essentially follows from [18] and duality of convex optimization [23] , and can be found in the longer version [22] of this technical note.
Thus, since for s ∈ S t , Δ(s) is compact, we can solve the S-robust action in polynomial time if all K i s are "easy" cones such as linear, conic quadratic or semidefinite cones. Moreover, using Theorem 1, by backward induction, we can obtain the S-robust strategy efficiently.
By virtue of the lifting technique [18, Theorem 5], we show below several widely used ambiguity sets are indeed special cases ofC s defined in (1) . We further derive their corresponding S-robust problems. See [22] for additional examples (variance and expected Huber loss function). 
This example can also be treated via "classical" robust optimization by virtue of Lemma 1.
The finite horizon DAMDP can be easily extended to discountedreward infinite horizon setup. We can generalize the notion of S-robust strategy, which turns to be distributionally robust in both stationary and non-stationary models. This extension is similar to [1] and can be found in [22] .
In this section, we study two synthetic numerical examples: a machine replacement problem and a path planning problem. In the machine replacement problem, the reward parameters are uncertain; whereas in the path planning problem, the transition probabilities are uncertain. All results were generated on desktop with Intel Core i5-3570 CPU of 3.40 GHz clock speed and 8 GB RAM. The S-robust problems are solved in Matlab using the CVX package [24] .
We consider a machine replacement problem similar to the one in [12] . Consider the repair cost incurred by a factory that holds a large number of machines, given that each of these machines is modeled with a same underlying MDP for which rewards are subject to uncertainty.
We first consider a machine replacement problem with 50 states, 2 actions ("repair" and "not repair") for each state, deterministic transitions, a discount factor of 0.8, and uncertain rewards following Gaussian distributions independently [see Fig. 2(a) ]: For the first 48 states, the "repair" action has a cost N (130, 1) . The 49th and 50th states of the machine's life are designed to be risky: not repairing at state 50 incurs a highly uncertain cost N (100, 800), while repairing at both states is a more secure but still uncertain option with a cost N (130, 10) . The detailed implementation is as follows: We use the mean value of uncertain rewards to compute the nominal strategy. For both robust and distributionally robust strategy, we construct confidence sets usinĝ m ± 3σ for the first 49 states, andm ± 4σ for state 50 wherem andσ 2 are mean and variance estimated from samples (see [22] for details), as it is more risky and thus hard to estimate. In addition, we construct an extra confidence set (centered at the mean) with 60%-70% confidence level (i.e., α 1 50 = 0.6, α 1 50 = 0.7) for distributionally robust strategy. The optimal paths followed by three strategies are shown in Fig. 2(a) .
The performance of the strategies obtained by using the nominal, the robust and the distributionally robust approaches is presented in Fig. 3 . The corresponding average total discounted rewards and computational times are shown in Table I . The nominal strategy results in the highest average total discounted rewards. This is well expected as we are using the exact mean value of the reward as the nominal Fig. 2 . Two instances of a machine replacement problem. Fig. 2(a) shows Gaussian uncertainty in the rewards, while Fig. 2(b) shows mixed Gaussian uncertainty in the rewards. parameter. However, the nominal strategy is highly risky: it cannot prevent bad performance (e.g., −0.025) from happening, which is undesirable. While the nominal strategy, blind to any form of risk, finds no advantage in ever repairing, the robust strategy ends up following a highly conservative policy (repairing the machine at state 49 to avoid state 50). In contrast, the distributionally robust optimal strategy makes use of more distributional information and handles the risk efficiently by waiting until state 50 and then repair the machine. Therefore, this strategy beats the nominal and robust strategies in that it strikes a good tradeoff between high mean reward and low variance over 10,000 different trials. These results coincide with what one would typically expect from the three solution concepts. Fig. 4 . Illustration of the confidence sets for two distributionally robust strategies.
The second experiment has a similar setup as the previous one, except that not repairing at the 50th state has a reward which follows a mixed Gaussian distribution [see Fig. 2(b) ]. This experiment illustrates the effect of the two different nested-set structures shown in Fig. 1 . In specific, we apply the two different distributionally robust approaches (proposed in [1] and this technical note respectively), and show that our method outperforms. The detailed implementation is as follows: For the robust and two distributionally robust strategies, we construct uncertainty set corresponding to 99% probability support of the rewards for the first 49 states, and 99.9% for the 50th state that is more risky, using estimated mean and variance (see [22] for details). For the first distributionally robust strategy proposed in [1] , we construct two additional nested confidence sets O 1 50 and O 2 50 [see Fig. 4(a) ], which w.p. 40%-50% and 60%-70% respectively the uncertain rewards belong to. In contrast, for the second distributionally robust strategy proposed in this technical note, we construct two disjoint confidence sets O 1 50 and O 2 50 [see Fig. 4 (b)] with 70%-80% and 0%-10% confidence level, respectively. Specifically, we select these two intervals around the peaks of the two Gaussian elements [i.e., N (100, 10) and N (140, 2)] to better model this mixed distribution. The optimal paths followed for the three strategies are shown in Fig. 2(b) .
The performance of the three strategies obtained is presented in Fig. 5 . The corresponding average total discounted rewards and computational times are shown in Table II . As expected, the robust strategy ends up following a highly conservative policy repairing the machine at state 49 to avoid state 50. The first distributionally robust strategy, not modeling the mixture Gaussian distribution well, finds it advantageous to repair at the 50th state. In contrast, capable of capturing the distribution information in a more flexible way, the second distributionally robust strategy better models the uncertainty and finds not repairing the machine at state 50 is optimal. The performance comparison clearly shows the second distributionally robust strategy is more desirable, which highlights the distributionally robust approach with general structure of confidence sets can be beneficial in practice.
We remark that, in practice, one can obtain the modality structure of uncertain parameters in a data-driven way by applying clustering algorithms to an initial primitive data set. For example, one may check the histogram of historical observations. If the data concentrates on several distinct and disjoint bins, our multi-model DAMDP approach can be applied. Moreover, we note that networked control systems (NCSs) have recently emerged as a topic of significant interest in the control community. A typical application of NCSs is in modern [25] and [26] proposed a novel two-layer structure to solve the setpoints compensation problem for industrial processes under network-based environment.
We now consider a path planning problem, similar to the one presented in [1] : an agent wants to exit a 4 × 21 maze [shown in Fig. 6(a) ] using the least possible time. Starting from the upper-left corner, the agent can move up, down, left and right, but can only exit the grid at the lower-right corner. Here, a white box stands for a normal place where the agent needs one time unit to pass through. A shaded box represents a "shaky" place: if an agent reaches a "shaky" place, then he may risk jumping to the starting point ("reboot"). The true transition probability of the jump follows a distribution
The four approaches are implemented as follows: The nominal approach neglects this random jump. The robust approach takes a worst-case analysis, i.e., it assumes that with 30%, the whole probability support of transition, the agent will jump to the spot with the highest costto-go. The first distributionally robust approach takes into account an additional information by using two nested confidence sets: the jump probability parameter belonging to 9%-11% is of a confidence 1 − λ. The second distributionally robust approach, which is proposed in this technical note, incorporates more information. In specific, we construct an extra confidence interval disjoint with the above 9%-11% interval. It states that the chance of jumping with probability 20% is λ.
The performance of strategies of the nominal, the robust and the two distributionally robust approaches is shown in Fig. 6(b) , where the error bars show the standard error of the expected time to exit. The CPU times of computing optimal policies for four strategies are 0.461, 549, 642, and 654 seconds, respectively. The second distributionally robust approach achieves the best performance over virtually the whole spectrum of λ. This is well expected, since additional probabilistic Fig. 6(a) illustrates the maze for the path plawnning problem. Fig. 6(b) shows the performance comparisons between nominal, robust and two distributionally robust strategies over 3,000 runs of the path planning problem.
information is available to and incorporated by the second distributionally robust approach which considers ambiguity sets with more general structures.
In this technical note, we considered Markov decision problems with uncertainty. Specifically, we generalized the distributionally robust approach proposed in [1] to incorporate more general ambiguity sets proposed in [18] to model a-priori probabilistic information of the uncertain parameters. We proposed a way to compute the distributionally robust strategy through a Bellman type backward induction. We showed that the strategy, which achieves maximum expected utility under the worst admissible distributions of uncertain parameters, can be solved in polynomial time under some mild technical conditions. We believe that many important problems that are usually addressed using standard MDP models could be revisited and better resolved using the proposed models when parameter uncertainty exists, as this formulation naturally enables the decision maker to account for more general parameter uncertainty.
The ability to explore unknown spaces independently, safely and efficiently is a combined product of motor, sensory, and cognitive skills. Normal exercise of this ability directly affects an individual's quality of life. Mental mapping of spaces and of the possible paths for navigating these spaces is essential for the development of efficient orientation and mobility (O&M) skills. Most of the information required for this mental mapping is gathered through the visual channel (Lynch, 1960) . People who are blind lack this information, and in consequence they are required to use compensatory sensorial channels and alternative exploration methods (Jacobson, 1993) . This research is based on the assumption that the supply of appropriate spatial information through compensatory sensorial channels, as an alternative to the (impaired) visual channel, may help to enhance the ability of people who are blind to explore unknown environments (Mioduser, in press) .
The research on the exploration process of known and unknown spaces by people who are blind refers to the use of both low and high technologies. These technologies serve as alternative sensorial or cognitive channels to the impaired visual channel. There are two types of informationtechnology devices: (a) passive devices -providing the user with information before her/his arrival to the environment (e.g., verbal description, tactile maps and physical models) and (b) dynamic devices -providing the user with information in-situ (e.g., Sonicguide, Kaspa, Talking Signs and Personal Guidance System). Ungar, Blades and Spencer, (1996) report on differences in exploration performance of people who are blind using various technologies (e.g., verbal description, tactile maps and physical models). Warren and Strelow (1985) studied the use of the Sonic-guide device and Easton and Bentzen (1999) focused on the users' ability to navigate using the Kaspa laser-guided device. Additional examples of O&M support under study are the talking signs embedded in the environment (Crandall, Bentzen, Myers, & Mitchell, 1995) , and the global positioning system (GPS), based on satellite communication (Golledge, Klatzky, & Loomis, 1996) .
Research on mobility in known and unknown spaces by people who are blind (Golledge, Klatzky, & Loomis, 1996; Ungar, Blades, & Spencer, 1996) , indicates that support for the acquisition of spatial mapping and orientation skills should be supplied at two main levels, perceptual and conceptual. At the perceptual level, hearing, smell, and touch are powerful information suppliers about known as well as unknown spaces. The auditory channel supplies essential information about events, or the presence of other people (or machines or animals) in the environment. In indoor spaces
ORLY LAHAV DAVID MIODUSER Tel Aviv University, School of Education Exploration of unknown spaces is essential for the development of efficient orientation and mobility skills. Most of the information required for the exploration is gathered through the visual channel. People who are blind lack this crucial information, facing in consequence difficulties in mapping as well as navigating spaces. This study is based on the assumption that the supply of appropriate spatial information through compensatory sensorial channels may contribute to the spatial performance of people who are blind. The main goals of this study were (a) the development of a haptic virtual environment enabling people who are blind to explore unknown spaces and (b) the study of the exploration process of these spaces by people who are blind. Participants were 31 people who are blind: 21 in the experimental group exploring a new space using a multi-sensory virtual environment, and 10 in the control group directly exploring the real new space. The results of the study showed that the participants in the experimental group mastered the navigation of the unknown virtual space in a short time. Significant differences were found concerning the use of exploration strategies, methods, and processes by participants working with the multi-sensory virtual environment, in comparison with those working in the real space.
people who are blind can use echo feedback (i.e., by whistling, clapping hands, or talking) to estimate distances (Hill, Rieser, Hill, Hill, Halpin & Halpin, 1993) . The smell channel supplies additional information about particular situations (e.g., perfumery, bookstore, or bakery in a shopping center) or about people. Haptic information appears to be of great potential for supporting appropriate spatial performance. Fritz, Way, and Barner (1996) define haptics as encompassing touch along with kinesthetic information, or a sense of position, motion, or force. For people who are blind, haptic information is commonly supplied by the cane for lowresolution scanning of the immediate surroundings, by palms and fingers for fine recognition of objects form, texture and location, and by the feet regarding surface information.
As for the conceptual level, the focus is on supporting the development of appropriate strategies for the efficient exploration of the space and the generation of efficient navigation paths. For example, Jacobson (1993) , described indoor environment familiarization process by people who are blind as one that starts with the use of a perimeterrecognition-tactic -walking along the room's walls and exploring objects attached to the walls, followed by a gridscanning tactic, aiming to explore the room's interior.
Advanced computer technology offers new possibilities for supporting acquisition of orientation and mobility (O&M) skills by people who are blind, and the development of alternative navigation strategies, at both the perceptual and conceptual levels. Current virtual reality (VR) technology facilitates the development of rich virtual models of physical environments and objects to be manipulated, offering people who are blind the possibility to undergo learning or rehabilitation processes without the usual constraints of time, space, and a massive demand of human tutoring (Loomis, Klatzky & Golledge, 2001; Schultheis & Rizzo, 2001; Standen, Brown & Cromby, 2001) . Research on the implementation of haptic technologies within VR spatial simulation environments reports on its potential for supporting rehabilitation training with sighted people (Darken & Banker, 1998; Darken & Peterson, 2002; Waller, Hunt & Knapp, 1998; Witmer, Bailey, Knerr & Parsons 1996) , and perception of virtual textures and objects by people who are blind (Colwell, Petrie, & Kornbrot, 1998; Jansson, Fanger, Konig, & Billberger, 1998; ) .
The research reported in this paper follows the assumption that the supply (via technology) of compensatory perceptual and conceptual information may contribute to effective acquaintance with unknown environments by people who are blind. This approach differs from previous research lines and practices in several ways. First, it integrates existing knowledge from different disciplines (namely O&M, learning processes by people who are blind, virtual environments and haptic devices R&D) into a common conceptual framework for the study of O&M skills acquisition using technology. At an additional level it deals with two main drawbacks of technologies currently in use: (a) the need for prerequisite knowledge about the space to be navigated (e.g., the talking signs or GPS systems) and (b) the lack of appropriate resolution of the information supplied about the unknown space (e.g., verbal descriptions or tactile maps). The virtual tool used in this study supplies all required prerequisite knowledge, at a resolution compatible with the features of the simulated environment. To examine the above assumption we developed a multi-sensory virtual environment (MVE) and studied the exploration process of an unknown space by subjects who are blind using the MVE. Their performance was compared to that of a control group of people who are blind who explored the real environment simulated in the MVE. The main research questions of this study were:
1. What exploration strategies do people who are blind use working with the MVE, in comparison to those used by people whom are blind working directly in the real environment? 2. What characterizes the exploration processes used by people whom are blind working with the MVE, in comparison to the exploration processes used by people whom are blind working in the real environment? 3. What information collection and storage did participants use, in both the experimental and control groups?
The MVE prototype developed for this study comprised two modes of operation: (a) developer/teacher mode and (b) learning mode. The core component of the developer/teacher mode was the virtual environment editor, which included three tools: (a) a 3D environment builder, (b) a force-feedback effects editor, and (c) an audio feedback editor (see Figure 1) . By using the 3D-environment editor the developer can define the physical characteristics of the space, e.g., size and shape of the room, or type and size of the objects (i.e., doors, windows and furniture). Using the force feedback effects (FFE) editor the developer was able to attach haptic effects to all objects in the environment. Examples of FFE's were vibrations or attraction/rejection fields surrounding objects. The audio editor allowed the attachment of three kinds of auditory feedback to the objects: (a) labels (e.g., bird chirps) as representative for the windows, (b) explicit names (e.g., first door or second cube), and (c) a guiding agent reporting on features of the objects (e.g., the proximity of corners or required turns). All environments used in this study were composed by the researchers using the system editing tools.
In the learning mode, the users navigated the environment by means of the force feedback joystick (FFJ). While walking via the FFJ they interacted with the simulated space components (i.e., they perceived the form, dimensions, and relative location of objects; or identified the structural configuration of the room including presence and location of walls, doors, and windows. As part of these interactions the users got haptic feedback through the FFJ along with audio feedback. Figure 2 shows the user-interface screen. The red circles indicate the hot spots that triggered the guiding agent's intervention.
Several additional features were offered to the teachers during and after the learning session. Monitoring frames, for example, presented updated information on the user's navigation performance (e.g., position or objects already reached). Another feature allowed the recording of the user's navigation path and its replay for analysis and evaluation purposes, as shown in Figure 3 .
The study included 31 participants who were selected on the basis of the following seven criteria: (a) total blindness, (b) minimum of 12 years old, (c) not multi-handicapped, (d) received O&M training, (e) Hebrew speaker, (f) onset of blindness at least two years prior to the experimental period, and (g) comfortable with the use of computers. The participant age range was 12-70 years old. We defined two groups that were similar in gender, age and age of vision loss (i.e., congenitally blind or late blind. The experimental group included 21 participants who explored the unknown space by means of the MVE, and the control group had 10 participants who explored the real unknown space (see Table 1 ).
To evaluate the participants' initial O&M skills, all completed a questionnaire on O&M issues. The questionnaire results showed no differences in initial ability among both groups' participants.
The independent variable in this study was the type of environment (i.e., the multi-sensory virtual environment (MVE) and the real environment.
Three groups of dependent variables were defined, concerning (a) exploration strategies, (b) characteristics of the exploration process, and (c) the use of information and storage aids during the exploration.
Variables related to the exploration strategies included: 1. Exploration strategies -alternative strategies used by the subjects in their navigation: the "perimeter" strategywalking along a room's walls (see Figure 4 , Route 1); the "grid" strategy -exploring a room's interior by scanning the room (see Figure 4 , Route 2); the "object-to-object" strategywalking from one object to another (see Figure 4 , Route 3); the "points-of-references" strategy -walking in the environment and creating landmarks (see Figure 4 , Route 4), or other (new) strategies.
2. Frequency -the number of times each strategy was implemented during the exploration.
3. Distance traversed -distance traversed using each strategy.
Variables related to the characteristics of the exploration process:
1. Total duration -the total time spent accomplishing the task.
2. Total distance -the total distance traversed. 3. Strategy-switch -the frequency of strategy changes during the exploration task.
4. Sequence -the first sequence of two strategies used in the exploration (e.g., pattern strategy first then grid strategy).
5. Stops -the number of pauses made during the exploration. Two values were defined: short pauses (4-10 seconds) introduced for technical purposes (e.g., changing the hand that holds the joystick) and long pauses (more then 10 seconds) supposedly used for cognitive processing (e.g., memorization or planning).
Variables related to the use of information and storage aids included:
1. Aids -use of aids of two types: measurement aids (e.g., counting steps or using echo feedback) and informationretaining-aids (e.g., producing a verbal reconstruction of landmarks or using metaphors).
The main instruments used in the study were: 1. The unknown space -the space to be explored, both as real physical space and as virtual representation in the MVE (see Figures 5-6 ). The space was a 54 square meters room with three doors, six windows and two columns. There were seven objects in the room, five of them attached to the walls and two placed in the inner space.
2. Exploration task -each participant was asked individually to explore the room, without time limitations. The experimenters informed the participants that they would be asked to describe the room and its components at the end of their exploration.
In addition a set of three instruments was developed for the collection of quantitative and qualitative data:
1. Orientation and mobility (O&M) questionnairecomprising 46 questions concerning the participants O&M ability indoors and outdoors, in known and unknown environments. Most of the questions were taken from O&M rehabilitation evaluation instruments (e.g., Dodson-Burk & Hill, 1989; Sonn, Tornquist & Svensson, 1999) . The O&M questionnaire included four parts: (a) 19 descriptive questions (e.g., age; gender; age of vision loss); (b) 8 questions on the subject's O&M ability in known indoor environments (e.g., home; school; work; etc); (c) 12 questions about the subject's O&M ability in known outdoor environments (e.g., street crossing; using public transportation; walking in shopping centers; etc); (d) 7 questions on subject's O&M ability in unknown indoor environments (e.g., what are the O&M devices you use in unknown indoor environments?; next week you are going to move to a new office or classroom. You will be visiting the new place today. What do you need to do to ensure yourself appropriate orientation in the new space next time?). Among the questions 23 O&M-related questions were answered in a four-level ability scale: (i) I cannot do the task, (ii) I need assistance from a sighted person, (iii) I need to use an O&M device, (iv) I can do the task independently.
2. Observations were video-recorded -the participant's exploration was video-recorded during the task. The information from these recordings was combined with the computer recording.
3. Computer recording -The computer's recording data enabled the researchers to track the user's exploration in the MVE, in two ways: through a data log and through a film. This enabled the researcher to collect information about users' exploration strategies, distances, total duration, switches of strategies and stops (see Figure 3) .
Two data evaluation and coding schemas were developed, one for the participant's O&M skills and the other for his or her acquaintance process with the new space.
All participants worked and were observed individually. The study was carried out in three stages. The first stage focused on the evaluation of the participants' initial O&M skills using the O&M questionnaire. In the second stage the experimental group became acquainted with the virtual environment's components and operation modes. The series of tasks administered at this stage included (a) free navigation, (b) directed navigation, and (c) a task aimed to introduce the auditory feedback. This stage lasted about three hours (two meetings). At the end of it participants learned to work independently with the FFJ, were able to walk directly toward the objects, could say when they bumped into an object or got to one of the room's corners, and could walk around the objects and along the walls by using the FFJ and the audio feedback. The third stage, the main part of the study, focused on participants' exploration of the unknown space. The experimental group explored the space using the virtual environment, while the control group directly explored the real environment. This stage lasted about 1.5 -2.5 hours, the task was video-recorded. For the experimental group the video-recording was combined with computer-recording. The last stage consisted of the processing and analysis of the collected data.
The research results and conclusions are based and represent only the research participants' performance and achievements (n=31). The population target on this research were Israeli people who are blind, selected using the seven criteria above mentioned and that agree to take a part on this study. The actual size of the study's population did not allow a detailed examination of the effect of otherwise relevant variables (e.g., gender or age).
The results regarding the exploration strategies, methods, and processes manifested by the participants working with the MVE, in comparison with those working in the real space, are presented according to our main research questions.
Research Question 1: What exploration strategies do people who are blind use working with the MVE, in comparison to those used by people whom are blind working directly in the real environment? The participants in both groups implemented similar exploration strategies, mostly based on the ones they used in their daily navigation in real spaces. Examples of strategies implemented were: (a) perimeter (e.g., walking along the room's walls and exploring objects attached to the walls), (b) grid (e.g., exploring the room's inner-space), (c) object-toobject (e.g., walking from one object to another), and (d) points-of-references (e.g., walking in the environment and creating landmarks). However, an interesting additional finding surfaced in that several participants in the experimental group developed a few new strategies while working within the virtual environment. A constant scanning strategy was identified by which the user collected information about the room's interior while simultaneously 
collecting perimeter information (e.g., resembling the use of a long cane in real space -as shown in Figure 7 ). Those strategies could be generated only within the MVE, representing an important added value of the work with the computer system.
As already mentioned, no substantial difference between groups was observed as regards the types of strategies used, but significant difference was found concerning the frequency of use of the strategies, and distance traversed using each strategy. Data in Table 2 indicate that the strategy most frequently used by the experimental group was grid, followed by the perimeter strategy. In contrast, the control group preferred to explore the room's perimeter, and next to use the object-to-object strategy. Examining the distance traversed using each strategy, we found that both groups traversed the longest distance using the perimeter strategy.
Research Question 2: What characterizes the exploration processes used by people who are blind working with the MVE, in comparison to the exploration processes used by people whom are blind working in the real environment? Five aspects are of interest as regards to the exploration processes used in the two groups: (a) the duration of the exploration, (b) the distance traversed, (c) the number of switches among strategies, (d) the sequence of main implemented strategies, and (e) the number and kinds of stops made while examining the new space.
Concerning the duration of the exploration, it should be noted that the participants were not limited in time for accomplishing the task. Participants from the experimental group needed four times more time to explore the new environment (average time of 38 minutes) than the ones from the control group (average time of 10 minutes). This difference was significant (t (28)=7; p=.000). Significant difference was found also for the total length of the exploration path (t (29)=5.44; p=.000). Participants in experimental group traversed an average of three times more distance (M=6.3 m) than the control group subjects (M=1.9 m).
The experimental group made frequent switches of strategy during their walk in the MVE, in contrast with the control-group performance in the real space. This behavior is reflected in the total and average frequency of use of the various strategies by both groups (see Table 2 ), total frequency of 292 and mean of 14 for the experimental group, and total frequency of 64 and mean of 6.4 for the control group.
Significant difference was also found between the groups in the sequence of main strategies implemented (c2(2)=7.55; p<.05). Data in Table 3 indicate that most experimentalgroup participants (62%) used the grid strategy first and then the perimeter strategy. In contrast, most control-group participants (90%) preferred first to explore the room's perimeter and then the objects located in the inner space of the room.
Participants from both groups made many pauses during their walk, suggesting that different cognitive operations related to the task in process were activated during these intervals. In terms of duration and function, we defined two types of pauses, short and long. Short pauses (i.e., 4-10 seconds) were used for technical purposes (e.g., changing the hand that holds the force-feedback joystick) or for reflection on a recent action. Long pauses (i.e., more than 10 seconds) were used for memorizing spatial information, reflection on a recently implemented exploration strategy, or planning. As shown in Table 4 , significant difference was found between the groups (t(26)=7.65; p<.001; t(25)=2.56; p<.05 ) for both short and long pauses. The experimental group made about 3 times more short pauses, and 6 times more long pauses.
As the results indicate, significant differences were found between the experimental group and the control group concerning the characteristics of the exploration process. These differences were related to four dependent variables: (a) the total duration of the exploration, (b) the total distance traversed, (c) the sequence of main implemented strategies, and (d) the number of pauses made while exploring the unknown space. The experimental group participants, in comparison with the control group, used a more varied range of strategies to explore the room, walked a longer distance to complete the exploration, and made more pauses for technical or reflective purposes.
Research Question 3: What information collection and storage did participants use, in both the experimental and control groups? The collection and storage of relevant information is inherent to the process of exploring an unknown space. Although only a few participants in this study reported explicitly on the use of tactics and aids for performing these functions, their account on this matter is of interest. Excerpts of these participants' references to the use of such aids follow.
One important category of information-collection aids was related to measurement to support the estimation of dimensions and distances. One example is the case of T, a 25-year-old, late blind, woman who explored the room using the MVE. After 2 minutes in the system T began to walk and count out aloud steps: "The blackboard… ok, the wall, one, two, three, four, five…".
The use of echo was another useful measurement aid. For example, the control-group participants used echo for measuring their distance from the wall or from other objects, or the room size. During their exploration those participants spoke, sang or whistled to get echo information.
The participants in this study used various kinds of information-retaining means. One was verbal reconstruction of landmarks, by which the subject recalled out loud her/his exploration of the space. For example, G, a 25-year-old, late blind man who explored the room by using the MVE, after examining the room's perimeter for 13 minutes said: "I had door, a prism, a corner, when I walked I had a window at the left side and then I reached the cube, I passed it and in front of it I had a window, column, window, window, column, window…". A variant of this was to complement the verbal reconstruction with virtual drawing (i.e., accompanying the verbal description with hand movements mimicking the physical presence and distribution of spatial components). For example, G, a 12-year-old, congenitally blind girl working with the MVE, after 22 minutes of exploration discovered the second cube in the room and began to describe out loud: "second cube, this cube is in the corner of the lower wall, and the wall is here…so the cube is in this corner...of the left wall…[during this verbal reconstruction G moved with her hand on the table indicating where the surveyed objects were located] yes the left wall, yes this wall corner and that wall corner ...there is a cube...".
Another interesting aid for the reinforcement of acquired information was the use of metaphors. For example, M, a 39-year-old, congenitally blind woman, after 32 minutes of exploration said: "…now I am a tourist guide, you are standing in front of the room entrance, now you are going to follow me, we are turning to the left, ok follow me... you have reached the prism, look at the prism, it is a beautiful object. We can not walk to the left, we are stuck in… we are walking forward… and we are arriving at the wall…".
Only some of the participants explicitly reported on the use of any aid. Table 5 indicates that the participants from the experimental group who reported on the use of aids mentioned mainly retention reinforcement aids (54%). In contrast, half of the control group mentioned such aids (50%), and even more mentioned the use of measuring aids (70%).
The research reported here is part of an effort aimed to understand if and how, the work with a MVE supports the exploration of unknown environments by people who are blind . Gathering comprehensive information about new spaces is a prerequisite for the construction of effective cognitive maps of these spaces, and for supporting people's ability to navigate them. The results of this study helped uncover several issues concerning the contribution of the MVE to the exploration strategies and learning process of unknown spaces by people who are blind.
Walking in the MVE gave participants a comprehensive and thorough acquaintance with the target space. The high degree of compatibility between the components of the virtual system and of the real space on one hand and the exploring methods supported by the MVE on the other, contributed to Journal of Special Education Technology the users' relaxed and safe walking. These features also enabled participants to implement exploration patterns they commonly used in real spaces, but in a qualitatively different manner. The use of real walking strategies in virtual environments was reported in previous studies on spatial performance by sighted participants (Darken & Peterson, 2002; Witmer, Bailey, Knerr & Parsons 1996) . But this study's MVE participants applied the known strategies in novel ways. For example, they preferred to explore the inner part of the room first and only then its boundaries, in contrast with the exploration patterns described by Jacobson, 1993 . Moreover, the MVE participants created new exploration strategies, such as the one simulating walking with a long cane enabling them to walk the perimeter of the room and at the same time to explore its corresponding inner areas -a strategy only possible within the MVE.
Operation features of the MVE (e.g., the game-like physical interface, various types of feedback) contributed to participants' performance with the system while exploring the unknown space. As a result, the exploration process showed interesting qualities concerning spatial, temporal, and thinking-related aspects. Examples of spatial and temporal qualities were the range of scanning strategies implemented, the inclusion of a large number of long and short breaks, or the time spent in examining the space. In addition, the MVE users traveled as much as three times more distance than the control-group participants, allowing them to collect information about the environment at different resolution levels, and to re-evaluate the information already gathered. All these were indications of the richness and comprehensiveness of the exploration process as accomplished by the MVE participants. Although the time measures collected were similar to those reported in Darken and Banker (1998) and Waller et al. (1998) , both studies of sighted participants exploring spaces by means of VEs, it could be expected that exploration time would become shorter as participants got gradually used to working with these systems as tools for learning unknown spaces.
Concerning thinking-related aspects of the process, interesting examples were the long breaks made by the participants with the aim to reflect on the exploration steps or to memorize data concerning an explored area, or the use of virtual drawing of spatial features under examination on the table's surface as a reinforcement aid.
An important byproduct of the study is related to the definition of specifications and constraints for the appropriate design of haptic virtual learning environments for people who are blind (e.g., force-feedback in high resolution, audio feedback). We expect these virtual environments to become powerful tools for people who are blind in learning processes in which spatial information is crucial, both for understanding new concepts and phenomena, as well as for acting and performing in the real world.
Further studies should examine the participants' construction of spatial cognitive maps of spaces using the MVE and, consequently, their use of these maps for navigating in the real environments. Additional variables to be studied should relate to properties of the environment (e.g., indoor or outdoor spaces, complex public spaces, and irregular surfaces). Finally, a comparison with traditional methods used by people who are blind to learn about unknown environments (e.g., tactile maps, verbal descriptions, human guidance) may serve for comprehensive evaluation of the contribution of the virtual tools to people's spatial performance. Finally, at the implementation level the virtual tool could play a central role in training and rehabilitation processes as well. One possible application is for supporting the acquisition of O&M skills and strategies by persons who are late blind as part of their rehabilitation process. At another level, the development of more comprehensive environmentediting tools for the MVE will support the creation of a variety of models of spaces (e.g., public buildings, shopping areas) enabling pre and post-visit exploration and recall of unknown spaces by people who are blind. These implementations may also serve the research and practitioners community as models for the further development of technology-based tools for supporting learning processes and performance of people with special needs. 
name and sent by a colleague who has participated in a study of mine before, made me feel that his request was the first I should respond to.
In an ideal world, we, fellow academics, irrespective of the difference in rank, gender, or seniority, would support each other's research (and hence publications) by responding to requests for helppresumably every response, every bit of contribution to their data would count. (It should be noted that such appeals to fellow academics for help are typically made by academics in education, including language education, where they may have research interests that necessitate the participation of fellow academics across disciplines.) The literature, especially that of health research, has discussed the pros and cons of undertaking qualitative interviews with one's peers in the same profession [1, 2] . However, little reflection is found in the literature on the implications of working with fellow academics who are not necessarily in one's own field of profession or discipline. At the same time, while research methodology books give ample advice on accessing participants beyond academia, little discussion can be found on working with fellow academics as research participants. The present paper will address this gap in the literature. The observations shared here represent personal views, which have not been tested by systematic research. However, a discussion of the phenomenon is worthwhile as accessing target research participants is an important issue for many academics in social sciences.
In the following, I will begin with an example from Braine [3] to illustrate what can result from a lack of cooperation from fellow academics as potential research participants. Then, I will draw examples from the literature to sketch a picture of some studies where researchers in education have tried to involve fellow academics in U.S., European, and Chinese universities, respectively, as their research participants. I will then discuss some related issues by drawing upon my own experiences of working with Chinese academics as my research participants, as a female junior academic based at a university in Hong Kong.
We normally take precautionary measures against a potentially low response rates. For instance, we may aim to reach an extensive pool of a target population, use a "snowball strategy" when selecting participants, or personalize the invitations sent to target participants. When little can be done to change a low level of "cooperation," however, our research purposes can suffer. An example was given by George Braine [3] in an article entitled "When professors don't cooperate: A critical perspective on EAP research" (published in the journal of English for Specific Purposes, a flagship journal in a field where cooperation from colleagues across disciplines is often vital). For his doctoral research, conducted at the University of Texas (Austin) in the early 1990s, on undergraduate writing tasks in engineering and natural sciences, Braine received enthusiastic support when requesting written assignment prompts from professors across disciplines at the university; however, when trying to replicate the study at the Chinese University of Hong Kong a few years later (in mid-to late-1990s), as a faculty member in the English Department, he ran into difficulty. With the support of two research assistants, they sent requests to "223 teachers in the engineering and science faculties who were listed in the timetable as teaching in English or in Cantonese and English, inviting them to participate in the project by sending us their course syllabi and writing assignments" ( [3] , p. 297):
"Within a week, 80 had replied, citing their reasons for being unable to participate: some were too busy, others were not teaching that semester/year, and the rest did not give writing assignments in their courses. In response to follow-up requests, phone calls, and email messages, only five teachers from engineering and four from science agreed to participate in the project. Despite requests through e-mails and telephone calls, 134 teachers did not respond at all." (pp. [297] [298] In all, the research team collected 29 assignments from engineering, 22 of which were from two courses, in the form of "instructions for laboratory experiments" but were "too succinct for detailed analysis"; and none came from science (p. 298). The researchers did, instead, receive a generous donation of student reports, and they ended up trying to "retrace and reconstruct laboriously, from the students' reports, the teachers' expectations" (p. 298).
Reflecting on why colleagues in engineering and science faculties were reluctant to share their writing assignments, Braine [3] suggested that "these teachers may have received little or no instruction in writing during their secondary, undergraduate and graduate studies" (p. 299); and even having pursued graduate studies in North America usually also means a lack of training in writing for some. Hence, the professors may either take writing assignments from textbooks or give few writing assignments (p. 298 and p. 302). Another reason for the teachers' reluctance to share the materials requested, Braine suggested, might be that they did not want to let an English teacher see their "poorly written or poorly designed texts" ([4], p. 33) (cited in [3] , p. 302).
Nearly two decades after Braine's effort to reach out to fellow academics at the Chinese University of Hong Kong, there has apparently been heightened awareness for writing across disciplines in Hong Kong universities, and professors across faculties generally seem to possess relatively strong English writing skills, as, after all, a track-record of English publications, coupled with a PhD degree earned at an English-speaking country, has often been an instrumental factor in faculty recruitment. However, there is no guarantee that Braine would have received more positive responses to his request now than over a decade ago.
In Braine's [3] study, another factor seems important for the contrast of the responses he received from subject professors at the University of Texas (Austin) and later at the Chinese University of Hong Kong: there was a long-established Writing Across the Curriculum (WAC) program at the former, so that, presumably, the subject professors were accustomed to working with language teachers on a regular basis; while, at the latter university, with the absence of such a program and with a language specialist-subject specialist partnership largely an alien concept at the time of Braine's study, the subject specialists might not feel comfortable moving out of the tradition. However, other than this divergence at the two sites, one may wonder if the difference in responding to the same request might reflect some sort of "cultural" difference among the professors on the two sides? Namely, do U.S. academics tend to be more responsive to research-support requests from fellow academics than (Hong Kong) Chinese academics? Such a question will be difficult to answer. However, it may be useful to look into the literature to get a sense of the extent to which academics in different parts of the world are willing to become research participants, by responding to fellow academics' request of completing a questionnaire.
In the following, I will provide an overview of some studies, which were based on data gathered through questionnaires administered to academics across disciplines in U.S., European, and Chinese universities, respectively. I focus on questionnaire-based studies rather than interview-based ones here as the former tend to give clear indications of the response rate (i.e., the percentage of the responding fellow academics in the total target population). Most of the studies examined below were conducted by researchers in language education (which is my own disciplinary area) and a few by researchers in education more generally. The target questionnaire respondents were typically fellow academics across disciplines rather than in the researchers' own discipline (an exception being Min's [5] study, in which the participants were from the researcher's own disciplinary area, i.e., applied linguistics). It should be noted that my selection of the studies is more of a result of my having knowledge of these studies than due to any other reason. For consistency, five studies have been selected from each of the three geographical locations: the U.S., Europe, and several Chinese-speaking regions. The purpose of the overview is to offer a glimpse into some studies that involved academics responding to peers' request for participation in research. There is no intention to pick "representative" studies or to make any generalizations based on these studies. Table 1 summarizes five questionnaire-based studies, conducted by language professionals teaching in U.S. universities. These studies, with an aim of informing English for Academic Purposes (EAP) pedagogy, mostly focused on the communication or, especially, the academic writing requirements for students across disciplines. 1518 foreign language (FL) instructors working at the chosen universities, receiving an email containing a secure URL link providing access to an online survey system
The extent to which the current university FL instruction is informed by second language writing research 153 (9.92%) FL instructors completed the survey Ferris and Tagg [8] described the response rate they received from fellow academics (at 25.4%, with 234 responses to the 921 delivered surveys) as "fairly low" (p. 37). In another study, not included in Table 1 but apparently drawing data from the same questionnaire described in Ferris and Tagg [8] , the same authors [11] focused on listening/speaking tasks for ESL (English as a Second Language) students, and pointed to an (understandable) connection between fellow academics' decision over whether to respond and their perceived relevance of the topic of the study for them: "As the survey was rather long, it is likely that the respondents were primarily those who had strong interest in or concerns about their ESL students and may thus not be representative of all instructors" (p. 303).
While the surveys conducted by Casanave and Hubbard [6] , Jenkins, Jordan, and Weiland [7] , and Ferris and Tagg [8, 11] , in the late 1980s to early 1990s, were paper-based with hard copies of questionnaire mailed to target participants, over time it has increasingly become a norm to conduct surveys using the Internet, by distributing the questionnaire directly via email (as in [9] ) or sending an invitation email containing a URL link to an online survey (as in [10] ). Of the latter, as the researchers Hubert and Bonzo ([10,] p. 521) described: "The survey was administered using an online instrument to which potential respondents were provided access via a secure URL link. This link was provided to potential respondents via email." Table 2 shows five survey-based studies conducted in European universities also by language professionals. In these studies, questionnaires were administered to fellow EAL (English as an Additional Language) academics to find out about their attitudes toward the dominance of English in academia in relation to their first language, perception of difficulties in writing for publication in English, and their related practices in overcoming the potential language barrier. Table 2 . A summary of five surveys conducted by language professionals in European universities.



Duszak and Lewkowicz [12] A university in Poland
The questionnaire was emailed to "academics in medicine, psychology and language studies" (p. 111) Polish academics' perception of difficulties in writing for publication in English "99 completed questionnaires were received"; response rate is unknown as the population size is not indicated Ferguson, Pérez-Llantada, and Plo [13] University of Zaragoza, Spain A questionnaire (written in Spanish) was emailed "through a university server to all 3,000 academic and academic-related staff" (p. 47)
Scientists' perception of disadvantage in using English in academic/scientific publication "a modest though not impossibly low response rate of 10 per cent", with 300 questionnaires returned (p. 47) "online questionnaires which were posted on the university server during the period April-June 2009" (p.
433)
The use of English at undergraduate and postgraduate levels and across disciplines, and the target respondents' attitudes toward the use of English in teaching and research "highly satisfactory" response rates: 19% of the target students responded, and 668 (40%) out of 1683 staff responded; excluding administrative personnel and PhD students from the staff category, "498 teaching and research staff remained" (p. 433) Table 2 shows that from the researchers' point of view, the response rates ranged from "disappointingly low" [15] , "modest" [13] , to "highly satisfactory" [16] , but, apparently, tipping toward the modest and low side. The top reason for a colleague not to respond, as some suggested, is "pressures on academics' time" ( [13] , p. 47). In addition, Duszak and Lewkowicz [12] noted that 62% of their respondents were aged between 22 and 45, which is "likely to reflect the fact that younger academics are more willing to respond to requests for data of this nature" (p. 111). This suggestion can be interpreted as implying that younger academics may find the topic of the survey, namely perception of difficulties in writing for publication in English, particularly relevant for them and are therefore more willing to respond. A similar point was made by Ferris and Tagg [8] , as noted earlier.
Finally, Table 3 includes five questionnaire-based studies concerning academics' teaching/research activities, conducted in universities in Mainland China, Hong Kong, and Taiwan, respectively. It can be seen from Table 3 that the studies of Li et al. [17] and Min [5] achieved particularly high response rates: 89.3% (268 responses out of a target of 300) and 76% (38 responses out of 50), respectively. However, this is perhaps not surprising, given that, as indicated in the table, Min [5] identified her target respondents from applied linguists (language educators) she had met at local conferences, so that presumably the researcher and the target respondents were members of a local academic community based on professional relationships. In the case of Li et al.'s [17] study, "printed surveys were handed out" (p. 282) to the target respondents, apparently through personal connections and with facilitation of the approval of the study by "the leaders of the institution" (p. 279). The potential implication of personal connections (or guanxi) in conducting research in the Chinese context will be re-visited later in the present paper, when I reflect upon my own experience of working with Chinese academics.
Incidentally, questionnaire-based research conducted by language educators with fellow academics across disciplines in mainland Chinese universities is hard to find. This seems to echo the current general lack of exchange and collaboration in EAP instruction between language and subject specialists in the country [21, 22] . In this sense, it can be suggested that access is only likely to occur when there is a desire to access. On the other hand, potential accessibility of subject specialists and language specialists to each other in Chinese universities might, in fact, be a reminder of the existence of opportunities for collaboration between the two parties in EAP teaching and research, with such collaborations having long been advocated in the literature (e.g., [23, 24] ).
In the above, I summarized a total of 15 questionnaire-based studies conducted by researchers in education, especially language education, among fellow academics mostly working in disciplines other than their own. It can be seen that when questionnaires are distributed within a relatively closely-knit academic community, where the researcher has personal connections, either in the context of one university [16, 17] , or a disciplinary/professional community of which the researcher is a member [5] or to which the researcher has access through a listserv [9] , a high response rate is likely to achieved. We also notice that researchers may express an emotional attitude toward the response rates: while for Ferris and Tagg [8] , a response rate of 25.4% was "fairly low" (p. 37), for Ferguson et al. [13] , 10% was "modest" (p. 47), and for Olsson and Sheridan [15] , 17.5% was "disappointingly low" (p. 39); in addition, a response rate of 40% was "highly satisfactory" (p. 433) for Bolton and Kuteeva [16] . Thus, researchers do care about the response rates they receive.
Casanave and Hubbard ( [6] , p. 35), Jenkins, Jordan, and Weiland [7] , and Ferris and Tagg ( [11] , p. 303) all admitted that their surveys were long and complex. The trend over the past two decades is probably for questionnaires targeted at academics to become shorter and easier to respond to. Nevertheless, Ferguson et al. [13] suggested that curtailing the number of questions in a questionnaire, including by "overriding the customary practice of including multiple items focusing on the same target", "can sometimes impact on reliability" (p. 56). Researchers are thus caught in a dilemma: in trying to overcome low response rates, which may threaten the authenticity of the data (e.g., by not being representative of the larger population), they may feel compelled to adopt strategies that can potentially undercut the reliability of the questionnaire. When they do get a low response rate, researchers may have to make a case that the data are trustworthy and usable (presumably justified in doing so). Ferguson et al. [13] , for example, suggested that despite the "modest" response rate of 10% that their questionnaire received from their target Spanish academics respondents, the data "do appear utilizable within the inherent limitations of the methodology", "bearing in mind that the sample size is not lower than in many comparable surveys", plus their survey study was exploratory and would be followed up by interviews with a selection of academics (p. 47).
In the early 2000s, when I first embarked on an academic career, I had ingrained beliefs to dismantle first: having a scholar father specializing in historical Chinese linguistics, I grew up assuming that research should be based on books and study of texts; collecting "data" from human beings (including students) for research felt unnatural, and even unethical, to me. However, having decided to study what difficulties Chinese doctoral science students experience in trying to meet the degree conferment requirement of SCI (Science Citation Index) publication (a requirement becoming increasingly popular at Chinese universities at the time), i.e., publishing in (English-medium) international journals included in the SCI, and how they can be supported in the endeavor, I seemed to have little choice but to muster the courage to approach students to collect "data" from them. With little training in research methodology at the time, it was curiosity and a desire to learn that sent me onto a track of "empirical research"-which was a privileged term in the language education circle in China at the time (and it still is), in reaction to the much-criticized "impressionistic," "reflective" tradition in Chinese research [25] . I remember the encouragement I received from an American professor at the time: that my project of working with science students and scientists would be very "doable," as long as I could "penetrate" their "academic tribes" [26] .
Over the past decade, in a number of projects on scholarly publication, I have interviewed academics across disciplines in Hong Kong and Mainland China on their attitudes toward and practices in linguistic choice between English and Chinese in research and publication; I have conducted case studies of novice scientists (doctoral science students) in mainland universities writing for publication in English, which included an examination of how supervisors revise papers for novices, and, in particular, how supervisors perceive and tackle the issue of text-based plagiarism in novice texts. Other than working with university-based academics and students, I also accessed the orthopedics department of a Chinese hospital to investigate how medical doctors with doctoral degrees engage in research activities amidst their busy schedules of clinical practice. The challenge of gaining access to, and maintaining, contact with participants in these projects has not been small. On reflection, I would say that, on the whole, opportunities for access have come mostly from personal contacts, while constraints seem to be often related to my geographic location.
Personal contacts were formed by my having taught English at a university in a major city in east China, so that I was able to invite my students-doctoral students across disciplines-as my initial participants in my doctoral research project on Chinese novice scientists writing for international publication. The literature is rich with examples of English language professionals working with ESL students to study the latter's academic literacy pratices (e.g., [27, 28] ). In conducting such research, language professionals are able to draw upon their insider knowledge [29] while capitalizing on the advantage of access. As Casanave [30] commented on Spack's [28] three-year study of a Japanese student (named "Yuko") at a U.S. university: "Like many other qualitative studies of writing, this case study used Spack's insider knowledge, contexts, and contacts at her own university to comfortably get access to what she needed from and about Yuko."
During my doctoral years I made painstaking efforts to stay in touch with my participants, so that the number of words I wrote in emails greatly surpassed the total number of words of my doctoral dissertation. In those emails, I played the roles of friend, English teacher, and researcher. I observed the principle of reciprocity [31] , and did what I could for my student participants, above all by editing English papers for them. Over time I got to know some of the students' supervisors, who also participated in my research. Later through the introduction of colleagues and friends, I reached more academics across disciplines, who, regarding me as a previous colleague, kindly accepted my invitation to participate in interviews. Additionally, the relevance of my topic of research, namely the impact of English and English publication requirements, for my target participants may have been a facilitative factor in my access. Examples of such reciprocal relationships between EAP language specialists and EAL academics, or students in disciplinary areas who need to publish in English, can be found in the literature: the former providing instruction, training, or editorial support to the latter, and the latter becoming research participants in interviews, case studies, or questionnaire surveys in return (e.g., [32, 33] ).
Other personal relations, or family relations to be more specific, have facilitated my access to researchers beyond academia, i.e., doctors and medical students at a Chinese hospital engaging in research for publication (e.g., [34, 35] ). Evidence can be gleaned from the literature to indicate that family relations can sometimes be an important factor in shaping one's research path. For example, we learned that for Dorothy Winsor, an accomplished professional and technical writing researcher, access to engineers through family relations has been critical: "Winsor realized that with her access to engineers via her husband's profession she was in a position to learn something about engineering writing" ( [36] , p. 355). Similarly, we learned from Dressen (or "Dressen-Hammouda" in her later publications) [37] that her husband was a geologist so that she had opportunities to learn about the discipline "through osmosis" (p. 284) and built her research path over time around geology text and geologists' genre mastery.
To employ notions widely adopted in the literature, in terms of social science power, I am located in a "semi-periphery" region, but have conducted research in a "periphery" setting, and published in "center"/"semi-periphery" journals [38, 39] . ("Semi-peripheral social science power", according to Alatas ([38] , p. 606), "may be defined as a social science community that is dependent on ideas originating in the social science centres, but which themselves exert some influence on peripheral social science communities". From this perspective, Hong Kong may be regarded as a semi-peripheral social science power.) I am aware of the connection between my practice and the scenario described by Canagarajah [40] : that scholars from better-resourced and English-dominant countries/regions utilizing research data in the periphery to develop interpretations, typically through the lens of center-origined theories, and, henceforth, claiming leadership in scholarship by publicizing their work in Englishmedium journals. In contrast to this scenario, had local scholars "enjoyed similar resources", they "would have also possessed the power to orchestrate the whole research enterprise under their own leadership" and "present the discovery in a manner favoring their community's interests, knowledge, and values" ( [40] , p. 5). Canagarajah is sharp in his observation.
As a Hong Kong-based academic conducting research with Chinese mainland fellow academics, I am conscious of the way I might be perceived: unlike some of my target participants, I am not a returnee (having completed graduate studies outside Mainland China and returned to become a mainland-based academic). I suspect that this kind of background gives me both an advantage and disadvantage when I approach my target participants. There may be perceptions that I cannot change; but I do have a clear goal in my research. With the advantage that I may enjoy-e.g., resources and, perhaps, the name of my home institution-my central concern in my work with Chinese academics has been to let the outside world hear their voices. I am indebted to their support of my research; and I decided that the best thing I can do for them in return is to do good research and disseminate my findings so that there will be a more balanced view about Chinese academics on the international stage: that other than corruptive practices that seem to be wide-spread in some quarters of Chinese academia [41, 42] , the general Chinese academics' diligence and studious efforts to achieve success in research and international visibility and impact should also be made known.
I am also aware that I am just one of many education and social science academics in Hong Kong universities who conduct research in Mainland China and publish in English journals (for illustration of this phenomenon, see, for example, [43] ). The steady growth of the number of SSCI (Social Science Citation Index) journal articles co-authored by Chinese and overseas academics [44] seems to indicate a growing interest in exploring Chinese research sites. As the joint publications would indicate, a primary means for academics outside China to access Chinese research sites is to form research partnerships with their Chinese counterparts, with the latter's role possibly ranging from mere data collection to participating in the design of research and writing for publication. Over the years I have not made special efforts to forge research partnerships with mainland Chinese counterparts (of my discipline), partly due to the absence of linguistic or cultural barriers in my doing research in Mainland China-an advantage enjoyed by multilingual researchers, as having been discussed in the literature (e.g., [45] ). Of course, I have often needed the assistance of "gatekeepers" [46] in my efforts of access. Yet I have perhaps been viewed as an "outsider" sometimes. Outsiders from a reputable university in Hong Kong, a privileged semi-periphery region, are likely to be treated with some politeness; but this does not guarantee their target participants' cooperation. It has been suggested that collecting data in "emerging societies" is difficult, "as there is little tradition of independent enquiry" and "asking questions in any form is viewed with suspicion" ( [47] , p. 164). If this applies to collecting data in Chinese business organizations [47] , this should arguably be less true when working with Chinese academics. However, connections, or what is commonly known as guanxi in the Chinese culture, would still be crucial.
While having mostly worked with mainland Chinese academics, I have also reached out to academics in Hong Kong on two occasions, the first to interview some scholars in humanities and social sciences to find out how they negotiate between local engagement and international participation through publication [43] , and the second in a project on university students writing from sources, with my own university as the research site. In the first study, I approached the target interviewees one-byone by emailing them, and most agreed to be interviewed. In the second study, in an early attempt, I only got three positive responses to my email invitations sent to over three-hundred Turnitin instructors (academic staff who have a Turnitin account) on campus to recruit interview participants in order to explore the issue of plagiarism among students. All these three respondents were expatriates (i.e., English-speaking academics, two from North America and another from the UK). I surmised what has led to the poor response to my request: firstly, an interview is perceived to be more timeconsuming than answering a questionnaire online; secondly, the topic of plagiarism is perhaps not an attractive one for busy colleagues; and thirdly, the fact that three expatriate colleagues responded might indicate that they are probably a little more interested in talking about the issue of plagiarism than the general Cantonese-speaking professors, having come from the Anglo-American academic culture where a concern of the issue has been long institutionalized. As the invitation for voluntary participation did not work, later, with the help of a research assistant, I utilized personal contacts and a snowball sampling approach ( [48] , p. 89) to access target interviewees.
Reciprocity [31] can be an important principle in our research relationships with fellow academic participants, an obvious example being an English language professional editing English papers for her EAL participants, as mentioned earlier. In this scenario, the two sides are assisting each other in a high-stakes commitment, i.e., research and publication. Nevertheless, such reciprocity, in the form of the researcher giving support to the researched on the latter's work, is not always possible or required. In this situation, I have tried to pay my participants. Some may think paying academics for their participating in research is "unusual" (as a reviewer has written in commenting on the budget portion of a research proposal of mine). However, if it is appropriate to compensate research participants, such as by paying patients in health research [49] , it seems reasonable to pay fellow academics a certain token amount to express appreciation of their time and contribution of data, if it is also considered culturally acceptable to do so in a given context. Indeed, the payment can only be a symbolic compensation. An academic's hour spent on being interviewed cannot be measured by a modest honorarium. At the same time, it has to be said that the amount of such a payment is restricted, both by budget and by the consideration that it is important not to give an impression of bribing fellow academics into participation. A question here may be: how much is considered appropriate as a token fee (rather than an act of corruption) in paying a participant academic? It seems that the researcher's judicious judgment, in light of the local culture, will have to be relied upon in the decision. One may also suggest that if it is justifiable to pay a token fee to a fellow academic who participates in an interview, then one who completes a questionnaire should also be paid-yet, the latter is not usually possible, given the typical anonymity of respondents in questionnaire surveys.
It should be pointed out that, when participant fellow academics are one's own colleagues at the same university, or have been colleagues with the researcher at one time, payment of token fees may be inappropriate. It is also very likely that, despite not having been a colleague of the researcher, a fellow academic may decline to be paid. In this case, needless to say, the fellow academic's preference should be respected. To be fair, when an academic does become a fellow academic's research participant, and gets paid a token fee, the incentive should not be the honorarium, but rather, a desire to support the fellow academic. This is "academic citizenship" [50] at play.
It seems that nowadays whether we as academics are willing to sacrifice some of our time to help a fellow academic is often hinged on our schedule: the pressure of publication, coupled with the often growing, rather than reducing, load of teaching and administrative duties, means that we have become highly selective in whether or not to respond, not to say that the frequency of such requests that we receive may have led to a sense of fatigue. Of the large number of emails we receive on a daily basis, those addressed to us personally from students, department colleagues, research collaborators, and certain professional colleagues are to be filtered in; while the rest is likely to be filtered out. It does take some conscious collegiality and a desire to support fellow academics for us to respond to a request to fill in an online questionnaire, to participate in an interview, or to share our syllabi or writing assignments (the request made by Braine [3] to fellow academics). It may be fair to suggest that academics nowadays, in general, have become more pragmatic than before: more often than not, we invest time only when something is judged to be "relevant" to our personal interest or duty.
It might be suggested that if Braine's [3] study were to be conducted today, nearly two decades after it actually took place, he probably would not even get as many as 80 (36% of his total targeted 223) responding and citing reasons for not being able to participate. Olsson and Sheridan ([15] , p. 39), who reported that their survey among Swedish fellow academics got "a disappointingly low response rate of 17.5%" with 35 completed questionnaires, noted their questionnaire was partly based on Phillipson and Skutnabb-Kangas's [51] questionnaire, which was administered to Danish scholars two decades earlier and received a nearly 50% response rate with 83 completed questionnaires. Evidence is lacking to claim that the difference in the response rates between the surveys represents any general trend, but we probably have a telling pair of examples here.
Macfarlane [50] has insightfully discussed the "apparent decline of academic citizenship" (p. 296) and emphasized the essential role of "service" to "the preservation of community life" (p. 299). "Service in academic life", as Macfarlane put it, "is fundamentally about citizenship inasmuch that it demands participation as a member of a community of scholars rather than simply the individualised (and perhaps, selfish) pursuit of research and teaching interests" (p. 300). Clearly, supporting fellow academics by playing the role of being their research participants, thus indirectly helping to push forward understanding of educational issues, Macfarlane would agree, is a form of service, just as "teaching observation, mentoring, reviewing of academic papers and the organisation of conferences" are (p. 299). However, institutional forces, propelled by a performative culture which emphasizes "efficiency, effectiveness and profitability" ( [52] , p. 17), threaten to "disengage" [50] academics from their service role. Macfarlane ([50] , p. 309) called for "more explicit emphasis on the importance of the service role within reward and recognition structures" to reverse the trend of disengagement. It would be fair to suggest that, even if such structural adjustment cannot occur in the near future, intellectuals can still justifiably resist the trend of disengagement on a personal level, and, henceforth, on a community level.
Researchers may be taking many issues into consideration in order to maximize the chance of fellow academics' participation: e.g., the mode of surveying (which, nowadays, is typically sending an email invitation, embedded with a link to an online questionnaire, as noted earlier), the design of the questionnaire (user-friendly and focused), the timing of administering the questionnaire, and the mobilization of personal contacts. In addition, researchers make sure the cover letter or invitation is composed professionally, shows their credentials, and connects to the target respondents in a personalized way; they would also aim to clearly state the purpose of the study, what participation in a study involves, what sorts of questions will be asked, etc., all of which are, of course, also required in the application for ethical clearance. On the whole, they may demonstrate an understanding of the pressures in academia, and remarkable resilience and willingness to accommodate the target participants' schedule. An example of a request follows: "E-mail Request for Participants" "I know you are very busy, but I am asking for about half an hour to interview you about intellectual property issues from an administrator's point of view… … I am willing to conduct the interview at your convenience during the next two months. … … Please reply if you would be willing to help me with this, and I'll try to arrange a time that works for you." ( [53] , pp. 154-155)
After every effort has been made to incentivize fellow academics' participation, how many will give a positive response may be out of a researcher's control.
Silverman [54] reminded us that, in qualitative research in particular, we may aim to turn the negative (such as the difficulty in access) into a positive factor. He said: "Remember that the beauty of qualitative research is that it offers the potential for us to topicalize such difficulties rather than just treat them as methodological constraints. This is an issue of the creative use of troubles." (p. 153) Silverman did not illustrate how "creative use of troubles" might be achieved. However, it should not be difficult for an experienced researcher to name a couple of examples of how this might be done. Researchers, including those education researchers who need to engage fellow academics as participants, are resilient. Still, lack of cooperation from fellow academics may defeat our research purpose, as suggested earlier in the present paper.
By the time I finished writing this paper, I had responded to all three requests from fellow academics, mentioned at the beginning of this paper. I even emailed the requesting colleague in my faculty, after responding to his online questionnaire, that I would be happy to participate in an interview at a later stage if necessary, to elaborate on my responses in the questionnaire. I am glad I have added my two cents in a few fellow academics' research projects.
Traffic engineering is one of the active research areas in communication networks. The traditional form of routing and resource allocation, as the two major building blocks of traffic engineering cannot address quality of service requirements of flows while optimizing network utilization for complex communication networks. In this paper we consider ant colony algorithms to address this problem. In this approach foraging ants find the shortest path in a synergistic way. While moving back and forth between nest and food, ants mark their paths by secreting pheromone.
Step-by-step routing decisions are biased based on the local intensity of pheromone field which is the colony's collective and distributed memory. Ants will follow the most dense route in a maximum likelihood way. The actual algorithm implemented in nature by real ants is slow in convergence.
Our studies show that the ant-based routing models are sensitive to initial parameters settings. Only careful adjustments of these initial parameters results in an acceptable convergence behavior. The robust behavior of the real ant compared to the routing algorithms derived from it justifies the investigation of these algorithms in depth to find the reasons behind their shortcomings. We present results from our study of ant behavior in a quest for a robust algorithm. Most of the ant-based algorithms have been studied with limited sourcedestination traffic. In this work we have extended the algorithm to a more realistic environment in which multiple sourcedestination flows compete for the resources. We study the routing and load balancing behavior that emerges and show how the behavior relates to analytical approaches for optimal minimum delay algorithms by Gallager [3] , Mitra [6] , and others [4] , [5] . We show the results using simulations in OPNET and derive recommendations on the improvement of the ant-like algorithms to achieve load balancing.
The rest of this paper is organized as follows. In chapter 2 we highlight the problems of traffic engineering. Chapter 3 is dedicated to the ant algorithm to provide the overall view of the ant approach. In chapter 4 we briefly describe the outstanding analytical methods introduced to address the problem of dynamic routing and flow assignment. Our experiments and the results including our view of the ant algorithm is discussed in chapter 5. Finally we conclude the paper in chapter 6.
Traffic engineering is the process of mapping traffic flows onto the physical topology to meet traffic requirements, to enhance overall network utilization and create a uniform distribution of traffic throughout the network.
Traffic engineering in the traditional Internet is achieved by manipulating routing metrics, such as monetary cost, hopcount, bandwidth, reliability and delay. Since IGP (Interior Gateway Protocol) route calculation is topology driven and based on a simple additive metric such as the hop-count, it does not consider other important dynamic criteria such as bandwidth availability. As a result, traffic can be unevenly distributed across the network causing inefficient use of resources. Uneven distribution of traffic is complicated since it can be the product of the dynamic routing protocols such as OSPF and IS-IS, that select the shortest paths to forward packets. Hence a solution is required that takes into account more factors than the common path-metrics. While using shortest path conserves network resources, it may cause some other problems, such as congestion on some paths and under utilization of other paths.
These known problems with routing and the trends in networking and telecommunication provide incentive to look for another approach in routing and flow assignment as two main parts of traffic engineering.

Nature has always been an important source of inspiration for academic research. In particular there is much interest in the behavior of ants. Individual ants seem to move at random, do nothing but wander off, and yet groups of ants can accomplish complex tasks. Somehow, a collective intelligence is formed out of many simple elements which is called swarm intelligence. Each agent (ant) processes a very simple algorithm. The collective outcome realizes a much more complex algorithm. The whole system is distributed and adaptive. Ants cannot see or hear. They only sense the environment, and also the food. Ants cannot talk either, they communicate indirectly through the environment. An ant can leave a trial of pheromones which are materials with particular fragrance. Ants can smell and sense the pheromones left by other ants, moreover, ants can detect the density of the pheromones.
Ants find the shortest path to food according to this procedure: two ants start their random walk. They both eventually find the food. The one taking the shorter path finds the food first. Each ant leaves a trail of pheromones behind. Having taken the food the ants follow their pheromone trail towards the nest. The one with the shorter path returns first and arrives back to the nest first. Now a third ant wants to search for food. The ant realizes the trials left behind by its predecessors. Most likely it follows one of the existing trials rather than initiating a new trial and most likely it follows the trial with the higher density of pheromones. This results in even denser pheromone trial on the shorter path and in the long run this results in most ants using the shortest path.
When an ant starts its walk with some small probability it starts a new trial. The first ants may not necessarily have chosen the shortest path but starting new paths helps continue the quest for shorter paths until finding the shortest one and eventually the ants emerge around the shortest path. The pheromones evaporate over time. This is an essential requirement for the dynamism of the algorithm. The algorithm is adaptive because of the evaporation and the fact that ants keep starting new paths with some probability.
Ants in networks are emulated by mobile agents. Mobile agents are carried by packets. Special packets can be used as mobile agents (ants). Pheromones pass the information about the length of the path (time) to other ants. The agents can pass the same information to data packets at the nodes. Ants decide based on the density of the pheromones and some probability values. The probability values can be calculated based on the path information and listed in routing tables in the nodes. Starting with a static routing table for each node, every individual routing table stores the probabilities of using the next hops to reach all possible destinations. The sum of all the probabilities at each row should be equal to one.
Different methods have been used in the literature for implementing and updating the routing tables using the ant approach such as AntNet [1] .
In this section we will briefly review existing analytical approaches to addressing the traffic engineering issue. Analytical routing algorithms can be distributed or centralized and also static or dynamic. Static routing algorithms cannot keep themselves up-to-date with the continuing changes in networks. Centralized methods suffer from the lack of scalability and having a single point of failure. Other distributed and dynamic routing and flow assignment algorithms have stability and convergence issues.
A common characteristic of these methods is their dependence on one or more heuristic parameters that are found based on experiments. In Gallager's algorithm [3] in order to avoid loops the algorithm uses a parameter η that should be globally chosen and every router must use it to ensure appropriate behavior but this parameter depends on input traffic pattern. It is impossible to find one working value for all input traffics.
In Mitra's approach [6] also a heuristic parameter is used which is critical for the robustness of the algorithm. This factor is called "bandwidth protection" R. In [5] again a heuristic is used which is in the form of a function.
In this section we present our study of the ant colony algorithm based on simulation and discuss the results. We followed [1] for implementing ant colony algorithm, but we also examined modifications to show how the ant approach can be exploited to achieve load balancing.
Our simulations of the ant algorithm are applied to a fishlike network configuration as illustrated in figure 1 . The network consists of four routers and four hosts at the edges. Ants are generated at routers regularly and addressed to the destination hosts randomly. We use a uniform distribution to assign the destinations to the ants which are called forward ants at this stage.
The forward ants are routed to output links at each router until they find their way to their assigned destination hosts. A trace of the route traversed by the ant is also stored in the ant. At the host the ants are transformed to backward ants and sent back to the source through the same path that they took to arrive to the destination. At each router across the path the ants will then update the routing table that is used to route data packets. In the original ant colony algorithm the same table is used to route forward ants as well.
Data packets are generated by the hosts and addressed to certain destination hosts to create data traffic flows. At each router data packets will be routed to output links based on the information listed in the routing table. In an ant colony algorithm the routing table contains a probability number for every destination host though every existing output link. Packets are directed to an output link in proportion to the link probability.
The forward ants measure the travel time from each interim router to the final destination. The travel time which is a reflection of the route conditions is used to update the probability table when the backward ants come back to the router.
Except for the routing table, each node also keeps a table with records of the mean and variance of the trip time to every destination (delay). At each node, backward ants update the trip time statistics to the destination in addition to the output link probability. We derived these equations for updating the probabilities [ This equation gives an interim probability value for destination i from this router. In this equation j refers to all output links and k is the link that backward ant came from.
The probability values are filtered according to the following equation before being set into the routing table. This is to reduce the variations in the table because of temporary increases in the travel times. 
We examined three different approaches. In the first approach we used the original ant colony algorithm. In this approach forward ants are routed at the routers using the probability values in the routing tables similar to data packets.
In the second approach we used round-robin to forward the forward ants to the existing output links. The probability values are used to route only data packets.
In the third approach we modified the probability calculation algorithm while using the same round-robin approach for forward ants.
First we consider the results from the first set of experiments based on the first approach. In the majority of scenarios using various settings of the parameters, the ant algorithm reveals a strong tendency towards finding and using a major route to the destination (which is most likely the fastest one). Other routes exist but with much less share of the carried traffic.
As shown in figure 2 , even though links 3 and 4 of router 0 are exactly alike, the probability value for link 3 rapidly rises and as a result the traffic from source 1 to destination 2 eventually flows entirely through link 3.
This result seems to be in agreement with the actual behavior of ants in the real world which is basically directed in finding the best path to the food source while exploring other paths. This behavior leads to finding a robust path to the destination with enough dynamism to adapt to changes and to find new and better paths. multiple routes from source to destination in a balanced way such that overall delay and utilization of the links is optimized over the network. The ant colony implementation of ant behavior does not seem to be the best solution to this situation according to our observations of the results from the first approach.
In analytical solutions for the multi-commodity flow assignment problem, information about the links is used at the edge routers in a distributed way to calculate the best distribution of traffic into possible routes to achieve load balancing aimed at reducing delay and increasing link utilization.
Using probability tables for forwarding ants in the ant colony algorithm creates a feedback loop in favor of links that have better trip time results. This is the reason behind the convergence behavior of this method.
In the second approach we eliminate this mechanism and replace it with a round-robin selection of output links for forward ants. As expected the result as shown in figure 3 , shows a better load balancing between existing possible links to the destination represented in the form of closer probability values.
Inspired by the analytical solutions, in a third approach we used local processing of the link information gathered by the ants to achieve a better load balancing, which also results in a better delay performance and link utilization. Our results, as shown in figure 4 , show that a better and more robust load balancing (closer and more stable probability values) is achieved between link 3 and 4, compared to the previous cases.
Our experiments revealed that somewhere a friendly relationship between analytical and bio-inspired approaches has to be made. We believe the real ant in nature has a robust behavior but this process is slow and also is not necessarily promoting load balancing. While the evaporation property of ant pheromone brings some relevance to the concept of loadsharing, in fact the evaporation in ants is to provide the dynamism necessary for adaptive behavior. In other words ants have completely robust, adaptive and distributed behavior but this behavior is not aimed at load-balancing as an objective.
On the other hand there are some analytical solutions to balance the load in the whole network and to do resource allocation and resource assignment as a direct result.
Our findings show that there is opportunity for thinking differently. We can be inspired by the wonders of the nature but we need not imitate them. We are free to modify the bio-inspired approaches (ant in our discussion) to obtain new desired behavior. 
In conclusion, our suggestion regarding the use of ant algorithms is that improved behavior is possible by augmenting it with the analytical computation.
Our current work involves applying analytical solutions to ant like algorithms for routing and flow assignment to extract a robust and autonomic routing algorithm capable of tolerating predictable changes in traffic patterns and learn the unpredicted ones. 
Nowadays, the possibility to collect, store, and process very large amounts of data in combination with powerful data transformation and analysis techniques have raised privacy concerns such as when in early 2018, Cambridge Analytica had been granted access to millions of Facebook user profles for political campaigning without users being aware of it. Statutory counter measures to curb data misuse have in particular been undertaken by the European Union in May 2018 by adopting the General Data Protection Regulation (GDPR). The GDPR imposes strict rules on data processing, ownership, information obligation (including raising consent), transparency, and collection of user-related data. Since data protection laws penalize misuse of personal data reactively, they do not actively prohibit misuse on a technical level. In the literature, three major recommender system architectures have been proposed to address privacy issues arising from collecting large authoritative data pools.
Federated learning produces personalized recommendation models by communicating model building between a central server and mutually disconnected peers holding personal information [10, 22] . Thus, federated learning enacts model building centrally on data that is distributed across personal data owners.
In contrast, decentralized recommender systems feature direct interactions between distributed peers without a central server. They are commonly built on top of fle-sharing peer-to-peer networks [3, 21, 28, 31] that use gossip mechanisms [19] in order to establish a logic overlay network for fast network search and network resilience in view of peers joining or churning the network. In short, dissimilar peers are dropped from and similar peers are added to views (lists of visible peers in the network) iteratively. In so doing, the network establishes homogenous interest groups, which share recommendations explicitly among each other.
Pervasive (or ubiquitous) recommender systems [23, 27] are systems that often revolve around location-aware recommendations of for instance items in a nearby shop, restaurants, or events in proximity. The location-based approach naturally circumvents transmitting personal data to a central remote authority for recommendation by instead requesting closeby profle as well as context information.
Pervasive recommender systems are naturally confronted with limited profle data, for usually only a small subpopulation is available in proximity. Fortunately, there are indications that integrating context data into the recommendation process allows to outweigh profle data scarcity [1] .
Among these three approaches we believe that pervasive recommender systems yield the highest potential for data privacy for two reasons. First, the availability of context data as well as mobile compute power on smartphones increases rapidly rendering more and more complex recommendation algorithms feasible on smartphones holding personal data. Second, the model building process happens on-device and does not require connectivity to the network. Consequently, peers become invisible in the network when they do not interact with other peers thus adding privacy on the level of model building. In spite of their potential for recommendation, we see two main burdens. First, pervasive recommender systems are susceptible to data privacy issues since the recommendation mechanism usually builds on the exchange of raw profle data with nearby peers. Second, local scarcity of profle data renders item recommendations on location-independent items taxing. We believe that introducing gossip-based mechanisms and data sampling strategies to the feld of pervasive recommender systems can alleviate both issues. We present the following preliminary results:
• The design of Propagate and Filter, a gossip-based method that addresses the problem of data privacy and data scarcity in pervasive recommender systems. • An implementation of the propagation part in the form of an Android mobile application utilizing Google's Nearby Connections API and its evaluation.
In the present work, we propose a method for disseminating recommendations epidemically in combination with an on-device fltering process for which we proposed a mobile software architecture in [5] . The most similar work is that of Barbosa et al. [25] that propose device-to-device raw profle exchanges in an opportunistic networking scenario. Yet, it difers in that they neither address scalability nor privacy issues. Other related work subsumes: Information Dissemination: Technical works on device-to-device information exchange in proximity are key enablers for collaboratively built recommender systems on mobile devices. The Haggle API [24] implements data-centric message forwarding using Bluetooth, Ethernet, and WiFi 1 . In [26] , the authors implement a Bluetooth-based middleware to create opportunistic networks between passing users. On top of such enablers, the authors in [7] leverage real-world social phenomena such as large crowds at sports events for event-based mobile communication.
Information Filtering: Many approaches to collaboratively and decentrally build recommender systems on mobile devices in pervasive computing environments disseminate information epidemically and generate recommendations on top of user similarity. User similarities have been found to be reliably estimated by contextual information in the form of proximity at a music festival [11] 1 http://user.it.uu.se/~erikn/papers/haggle-arch.pdf or spatio-temporal information in the tourism domain [16] . Estimating user similarity on rating vectors has been performed on subcommunities [13] or afnity networks [12, 29] .
At the proposed information dissemination method's core is the social movement of people carrying mobile devices. On top of that core movement, we conceive a background data exchange between devices whenever they are geographically close to each other followed by an on-device customizable fltering process. We call this method Propagate and Filter.
Each device carries four types of data (see Figure 1 ), where the entire stack of data 2 is used for the derivation of local personalized recommendations.
• Peer Preference List: A list of items rated by the peer 3 . It can contain binary or scalar ratings. In our prototype implementation, the peer can rate movies, where each movie is identifed with a unique identifer provided by the publicly available Internet Movie Database 4 (IMDb). Peer preference lists are kept on the device. • Neighborhood Preference List: Every peer mixes 5 previously collected neighborhood preference lists received from the k most similar peers into a single list of item ratings. It is thus an aggregated preference list of an unknown subset of peers. Neighborhood preference lists get propagated to other peers. Note that every peer controls the amount of his/her own peer preference list that gets propagated to nearby peers. • Similarity Data: Any kind of data that can be used for peer similarity comparison. Similarity data gets propagated to nearby peers and therefore has to be privacy-preserving. Privacy-preserving similarity comparison can among others be performed on item vectors [4] as well as texting data [15] . • Context Data: Data that characterizes the encounter such as location, time, weather, or peer activity (running, eating, commuting) that can be sensed (for example via sensors) or retrieved (for example from the web) [6, 30] .
When two or more peers are geographically close to each other, their smartphones establish pairwise fast and secure connections and exchange their neighborhood preference lists and similarity data (propagation). At propagation time the received data is enriched with context data such as time or location characterizing the encounter. 
Data collection in the propagation step includes unfltered data from every encounter. It is necessary to flter by similarity in order to arrive at relevant information. Upon receiving data from another peer, the fltering process starts on the device. Three steps happen:
(1) Similarity Comparison: Similarity data is used in order to compare peer similarity between sender and receiver. (2) Sample Neighborhood Preference List: If the peer similarity is above the k-th highest, resample the neighborhood preference list on the basis of the peer's preference list and the neighborhood preference lists by the k most similar peers. (3) Update Personal Recommendations: Run a recommendation algorithm 6 on the locally available data (see Figure 1 ) in order to derive new recommendations or update ratings of previously generated recommendations.
Propagate and Filter's propagation step establishes wireless connections between smartphones in proximity in an ad-hoc fashion, exchanges similarity data and neighborhood preference lists, and thereafter terminates the connection. Consequently, the network topology is essentially disconnected and no information on interpeer relationships -such as is the case with Peer-to-Peer network overlays in social networks [20] , recommender systems [3] , or vehicular networks [17] -are exploitable. The entire peers' local databases form a geographically distributed and essentially disconnected database, where every peer holds only a very limited portion of data. Data query and data search are thus not possible at will. Access to other peers' data is limited to the time of contact and amount of data individually made available by nearby peers. 6 Propagate and Filter is independent of any specifc recommendation algorithm. The fltering techniques described in [2] are viable approaches leveraging contextual data.
We call this property Privacy by Disconnection. It is Propagate and Filter's contribution to data privacy.
Gossip protocols require a connected peer-to-peer network in order to converge similar peers toward each other, where network connectivity is retained by peer sampling [18] . In traditional decentralized recommender systems, peer sampling that requires network connectivity is applicable since neither items nor peers are spatial, that is both can be moved in the network at will. In the Propagate and Filter scenario, peers are spatial and cannot be moved in the network at will, though items can. Therefore, Propagate and Filter proposes to converge the recommendation list of latent interest communities in the following way: Peer sampling does not have to be administered by any protocol since it is performed by the global movement of self-organized agents carrying smartphones. Propagate and Filter unfolds its dissemination potential at locations of high degrees of peer mobility such as urban areas [9] . When a peer receives data from a similar peer, he/she resamples his/her neighborhood preference list and else does nothing. As a consequence, Propagate and Filter creates a constant fow of item recommendations that fows between similar peers and dries out between dissimilar peers. This property allows to relay recommendations between peers that have never been geographically close to each other and avoids having to transmit any information on dissimilar peers. In that sense Propagate and Filter addresses the profle data scarcity problem prone to pervasive recommender systems.
We present an Android mobile application that implements a prototype of the propagation step described in Section 3.2. We restrict to the transmission of peer preference lists for reasons of simplicity. The application is available in the Google Play Store 7 .
The prototype allows to search and rate movies locally which are registered and uniquely identifed in the IMDb. Movie ratings are scalar, ranging from 1 to 5 stars, and stored in the format (userID, movieID, scalarRating). A list of movie ratings implements the peer preference list introduced in Section 3.1. Once ratings have been specifed by the user, he/she can activate sharing.
The application's active sharing mode applies advertise -the device broadcasts its existence to other devices in proximity -and discover -the device listens on other device's broadcasting. The sharing process is handled by the Google Nearby Connections API 8 and entirely happens in the background. The Google Nearby Connections API connects devices using one of the three wireless technologies Bluetooth, Bluetooth Low Energy (BLE), or WiFi, automatically selecting the most efcient one in each scenario. When two smartphones establish a connection, ratings are exchanged immediately and stored locally. Note that ratings can be exchanged without a connection to the internet as it is commonly d the case in underground trains. As soon as the internet connection is re-established, detailed movie information such as the movie genre, cast or, trailer can be queried via movieIDs.
We evaluate the prototype implementation in view of its potential to facilitate the proposed pervasive recommender system sketched in Section 3 in particular in urban areas. We conducted six distinct experiments, where every experiment was conducted with Nexus 5 smartphones that support Bluetooth up to version 4 (including BLE) and Wif direct. We re-ran experiments 10 times by default, unless stated otherwise, with distinct experiment setups in order to have empirical evidence of the results' soundness.
(1) Share Large Amounts of Ratings: We share a bulk of 1000 ratings, accounting to roughly 100 kB uncompressed. As soon as the connection is established, all 1000 ratings get transmitted instantly and reliably without any data loss. (2) Share Data Between Multiple Devices: We share ratings between four smartphones simultaneously holding mutually disjoint ratings. Ratings get transmitted correctly and losslessly. (3) Share Data in Public Transportation: We successfully share ratings between three devices in the bus and the underground in the urban city of Berlin being exposed to many WiFi and Bluetooth disturb signals. The loss of internet connectivity does not impair the data transfer. As soon as internet connectivity is re-established, the shared placeholder recommendations get flled with movie metadata fetched from the Open Movie Database 9 (OMDb) API. (4) Efective Transmission Range: Recall that the Nearby Connections API comprises Bluetooth, BLE, and Wif direct under its hood. All three allegedly provide an efective transmission radius of 10 meters for class 2 devices such as smartphones. We tested transmission ranges between 3 and 12 meters, both outdoors and indoors, and with and without obstacles. The results are shown in Table 1 . We conclude that the efective radius of ratings propagation is between 3 and 6 meters. (5) Average Initial Connection Delay: We measure the initial connection delay for two devices, that is the elapsed time before a connection gets established. We place the devices at a distance of 1 meter in order to guarantee connectivity, where the connections were made with distinct app sessions (partner device's id not in cache). The average connection delay (pre-connection) has to be performed continuously, we would like to know whether or not advertising, discovering, and sharing information in the background for extended periods of time is feasible or not. Since we cannot experiment in a real-life scenario with at times no devices to connect to, and then multiple devices to connect to, we only measure the application's pre-connection battery drainage, which therefore presents a lower bound for battery drainage. We use two devices reset to factory settings. We track the battery levels for three distinct scenarios (a) application running in the background with sharing on, (b) with sharing of, and (c) factory settings, where in all three cases the displays were of. The results are shown in Table 2 .
The experimental fndings indicate that the Propagate and Filter's propagation step works reliably with larger amounts of ratings, in multi-device scenarios, and in areas without internet connectivity such as underground trains. Yet, the implementation has certain limitations. First of all, information can only be disseminated reliably within a radius of 6 meters, which on the one hand strengthens privacy, and on the other limits the number of potential peers to share information with. Furthermore, the average initial connection delay of 25.9 seconds is considerable and does not allow to share information between for instance passing pedestrians, yet includes scenarios such as waiting at the trafc lights, sitting next to each other in a café or restaurant, or taking public transportation. Last but not least, battery drainage is relatively high at at least 5% per hour, which thus prohibits continuous advertising and discovery. Apart from ever improving transmission standards and hardware, we believe that it is possible to limit activity and inactivity of the sharing process efciently on the logical level leveraging sensor information on smartphones.
Future work includes the launch of the mobile application for the collection of usage data. Recall that we left the sampling process and the recommendation algorithm as placeholders (see Section 3.3 (2) and (3) respectively). Once usage data is collected it will be possible to test distinct combinations of sampling and recommendation strategies. Furthermore, the evaluation of feedback on the application including the user interface and user experience is due.
Over the years, the performance of monocular 3D Human pose estimation has improved significantly by leveraging complex CNN models. [37, 18, 35] . However, these methods rely heavily on large-scale 3D pose annotated training data, which is difficult and costly to obtain, especially under in-the-wild setting for articulated poses. The two most popular 3D ground-truth annotated datasets, Human3.6M [7] and MPI-INF-3DHP [13] , have 3.6M and 1.3M annotated poses, respectively. Unfortunately, these datasets are biased towards typical indoor setting like uniform background and illumination and lack real-world environment variations [37] . However, it is relatively easier to obtain time-synchronized video streams of human poses from multiple different viewpoints. Therefore, techniques that can employ un-annotated multi-view human-pose data to learn the 3D structure and geometry could prove benefi- * -equal contribution cial for human-pose estimation with small amount of annotated data. To this end, we propose a metric learning based approach to jointly learn a 3D human pose embedding and pose regression using the embedding from synchronized videos of human motion with very limited pose annotations. Our approach doesn't require camera extrinsics or prior background extraction. Therefore, it can be easily extended to train with further un-annotated in-the-wild data. We seek motivation from a recent work in [22] , where image generation in different views via a geometry-aware latent space is used to improve pose-estimation under limited 3D supervision. This method, however, requires camera extrinsics and static background during training, which limits its application to indoor datasets. Our proposed approach is free from these constraints and, therefore, can potentially be used for in-the-wild setting. Moreover, we also show superior performance with faster inference.
We utilize our framework to improve pose estimation accuracy under limited 3D supervision. We show that weak supervision in learning the embedding ensures that our model's performance degrades gracefully when 3D supervision is progressively reduced. Additionally, we eliminate the subject-specific appearance information from our latent embedding with the help of an adversarial mechanism which leads to further improvements and outperforms the current state-of-the-art [22] . Lastly, we use smaller network architecture that affords 3X faster inference time. A simplified overview of our approach and its utilization is shown in Fig. 1 . The formulation of our loss function leads to a view-invariant embedding, and in Sec. 5, we demonstrate the richness of our learned embedding to capture human pose structure invariant to viewpoint by way of carefully designed pose retrieval experiments and establish novel benchmarks on Human3.6M and MPI-INF-3DHP to facilitate future research. A summary of our contributions is,
Figure 1: A schematic diagram explaining the motivation of our work of learning an pose embedding from multi-view images and utilizing the embedding for 3D pose estimation and view-invariant pose retrieval. The learned embedding space lies on the surface of a multi-dimensional unit hyper sphere. A detailed 2D T-SNE visualization of the embedding space in presented in the supplementary material.
• Formulating view-invariant pose retrieval benchmarks based on Human3.6M and MPI-INF-3DHP datasets.
In this section, we first review prior approaches for learning human-pose embedding followed by a discussion of previous weakly supervised methods for monocular 3D human pose estimation to bring out the differences between our approach and the previous art. Later, we discuss the usage of deep metric learning in capturing image similarity.
Historically, human-pose embedding have been employed in tracking persons [34, 10] . Estimation of 3D human pose and viewpoint from input silhouettes via learning a low dimension manifold is shown in [4] . Pose regression and retrieval in 2D by learning pose similarity embedding is shown in [9, 16] , but they require 2D annotations. In [27] , the need for 2D annotations is eliminated by using human motion videos and temporal ordering as weak supervision with a metric learning based loss. Unlike the aforementioned approaches, we learn a view-invariant 3D human pose embedding by taking advantage of semantically similar images in synchronized multi-view videos. In [29] a 3D pose embedding learnt using an over-complete autoencoder for better structure preservation, however unlike us they requires require full 3D annotations.
Majority of supervised 3D Human Pose Estimation algorithms [14, 12, 31, 21, 17] use 3D pose labels to train a model for regressing 3D joints locations from images or decouple the problem into 2D joint regression followed by 2D-to-3D lifting. In either case, they need large amount of annotated 2D and 3D training data. Another line of work [33, 26] focuses on training for 3D estimation with datasets capturing the scene in multi-view images. In [19] , approximate 3D human joint labels for supervision are generated by triangulating its corresponding 2D annotations from multiple view images. Utilizing multi-view images during training has recently been proposed in [23, 22, 20, 3] . Methods using multi-view images can further be classified into the following categories, strong 2D and limited 3D supervision -Methods mentioned in [23, 20, 3] use full 2D supervision from in-thewild datasets like MPII [1] to either estimate 3D pose from images or perform 2D to 3D pose lifting. In [3] , a latent embedding capturing 3D pose is learned by reconstructing 2D pose from the embedding in a different view. A shallow network requiring much less supervision is subsequently learned to regress 3D pose from the embedding. In [20] , back-projection of predicted 3D pose to its 2D representation and its difference with the input 2D pose is used as a weak supervision in [20] . Additionally, it uses multiple temporally adjacent frames at inference to refine predictions. A network with pre-trained weights for 2D pose estimation is used for 3D estimation in [23] . limited 3D supervision -To alleviate the need for a large amount of 2D annotations, [22] learns an unsupervised embedding and estimates pose from it with limited 3D supervision. Novel view synthesis using synchronized videos from multiple views is used to learn a geometry aware embedding capturing human pose. This method however still requires camera extrinsics and background extraction.
Our proposed method also utilizes synchronized videos from multiple views to learn a pose embedding but unlike [22] does not require camera extrinsics and background information. Moreover, due to our metric learning based approach, we do not require to perform image-reconstruction that affords smaller networks, Resnet-18 [6] vs. Resnet-50. Subject-specific appearance disentanglement from human pose embedding has been shown in [22] using appearance swap followed by an image reconstruction task. Such swapping mechanism doesn't guarantee removal of appearance from pose embedding as the network has an alternate information pathway through the pose branch. We, on the other hand, adopt the method in [11] using adversarial losses to remove the subject-specific appearance information from our pose embedding.
To learn image similarity, images are mapped to a low dimensional embedding space via a CNN and trained with a contrastive or triplet loss. In contrastive loss [25, 36, 5] , semantically similar image pairs (positive pairs) are mapped close together in the embedding space while those dissimilar content (negative pairs) are mapped far apart. In triplet loss [2, ?] , the hard constraint of contrastive loss is relaxed by ensuring a relative separation between positive and negative image pairs by a pre-determined margin. Hence, the euclidean distance between two images in the embedding space gives the measure of their similarity. For our application a pair of images are semantically same if they represent humans with the same underlying 3D pose.
The performance of models using either of the losses is highly dependant on the quality of dissimilar samples used during training [25, 36] . The current state-of-the-art image descriptor learning framework Hardnet [15] provides a good-trade-off between performance and training time by selecting the hardest negative within a batch. Inspired by its performance and simplicity in training, we adopt the hardnet to learn our pose embedding.
Our proposed approach is comprised of two modules i) learning an embedding capturing human pose information from multi-view time synchronised videos using metric learning ii) regressing 3D human pose from the embedding using minimal 3D supervision. We jointly learn the two modules as shown in Fig. 3 . Metric learning provides a weak supervision and reduces the dependency on large 3D annotations in our framework while pose regression guides the framework to learn pose specific features. The following sub-sections explains the two modules,
To learn our pose embedding via metric learning, we utilise Hardnet framework [15] due to its the state-of-the-art performance in image patch matching invariant to camera viewpoints. The datasets used for training have the following generic format. The entire data is divided into images belonging to one of S = {S 1 , S 2 , . . . S n } set of subjects. The set P ⊂ IR 16×3 is the set of all possible poses and each pose is viewed from V = {v 1 , v 2 , . . . v q } set of viewpoints.
In the hardnet training regimen, each batch consists of paired anchor( § va p ∈ X ) and positive( § v b p ∈ X ) images that share same pose p ∈ P but taken from different viewpoints v a and v b . X ⊂ IR 3×256×256 is the set of all images. It is to be noted, since we use time synchronization to choose a pair of anchor and positive, it is implied that they share the same subject. However different anchors within a batch can be from different subjects. We pass both the anchor and positive images through feature extractor (F θ F : X → Ψ; Ψ ⊂ IR 512×4×4 ) to generate features {ψ
The feature extractor network is parameterised by θ F . The features are then finally passed through an embedding generating network (G θ G : Ψ → Φ; Φ ⊂ IR dim φ ; where dim φ is dimension of our embedding). Let's assume we feed anchor and positive images to F in batches of m. Once corresponding features {φ pi respectively. Mathematically, the sampling is formulated in Eq. 1. Here, α denotes the margin.
The average triplet loss over the batch is then given by,
Similarly, the average contrastive loss is given by,
Our anchor and positives examples always share the same subject and it results in unwarranted appearance bias in the embedding. Hence, to improve pose accuracy, it is necessary to disentangle appearance information from our learned embedding. To this end, we introduce an adversarial loss on our ResNet feature extractor F θ F so as to fool an ap-
Our formulation is inspired from [11] where adversarial training is used to disentangle individual identity and other facial information from images of faces. Formally, we define our adversarial formulation with input image x i and subject label y i ∈ Y and predictionŷ i in Eq. 4,
In Eq. 4, L class tries to make the classifier M predict higher probability for the correct target while L adv tries to fool the classifier to predict uniform probability for all subjects by tuning the ResNet feature generator F. At equilibrium, F generates features Ψ which are devoid of any subject appearance information. Note that our weak supervision losses namely, L class , L adv , L cnstr / L trip do not require camera extrinsics, background extraction, pose annotations etc. and the only sources of supervision are synchronizing the videos, annotating the subject and pre-trained ImageNet [24] weights.
Most 3D human pose estimation approaches focus on regressing pose in the local camera coordinate system. In this representation, frames captured from different camera views but of the same time instant will be associated poses with different 3D co-ordinates values of the body joints. However, the frames are all mapped to the same point in our embedding space irrespective of their viewpoint by our formulation. Hence, regressing pose in this representation from our learned embedding is ambiguous as the relation to be learned by the regressor is one-many. In this regard, one can utilise pose represented in the MoCap system's coordinate system. We term this representation as global pose. In this representation, frames captured from different viewpoints belonging to a particular time instant are associated with one pose. However, frames captured at different time instants can contain poses which are rigid body transforms of one another while having same set of 2D projections. In such cases, regressing pose from our embedding is again learning a one-many relation. In Fig. 2 , an example of such ambiguity is illustrated.
To estimate 3D pose from our embedding and bene- fit from the embedding loss, we formulate a corresponding view-invariant pose representation. We term this representation as canonical pose. To ensure consistency among poses captured from different camera views and at different time instants in our canonical pose representation, we ensure that the bone connecting the pelvis to the right hip joint is always parallel to XZ plane. In Human3.6M dataset, the upward direction is +Z axis while XY plane forms the horizontal. So, we rotate the skeleton about the +Z axis until the above mentioned bone is parallel to the XZ axis. This makes the depth aligned along Y axis. We don't require any translation since the joint positions are root relative with pelvis being the root. As an added bonus, unlike pose estimated in camera coordinates, our predicted canonical pose does not change orientation with variations in camera viewpoint. A similar approach to achieve a rotation invariant pose is suggested in [32] . Note that the canonical pose is constructed directly from MoCap system's coordinates and doesn't require camera extrinsics. We learn our canonical pose from the latent embedding Φ space mentioned. To this end, we use a shallow network (H θ H : Φ → P), as shown in Fig. 3 . We regress pose using L pose = p −p 1 , with target pose p ∈ P and predicted posep.
We train our framework with all the losses simultaneously by default and optimize different network parameters according to Eq. 5. Note that the the gradient from L class does not flow through the network F. We also provide ablation on the different losses to understand their impact on pose estimation accuracy.
The metric learning loss used to learn Φ serves as a weak supervision in canonical pose estimation. The L conrst loss ensures latent embeddings φ 
We build our architecture on the ResNet framework and choose the 18 layer version in our implementation. We only use the first 4 residual blocks and initialize them with pretrained ImageNet [24] weights. In addition, we modify the batch-norm layers by turning off the affine parameters as suggested in [15] . When the input image size is 256 × 256 , the output of the ResNet network is 512 × 8 × 8. We down- p are a pair of anchor and positive images taken from different camera views. F is the ResNet based feature extractor. G maps features extracted ψ from F to our embedding φ. The Hard Negative Sampling module performs in-batch hard mining as given in Eq. 1. Network H regresses posep from our embedding φ. Classifier M is used to classify subjects from set S from features ψ. L class , L adv and L pose are discussed in Sec. 3.2. Network blocks sharing same colour also share parameters. sample the ResNet output by half using a MaxPooling layer to get Ψ. The embedding network G maps it to the output embedding of dimension dim φ , using a Fully-Connected layer and BN layer followed by L2-Normalization as done in [15, 30] . The value of dim φ is 256 for all our experiments. The classifier network M consists of Conv(512, 256, kernel=1), BN, ReLU, Conv(256, 256, kernel=4), BN, ReLU, FC(256, |S|) with |S| = 5.
For regression, similar to [12] , we normalize the dataset for each joint. The pose regression network G consists of fully-connected layer FC(256, 48), with Φ ⊂ IR 256 . We choose the margin α for L conrst to be 0.6. Adam [8] with default parameters (α = 0.9, β = 0.99) is used as the optimizer with initial learning rate 10 −3 . The model is trained for 25 epochs with the learning rate dropped by 0.1 after every 15 epochs with a batch size of 128. A schematic diagram of our network architecture is shown in Fig. 3. 
We use the popular Human3.6M [7] and MPI-INF-3DHP [13] datasets.
• Human3.6M -The dataset contains 3.6 million frames captured from an indoor MoCap setting with 4 cameras(V). It comprises of 11 subjects (actors)(S), each performing 16 actions with each action having 2 sub-actions. Following the standard protocol [28] , Protocol 2, we use subjects (S1, S5, S6, S7, S8) for training and (S9, S11) for testing. As used by several other methods, we use images cropped using subject bounding boxes provided with the dataset and temporal sub-sampling to include every 5 th and 64 th frame for training and testing phase, respectively,
• MPI-INF-3DHP -This dataset is generated from a MoCap system with 12 synchronized cameras in both indoor and outdoor settings. It contains 8 subjects(S) with diverse clothing. We use the 5 chest height cameras(V) for both training and test purposes. Since the test set doesn't contain annotated multi-view data, we use S1-S6 for training and S7-S8 for evaluation.
We perform the same quantitative experiment as presented in Rhodin et. al [22] to establish the benefits of the learned embedding in pose estimation. We evaluate using two well adopted metrics, MPJPE and Normalized MPJPE (N-MPJPE) (introduced in [23] ) which incorporates a scale normalization to make the evaluation independent of person height as our evaluation metric. We compare our proposed approach and its variants against a baseline which only uses L pose . In addition, we compare our method against the approach proposed by Rhodin et. al [22] and [23] , although it estimates human poses in the camera coordinate system. [22] . Our proposed model outperforms the current state of the art.
We also report the performance of Rhodin et. al [22] using ResNet-18 as the feature extractor instead of ResNet-50. It is to be noted [22] uses additional information at training time in form of relative camera rotation and background extraction which requires sophisticated, well calibrated setup. We acknowledge existence of more accurate methods than [22, 23] on Human3.6M when abundant 2D and 3D labels are available. However, like [22] to highlight the point of limited supervision, we omit them in our comparison. We also report performance of [3] which requires limited 3D supervision but uses full 2D supervision from MPII [1] dataset. We did not include the results of [20] as it requires multiple temporally adjacent frames at inference. We report both N-MPJPE values when our model is trained and tested on Human3.6M dataset with progressively less supervision for pose regression in Fig. 4 . The amount of supervision is reduced gradually from full supervision using all 5 subjects, to S1+S6, only S1, 50% S1, 10%S1 and finally 5% S1. Our proposed model clearly outperforms the baseline as 3D supervision is reduced with a gain of 33 mm (21.5%) N-MPJPE when only S1 is used for supervision). The performance of our model shows little degradation even on further reduction in supervision. A variant of our proposed model with appearance not disentangled also beats the baseline convincingly but performs worse compared to when appearance is disentangled. The observation validates the importance of L conrst in providing weak supervision capturing 3D pose and the need to eliminate appearance bias. Qualitative comparison of our method against the baseline is shown in Fig. 4 . In Table. 1, we compare MPJPE and N-MPJPE values of our approach against baseline and [22] . Considering N-MPJPE, our method outperforms [22] by 14 mm when fully supervised on 3D data and by 2 mm when supervision is limited to S1. When MPJPE is considered the difference is 4 mm. Interestingly as mentioned in [22] , the performance of [23] drastically falls when pre-trained weights from strong 2D pose supervision is not used (reported in Table 1 as Rhodin [23] * and Rhodin [23] ). Table 1 : Comparing N-MPJPE and MPJPE values between different approaches on Human 3.6M dataset when supervised on all 5 subjects and on only S1. Note: Pre-trained ImageNet weights are used to initialize the networks by all the methods. Methods or its variants marked with '*' are supervised with large amount of in-the-wild 2D annotations from MPII [1] dataset either during training or by means of a pre-trained 2D pose estimator. All other methods use much weaker supervision by assuming no 2D annotations and ours outperforms the state-of-the-art [22] in such settings.
Method N-MPJPE MPJPE We additionally compare performance of our learning framework when target pose is represented in MoCap's(global pose) against our canonical representation in Table. 2. The increase in N-MPJPE by 43mm for global pose validates the importance of our canonical representation to the efficacy of our approach.
An additional benefit of our proposed framework is that it uses a much smaller ResNet-18 feature extractor as compared to ResNet-50 used in Rhodin et. al [22] . This enables our model achieve an interference time of 24.8 ms in comparison to 75.3 ms obtained obtained by [22] averaged over batch size of 32 using NVIDIA 1080Ti GPU. Hence, our method performs roughly 3X faster inference while attain- Figure 5 : Qualitative results on canonical pose estimation by our proposed framework (Ours) against our Baseline on Human 3.6M test split (S9, S11). Both the models are trained with supervision from labels of subject S1. Our method produces more accurate estimates for even for challenging poses like 'sitting', 'kneeling', 'bending' ing better accuracy.
In this section, we demonstrate the quality of our learned embedding through a series of retrieval tasks and provide benchmarks against an oracle on popular human pose datasets. Given a query image of a human from a particular view, our learned embedding ensures that images from all the other views are mapped close to it in the embedding space.
To quantify the view-invariance, we formulate Hit@K which measures the percentage of queries with the exact pose among the top K poses retrieved through the embedding. For example, 85% Hit@5 indicates that out of 100 queries, 85 queries have atleast one of the images with exact pose but from a different viewpoint in the top 5 retrievals. We also define Hit@K All which registers a hit when all of the images with exact pose are present in top K retrievals.
Further, we want to ensure that the images with similar pose should be clustered together in the embedding space. In this regard, we propose Mean PA-MPJPE@K which measures the Procrustes Aligned Mean Per Joint Position Error(PA-MPJPE) of K closest neighbours from other views. The retrieved poses, although similar to the query in terms relative skeleton configuration, can have different orientations. Hence we use PA-MPJPE, which is MPJPE calculated after rigid alignment of retrieved pose with the ground truth of query pose, for a fair evaluation.
We compare our model against an oracle which uses ground truth 3D annotations. Given a query image, we ensure that the retrieval database contains images taken from viewpoints other than that of the query image. It is done to clearly bring out the view invariance property of the proposed embedding. The aforementioned two performance metrics are used to quantify the pose retrieval performance, namely Mean PA-MPJPE@K and Hit@K. First, we report the Mean PA-MPJPE@K between query pose and its K nearest neighbors in the embedding space. In Fig. 6 , we show comparison of Mean PA-MPJPE@K of retrieved poses when retrieval is done from images with: Case 1: all test subjects including that of query's. Case 2: all test subjects except that of query's, termed as cross. We report our results relative to the oracle. The horizontal plots with low errors suggest that our model picks poses similar to that of oracle irrespective of K. The error is lower for Case 1 than Case 2 due to the presence of images from different viewpoints sharing the exact pose as that of query's.
Our second metric, similar to [9] , is computing Hit@K which measures the occurrence of a correct pose among top K retrievals using nearest neighbors. A retrieved pose is considered correct if it is exactly same as the query pose but from a different viewpoint. However, unlike [9] , we do not retrieve poses having same viewpoint as that of query. We measure Hit@K under Case 1 settings previously mentioned. Under easy('E') setting, a hit is registered when a retrieved image with the correct pose differs in viewpoint with the query by less than 90
• . Under hard('H'), the viewpoint difference has to be more than 90
• . Unlike, 'E' and 'H', in 'All', a hit is considered when correct poses from every viewpoint other than that of query are present in the top K. The evaluation of this metric on our model is shown in Fig. 7 . We achieve high accuracy rates of more than 85% for easy('E') viewpoint differences on Human3.6M even for low values of K = 2, 5. For hard ('H') viewpoint differences, the performance goes down but still remains above 70%. However, the most impressive performance of our model is shown when retrieving 'All' other views with K = 5. The accuracy is close to 50%. This implies our model retrieves all the 3 corresponding views in the top 5 slots, half of the time. The performance on MPI-INF-3DHP is comparatively lower than Human3.6M, specially on low K values. We attribute this to MPI-INF-3DHP having smaller training data. We have included qualitative results of our retrieval experiments in the supplementary material.
An interesting observation from pose regression results shown in Fig. 4 , is that our proposed model performs worse than the baseline under full 3D supervision. This is also observed in [22] , and one possible explanation is that the additional weak supervision losses lead to a joint optimum which is sub-optimal for pose regression. But in case of reduced 3D supervision, all the losses work in synergy and produce a high improvement over the baseline pose accuracies. To analyse this further, in Table. 3 we show the result of adding progressive pose supervision on Mean PA-MPJPE@5 for cross subject retrieval on Human 3.6M dataset. We observe that even a limited amount of pose supervision (5% S1), reduces Mean PA-MPJPE@K by 13.23mm. We can also see that using only L pose supervision on S1 without other losses, leads to poor retrievals. Here, we note that a single embedding trained with both L pose and L conrst outperforms the respective task specific embeddings, when 3D supervision is limited. 
In this paper, we demonstrated a metric learning approach to capture 3D human structure and its effectiveness in both pose estimation and pose retrieval tasks. More specifically, the information from our embedding reduces the need for 3D supervision when regressing human pose, enabling our method to outperform contemporary weaklysupervised approaches even while using a smaller network. Further, we provided strong benchmarks for view-invariant pose retrieval on publicly available datasets.
In future, we plan to use multi-view synchronised videos captured in-the-wild and synthetically generated, consisting of images taken from a large no. of viewpoints with diverse appearances, to improve the quality of the embedding and in the wild generalisation. Also, we plan to apply our approach to recognize actions from unseen viewpoints. Figure 8 : Shows top 5 pose retrievals using our embedding on Human3.6M [7] and MPI-INF-3DHP [13] datasets. The top row marked in Red is the query image, the next five rows shows retrieved images taken from different viewpoint having similar poses.

In Fig. 8 , we provide examples of retrieved poses from viewpoints different to that of the query from the popular Human3.6M [7] and MPI-INF-3DHP [13] datasets.
In Fig. 9 , a 2D T-SNE visualisation of our leaned pose embedding space is shown. Figure 9 : The top image shows a 2D T-SNE visualisation of our learned embedding space on test split of Human3.6M [7] . In bottom left and right images, zoomed in views of green and red boxes from top are shown respectively. One can observe the clusters of similar poses formed in the zoomed in boxes. Note: The 2D visualization provided is an approximation of the original embedding space which lies on the surface of a multi-dimensional unit hyper-sphere. Hence, there are inconsistency in smoothness in the pose space at some places.
transcriptional and metabolic changes that increase production and accumulation of the compatible solute glycerol. Mounting a rapid response to increased osmolarity is essential to yeast survival [3, 4] . Accordingly, S. cerevisiae can activate the HOG pathway within one minute of experiencing an osmotic shock [5] . Yeast can also effectively respond to rapid periodic oscillations (with frequencies up to 0.0046 Hz) between low and high external osmolyte concentrations [3] .
Despite its importance during periods of increased osmolarity, unintended activation of the HOG pathway during growth in normal osmolarity conditions is severely deleterious [6, 7] . The Sln1-Ypd1-Ssk1 three-component phospho-relay is responsible for maintaining inactivation of the HOG pathway under normal conditions. This three-component phospho-relay is a variant of the twocomponent signaling systems used by many prokaryotes for osmoregulation, chemotaxis, and other key cellular processes. Sln1 is active in vivo as a membrane-bound dimer [6] . Under normal osmolarity conditions, Sln1 autophosphorylates on a histidine residue and then irreversibly transfers the phosphate to an aspartate in its response regulator (RR) domain (Figure 2A ). Aspartatephosphorylated Sln1 binds to the histidine-containing phospho-transfer (HPt) protein Ypd1 and reversibly transfers its phosphate to Ypd1 ( Figure 2B ). Finally, phospho-Ypd1 transfers its phosphate to dimeric Ssk1, preventing it from interacting with Ssk2 (or the functionally redundant Ssk22) and inhibiting HOG pathway activity [5, 8, 9] . The sequence of phosphate transfers in the three-component relay is summarized in Figure 2A . In response to osmotic shock, the phospho-relay is inactivated, and Ssk1 is rapidly dephosphorylated through an as-yet unknown mechanism. Unphosphorylated Ssk1 then activates Ssk2 and Ssk22, leading to induction of the HOG pathway [1, 2] . It is thus the essential controller of HOG pathway activity, and variations in its concentration could compromise fitness.
There is limited existing experimental evidence that the phospho-relay is able to maintain robust phosphorylation of Ssk1 and inactivation of the HOG pathway despite changes in the levels of some pathway components [7, 9, 10] . We undertook a comprehensive characterization of the sensitivity of HOG pathway activation to changes in the expression levels of the phospho-relay proteins Sln1, Ypd1, and Ssk1. We systematically under-and overexpressed the three proteins using the GEV artificial induction system, which allows for rapid and nearly gratuitous induced expression of individual yeast genes [11] . We found that the phospho-relay maintains inactivation of the HOG pathway even after moderate perturbation of Sln1, Ypd1, and Ssk1.
We developed a detailed, biochemically realistic mathematical model of the HOG pathway three-component phospho-relay to elucidate the mechanism underlying this robustness ( Figure 2C ). Our model incorporates extensive structural and mechanistic information about the phospho-relay and considers nearly all possible interactions between the three relay proteins. We used massaction kinetics and algebraic calculations to characterize the steady-state behavior of the model. Steady-state algebraic models are a useful alternative to existing computational models of the HOG pathway for understanding robust behavior [3, 7, 12] . Unlike numerical simulations [3, 7, 12] , algebraic manipulations can be done without ever assigning specific values to the parameters (i.e., the rate constants in the reaction network), many of which are difficult or impossible to measure experimentally [13] . This advantage enabled us to design and analyze a more biochemically realistic model. A steady-state approximation Phospho-transfer proceeds from Sln1 (after autophosphorylation on H576 and transfer to D1144) to Ypd1 to Ssk1, as indicated by the numbers in circles. B Crystal structure of Ypd1 (green) in complex with the response regulator domain of Sln1 (Sln1-R1, red). Drawn from data presented in [18] . C Reaction network diagram describing our model of the phospho-relay. The network includes nearly all possible interactions between the three proteins subject to the biochemical assumptions outlined in the main text. For clarity, the reaction network is color-coded to indicate the groups of reactions involved in each phosphorylation event. S denotes Sln1, Y denotes Ypd1, and K denotes Ssk1. Phosphorylated residues are denoted by p, unphosphorylated residues by o. D Directed graph describing the subnetwork involving phosphorylation and dephosphorylation of Ssk1. The graph contains four loops that are connected as a branched tree.
is appropriate because previous studies have shown that activation of the HOG pathway does not vary under normal growth conditions [3, 4, 12, 14, 15] .
Our steady-state analysis predicted that relative levels of dephosphorylated Ssk1 depend solely on Ypd1 levels and that robustness is achieved by maintaining Ypd1 in large excess. We experimentally tested this prediction by perturbing protein expression levels so as to deplete this buffering pool of Ypd1. All such perturbations compromised the ability of the phospho-relay to inhibit the HOG pathway, leading to hyperactivation in normal osmolarity conditions. The presence of a large buffering pool of an intermediate phospho-relay component is a previously underappreciated mechanism for robustness and suggests a possible advantage of a three-component relay over a two-component system.

Inappropriate HOG pathway activation during normal osmolarity growth unnecessarily alters transcription and metabolism [7, 9, 10] . To assess the robustness of HOG pathway inhibition by the three-component phosphorelay, we created strains capable of overexpressing Sln1, Ypd1, and Ssk1 in response to β-estradiol. For these overexpression experiments, we used diploid strains homozygous for the GEV artificial transcription factor [11] . GEV consists of the Gal4 DNA-binding domain, the estrogen receptor, and the VP16 activation domain. Upon treatment with the hormone β-estradiol, GEV rapidly translocates to the nucleus, where it activates transcription from promoters containing the Gal4 DNA-binding target sequence. The GEV system enables rapid induction of individual yeast genes with limited off-target effects [11] . To make a given phospho-relay gene GEVinducible, we placed it under the control of the GAL1 promoter (SLN1/P GAL1 -SLN1, YPD1/P GAL1 -YPD1, and SSK1/P GAL1 -SSK1), as described previously [11] .
Inappropriate activation of the HOG pathway is known to cause a growth defect [7, 9, 10] . We therefore measured growth of these GEV strains after induction with β-estradiol at a range of concentrations to screen for HOG pathway hyperactivity. A strain carrying an inducible allele of Pbs2 (P GAL1 -PBS2/PBS2) was used as a control because Pbs2 overexpression is known to cause severe growth defects from inappropriate activation of the HOG pathway [7] . As shown in Figure 3A , strains overexpressing Pbs2 exhibited a measurable growth defect, while strains overexpressing components of the phospho-relay (Sln1, Ypd1, and Ssk1) showed no significant change.
We also assayed for Hog1 phosphorylation following GEV induction of phospho-relay components to obtain direct evidence that moderate overexpression does not cause HOG pathway hyperactivation. We used overexpression of Pbs2 and Ssk2 (also known to cause hyperactivation of the HOG pathway [7] ) as positive controls. After 30 minutes of GEV induction, there was no detectable increase in Hog1 phosphorylation in strains overexpressing phospho-relay components ( Figure 3B ). In contrast, Pbs2 and Ssk2 overexpression caused phosphorylation of Hog1. Interestingly, overexpression of the Ssk2 homolog Ssk22 had the strongest effect on Hog1 phosphorylation.
We then constructed diploid strains with a single inducible copy of the HOG phospho-relay gene of interest and a single P STL1 -YFP reporter to assay for HOG pathway transcriptional activity in response to overexpression of relay components. Stl1 is a glycerol/H + symporter whose expression is strongly upregulated in response to osmotic shock [16] . We overexpressed all three relay components, Pbs2, and Ssk22. Here we used Ssk22 as a control instead of Ssk2 because it showed a strong effect on Hog1 phosphorylation in the previous experiment. After 120 minutes of induction with 10 μM β-estradiol, Ssk22 and Pbs2 overexpression led to HOG-dependent transcription from the STL1 promoter, as indicated by an increase in YFP fluorescence ( Figure 4 ). Over the same period of time, overexpression of the phospho-relay components (Sln1, Ypd1, and Ssk1) caused almost no transcription from the STL1 promoter. After 19 hours, overexpression of Ssk1 and Sln1 did increase expression of YFP from the P STL1 -YFP reporter. These effects on longer time scales may have been due to factors beyond Ssk1 and Sln1 overexpression, however, as there was a population expressing YFP even in the control strain at 19 hours.
Growth, phosphorylation, and transcriptional measurements of HOG pathway activity all indicated that HOG pathway activation is robust to fluctuations in the Sln1-Ypd1-Ssk1 phospho-relay components. These results prompted us to investigate the mechanistic basis of this robustness using a biochemical model. The reaction network underlying our model ( Figure 2C ) has 37 nodes and involves 13 species. In this section we discuss the biochemical justification for key assumptions in the model.
There exist high-resolution crystal structures of Ypd1 alone and in complex with the Sln1 receiver domain ( Figure 2B ) [17, 18] . Genetic and biochemical evidence suggest that Sln1 forms an obligate homodimer and that Ypd1 can interact with either half of the dimer [6, 19] , which implies that formation of a Ypd1-Sln1(dimer)-Ypd1 ternary complex is possible. Accordingly, we include Sln1 in the reaction network as a dimer with four relevant phosphorylation sites. The Sln1 dimer is referred to as S H1D1H2D2 , where H1 and D1 denote the phosphorylatable histidine and aspartate residues in one half of the dimer and H2 and D2 denote the corresponding residues in the other half. The dimer is allowed to autophosphorylate on either histidine residue. Phospho-transfer from the histidine to the aspartate in the RR domain is treated as irreversible. A reverse reaction is included with all histidine autophosphorylation steps to account for possible hydrolysis of the phosphate prior to transfer [2, 20] . Each half of the dimer is assumed to be independent from the other. Coincident phosphorylation events (e.g., S OOOO forming S POPO in one step) are therefore considered to be unlikely and are excluded from the model. Following these assumptions, the model includes nine different forms of free Sln1 that are interconverted as shown in the reaction network.
In the second leg of the phospho-relay, any Sln1 phospho-form with at least one phosphorylated aspartate is allowed to reversibly associate with unphosphorylated monomeric Ypd1 (Y ) to form a series of binary complexes (YS OOOP , YS OPOO , YS POOP , YS OPPO , and YS OPOP ). Sln1-Ypd1 phospho-transfer has been shown to be reversible [21] . As such, all reactions that produce phospho-Ypd1 (Y P ) are treated as reversible. No
A Homozygous GEV diploid strains with a single inducible copy of a phospho-relay gene were grown to saturation in different concentrations of β-estradiol. The optical density (OD 600 ) after 13 hours of growth is plotted. Overexpression of Pbs2 caused a growth defect at higher concentrations of β-estradiol, while no significant growth defects were observed following overexpression of relay components (beyond the defect in the wild-type due to overexpression of Gal4 target genes) . Each point represents the mean and standard deviation over four replicates. B We assayed for Hog1 phosphorylation using an antibody specific to doubly phosphorylated Hog1 to confirm that moderate overexpression of phospho-relay components does not lead to activation of the HOG pathway. Overexpression of phospho-relay components using a saturating dose (10 μM) of β-estradiol did not lead to Hog1 phosphorylation. Overexpression of the positive controls Ssk22, Ssk2, and Pbs2, however, led to clear upregulation of Hog1 phosphorylation after 30 minutes of induction. Total Hog1 is shown as a loading control.
assumption is made about which half Ypd1 binds to in the YS OPOP binary complex because the halves of the Sln1 dimer are considered to be indistinguishable. Accordingly, the YS OPOP complex is allowed to form from either Y P + S OOOP or Y P + S OPOO . Additionally, YS OPOP can bind to a second Y molecule to form a ternary complex (YS OPOP Y ) that produces Y P using either of the Sln1 phospho-aspartate residues.
Phospho-Ypd1 then binds to and phosphorylates Ssk1, which is modeled as a dimer with two phosphorylation sites (K OO ) [9] . The two monophosphorylated forms of Ssk1, which are known to be fully inactive [9] , are assumed to be identical (i.e., K PO = K OP ). Y P can form a complex with K OO , leading to the production of K OP , and in turn Y P can bind to K OP and transfer a phosphate to produce K PP . The network includes one further interaction between Ypd1 and Ssk1 deduced from kinetic data. The half-life of phospho-Ssk1 in vitro has been measured to be dramatically different with and without the presence of Ypd1 (over 40 hours vs. 13 minutes, respectively), suggesting that Ypd1 binds to K OP and K PP to prevent hydrolysis of the phosphate [20, 22] . Accordingly, the reaction network includes the reversible formation of dead-end complexes between Ypd1 and phospho-Ssk1. Finally, unstabilized K PP and K OP are allowed to lose phosphates via spontaneous hydrolysis. Inclusion of these hydrolysis reactions ensures that there is a complete cycle for Ssk1 modification/demodification and that the system can reach a stable steady state. We emphasize, however, that spontaneous hydrolysis of complexed Ssk1 is likely not the mechanism for rapid dephosphorylation of large quantities of Ssk1 in response to osmotic shock. The mechanism for this rapid activation remains unknown but is irrelevant for our model, which is restricted to yeast growing in steady-state normal osmolarity conditions.
Robust inactivation of the HOG pathway requires that only a small fraction of total Ssk1 (K T ) be in the active (K OO ) modification form at steady state. The goal of this section is to derive a simple steady-state expression (an invariant) for the ratio of active to total Ssk1. We find an invariant of the form
where the coefficients are combinations of the rate constants. In this section we derive Eq. 1, and in the following section we discuss experimental tests of its predictions. The subnetwork involving Ssk1 contains four loops, which are linked in a branched tree ( Figure 2D ). It is a general feature of such networks that, at steady state, each individual loop is at steady state, irrespective of any other loops in which the components participate [23, 24] .
If each individual loop is at steady state, then the forward flux through each loop must be balanced by the backward flux, which yields the following four equations:
Because the intermediate complexes Y P K OO and Y P K OP are also at steady state, we can write
from which we deduce that
Substituting Eq. 3 into Eq. 2, we obtain expressions for K OP , K PP , YK OP , and YK PP in terms of K OO , Y , and Y P . From Eq. 3 we already have expressions for Y P K OO and Y P K OP in terms of K OO and Y P . As such, we are able to calculate the total amount of Ssk1 (K T ) in terms of just K OO , Y , and Y P . We have
Substituting for the individual terms, we obtain
The relative concentration of K OO is thus given by the invariant in Eq. 1.
The tree of loops structure of the Ssk1 network has an important consequence. It implies that the steady-state ratio of active to total Ssk1 is independent of the upstream biochemistry (i.e., the mechanistic details of the various Sln1 and Ypd1 reactions) as long as some process exists to generate positive levels of Y and Y P . In that case, the ratio will always be given by Eq. 1, although the numerical value will of course differ depending on steady-state concentrations of Y and Y P . The implications of this result, including its suggestion that robustness in the HOG pathway is independent of putative Sln1 bifunctionality, are considered in the conclusion.
The invariant derived from our mathematical model (Eq. 1) suggests that Ypd1 levels are critical to robustness. The denominator of the invariant is quadratic in the concentration of free Y P and linear in the concentration of free Y . Provided that the upstream network favors production of Y P over Y and that there is substantially more Ypd1 than Ssk1, the denominator of Eq. 1 will be large, and the relative concentration of K OO will be maintained at a low level. This situation allows for considerable underor overexpression of pathway components without spurious activation of the HOG pathway, in agreement with our experimental findings.
The invariant predicts that massive overexpression of Ypd1 should not cause phosphorylation of Hog1. In fact, additional Ypd1 would drive the [K OO ] K T ratio even closer to zero, lowering the amount of unphophorylated Ssk1 required for HOG pathway activation. In contrast, Ypd1 underexpression should increase the ratio, potentially compromising fitness due to inappropriate activation of the HOG pathway. Similarly, massive overexpression of Ssk1 should deplete free Y and Y P due to increased levels of the four intermediate complexes (Y P K OO , Y P K OP , YK OP , and YK PP ). Under the assumption of tight binding between Ypd1 and Ssk1 in each of these complexes, which is well-supported by existing kinetic data [20, 22] , very little free Ypd1 will be present at steady state if there is much more Ssk1 than Ypd1. As such, Eq. 1 predicts that the ratio will be higher following massive overexpression of Ssk1 than under wild-type conditions.
We experimentally validated these three predictions. The GEV system can achieve at most a 10-fold increase in protein expression from a single inducible allele [11] . We created haploid GEV yeast strains carrying high-copy 2μ plasmids with a GEV-inducible allele (P GAL1 -GENE) of a gene of interest, which allowed us to test the model prediction that massive overexpression of Ssk1, but not of Ypd1, should lead to inappropriate HOG pathway activation. These high-copy yeast plasmids are estimated to be present at 15-50 copies per cell [25] [26] [27] [28] .
We measured the growth of these strains in different concentrations of β-estradiol to assay for growth defects that might be due to HOG pathway hyperactivation. Massive overexpression of both Sln1 and Ssk1 caused a growth defect over a range of β-estradiol concentrations, but the strain with Ypd1 overexpressed grew as well as a wild-type strain carrying only the empty vector (P GAL1 2μ scURA3) ( Figure 5A ). Examination of growth over a finer range of β-estradiol concentrations indicated that overexpression of Ssk1 caused a more severe growth defect than overexpression of Sln1 ( Figure 5B ). These defects were also visible on solid media ( Figure 5C ). As such, extreme overexpression of Ssk1 compromises fitness.
We again assayed phospho-Hog1 levels to check if overexpression of Sln1 and Ssk1 causes activation of the HOG pathway in normal osmolarity conditions ( Figure 6 ). We measured Hog1 phosphorylation levels after GEVinduction of relay components and of the positive controls Pbs2 and Ssk22 from a multi-copy plasmid. Overexpression of Pbs2, Ssk22, and Ssk1 caused a significant change in Hog1 phosphorylation after 30 minutes ( Figure 6B ). Although some Hog1 phosphorylation was observed after overexpression of Sln1, the increase was insignificant. Interestingly, overexpression of Ypd1 did not cause an increase in the level of phosphorylated Hog1. In fact, levels of Hog1 phosphorylation were reduced in the Ypd1 overexpression strain (p = 0.0246, two-way ANOVA).
We could not perform underexpression experiments in a ypd1 background because deletion of Ypd1 is lethal. To underexpress Ypd1, we instead sporulated the diploid strain (P GAL1 -YPD1/YPD1) containing a wild-type copy of YPD1 and a single copy under the control of P GAL1 onto media containing 10 nM β-estradiol. We reasoned that 10 nM β-estradiol would give sufficient expression of Ypd1 for cell growth, which was confirmed by observation of four viable spores. We then grew these spores on media containing a range of β-estradiol concentrations (Figure 7) . Ypd1 underexpression caused a clear growth defect compared to the wild-type control on 0 nM and 5 nM β-estradiol, indicating that Ypd1 underexpression is toxic. In contrast, underexpression of Sln1 and Ssk1 did not cause a growth defect (Additional file 1: Figure S1 ).
As discussed above, we observed a slight but nonsignificant increase in Hog1 phosphorylation following massive overexpression of Sln1 ( Figure 6 ). We therefore investigated whether the growth defect in response to Sln1 overexpression is only partially due to HOG pathway activation by creating yeast strains null for SSK1. Activation of the HOG cascade through the Sln1 branch requires Ssk1, so in ssk1 strains it is not possible for Sln1 overexpression to activate the HOG pathway.
Overexpression of Sln1 from a 2μ plasmid using GEV was still detrimental to growth in the ssk1 strain (Figure 8) , indicating that the growth defect due to Sln1 overexpression is only partially due to HOG pathway activation. This result held both for growth in liquid cultures ( Figure 8A ) and on solid media ( Figure 8B ). It is consistent with Sln1 overexpression causing a smaller effect on Hog1 phosphorylation levels ( Figure 6 ).
Robustness of the Sln1-Ypd1-Ssk1 phospho-relay is essential to prevent spurious activation of the HOG pathway, which severely compromises yeast fitness. We established that the phospho-relay is robust to perturbations in the concentrations of the three relay components. A theoretical analysis suggested that a large pool of the intermediate component Ypd1 can buffer fluctuations in other pathway components to maintain robustness. This suggestion was consistent with earlier published measurements indicating that Ypd1 is at least 5 times more abundant than Ssk1 at normal expression levels [7, 29] . Although Ypd1 may also bind to the protein Skn7, combined levels of Ssk1 and Skn7 have been measured to be below total Ypd1 levels [29] . Our subsequent experiments confirmed that depletion of this buffering pool of Ypd1 leads to inappropriate activation of the HOG pathway.
The differential expression of Ypd1 and Ssk1 enables phosphorylation of excess Ssk1 and stabilization of the new phospho-Ssk1, buffering HOG pathway activation to fluctuations in Ssk1 levels. This novel mechanism of robustness suggests an advantage of a three-component Figure 5 Massive overexpression of phospho-relay components leads to growth defects. A Haploid GEV strains carrying a high-copy plasmid with an inducible HOG pathway gene were grown in different concentrations of β-estradiol. The OD 600 after 36 hours of growth is plotted as a function of β-estradiol concentration. Each point represents the mean and standard deviation of four replicates. Overexpression of Sln1 and Ssk1 (but not Ypd1) caused a growth defect. B The same strains were grown over a finer titration of β-estradiol concentrations. The OD 600 after 36 hours is plotted. At this resolution, it is clear that the growth defect from Ssk1 overexpression is more severe than the growth defect from Sln1 overexpression at low β-estradiol concentrations. C The same strains were frogged onto plates containing different concentrations of β-estradiol. Massive overexpression of Sln1 and Ssk1 again caused a growth defect comparable to that from overexpression of Pbs2. In all experiments, the parent strain carrying the empty vector plasmid [2μ P GAL1 scURA3] was used as a negative control.
architecture over a two-component one. In particular, the implementation of an analogous buffering strategy in a two-component system would be difficult because it would require expressing the sensor histidine kinase at very high levels. This situation might lead to imprecise sensing and various other off-target effects. In contrast, Figure 6 Growth defects following massive overexpression of phospho-relay components are due to activation of the HOG pathway. A We assayed for Hog1 phosphorylation after overexpression of relay components (Sln1, Ypd1, Ssk1) and positive controls (Pbs2, Ssk22). The parental strain carrying the empty plasmid vector was used as a negative control. B We quantified the amount of phosphorylated Hog1 (relative to Hog1) in five biological replicates of this experiment. Error bars represent the standard error. Pbs2, Ssk22, and Ssk1 caused a significant ( * ) change in Hog1 phosphorylation levels after overexpression for 30 minutes (p = 0.0395, 0.0096, and 0.0224, respectively; paired t-test). Hog1 phosphorylation levels were also significantly lower in the Ypd1 overexpression strain (p = 0.0246, two-way ANOVA).
the use of an intermediate transfer protein enables robust buffering with both the sensor and response regulator expressed at comparable levels. Our work has thus identified a potential mechanism for circumventing a tradeoff between efficient sensing and robust control. There are other possible advantages for a three-component architecture, including combinatorial control of response regulators by sensor proteins through a common phosphotransfer protein or segregation of sensing and activation functions between the nucleus and cytoplasm. Intriguingly, deletion of YPD1 has recently been shown to cause constitutive activation of the HOG pathway in Candida albicans, suggesting that its buffering capacity might also be important in this organism [30] .
Robustness in real biological systems is necessarily approximate and apt to be compromised at extreme expression levels of cellular components. In many systems, however, it has proven difficult to characterize where robustness breaks down and to reconcile such results with mathematical models, which often predict exact robustness [31] . Our combined theoretical and experimental results specify a single condition (Ypd1 in large excess) for robust regulation of the HOG pathway.
The link between bifunctionality and robustness is well-established [31] [32] [33] [34] [35] , and it is known that bifunctionality of EnvZ is essential to robustness in Escherichia coli osmoregulation [36, 37] . As such, it is intriguing that our model suggests that robustness in S. cerevisiae osmoregulation is not dependent on bifunctionality of Sln1. It is important to emphasize, however, that bifunctionality would not compromise robustness. Rather, the model indicates that any upstream process that produces non-zero levels of Y and Y P should enable the same fundamental behavior predicted by Eq. 1. The possibility that Sln1 exhibits phosphatase activity warrants further experimental investigation.

All yeast strains used in this study are listed in Additional file 2: Table S1 . The homozygous GEV diploid strain, which served as the wild-type background strain for all diploid overexpression experiments, was created by mating haploid GEV strains yMM598 and yMM1101 [11] and picking zygotes to create yMM1104. As described previously [11] , diploid yeast strains capable of overexpressing the desired HOG pathway protein from a single locus (yMM1263, yMM1272, yMM1259) were created by transforming [38] the homozygous GEV diploid strain yMM1104 with the KanMX-P GAL1 cassette amplified from yMM1100 genomic DNA using appropriate oligonucleotide pairs. Transformants were verified by colony PCR and sequencing.
Yeast strains containing 2μ plasmids for massive overexpression of pathway components (yMM1313-yMM1318) were constructed using recombination-mediated plasmid construction [39, 40] to generate the overexpression plasmids pMM330-pMM334 in vivo (as described below). Positive transformants were selected for and maintained on SC-Ura media [38] .
Yeast strains containing the P STL1 -YFP reporter of HOG pathway activity and one estradiol-inducible allele of a HOG pathway gene (yMM1296, yMM1298, yMM1300, yMM1301, yMM1304, yMM1305) were constructed by transforming the heterozygous diploid GEV yeast strains (yMM1104, yMM1272, yMM1264, yMM1259 yMM1286, yMM1287) already containing an inducible allele with the product of PCRing yECitrine-HphMX off plasmid pMM280 using appropriate oligonucleotides. The oligonucleotides contained homology such that the STL1 ORF was replaced with the yECitrine-HphMX cassette. Transformants were verified by colony PCR, and expression of YFP in 1M sorbitol was assayed.
Yeast strains null for the SSK1 gene (ssk1 ) were created by deleting the SSK1 ORF using appropriate oligonucleotide pairs to amplify KanMX from pMM131 and transforming it into yMM630. Transformants were selected for drug resistance and verified by colony PCR and sequencing.
Standard yeast media was used as noted. Low fluorescence yeast media was prepared as described previously [41] .
All plasmids used in this study are listed in Additional file 3: Table S2 . Plasmid pMM329 (P GAL1 scURA3 2μ) was constructed by PCR of the native GAL1 promoter from genomic DNA prepared from yMM1100 using appropriate primers. This promoter was ligated in pMM12 between the restriction sites KpnI and XhoI (scURA3 2μ) [42] . The resulting plasmid served as a template to create a series of overexpression plasmids with different HOG pathway genes under the control of the GAL1 promoter using yeast recombination-mediated plasmid construction [39, 40] . Appropriate primer pairs were used to amplify Pbs2, Ssk22, Sln1, Ypd1, and Ssk1, respectively, from yMM1100 genomic DNA. These PCR products were then co-transformed with pMM329 linearized Figure 8 Growth defects following Sln1 overexpression are not completely due to HOG pathway activation. A We assayed growth in wild-type and ssk1 strains overexpressing Sln1 in response to β-estradiol or carrying an empty P GAL1 vector control. Deletion of Ssk1, which prevented HOG pathway activation by Sln1, partially alleviated the growth defect due to Sln1 overexpression. B Growth of cells on plates containing 10 μM β-estradiol indicated that ssk1 reduced the toxicity of Sln1 overexpression.
with XhoI and SalI. The primer pairs used to amplify the HOG pathway genes contained homology with the pMM329 backbone such that the gene of interest was integrated after P GAL1 to create a P GAL1 -HOGGENE plasmid (pMM330-pMM334). Positive transformants in which the plasmid had been repaired were selected for on SC-Ura media. Plasmids were purified from these transformants and verified by sequencing.
Yeast strains were grown overnight to saturation in appropriate media (YPD or SC-Ura media to maintain plasmids). These saturated cultures were serially diluted in 10-fold increments and frogged onto YPD or SC-URA plates containing 0 nM, 10 nM, 100 nM, 1 μM, or 10 μM β-estradiol (Tocris Biosciences). These plates were incubated at 30°C for two days before imaging.
Yeast strains were grown overnight to saturation in YPD or SC-Ura media. In the morning, each strain was diluted 1:2000 into 200 μL of the same media containing 0 nM, 100 nM, 1 μM, or 10 μM of β-estradiol in a well of a 96-well flat-bottom plate (Costar). Each strain/estradiol combination was run in four replicates on the same plate. Growth curves were generated using a Synergy H1 microplate reader (BioTek). Cells were grown at 30°C with continuous, double-orbital (555 cpm) shaking, and OD 600 was measured every 20 minutes. Growth rates were calculated from the growth curves using spline-fits determined with the R package grofit [43] . OD was plotted at the time points indicated in the figure legends.
Diploid GEV strains yMM1104 (control), yMM1259 (SLN1/KanMX-P GAL1 -SLN1), yMM1263 (SSK1/KanMX-P GAL1 -SSK1), yMM1264 (SSK1/KanMX-P GAL1 -SSK1), and yMM1272 (YPD1/KanMXrev-P GAL1 -YPD1) were sporulated in 1% potassium acetate for 3 days and dissected onto YPD plates containing 10 nM β-estradiol. Two spores from each tetrad contained the wild-type HOG pathway gene (Sln1, Ypd1, or Ssk1), while the other contained the same gene under the control of the GAL1 promoter (KanMX-P GAL1 -SSK1, KanMX-P GAL1 -SLN1, KanMX-P GAL1 -YPD1). After all spores had grown to a sufficient size, they were diluted into YPD and frogged onto YPD plates containing 0 nM, 5 nM, 10 nM, 100 nM, 1 μM, or 10 μM β-estradiol. Spores were allowed to grow at 30°C for 2 days prior to imaging.
We created diploid GEV strains that carried both an inducible HOG gene under the control of P GAL1 promoter and a HOG pathway transcriptional reporter (P STL1 -yEVenus) to assay for downstream transcriptional activation in response to overexpression of various HOG pathway proteins. Strains were grown with agitation in low fluorescence media at 30°C to mid-log (Klett 80), at which point 200 μl of cell culture was sampled for flow cytometry by adding it to 800 μl of cold PBS + 0.1% Tween 20 stored at 4°C. Each culture was induced by adding β-estradiol to a final concentration of 10 μM. Cultures were sampled for flow cytometry after induction with β-estradiol at T = 2 hours and T = 19 hours. Fluorescence was analyzed by flow cytometry on a BD LSRII Multi-Laser Analyzer with HTS (BD Biosciences).
We measured levels of phosphorylated Hog1 following both moderate and massive overexpression of pathway components. For the moderate overexpression experiments, diploid GEV yeast strains yMM1104, yMM1263, yMM1259, yMM1272, and yMM1287, which each contained one estradiol-inducible copy of a HOG pathway gene, were grown to mid-log (Klett 80) in YPD at 30°C with shaking. To assess the effect of massive overexpression of HOG pathway proteins, yeast strains containing the P GAL1 -HOGGENE scURA3 2μ overexpression plasmids (yMM1313-yMM1318) were grown in SC-Ura media to mid-log (Klett 80) at 30°C with shaking. For all strains, at T = 0 expression of the gene of interest was induced by addition of 10 μM β-estradiol (final concentration). At indicated timepoints, 1.5 ml of culture was sampled.
Protein was prepped from samples immediately after each time point. Each sample was centrifuged (1320 RPM) and the supernatant aspirated. The resulting cell pellet was resuspended in 100 μl of 1X sample buffer (Invitrogen) with β-mercaptoethanol (final concentration of 10%), protease inhibitor (Roche), and phosphatase inhibitor (Fisher Scientific). Samples were heated at 95°C for 5 minutes, vortexed for 2 minutes, and then rapidly frozen in liquid nitrogen and stored at -20°C.
Prior to western blotting, samples were thawed and centrifuged at 1320 RPM for 5 minutes. They were run on 4-10% Bis-Tris gels (Invitrogen) and transferred to PVDF membranes (Invitrogen) by electrophoresis at 13 V for 4 hours. Membranes were blocked for 1 hour at room temperature with agitation in 1X TBS, 0.1% Tween-20, and 5% milk. Membranes were then probed with primary antibody overnight at 4°C.
The following antibodies were used to detect phosphorylated Hog1, total Hog1, and actin, respectively: antiphospho-p38 MAPK rabbit monoclonal antibodies (Cell Signaling Technology #9215), anti-c-myc goat polyclonal antibodies (Santa Cruz Biotechnology sc-6815), and antiβ-actin antibody (Abcam ab8224). All primary antibodies were diluted 1:1000 in 1X TBS, 0.1% Tween-20, and 5% milk. Following incubation with the primary antibody, membranes were washed (4 × 5 minutes) with TBST (1X TBS, 1% Tween-20) and then incubated for 1 hour at room temperature with the appropriate secondary antibody conjugated to HRP (anti-rabbit IgG (Cell Signaling #7074, 1:5000 dilution), rabbit anti-goat IgG (Santa Cruz sc-2768, 1:5000 dilution), and anti-mouse IgG (Abcam ab97023, 1:20000 dilution), respectively). All secondary antibodies were diluted in 5% milk, 1X TBS, and 1% Tween-20. After incubation with the secondary antibody, membranes were washed 4 × 5 minutes with 5% milk, 1X TBS, and 1% Tween-20.
Western blots were quantified using chemiluminescence. Membranes were developed using the Pierce Supersignal Femto kit following the manufacturer's protocol. Chemiluminescence was quantified using HyBlot CL Autoradiography film. Developed film was scanned on a Epson Perfection 4490 Photo Scanner in transmission mode. Protein levels were quantified by densitometry using the Gel Analysis plug-in in ImageJ [44] .
Each membrane was probed for phospho-Hog1, betaactin, and total Hog1 (in that order). After the chemiluminescence assay but before re-blocking, the membrane was stripped by washing in stripping buffer (2 × 10 minutes), phosphate-buffered saline (2 × 10 minutes) and 1X TBS + 1% Tween-20 (2 × 5 minutes). Stripping buffer contained 15 g glycine, 1 g SDS, and 10 mL Tween 20 in 1 L ultrapure water, with the pH adjusted to 2.2 using concentrated HCl.
Scientific workflow has become increasingly popular in modern scientific computation as more and more scientists and researchers are relying on workflow systems to conduct their daily science analysis and discovery. With technology advances in both scientific instrumentation and simulation, the amount of scientific datasets is growing exponentially each year, such large data size combined with growing complexity of data analysis procedures and algorithms have rendered traditional manual processing and exploration unfavorable as compared with modern in silico processes automated by scientific workflow systems (SWFS). While the term workflow speaks of different things in different context, we find in general SWFS are engaged and applied to the following aspects of scientific computations: 1) describing complex scientific procedures, 2) automating data derivation processes, 3) high performance computing (HPC) to improve throughput and performance, and 4) provenance management and query.
Workflows are not a new concept and have been around for decades. There were a number of coordination languages and systems developed in the 80s and 90s [1, 7] , which share many common characteristic with workflow systems (i.e. they describe individual computation components and their ports and channels, and the data and event flow between them). They also coordinate the execution of the components, often on parallel computing resources. Furthermore, business process management systems have been developed and invested in for years; there are many mature commercial products and industry standards such as BPEL [2] . In the scientific community there are also many emerging systems for scientific programming and computation [5, 22] . Before we jump on developing yet another workflow system, a fundamental question to ask is whether we can use existing technologies, or we should invent new languages and systems in order to achieve the four aspects mentioned earlier that are essential to scientific workflow systems. This paper identifies the challenges to workflow development in the context of scientific computation; we present an overview of some of the existing technologies and emerging systems, and discuss opportunities in addressing these challenges.
Software development has been on a free ride for performance gain as chipmakers continue to follow Moore's Law in doubling up transistors in minuscule space. Little consideration has been given to code parallelization since it has not been essential for the average computer user until recently, when single CPU core performance growth stagnated and multi-core processors emerged on the market in 2005.
Due to the limitations to effectively increasing processor clock frequency, hardware manufactures started to physically reorganize chips into what we call the multi-core architecture [10] , involving linking several microprocessor cores together on the same semiconductor. Various manufactures from Intel, AMD, IBM, Sun, have released dual-core, quad-core, eight-core, and 64-threaded processors in the past few years [13, 21] . Given that 128-threaded SMP systems are a reality today [21] , it is reasonable to assume that 1024 CPU cores/threads or more per SMP system will be available in the next decade.
The new multi-core architecture will force radical changes in software design and development. We are already seeing significant increase of research interests in concurrency and parallelism, and multi-core software development. The number of multiprocessor research papers has increased sharply since year 2001, surpassing the peak point in all the past years [10] . Concurrency is one of the next big challenges in how we write software simply because our industry has been driven by requirements to write ever larger systems that solve ever more complicated problems and exploit the ever greater computing and storage resources that are available [18] .
Within the science domain, the data that needs to be processed generally grows faster than computational resources and their speed. The scientific community is facing an imminent flood of data expected from the next generation of experiments, simulations, sensors and satellites. Scientists are now attempting calculations requiring orders of magnitude more computing and communication than was possible only a few years ago. Moreover, in many currently planned and future experiments, they are also planning to generate several orders of magnitude more data than has been collected in the entire human history [9] .
For instance, in the astronomy domain the Sloan Digital Sky Survey (http://www.sdss.org) has datasets that exceed 10 terabytes in size. They can reach up to 100 terabytes or even petabytes if we consider multiple surveys and the time dimension. In physics, the CMS detector being built to run at CERN's Large Hadron Collider (http://lhc.web.cern.ch/lhc) is expected to generate over a petabyte of data per year. In the bioinformatics domain, the rate of growth of DNA databases such as GenBank (http://www.psc.edu/general/software/packages/genbank/) and EMBL (European Molecular Biology Laboratory, http://www.embl.org) has been following an exponential trend, with a doubling time estimated to be 9-12 months.
To enable the storage and analysis of large quantities of data and to achieve rapid turnaround, data needs to be distributed over thousands to tens of thousands of compute nodes. In such circumstances, data locality is crucial to the successful and efficient use of large scale distributed systems for data-intensive applications [19] . Scientific workflows are generally executed on a shared infrastructure such as TeraGrid (http://www.teragrid.org), Open Science Grid (http://www.opensciencegrid.org), and dedicated clusters, where data movement relies on shared file systems that are known bottlenecks for data intensive operations. If data analysis workloads have locality of reference, then it is feasible to cache and replicate data at each individual compute node, as high initial data movement costs can be offset by many subsequent data operations performed on cached data [15] .
Modern scientific workflow systems need to set large scale data management as one of its primary objectives, and to ensure data movement is minimized by intelligent data-aware scheduling both among distributed computing sites (assuming that each site has a local area network shared storage infrastructure), and among compute nodes (assuming that data can be stored on compute nodes' local disk and/or memory).
Supercomputers had their golden age back in the 80s when there were virtually no other choices in dealing with compute-intensive tasks. They were applied mostly to scientific modeling and simulation in various disciplines such as high energy physics, earth science, biology, mechanical engineering etc. Some typical applications included weather forecasting, missile trajectory simulation, airplane wind tunnel simulation, genomics etc. However, supercomputers are expensive and scarce resources where only national laboratories, government agencies and some universities have access to them; and the parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed, such as special-purposed FORTRAN compilers, PVM, MPI and OpenMP [9] .
Over the last decade, we have observed processor speeds, storage capacity per drive, and network bandwidth increase 100~1000 times. As a consequence, cluster computing and Grid computing environments that leverage the cheaper commodity computing and storage hardware have been actively adopted for scientific computations. Cluster computing usually involves homogeneous machines interconnected by high speed network with locally accessible storage in one administrative domain, where Grid computing focuses on distributed resource sharing and coordination across multiple "virtual organizations" that may span many geographically distributed administrative domains. Grids can also be categorized into Computational Grids and Data Grids, where the former mostly tackle computation intensive tasks, and the latter target data-intensive sciences.
With the introduction of multi-core architectures, the separation between Grid Computing and Supercomputing is becoming less clear. Many supercomputers are being built on multi-core chips with high speed interconnection. The Cray XT5 system (http://www.cray.com/products/xt5/index.html) uses thousands commodity Quad-Core AMD Opteron™ processors and has a unified Linux environment. The latest IBM BlueGene/P Supercomputer (BG/P, http://www.research.ibm.com/bluegene/) has quad core processors with a total of 160K-cores, and has support for a lightweight Linux kernel on the compute nodes, making it significantly more accessible to new applications [17] . Finally, a smaller system named SiCortex (http://www.sicortex.com/) is also worth mentioning; it boasts 6-core processors for a total of 5832-cores, and runs a standard Linux environment.
Supercomputers (e.g. IBM BlueGene) have traditionally been designed and used for tightly coupled massively parallel applications, typically implemented in MPI. They have not been an ideal preferred platform for executing loosely coupled applications that are typical in many scientific workflows. Grids have seen success in the execution of tightly coupled parallel applications, but they has been the platform of choice for loosely coupled applications mostly due to the flexibility and granularity of the resource management and the execution of single processor jobs with ease. Work is underway within both the Falkon [14] and Condor [20] projects to enable the latest BG/P to efficiently support loosely coupled serial jobs without any modifications to the respective applications, and hence enabling an entirely new class of applications that were never candidates as possible use cases for the BlueGene/P supercomputer.
Scalability and performance are top priorities for SWFS. To this end, it is necessary to leverage supercomputing resources as well as Grid computing infrastructures for large scale parallel computations.
DAGMan (http://www.cs.wisc.edu/condor/dagman) and Pegasus [6] are two systems that are commonly referred to as workflow systems and have been widely applied in Grid environments. DAGMan provides a workflow engine that manages Condor jobs organized as directed acyclic graphs (DAGs) in which each edge corresponds to an explicit task precedence. Both systems focus on the scheduling and execution of long running jobs.
Taverna [12] is an open source workflow system particularly focused on bioinformatics applications and services, and it is based on the XScufl (XML Simple Conceptual Unified Flow) language. Kepler [11] is a scientific workflow system that builds on the Ptolemy-II system (http://ptolemy.eecs.berkeley.edu/ptolemyII/), which is a visual modeling tool written in Java. Triana [4] is a GUI-based workflow system for coordinating and executing a collection of services. All these systems have some visual interfaces (also referred to as workbenches) that allow the graphical composition of workflows.
While all of the existing SWFS possess great features and address many aspects of workflow specification, execution and management problems, it is unrealistic to expect one system to cover all the bases. The Workflow Bus project [23] instead tries to leverage multiple existing workflow systems to compliment each other in implementing aggregated functions and services.
Finally, the evolutions of workflows themselves (explorations) are vital in scientific analysis. VisTrails [3] captures the notion of an evolving dataflow, and implements a history management mechanism to maintain versions of a dataflow, thus allowing a scientist to return to previous steps, apply a dataflow instance to different input data, explore the parameter space of the dataflow, and (while performing these steps) compare the associated visualization results.
In response to the pressing demand of scientific applications, and the hunger for computing power, there have been a few emerging languages and systems that try to tackle the problems taking unconventional approaches.
MapReduce [5] is regarded as a power-leveler that solves complicated computation problems using brutalforce computation power. It provides a very simple programming model and powerful runtime system for the processing of large datasets. The programming model is based on just two key functions: "map" and "reduce," borrowed from functional languages. The MapReduce runtime system automatically partitions input data and schedules the execution of programs in a large cluster of commodity machines. The system is made fault tolerant by checking worker nodes periodically and reassigning failed jobs to other worker nodes. MapReduce has been mostly applied to document processing problems, such as distributed indexing, sorting, and clustering.
The Fortress language (http://fortress.sunsource.net) recently released by Sun Microsystems is a new programming language designed for HPC, and aims to improve programmability and productivity in scientific computation. The language has been designed from ground up, supporting mathematical notation (in Unicode) and physical units and dimensions, static type checking of multidimensional arrays and matrices, and rich functionality in libraries. It supports transactions, specification of locality, and implicit parallel computation (e.g. parallel for loops). Although Fortress in a strict sense is not a workflow language, and its adoption remains to be seen, it provides the higher level abstractions and functionalities for building a parallel workflow language.
Microsoft Windows Workflow Foundation (WWF) [16] provides a generic framework for workflow development and execution. It is focused on integrating diverse components within an application, allowing a workflow to be deployed and managed as a native part of the application. The fundamental idea behind WWF is that each activity is modeled as a resumable program statement, and the invocation of an activity is asynchronously organized, thus a program can be compared to a bookmark, which can be frozen in action, serialized into persistent storage, and resumed after arbitrarily long time later. However, WWF is not a full-fledged workflow management system in that it lacks administration, monitoring, retry mechanism, load balancing, etc. for a production environment.
Star-P (http://www.interactivesupercomputing.com) approaches the integration of scientific applications and HPC via language extension -allowing scientists to work in their familiar programming environments such as MATLAB, Python, and R, with some parallel directives. Internally the system can schedule the execution of parallel tasks to a computation cluster preconfigured with scientific calculation libraries. The system has been applied to a wide variety of computation problems, but the performance improvement is mostly intra-application parallelization, instead of inter-component coordination and management.
Swift [22] is an emerging system that bridges scientific workflows with parallel computing. It is a parallel programming tool for rapid and reliable specification, execution, and management of largescale science and engineering workflows. Swift takes a structured approach to workflow specification, scheduling and execution. It consists of a simple scripting language called SwiftScript for concise specifications of complex parallel computations based on dataset typing and iterations, and dynamic dataset mappings for accessing large scale datasets represented in diverse data formats. The runtime system relies on the CoG Karajan workflow engine for efficient scheduling and load balancing, and it integrates the Falkon [14] light-weight task execution service for optimized task throughput and resource efficiency delivered by a streamlined dispatcher, a dynamic resource provisioner, and the data diffusion mechanism to cache datasets in local disk or memory and dispatch tasks according to data locality.
Existing technologies and systems already address many of the fundamental issues in scientific workflow specification and management, and many of them have been successful applied to various scientific applications across multiple science disciplines. However, modern multi-core architectures and parallel and distributed computing technologies, and the exponentially growing scientific data are bound to change the landscape and evolution of scientific workflow systems. As already being manifested by the few emerging systems, the science community is demanding both specialized, domain-specific languages to improve productivity and efficiency in writing concurrent programs and coordination tools, and generic platforms and infrastructures for the execution and management of large scale scientific applications, where scalability and performance are major concerns. High performance computing support has become a indispensable piece of such workflow languages and systems, as there is no other viable way to get around the large storage and computing problems emerging in every discipline of 21 st century e-science, although what may be the best approach to enabling scientists to leverage HPC technologies as transparent and efficient as possible remains unanswered.
In the science domain, there is an increasing need for programming languages to expose parallelism, whether it's done explicitly or implicitly, to specify the concurrency within a component, or across multiple independent components. There is a need for new parallel or workflow languages that adopt implicit parallelism where data dependencies can be discovered by its compiler, and independent tasks in the orders of hundreds of thousands can be scheduled to run in clustered or Grid environments. Such systems could achieve improvements in both manageability and productivity.
Scientific workflow systems aim to provide a simple concise notation that allows easy parallelization and supports the composition of large numbers of parallel computations, therefore they may not need all the constructs and features in a full-fledged conventional language, and implicit parallelism is preferred to explicit parallelism specification, as the latter requires expertise and attention to the details of parallel programming, which may be difficult for end users. But in the mean time sometimes scientists do need more control in specifying how to distribute their applications and datasets.
We are also in need of common generic infrastructures and platforms in the science domain for workflow administration, scheduling, execution, monitoring, provenance tracking etc. While business process management has industry agreed upon standards and steering committees, we don't have these in the science domain, where often time people reinvent the wheel in developing their in-house yet another SWFS, and there is no easy way in integrating various workflow systems and specifications. We also argue that in order to address all the important issues such as scalability, reliability, scheduling and monitoring, data management, collaboration, workflow provenance, and workflow evolution, one system cannot fit all needs. A structured infrastructure that separates the concerns of workflow specification, scheduling, execution etc, yet is organized on top of components that specialize on one or more of the areas would be more appropriate.
Francesco Bonchi is a senior research scientist at Yahoo! Research in Barcelona, Spain, where he is part of the Web Mining Group. His recent research interests include mining query-logs, social networks, and social media, as well as the privacy issues related to mining these kinds of sensible data. In the past he has been interested in data mining query languages, constrained pattern mining, mining spatiotemporal and mobility data, and privacy preserving data mining. He is member of the ECML/PKDD Steering Committee 
Orthogonal frequency division multiplexing (OFDM) has been widely applied in wireless communication systems, because it transmits at a high rate, achieves high bandwidth efficiency, and is robust to multipath fading and delay [1] . OFDM applications can be found in digital television and audio broadcasting, wireless networking, and broadband internet access. Current OFDM based WLAN standards (such as IEEE802.11a/g) use variations of QAM schemes for subcarrier modulations which require a coherent detection at the OFDM receiver and consequently requires an accurate (or near accurate) estimation of Channel State Information (CSI). The structure of OFDM signal makes it difficult to balance complexity and performance in channel estimation. The design principles for channel estimators are to reduce the computational complexity and bandwidth overhead while maintaining sufficient estimation accuracy.
Some channel estimation schemes proposed in literature are based on pilots, which form the reference signal used by both the transmitter and the receiver. This approach has two main challenges: (i) the design of pilots; and (ii) the design of an efficient estimation algorithm (i.e., the estimator).
There is a tradeoff between the spectrum efficiency and the channel estimation accuracy. Most of the existing pilotassisted OFDM channel estimation schemes rely on the use of a large number of pilots to increase to estimation accuracy; the spectral efficiency is therefore reduced. For example, there are approaches based on time-multiplexed pilot, frequencymultiplexed pilot, and scattered pilot [2] , all achieving higher estimation accuracy at the price of using more pilots. There have been attempts to reduce the number of pilots, i.e. J. Byun et al. in [3] . The solutions generally require extra "test signal" for channel pre-estimation. By sending out "test signal", they try to find out how many pilots are needed by firstly inserting a relatively small number of pilots and then, based on the results of the "test", the number of pilots are decided. Therefore, there is no guaranteed overall reduction of pilots insertion.
As a sensing problem, OFDM channel estimation can benefit from the emerging technique of compressive sensing (CS), which acquires and reconstructs a signal from fewer samples than what is dictated by the Nyquist-Shannon sampling theorem, mainly by utilizing the signal's sparse or compressible property. The field has exploded since the pioneering work by Donoho [4] and Candes, Romberg and Tao [5] . The main idea is to encode a sparse signal by taking its "incoherent" linear projections and recover the signal through algorithms such as 1 minimization. To maximize the benefits of CS for OFDM channel estimation, one shall skillfully perform the CS encoding and decoding steps, which are precisely the two focuses of this paper: the designs of pilots and estimator, respectively.
Contributions: CS has been applied to channel estimation in [13] [14] [15] [16] , which are reviewed in subsection III-E below. For OFDM channel estimation, there are papers [6] [7] [8] [9] , to which our work differs in various ways as follows. We skillfully design CS encoding and decoding strategies for OFDM channel estimation. Compared to existing work, we are able to obtain channel response in much higher resolutions and from much fewer pilots (thus taking much shorter times). This is achieved by designing pilots with uniform random phases and using a novel estimator. The pilot design preserves the information of high-resolution channel response during aggressive uniform down-sampling, which means that receiver ADC can run at a much lower speed. The estimator is tailored for OFDM channel response; in particular, instead of the generic 1 minimization, iterative support detection (ISD) [17] and limited-support least-squares are adopted in order to take advantage of the characteristics of channel response. The resulting algorithm is very simple and performs better.
The rest of this paper is organized as follows: Section II reviews the general OFDM system model and sets up the channel estimation formulation. Section III relates channel estimation to CS and present the proposed pilot design. In Section IV, the estimator based on iterative support detection and limited-support least-squares are introduced. Section V give the simulation results. Finally, Section VI concludes this work.
II. OFDM SYSTEM MODEL A baseband OFDM system is shown in Figure 1 . In this system, the modulated signal in the frequency domain, represented by X(k), k ∈ [1, N] , is inserted with pilot signal and guard band, and then an N -point IDFT transforms the signal into the time domain, denoted by x(n), n ∈ [1, N] , where a cyclic extension of time length T G is added to avoid inter-symbol and inter-subcarrier interferences. The resulting time series data is converted by a digital-to-analog converter (DAC) with a clock speed of 1/T S Hz into an analog signal for transmission. We assume that the channel response comprises P propagation paths, which can be modeled by a time-domain complex-baseband vector with P taps:
where α p is a complex multipath component and τ p is the multipath delay (0 
where ⊗ denotes convolution and ξ(n), n ∈ 
, where the guard band and pilot signal will be removed. For pilot assisted OFDM channel estimation, we shall design the pilots X (and thus x) and recover h from the measurements Y (or, equivalently y).

CS, which will be reviewed in the next subsection, allows sparse signals to be recovered from very few measurements, which translates to slower sampling rates and shorter sensing times. Because the channel impulse response h is very sparse, we are motivated to apply CS to recover h by using a reduced number of pilots so that the estimation becomes much quicker. Furthermore, in sharp contrast to conventional OFDM channel estimation in which ADC and DAC run at the same sampling rate, we can obtain a higher-resolution h by increasing the sampling rate of only the transmitter DAC, or we can reduce the receiver ADC speed which often defines the system cost. In other words, we have N > M. The rest of this section reviews CS and introduces our proposed approach for OFDM channel estimation.
CS theories [4] , [10] , [11] state that a S-sparse signal 1 h can be stably recovered from linear measurements y = Φh + ξ, where Φ is a certain matrix with M rows and N columns, M < N, by minimizing the 1 -norm of h. Classical CS often assumes that Φ, after scaling, satisfies the restricted isometry property (RIP)
for all S-sparse h, where δ > 0 is the RIP parameter. The RIP is satisfied with a high probability by a large class of random matrices, e.g., those with entries independently sampled from a subgaussian distribution. By minimizing the 1 -norm, one can stably recover h as long as M ≥ O(S log N ).
The classical random sensing matrices are not admissible in OFDM channel estimation because the channel response h is not directly multiplied by a random matrix; instead, as describe in Section II, h is first convoluted with x, the noise is added, and then the received signal z is uniformly down-sampled to y. Because convolution is a circulant linear operator, we can present this process by y =↓ Ω z =↓ Ω (Ch + ξ), where the sensing matrix C is the full circulant (convolution) matrix determined by x, and ↓ Ω denotes the uniform down sampling at points in
As is widely perceived, CS favors fully random matrices, which admit stable recovery from fewest measurements (in terms of order of magnitude), but C is structured and thus much less "random". This factor seemingly suggests that C would be not favored by CS. Nevertheless, carefully designed circulant matrices can deliver the same optimal CS performance.
To design the sensing matrix C, we propose to generate pilots X in either one of the following two ways: (i) the real and imaginary parts of X(k) are sampled independently from a Gaussian distribution, k = 1, . . . , N; (ii) (same as [15] ) X(k), k = 1, . . . , N, have independent random phases but a uniform amplitude. Note that X(k) of type (i) also have independent random phases. Let F denote the discrete Fourier transform. Following from the convolution theorem
F h, so the measurements y can be written as
Let us explain intuitively why (3) is an effective encoding scheme for a sparse vector h. First, it is commonly known that F h is non-sparse and its mass is somewhat evenly spread over all its components. The random phases of X by design are of critical importance. They "scramble" F h component wisely and break the delicate relationships among F h's components; as a result, in contrast to the sparse
is not sparse at all. Furthermore, X has a random-spreading effect. Due to a phenomenon called concentration of measures [12] , the mass of Ch spreads over its components in a way that, with a high probability, the information of h is preserved by down sampling of a size essentially linear in P -the sparsity of h (whether or not the down-sampling is equally spaced, i.e., uniform). Up to a log factor, the down sampled measurements permit stable 1 recovery. Both types (i) and (ii) of X have similar encoding strength, but X of type (ii) gives an orthogonal C, i.e., C * C = I, so x ⊗ h transforms h into a random orthobasis. Such orthogonality results in multiple benefits such as faster convergence of our recovery algorithm. Due to the page limitation, we omit rigorous mathematical analysis of (3) and its guaranteed recovery.
Note that the proposed sampling ↓ Ω (F −1 diag(X)F ) is very different from partial Fourier sampling ↓ Ω F . The latter requires a random Ω to avoid the aliasing artifacts in the recovery but the former, with random-phased X, permits both random and uniform Ω. Below we numerically demonstrate its encoding efficiency.
CS performance is measured by the number of measurements required for stable recovery. To compare the proposed sensing schemes with the well-established Gaussian random sensing, we conduct numerical simulations and show its results in Figure 2 . We compare three types of CS encoding matrices: the i.i.d. Gaussian random complex matrix, and the circulant random complex matrices corresponding to X of types (i) and (ii) above, respectively. In addition, 1 minimization is compared to our proposed algorithm CS-OFDM, which is detailed in the next section. The simulations results show that the random convolutions of both types perform as well as the Gaussian random sensing matrix under 1 minimization, and our algorithm CS-OFDM further improves the performance by half of a magnitude.

Random convolution has been used and proved to be an effective way of taking CS measurements that allow the signal to be recovered using 1 minimization. In [13] , Toeplitz 2 measurement matrices are constructed with i.i.d random row 1 (the same as type (i)) but with only ±1 or {−1, 0, 1}; their down sampling effectively takes the first M rows; and the number of measurements needed for stable 1 
[14] uses a "partial" Toeplits matrix, with i.i.d. Bernoulli or Gaussian row 1, for sparse channel estimation where the down sampling effectively also takes the first M rows. Their scheme requires M ≥ O(S 2 ·log N ) for stable 1 recovery. In [15] , random convolution of type (ii) above with either random downsampling or random demodulation is proposed and studied. It is shown that the resulting measurement matrix is incoherent with any given sparse basis with a high probability and 1 recovery is stable given M ≥ O(S · log N + log 3 N ). Our proposed type (ii) is motivated by [15] . On the other hand, no existing work proposes uniform down-sampling or shows its recovery guarantees. In addition, most existing analysis is limited to real-valued matrices and signals.
Our work is closely related to [14] and [16] . In [14] , i.i.d. Bernoulli or Gaussian vector is used as training sequence, and downsample is carried out by taking only the first M rows. While channel estimation is obtained as a solution to the Dantzig selector. In [16] , MIMO channels are estimated by activating all sources simultaneously. The receivers measure the cumulative response, which consists of random convolutions between multiple pairs of source signals and channel responses. Their goal is to reduce the channel estimation time. 1 minimization is used to recover channel response.
Our current work is limited to estimating a signal h-vector. While our work is based on similar random convolution techniques, we have proposed to use a pair of high-speed source and low-speed receiver for the novel goal of high resolution channel estimation. Furthermore, we apply a novel algorithm for the channel response recovery based on iterative support detection and limited-support least-squares, which is described in details in Section IV below.

As a result of rapid decaying of wireless channels, P -the number of significant multipaths -is small, so the channel response h is a highly sparse signal. Recall that the nonzero components of h only appear in the firstÑ components. We shall recover a sparse high-resolution signal h with a constraint from the measurements y at a lower resolution of M . We define operation | · | as the amplitude of a complex number, h 0 as the total number of nonzeros of |h| and
where φ denotes ↓ Ω C in (3), the submatrix of C formed by its rows corresponding to the down-sampling points in Ω.
Generally speaking, problem (4) is NP-hard and is impossible to solve even for moderate N . A common alternative is its 1 relaxation model with the same constraints.
which is convex and has polynomial time algorithms. If y has no noise, both (4) and (5) can recover h exactly given enough measurements, but (5) requires more measurements than (4).
Instead of using a generic algorithm for (5), we design an algorithm to exploit the OFDM system features, including the special structure of h and noisy measurements y. At the same time, we maintain its simplicity to achieve low complexity and match with easy hardware implementation.
First of all, we can simply collaborate two constraints into one by letting the variables beh = [h 1 , h 2 , . . . , hÑ ] and dropping the rest components of h. Letφ be the matrix formed by firstÑ columns of φ. Hence, the only constraints arẽ φh = y, which reduces the size of our problem.
We also develop our algorithm CS-OFDM for the purpose of handling noisy measurements. The iterative support detection (ISD) scheme proposed in [17] has a very good performance for solving (5) even with noisy measurements. Our algorithm uses the ISD, as well as a final denoising step. In the main iterative loop, it estimates a support set I from the current reconstruction and reconstructs a new candidate solution by solving the minimization problem min{ i∈I c |h i | :φh = y}, and it iterates these two steps for a small number of iterations. The idea of iteratively updating the index set I helps catch missing spikes and erase fake spikes. This is an 1 -based method but outperforms 1 . Analysis and numerical performance of ISD can be found in [18] . Because the measurements have noise, so reconstruction is never exact. Our algorithm 
Returnh uses a final denoising step, which solves least-squares over the final support T , to eliminate tiny spikes likely due to noise. Our pseudocode is listed in Algorithm 1. In Algorithm 1, at each iteration j, (6) solves a weighted 1 problem, and the solution h j is used for support detection to generate a new I j+1 . After the main loop is done, a support T is estimated above a threshold, which is selected based on empirical experiences. If the support detection is executed successfully, T would be the set of all channel multipath delay. Finally,h is constructed by solving a small least-squares problem, andh i , ∀i ∈ T fall to zero.
This algorithm is efficient since every step is simple and the total number of iterations needed is small. The subproblem is a standard weighted 1 minimization problem, which can be solved by various 1 solvers. Since φ is a convolution operator, we choose YALL1 [19] since (i) it allows us to customize the operators involvingφ and its adjoint to take advantages of DFTs, making it easier to implement the algorithm on hardware, (ii) YALL1 is asymptotically geometrically convergent and efficient even when the measurements are noisy. With our customization, all YALL1 operations are either an DFT/IDFT or one dimensional vector operations, so the overall complexity is O (N log N ) . Moreover, for support detection, we run YALL1 with a more forgiving stopping tolerance and always restart it from the last step solution. Furthermore, YALL1 converges faster as the index I j gets closer to the true support. The total number of YALL1 calls is also small since the detect support threshold decays exponentially and bounded below by a positive number. Numerical experience shows that the total number of YALL1 calls never exceeds P . The computational cost of the final least-squares step is negligible because the associated matrixφ T has its number of columns approximately equal to the number of spikes in h, which is far less than its rows. For example, if the system has P multipaths, the associated matrix for least-squares has size M × P . Generally speaking, the complexity for this leastsquares is O(MP + P 3 ). Since P and M are much smaller than N , the complexity of the entire algorithm is dominated by that of YALL1, which O(N log N ).
In this section, we perform numerical simulations to illustrate the performance of the proposed CS-OFDM algorithm for high resolution OFDM channel estimation. We focus on the mean square error (MSE) of channel estimation as well as the multipath delay detection when channel profile and signal to noise ratio (SNR) changes.
We consider an OFDM system with 1k-point IDFT (N = 1024) at the transmitter and 64-point DFT (M = 64) at the receiver, where we have a compression ratio of 16. The number of silent sub-carrier which acts as guard band is 256 among 1024 sub-carriers. The channel is estimated based on 768 pilot tones with uniformly random phases and a unit amplitude, with the Gaussian noise level ranging from 10 dB to 30 dB. We assume the usage of cyclic prefix and the impulse response of the channel is shorter than cyclic prefix which means there is no inter-symbol interference. For all simulations, we test the total numbers of multipath from 5 to 15. Moreover, we use only one OFDM symbol, i.e. use all non-silent subcarriers only once to carry pilot signals. All reported performances will substantially improve if more pilots are inserted. Figure 3 is a snapshot of one channel estimation simulation. It suggests that the proposed pilot arrangement and CS-OFDM successfully detect an OFDM channel with 7 multipaths when the signal to noise ratio is 30 dB. Our method not only exactly estimates the multipath delays, but also correctly estimates the values of the corresponding multipath components. Figure 4 depicts the MSE performance on OFDM channels with the number of multipaths ranging from 5 to 15 and noise level ranging from 10 dB to 30 dB. When there are only a moderate number of multipaths on the OFDM channel, e.g. 10 multipaths, even when SNR is 20 dB, MSE is as low as −17 dB. Figure 5 shows the reconstructed SNR vs. the number of multipaths when the input SNR changes. We can see that CS-OFDM achieves a gain in SNR. For example, when the input SNR is 10 dB, we obtain a reconstructed SNR higher than 20 dB when there are 5 multipaths. As the number of multipaths increases, the SNR gain from the reconstructed signal to the input signal decreases. However, even when the number of multipaths is 10, we still have a 5 dB gain, e.g. reconstructed SNR is 15 dB when the input signal SNR is 10 dB. The similar SNR gain appears for input SNR= 20 dB and SNR= 30 dB cases. From the entire input SNR and the number of multipath range we have tested, there is an average gain of 6 dB from the input SNR to the recovered SNR. Figure 6 and Figure 7 depict the probability of correct detection (POD) of the multipath delay and the false detection rate (FAR) while we change the SNR and the number of multipaths. When the SNR is above 10 dB, simulation shows almost 100% POD when the number of multipaths changing from 5 to 12. When there is a relatively large number of multipaths, e.g. 15, the probability of correct multipath delay detection is higher than 95% as SNR≥ 10 dB. Even when SNR is low, as long as the number of multipaths does not exceed 10, we still have a POD of greater than 95%. The FAR performance shows the similar results, as the SNR decreases and the number of multipaths increases, performance decreases. But, in a large range, e.g. SNR≥ 10 dB, the number of multipath≤ 10, we have almost zero FAR.

VI. CONCLUSIONS Efficient OFDM channel estimation will drive OFDM to carry the future of wireless networking. A great opportunity for high-efficiency OFDM channel estimation is lent by the sparse nature of channel response. Riding on the recent development of CS, we propose a design of probing pilots with random phases, which preserves the information of channel response during the convolution and down-sampling processes, and a sparse recovery algorithm, which returns the channel response in high SNR. These benefits translate to the high resolution of channel estimation, the lower speed of the receiver ADC, as well as shorter probing times. In this paper, the presentation is limited to an idealized OFDM model, intuitive explanations, and simulated experiments. In the future, we will formalize the work with rigorous theorems and fuse it into more realistic OFDM frameworks. The results presented here hint a high efficiency improvement for OFMD in practice.
Multi-resolution data representations are becoming increasingly popular in image processing applications. Pyramid data structures, in particular, play an important role in coding, and are ideally suited for progressive image transmission [13, 15] . In these data structures, the image is represented hierarchically with each level corresponding to a reduced-resolution approximation. An example of such a coding scheme is the Laplacian pyramid proposed by Burt and Adelson in which the difference between successive levels of a Gaussian pyramid is transmitted [3] . This approach compares favorably with earlier techniques, such as transform or predictive image coding, especially when large compression ratios are desired [7] . Recent developments in pyramid image compression also include subband coding techniques [ 18, 20] , orthogonal pyramid structures [1, 12] and wavelet transforms [9] , which are all based on the concept of quadrature mirror filters (QMF) [4] .
The Laplacian pyramid coding technique described by Burt and Adelson relies on the use of two complementary functions: REDUCE and EXPAND. REDUCE computes a lower resolution level of the Gaussian pyramid by decreasing the resolution by a factor of two. EXPAND performs the reverse operation by mapping the coarser level onto a finer sampling grid. These two functions, as defined initially, were sub-optimal in two respects. First, the basic EXPAND function induces some image blurring, tending to increase the energy of the residual image. Second, the initial REDUCE function fails to minimize the loss of information (in the least squares sense) from one level to the next one. It will be shown here that these limitations can be corrected through the appropriate Signal Processing insertion of additional post-and pre-filtering modules. These operators have an infinite impulse response (IIR) and yet can be implemented very efficiently using simple forward and backward recursions, as discussed in Appendix A.
The presentation is organized as follows. Following a series of definitions, a brief review of the Laplacian pyramid coding concept is given in Section 2. A modified EXPAND function that guarantees an exact image interpolation is described in Section 3. The least squares Laplacian pyramid is introduced in Section 4 and the corresponding REDUCE function is derived. The performance improvement of this new approach is illustrated both qualitatively and quantitatively with some experimental results in Section 5. Finally, the present approach is reinterpreted in terms of quadrature mirror filters in order to bring out the relationship with recent subband (or wavelet transform) coding techniques.

The techniques described in this paper are intended for the processing of digital images. However, to simplify the presentation, we have chosen to concentrate on the pyramidal representation of a one-dimensional signal: {f(k)}k~. All subsequent results carry over directly to higher dimensions if one makes use of separable filtering kernels. In practice, for digital images, this means that a pyramid representation can be obtained from the successive application of one-dimensional operators along the rows and columns.
There are two operations that are particularly useful for our purpose: the up-sampling of a signal by an integer multiple m (in particular, m=2), which is defined as (2.6) and the successive coarser resolution levels are constructed iteratively using the REDUCE operator
This operation requires some form of lowpass filtering and decimation by a factor of 2. Two examples of pyramid representations are shown in Fig. 1 . Burt and Adelson [3] use a 5-point quasiGaussian pre-filter and their REDUCE function can be described as
We will rely heavily on the z-transform representation of a signal, which, as a reminder, is defined as
In particular, Burt's generating kernel [3] is central to the construction of the Gaussian or Laplacian pyramid and is conveniently represented as 8) where the generating kernel w2 is defined by (2.4). The complement of REDUCE is the EXPAND function, which performs a signal extrapolation to a finer resolution level,
This operation involves an up-sampling by a factor of two and some form of interpolation. Burt and Adelson use the following operator [3] :
This operator is symmetric and has a sum equal to two, independent of a. The decimated version of this kernel is 5) and has a sum equal to one.
These two procedures are summarized in Figs. 2(a) and 2(b).
The Laplacian pyramid captures the loss of information resulting from an application of the REDUCE function and is the difference between two successive levels of the Gaussian pyramid:
(2.11)
The Gaussian pyramid is a multi-resolution representation of a signal. It is characterized by a sequence of signals f0,fl ..... fn, with the number of samples reduced by a factor of two in each of the principal directions from one level to the next. The finer or zero level of the pyramid is given by The key idea in the Laplacian coding scheme is to transmit the sequence of difference images Afl ..... Af, with sample values less extensively correlated than are the initial image pixels. The original image is then recovered by progressively expanding and summing the levels of the Laplacian pyramid, starting at the coarsest level. The main advantage of this approach is that the entropy of GAUSSIAN PYRAMID : the difference images is usually smaller than that of the initial image. Thus, the amount of transmitted information can be reduced by source coding. If one is willing to accept some image degradation, a substantial improvement of performance can be further achieved through quantization. Burt and Adelson have shown that the degradation can be made almost imperceptible through a proper choice of the number of quantization levels. The scheme they propose uses more quantization steps Signal Processing for coarser levels of the pyramid. The sample values at coarser spatial resolutions have to be coded more carefully because their contribution affects a larger number of pixels in the final reconstructed image.
Although this approach achieves excellent image coding performance, we have evidence that it can be further improved. The reason for this is that Burt's construction of the Laplacian pyramid is sub-optimal by several criteria: 
The following sections will show how these criteria can be taken into account.
It is straightforward to verify that (3.2) (3.4) where g is the inverse filter given by
The poles of this filter are -2a+ 4x/~l Zl,2 -(3.6) 1 -2a
The major limitation of the method proposed by Burt et al. is that the EXPAND function defined by (2.10) does not produce a valid image interpolation in the sense that the pixel values at the nodes are not preserved when a coarser level is used to approximate the next finer level. In fact,~,~+ l(k) is a smoothed extrapolation off+ 6k) and the energy of the difference signal is therefore unnecessarily large. We have defined a modified EXPAND function that guarantees strict signal interpolation in the sense defined above. This constraint is formally expressed as
This condition can be satisfied by applying the previous EXPAND operation to an auxiliary sequence {p,÷t (k) } :
l=--o¢ chosen to satisfy the constraint
and form a reciprocal pair. For a >~ ~, these poles are real and the system can be decomposed as a cascade or a sum of causal and anti-causal simple exponential filters. For a = I, G(z) = 1 and an exact interpolation can be achieved with no filtering at all. Otherwise, this operator can be implemented recursively with as few as two adds and three multiplies per sample point, as shown in Appendix A (see Table A ). As the signals encountered in practice are of finite extent (e.g. { f(k) Lk = 1 ..... K}), we have chosen to implement both finite and infinite impulse response filters using the following boundary conditions:
This type of signal extrapolation using mirror symmetry is commonly used in image processing applications and has the advantage of suppressing border artifacts.
Our modified EXPAND operator is represented schematically in Fig. 2 (e) and is described formally
It differs from (2.10) only by the adjunction of a pre-filter (g).
A further refinement is to choose a compression scheme that minimizes the energy of the Laplacian. For this purpose, it is convenient to use the auxiliary coeffÉcient sequence {pi(k)} defined earlier and to express the Laplacian as
We now seek the series of coefficients {p~(k)} that minimizes the error criterion,
As demonstrated in Appendix B, the optimal sequence of coefficients pt(k) satisfies the following equation:
The solution is determined by first convolvingf_ l with w2, performing a decimation by a factor two, and finally filtering the resulting sequence with the operator h that implements the inverse of [w22]lz:
By determining [w22]~2(k) explicitly, the corresponding IIR filter is characterized in the z-transform domain,
The poles of this operator are simple and real for ~< a ~< ½. As shown in Appendix A, h can be implemented recursively with as few as five multiplications and four additions per sample point. The relevant filter parameters for different values of a are given in Table A . By substituting (4.4) in (4.1), we find that the least squares Laplacian is given by
Similarly, the corresponding REDUCE function is obtained by substitution of (4.4) in (3.3),
and differs from (2.8) by the inclusion of two additional levels of post-filtering provided by wl and h. By recalling that g, wffk)= ~(k), we note that (4.6) is fully compatible with both (2.11) and the modified EXPAND function defined by (3.8).
However, a direct evaluation of the least squares Laplacian through (4.6) is preferable for most practical purposes. It is more economical and also reduces the propagation of roundoff errors. These results are summarized in Fig. 2 which provides a block diagram representation of the EXPAND, REDUCE and LAPLACIAN functions and a comparison of the conventional and least squares Laplacian pyramids.

The experiments were performed with a =3, unless indicated otherwise. In these comparisons, the three following procedures were considered: (i) the initial Laplacian pyramid (LP) based on (2.8) and (2.10), (ii) the Laplacian pyramid with interpolation (LPI) based on (2.8) and (3.8) , and (iii) the least squares Laplacian pyramid (LSLP) based on (4.6), (4.7) and (3.8).
The Gaussian and least squares pyramidal representations for two test images are shown in Figs. 1 and 3, respectively. In both cases, the sharpness of the least squares pyramid is preserved at all resolution levels, while the corresponding images in the Gaussian pyramid seem increasingly blurred by comparison. The distinction between the two methods is even more striking if one looks at the Laplacian images displayed in Figs. 4 and 5. The same intensity scaling factors were applied to all images to facilitate the comparison. For the initial LP, the amount of information at each level is quite significant and the initial subject is still recognizable. In the case of the LSLP, the energy of the Laplacian is reduced drastically and only very high frequency details are visible in this representation. In a first stage, the performance of the decomposition can be assessed in terms of simple statistics These measures are given in Tables 1 and 2 for test images (a) and (b), respectively. For a = 2, the LPI is superior to the basic LP in all respects (e.g., reduced range, smaller standard deviation and entropy, and better signal approximation). As expected, the LSLP provides an even better signal approximation. In fact, the SNR values obtained for an LSLP extrapolation at a given level i are comparable to those obtained for an LP extrapolation at level i-1 with four times more sample values. The improvement of the LSLP is particularly striking at the finer resolution level at which the residual RMS error is approximately reduced by a factor of 2. Note, however, that this effect is reversed for the coarser levels and that the LSLP has the tendency to pack the energy into the top of the pyramid. In terms of image coding, this means that while fewer bits are required for representing the finer levels of the LSLP, more bits will be necessary for coding the coarser levels, a result consistent with the bit allocation strategy used by Burt and Adelson. For lossless image coding, the number of bits per pixel (bit-rate) necessary to transmit the top of the pyramid up to level i is approximately 
The rate-distortion curves for our test images are given in Fig. 6 . For both images, the LSLP achieves the best performance at all resolution levels. The LP is the worst and the LPI is in between. An aspect that must also be taken into account in this comparison is that the performance of the pyramid decomposition depends on the value of the parameter a. In principle, our modified scheme should result in some improvement for any value of this parameter, although this effect may not always be as dramatic as in the examples discussed above. A case of special interest occurs when a = 0.5 in which case the LP is equivalent to the LPI (i.e. g(k) = identity). The corresponding error statistics for the MRI image are given in Table 3 . The performances of the LPI are slightly superior to those obtained with a = 3. The LSLP performs best but the improvement is not as dramatic as in Table  2 . For comparison, we have also included the results for the LP with a---0.6, the parameter value that resulted in the greatest reduction in entropy and variance in the series of experiments reported by Burt and Adelson [3] . The improvement over 3 the LP with a=~ is substantial, emphasizing the importance of the optimization of this parameter. Despite these excellent results, the optimized LP is still less performant than the LSLP which provides its best results for a = 3.
The Laplacian pyramid coding scheme proposed by Burt and Adelson is especially suited for lossy image transmission [3] . The quantization scheme that they propose uses fewer bins for the higher resolution levels of the pyramid, which takes into account the fact that human contrast sensitivity decreases with high spatial frequencies. We have conducted some preliminary experiments to compare the efficiency of the different pyramid representations for this type of image coding. The experimental procedure is similar to the one used in [3] with some minor differences. The important features of the present compression algorithm are as follows:
(i) The coding and the decoding are performed in parallel starting at the coarsest level of the pyramid. In the present case, the pyramid has three levels and the coarsest (~) is coded precisely using all eight bits per node (256 gray level values). The corresponding contribution to the total bit-rate is only 8=0.125bits/ pixel. (ii) A Laplacian image is computed from the difference between a particular level of the Gaussian pyramid and the expanded version of the encoded image one level coarser. This technique takes into account quantization errors introduced at coarser resolution levels. (iii) The number of levels for each Laplacian image is fixed and should be determined using psychophysical information. The values of these levels are determined using a discrete form of the Max minimum error quantization algorithm [ 11 ] applied to the histogram of the Table 3 Comparison of performance measures at successive pyramid levels for the 'MRI' image with a=0.5 and a=0. images. The corresponding quantization levels are selected to minimize the approximation error and are not necessarily equidistant as was the case in the approach chosen by Burt and Adelson. In this series of experiments, the Laplacian images 2 and 3 were represented by 5 and 15 levels, respectively. The finest level of the pyramid was either not transmitted at all to achieve bit-rates lower than 1 bit/pixel or represented by 3 levels. (iv) The effective bit-rates are estimated from the entropies of the quantized images using (5.2). These estimates are somewhat optimistic as they ignore the transmission of the code book information. A practical approach to this problem is to summarize this information in terms of the coefficients of a parametric model of the Laplacian histogram (for example, the two parameters a and fl of a generalized exponential model p(Af) = Co e-~lAJ-I/~)~). These parameters can then be used to determine uniquely the optimal quantization levels in the Lloyd Max scheme and their corresponding code words in a variable length Huffman code [6] . Some examples of image coding with bit-rates as low as 0.7 bits/pixel are shown in Fig. 7 . The same number of quantization levels were used in all cases with the exception of Fig. 7(d) . This latter image is an improvement of Fig. 7 (c) obtained by adding a finer level of the LSLP quantized with three levels; it is visually indistinguishable from the original. The image obtained using LP (Fig. 7(a) ) appears to be out of focus and is of lesser quality (both qualitatively and quantitatively) than the results obtained with the LPI and LSLP. The LSLP scheme is clearly superior and appears to preserve most of the image details. The same qualitative behavior has also been observed for different compression ratios and test images. For the test image in Fig. 7 , we have also observed that the quality of the LP reconstruction is noticeably degraded for bit-rates lower than 1.5 bits/pixel, while for the LSLP greater compression ratios still produce acceptable results, as illustrated by Fig. 7(c) . In 197 these preliminary experiments, the performance of LSLP appears to be consistently superior.
Our experimental results show that both the LPI and LSLP should be superior to the standard LP proposed by Burt et al. Two types of improvements have been considered and both seem to be equally 1 helpful, at least for a < ~. The first is the requirement that an image extrapolation be a true interpolation of a lower level approximation. A simple way to enforce this constraint is to add a pre- filter  (g(k) ) to the basic EXPAND operation. The effect of this operator is less significant when a is close to ~, in which case the LP and LPI are essentially equivalent. The second is to minimize the amount of transmitted information. The only adjunction here is a post-filter following the basic REDUCE operation. The LSLP incorporates both of these mechanisms and has surprisingly good compression properties. This approach provides an attractive alternative to the standard LP and should allow greater efficiency in image coding. Since multi-resolution techniques are being used increasingly in image processing, there are many other potential applications including image segmentation [2, 16] , edge detection [10] , feature extraction and a variety of multi-grid algorithms for computer vision [14] .
The experimental results presented in Section 5.1 indicate a performance improvement in a lossless progressive data transmission scheme (cf. Fig. 6 ). The reduction of the RMS error also suggests that the LSLP should result in some improvement for lossy image coding as confirmed by our preliminary experiments (cf. Fig. 7 ). These results, however, are still preliminary and require further investigation. For instance, it seems important to determine an optimal bit allocation strategy for a given compression ratio and to compare the coding results for a variety of test images using objective psychovisual criteria. A detailed evaluation of the dependence of the relative performance of the algorithms on the parameter a may also be appropriate. As described in Appendix A, the additional preand post-filters can be implemented very efficiently and the increase in computation is negligible. For instance, the CPU times (standard 16 MHz Apple Macintosh Ilcx) required to compute the first level of the Laplacian of a 256 x 256 image using Burt's LP, the LSI and LSLP are 18 s, 25 s and 27 s, respectively. The complexity of the LSI and LSLP are comparable because the use of the interpolation pre-filter can be avoided in the second scheme (cf. Fig. 2(f) ).
The value a--83---0.375 was used for most of our experiments. It is close to the value 0.36 recommended by Burt for the greatest reduction of the side lobes of the transfer function [2] . Note that a =3 corresponds to an implicit choice of a Signal Processing quadratic B-spline interpolator [17] . In terms of performance, this value of a seems to be preferable over others (cf . Tables 2 and 3) , largely because of the smoothness and Gaussian-like shape of the corresponding interpolation kernel, which appears to be most appropriate for a large class of images. The unmodified LP, on the other hand, seems to perform best for a=0.6 [3] . An explanation for this observation is that the corresponding correction filters in our modified scheme have a very fast decay (i.e., g(k)= O(z~ ~1) and h(k)= O(plkl), where Zl = 0.084 and p = 0.074), and can be relatively well approximated by an identity filter. Another value 1 of interest is a = ~. This value leads to a triangular interpolation function and corresponds to image reconstruction by piecewise linear (or bilinear) interpolation. This scheme is equivalent to a firstorder spline interpolation.
Clearly, the theory presented here is not restricted to the particular form of interpolation function given by (2.4). It is straightforward to adapt these results to any given kernel w(k) ~ W(z). The only constraint is the stability of the approximation and interpolation filters, which, in the general case, are given by
There is another advantage for the use of the least squares pyramid. In a standard complete pyramidal representation the number of nodes is 1 increased by 5 when compared to the initial number of pixels. In the LSLP, the total number I of nodes can be reduced by a (e.g., made equal to the initial number of pixels) because the residual error at each step is orthogonal to the reduced resolution signal approximation. In other words, the LS REDUCE function is a projection operator with the property that
For a bi-dimensional image with M grid points, (5.6) provides us with a set of 1 gM linear constraints. The true number of degrees of freedom of the LS Laplacian is therefore 3M and not M as may be thought initially. In fact, we will show in the last section that the quadrature mirror filter (QMF) concept offers a simple solution for dealing with this redundancy. We will thereby also establish the relationship between the present approach and recent work in orthogonal pyramid structures [1, 12] , wavelet transforms [9] and subband coding techniques [ 18, 20] .
Quadrature mirror filters, introduced by Croisier et al. in 1976 [4] , provide an attractive method for splitting a signal into critically sampled filtered components. Such filter banks can be applied iteratively to produce a subband decomposition of the spectrum into octave bandwidth pieces [18] . The two attractive features of this technique are (i) the reversibility of the process (error free reconstruction) and (ii) the fact that the resulting signal decomposition uses no more samples than the initial representation. Recently, several authors have applied this concept to pyramid image compression and have reported substantial improvements in performance [1, 12, 18, 20] . QMF banks also provide an efficient way of computing wavelet transforms, as has been shown recently by Mallat and Daubechies [5, 8, 9] .
The block diagram of a QMF bank is represented in Fig. 8 . In the basic QMF design [12, 19] , the transfer functions of the filters are chosen such that
where F(z) is a lowpass filter prototype satisfying the perfect reconstruction property
(5.8)
To establish its relationship to the present approach, we will construct a QMF bank such that its lower branch (lowpass) precisely computes the least squares signal estimates derived in Section 4. We derive this result by manipulating the block diagram in Fig. 9(a) , which performs successively the REDUCE and EXPAND functions described in Sections 3 and 4. The first step is to note that Fig. 9 . QMF interpretation of the least squares approximation procedure: equivalent block diagrams.
the two central filters (W~ (z) and G(z)) cancel each other. Second, the filter H(z) is factored into a product of square-root components ( Fig. 9(b) ). Finally, the filters are moved on each side of the sampling modules by upsampling their impulse response by a factor of two (this is achieved by replacing z by z 2 in their z-transform) ( Fig. 9(c) ).
At the end of this process, we have an equivalent system (i.e., same input and output) for which the pre-filters and post-filters are identical and given by
Using (5.4), it is then easy to verify by substitution that this operator satisfies the perfect reconstruction property (5.8) . Since the final output of the QMF bank is equal to its input, it follows that the corresponding highpass branch precisely codes for the residual signal displayed in the least squares Laplacian pyramid. This approach is easily extended to higher dimensions by iterating the subband decomposition along the rows and columns according to the procedure initially described by Vetterli [ 18] . The main advantage of such a QMF decomposition is that the residual signal is now represented without redundancy (i.e., the sum of the number of lowpass and highpass samples is equal to the initial number of samples).
In order to obtain a decomposition closer to our initial design, we choose an alternative, but globally equivalent, factorization with
for which it can be verified that the filters Fo(z) and Go(z) are precisely those required for the REDUCE and EXPAND function described in Sections 3 and 4. The advantage of this latter decomposition is that it can be implemented recursively using the fast algorithms described in Sections 3 and 4 (see Figs. 2(d) and 2(e)) and Appendix A. The highpass components can be evaluated using the same procedure, provided that the FIR smoothing kernel (Wz(z)) (which is used as a pre-and post-filter) is replaced by its modulated and shifted counterparts: z W2(-z) and W2(-z)/z, respectively. We note that this particular choice of filters corresponds to a linear algebraic transform that is non-orthogonal, in contrast to a standard QMF bank as defined by (5.7) (5.8), which can be interpreted as an orthogonal transformation [12] . These results also suggest that a QMF implementation of the present least squares image pyramid could provide a further I improvement by z over the coding procedure used in the experimental part of this paper.
Two methods for improving the Laplacian pyramid proposed by Burt and Adelson for image coding have been described: (i) The EXPAND function has been redefined to ensure that the expansion of a coarser level onto a finer grid is an exact interpolation. (ii) An improved REDUCE function has been derived in order to minimize the loss of information occurring during resolution conversion.
It is easy to modify the initial scheme to incorporate these new functions. This is achieved by adding a pre-filter and a post-filter in the expansion and reduction modules, respectively. These filters can be coded very efficiently and the resulting increase of computations is moderate.
For lossless progressive data transmission, the performance improvement that can be achieved in this way is significant. The least squares scheme performs best according to the quantitative criteria used in this paper. Preliminary results suggest that this approach allows improved image coding according to the lossy scheme developed by Burt and Adelson. The least squares pyramid also stands as an interesting alternative to the widely used Gaussian pyramid and should be useful in a variety of multi-resolution image processing algorithms. It has also been shown that the present approach can be linked to the family of QMF image pyramids (e.g., orthogonal pyramids, wavelet transforms, subband coders). 
The implementation of these elementary units is based on the decomposition of H(z; z,% into a sum of simple causal and anti-causal first order systems, as given by the right-hand side of (A.3). The corresponding recursive filter equations are also more economical to combine the individual scaling factors in (A.1) and (A.5) or (A.7) into a single multiplication at the end of the process. The relevant filter parameters for implementing some of the operators described in Sections 3 and 4 using this strategy are given in Table A . This approach is also applicable in higher dimensions through the successive use of the same onedimensional filter along the various dimensions of the data. For digital images there is no need for floating point data storage other than the onedimensional array(s) required by the basic onedimensional filtering module.
We note that the second equation is borrowed from the sum decomposition and is required to initialize the backward recursion correctly.
All operations in (A.7) (respectively (A.5)) are real, and it is necessary to use one (respectively two) one-dimensional real array(s) for storing the filtered sequences with sufficient precision to avoid a recursive propagation of errors. It is relatively straightforward to write a general subroutine that implements (A. 1) from a succession of simple convolutions of the form (A.5) or (A.7); no additional intermediate storage is necessary for this task. It is
The error criterion (4.2) is decomposed as 
(B.1)
The partial derivative of (B. 1) with respect to p~(k)
is given by The optimal sequence of coefficients is obtained by setting this expression equal to zero, which results in (4.3). Q.E.D.
Albert Einstein discovered noise accidentally in 1905, when he observed that atoms move according to the Brownian molecular motion [1] . Following his discovery, numerous descriptions of physical and biological systems have made incidental reference to noise, without recognizing its essential contribution. Noise is often regarded as an unwanted component or disturbance to a system, even though it has a tremendous impact on many aspects of science and technology [1] , including medicine and biology. A typical example for such a statement is a field of engineering called signal processing. On one hand, many signal processing algorithms have been designed to remove noise from a system, since greater noise levels are associated with degraded performance of algorithms.
On the other hand, noise has been shown to enhance system performance in many areas of signal * Ervin Sejdić is with the Department of Electrical and Computer Engineering, Swanson School of Enginering, University of Pittsburgh, Pittsburgh, PA, 15261, USA. E-mail: esejdic@ieee.org. Ervin Sejdić is the corresponding author.
† Lewis A. Lipsitz is with Harvard Medical School, Beth Israel Deaconess Medical Center and Hebrew Senior Life, processing including stochastic optimization techniques, genetic algorithms, dithering, just to name a few. Similarly, another concept called stochastic resonance (SR), first proposed in 1981 (e.g., [2] , [3] ), describes a positive impact of noise in nonlinear systems. SR refers to the fact that at an optimal level of input noise, signal detection is enhanced [4] , [5] . SR is observed in both man-made and naturally occurring nonlinear systems [6] . For example, paddlefish were shown to use SR to locate and capture prey, implicating this phenomenon in animal behavior [7] . Also, small noisy input can influence the firing patterns of squid axons [8] , enhance breathing stability in pre-term infants [9] , improve postural control in human aging, stroke or peripheral neuropathy [10] , [11] , and stabilize gait in elderly people with recurrent falls [12] .
The intent of this manuscript is to inform researchers from multiple scientific disciplines that noise (i.e., stochastic processes) is a critical component of many biological and physiological systems that may be exploited in the future to develop interventions for the prevention and treatment of diseases. In other words, this manuscript is a crossover between a review paper and a position paper and as such is meant to initiate further discussions about the role of stochastic processes in modeling of physiological systems.
To gather previous contributions cited in this manuscript, we utilized PubMed and Google Scholar to find manuscript published in English using a variety of search terms (e.g., "noise physiology," "noise medicine," "noise brain," "noise aging"). These search terms yielded thousands of manuscripts and we focused only on representative publications from several fields. Extensive coverage of all topics is beyond the scope of this paper, since excellent extensive reviews of each have been previously published (e.g., [6] , [13] ).
The paper is organized as follows: Section 2 introduces various stochastic processes considered in biomedical systems, while also describing the physiological meaning of these processes. Section 3 discusses the important role of noise in fundamental biomedical systems. In Section 4, we discuss several translational applications of noise to treat diseases, while in Section 5, we provide concluding remarks along with an outline of possible future directions.
2 Noise and variability in physiological systems
By definition, noise is a stochastic process with specific spectral characteristics. While many different stochastic processes exist, we consider here the most common types discussed in the literature.
White noise is a stochastic process characterized by equal energy over all frequencies. In mathematical terms, its power spectral density is equal to:
where C w is a constant. The name "white" stems from the fact its power spectral density is the same at all frequencies in an analogy to the frequency spectrum of white light. A time-domain realization of the white noise is depicted in Fig. 1 (a), while its power spectral density is depicted in Fig. 1 
Pink noise (also called fractal or 1/f noise) is a stochastic process suitable for modeling evolutionary or developmental systems characterized by equal energy per octave as depicted in Fig 1(d) [14]. The power spectral density of pink noise is roughly inversely proportional to frequency [14] :
where C f is a constant and 0 < α < 2. 1/f noise is a stochastic process between white noise
(1/f 0 ) and red (Brownian) noise (1/f 2 ); hence, the name pink noise. Pink or fractal noise is found in numerous biological and physiological processes, including the organization of neural networks, Purkinje fibers in the heart, the vascular tree, bronchial tree, and bone trabeculae, as well as electroencephalographic rhythms, heart rate variability, and respiratory intervals [15] , [16] .The omnipresence of pink noise in many diverse applications has led researchers to speculate that there exists some profound law of nature that applies to all nonequilibrium systems and results in such noise [14] .
Because pink or fractal noise arises from the interaction of multiple physiologic or biologic control systems operating over different scales in time or space, it may confer system resiliency, adaptability, and structural integrity. For example, the structural (e.g. bone trabeculae) or functional (e.g., heart rate control) networks that generate such noise retain their integrity or functional ability if individual components are lost or interrupted. This fractal network organization also enables a system to adapt to stress by drawing on specific components and fine tuning its response to overcome a given perturbation [16] . Brownian or red noise is a stochastic process whose power spectral density, as depicted in Fig. 1(f), is defined as:
where C b is a constant.Mathematically, the Brownian noise can be defined as the integral of the white noise.
There are other types of noise specific to certain applications (e.g., blue noise, diotic noise, and dichotic noise). However, the extensive coverage of these topics is beyond the scope of the current manuscript.
Physiology teaches us that healthy systems are self-regulated to reduce variability and maintain physiologic constancy [17] . However, that is not the case in reality. Small amounts of noise, as depicted in Fig. 2(a) , can have a very beneficial role in physiological systems (e.g., [9] , [10] , [15] , [16] ). Also, the non-linear interactions of multiple regulatory systems and environmental influences operating over different time scales produce highly variable "noisy" behaviours in physiological processes that are far from constant [18] . For example, the normal human heartbeat fluctuates in a complex stochastic manner [17] , and can be modeled as a 1/f process (e.g., [19] ).
On the contrary, the stochastic properties of the heartbeat time series degrade in subjects at high risk of sudden death (e.g., congestive heart failure patients) becoming more characteristic of white noise. This situation is depicted in Fig. 2(b) , where a deviation from 1/f noise can result in reduced functional capacity and the onset of disease. Similar counterintuitive results have been obtained in other fields. For example, gene expression can be thought of as a stochastic process [18] , [20] . Stochastic gene variations can have both beneficial and harmful roles. Different patterns of gene expression can influence the stress response, metabolism, development, the cell cycle, circadian rhythms, and aging [18] . Therefore, elucidating the stochastic mechanisms involved in physiologic control systems and complex signaling networks is emerging as a major challenge in the postgenomic era [17] .
A number of studies elucidating the fundamental mechanisms of biological systems suggest that noise is an "essential ingredient" in these systems, without which they cannot function. For example, noise plays an important role in molecular transitions or interactions that control cellular behavior (e.g., how cells acquire fate) [21] . Furthermore, several mathematical models used to describe biological processes require a noise term to adequately model the behaviour of these processes.
Cellular processes, such as transcription and translation, chromatin remodeling and pathwayspecific regulation, are sources of stochastic events leading to cell-to-cell variability [18] , [22] . In fact, cellular behaviour varies in clonal cell populations despite their development in identical environments [23] .
Stochastic processes can have a dual role in these systems. One point of view is that the stochasticity obstructs the efficient functioning of cellular processes [24] . The accuracy of cellular processes, such as the circadian oscillator, is limited by noise in gene expression [20] . Noise can interfere with the operation of engineered genetic circuits [25] and cell-to-cell variability can be reduced by engineering a circuit with negative feedback [26] . There is also evidence that aging is associated with increased randomness in gene expression [18] . For example, cell type-specific gene expressions in individual murine cardiac myocytes [27] , murine muscle tissues [28] and C. elegans [29] become increasingly stochastic as the organism ages. While the mechanisms underlying these stochastic phenomena are still unclear, the process of aging may be dependent on the effects of stochastic gene expression [18] . Another interpretation of these observations is that changes in gene expression with aging are associated with a shift from the more adaptive 1/f or fractal-like noise, to more random or white noise-like behavior that cannot adapt to the metabolic demands of the aged cell. This notion will need experimental validation.
The second point of view is that noise might have beneficial properties [30] . For example, living cells usually acquire their fate deterministically by virtue of their lineage or their proximity to an inductive signal from another cell. However, a cell can choose to differentiate stochastically without apparent regard to environment or history [31] . This random behavior can arise from significant stochastic fluctuations (i.e., noise) in cellular components and biochemical reactions [30] , [32] . Additionally, differences in the micro-environments inhabited by individual cells and preexisting heterogeneity propagated to subsequent cell generations can be sources of such cell-to-cell variations [33] . Cell-to-cell variability is thus a complex function of regulation of gene expression and the regulatory and biochemical networks in which the gene products are embedded [30] .
Neuronal networks are known to have noisy, heterogeneous and compact structures [34] , [35] . There are two points of view regarding the role of noise in these networks. One point of view argues that noise lowers the signal-to-noise ratio causing the performance of these networks to degrade. The second point of view states that noise reduces spike-timing precision and therefore, the information rate is lowered. However, noise can play important and constructive roles for the amplification of information transfer in neuronal networks [36] , [37] .
Stochastic variations are an essential part of the nervous system [37] , and the effects of these variations in dynamical neurobiological systems have been studied extensively for both single neurons and neural networks. Pioneering works by Derksen and Verveen in 1966 [38] and by Katz and Miledi in 1970 [39] were the first to establish the probabilistic behavior of neurons in the central nervous system [34] . Derksen and Verveen investigated the role of membrane noise in the probabilistic behavior of neurons in the central nervous system [38] , while Katz and Miledi studied signal fluctuations associated with ACh receptor-mediated muscle depolarization [39] . The foundations set by these two groups were later applied to experimental data to gain an insight into the nature of transmembrane-conductance changes and information processing in the brain [34] . Subsequent publications showed that intracellular recordings of cortical neurons consistently display highly complex and irregular activity due to an intense and sustained discharge of presynaptic neurons in the cortical network [36] .
In addition to synaptic noise, the stochastic activity of ion channels is another significant source of noise in the nervous system. For example, thermal agitation causes voltage-gated ion channels in neuronal membranes to fluctuate randomly between conducting and nonconducting states inducing noisy membrane currents and subthreshold voltage fluctuations [40] , [41] . It is now understood that channel noise affects spike-timing reliability, action potential dynamics, signal detection, the tuning properties of the cell and overall has important effects on neuronal information processing capabilities [36] , [40] , [41] . Lastly, while noise often leads to increased responsiveness in the nervous system, empirical data and neuronal models demonstrate that noise can also subdue or turn off repetitive neuronal activity [8] , [42] .
Noise generated in the brain may influence brain behavior. Noise is generated in the brain by random spike firing times of neurons [13] . By influencing the variability of the firing of neurons, noise may influence decision-making, memory, and the stability of short-term memory and attention [43] . Furthermore, cognitive operations are also affected by stochasticity in N-methyl-D-aspartate activated receptors, which affect the stability of short-term memory and attention, and in alterations of gamma-amino-butyric acid receptor activated synaptic ion channel conductances which are predicted to influence how likely the system is to jump incorrectly into a pathological state of high activity [13] . Similarly, in motor learning, the brain uses movement errors to adjust planning of future movements. This physiologically plausible strategy is optimally tuned to the properties of motor noise, and likely underlies learning in many motor tasks [44] .
Overall, noise in the brain promotes decision-making, creativity and the shifting of attention to new tasks [13] . The presence of stochastic brain variations (e.g., due to stochastic variations in spiking of neurons and in synaptic transmissions) is helping investigators and clinicians to understand pathological brain stability states, such as schizophrenia and obsessive-compulsive disorder.
One notion is that there is a range of stability states in different individuals. Instability (e.g., due
to random firing of neurons) contributes to the symptoms of schizophrenia [45] , while too much stability contributes to the symptoms of obsessive-compulsive disorder [45] . Of potentially great importance is that by having a model that is based on the ion channel conductances affected by different neurotransmitters, it is becoming possible to make predictions about what could be favorable combinations of treatments for particular disorders [45] .
Noise is omnipresent in sensory systems, ranging from the emission of neurotransmitters from the presynaptic membrane to the behavioral results in visual and auditory experiments (e.g., [6] , [46] , [47] , [48] , [49] , [50] , [51] ). For example, a recent study suggested that the addition of an appropriate amount of external noise can improve the perception of an "uncertain" visual signal that is difficult to detect [52] , [53] . Figure 3 depicts how adding an appropriate amount of noise improves image contrast and then degrades as we add too much noise [54] . Noise of a particular magnitude (i.e., the SR effect) also tends to enhance visually evoked responses in electroencephalography (e.g., [55] ) and magnetoencephalography studies (e.g., [56] ). Similarly, in [57] , the authors showed that a certain amount of noise reduced the pedestal effect, i.e., the improved detectability of a grating in the presence of a low-contrast masking grating. Their results supported the idea that a single mechanism underlies the pedestal effect and stochastic resonance in contrast perception [57] . 1/f noise is also effective in driving hallucinatory pattern formation as shown in [58] , where the authors explored the relationship between ordinary stimulus-controlled pattern perception and the autonomous hallucinatory geometrical pattern formation that occurs for unstructured visual stimulation (e.g., empty-field flicker). Similar results were observed in human hearing experiments [4] . 
Noise is a necessary component even in modeling of certain biomedical systems. In some cases, noise has a specific physiological meaning, while in others, limited knowledge about the systems under investigation yielded creation of a noise category to capture variability observed in experimental data. The next few subsections briefly cover some of the most well-known models requiring a noise term in order to adequately describe a function of a biomedical system. Although there are many more mathematical models that require a noise term in order to accurately model the phenomenon under consideration, it is beyond the scope of this manuscript to review all these different models.
The Hodgkin-Huxley model, one of the most important models in biomedicine, describes membrane potential, activation of Na and K currents, and inactivation of Na current [59] . Specifically, the Hodgkin-Huxley model describes the spiking behavior and refractory properties of neurons and serves as a paradigm for spiking neurons based on the nonlinear conductance of ion channels [60] .
The model is given by four nonlinear coupled equations, one for the membrane potential V , and three for gating variables m, n, and h:
where m ∞ , h ∞ , n ∞ , τ m , τ h , τ n represent the saturation values and the relaxation times of the gating variables. The membrane potential is driven by three types of currents: ionic current I ion , external stimulus current I ext , and synaptic current I syn . I ion is related to the gating variables of m, n, h and describes the ionic transport through the membrane:
where V N a , V K , V l are the corresponding reversal potentials and the constants g N a , g K , and g l are the maximal conductances for ion and leakage channels. I ext is the external stimulus usually serving as a bifurcation parameter of the system. I syn is the sum of the current inputs from all synapses connected to the other neurons and can be modeled as:
where ξ(t) is Gaussian white noise, and σ and τ d are the intensity and the correlation time of the synaptic noise, respectively [60] . Fig. 4 examines the effects of varying σ on the membrane potential, V . As σ decreases, the potentials become highly regular as depicted in Fig. 4(d) . This shows that without noise in the organism, the human body would be a highly deterministic system, and would not be able to account for any changes in the environment.
The Hodgkin-Huxley-type models are important not only because their parameters are biophysically meaningful and measurable, but also because they allow us to investigate questions related to synaptic integration, dendritic cable filtering, effects of dendritic morphology, the interplay between ionic currents, and other issues related to single cell dynamics [59] and there are extensions to various other fields such as cardiology (e.g., [61] ) in the literature.
The Fitz Hugh-Nagumo model is a simple but representative example of excitable systems that occur in application ranging from kinetics of chemical reactions and solid-state physics to biomedical processes [62] . Originally it was suggested for the description of nerve pulses [62] , but it found its applications in other fields as well. The equations are:
where ε << 1 is a small parameter allowing one to separate all fast and slow motions; the parameter α governs the character of solutions; and the parameter σ governs the amplitude of the noisy external force ξ assumed to be additive white Gaussian noise with zero mean [62] .
Similarly as the Hodgkin-Huxley model, the Fitz Hugh-Nagumo model is sensitive to the magnitude of σ as depicted in Fig. 5 . The presence of noise is necessary in order for the model to accurately represent a biomedical process. 
Cancer is stimulated by successive somatic mutations [63] . Here, we briefly review a stochastic model for the computation of cancer risks based on the hypothesis of two successive mutations [63] .
The model assumes that cells likely to mutate will divide over the lifetime of the tissue. Next, the number of type 1 mutation cells produced over the lifetime of the tissue is distributed according to the Poisson distribution with mean µ. The branching process begins with the appearance of the first type 1 cell. This type 1 cell may die with probability 1 − p 1 . The second option is that the type 1 cell divides in two type 1 cells with probability p 1 . At each division of a type 1 cell, there is a probability p 2 for each daughter cell to be a type 2 cell. The probability that a branching process started by a single type 1 cell eventually gives birth to at least one type 2 cell may be computed exactly:
The number of type 1 branching processes that eventually produce at least one type 2 cell is given by the Poisson distribution with mean µP b (p 1 , p 2 ). Hence, the probability that cancer will occur in a particular tissue is given by
The parameter µ is crucial in this model as shown in Figure 6 , as its value will dictate the shape of the probability density function of cancer. For small values of µ, the probability of cancer is almost negligible, even when the probability of type 1 cell branching increases past 50%. However, as we increase the mean number of first mutations to µ = 100, the probability of cancer becomes 100%, as the probability of type 1 cell branching increases past 50%.
Here, we present several applications of noise to enhance health. Also, we briefly discuss how noise (i.e., stochastic processes) can be used to model the effects of aging or social networks.
Noise-based bioengineering techniques and medical devices can play an important role for treating diseases and enhancing health overall. From a clinical standpoint, noise-based techniques and devices have been used to enhance signal detection in patients with significant sensory deficits, such as older adults [64] , [65] , patients with diabetic neuropathy [66] , patients with stroke [67] , or profoundly deaf people receiving speech cues by direct electrical stimulation of the cochlear nerve [68] . Nose-based devices have been used to increase tactile sensations [69] , [70] to help post-partum women achieve higher pelvic floor muscle activation [71] , or to alleviate postural instability due to ankle sprains [72] , [73] and lower back pain [74] . Noise-based solutions can even be used for the enhancement of brain-to-computer interfaces [75] .
Noise-based devices, such as randomly vibrating shoe insoles [10] that apply noise during specific activities or throughout the day, may enable people to overcome functional difficulties due to age-related sensory loss [11] , [64] , [76] , [77] , [78] , [79] . Furthermore, noise-based mechanical ventilators can improve gas exchange and could have a significant effect on morbidity by breaking the chain of injury propagation in acute lung injury [80] . These devices could potentially reduce the morbidity associated with various health issues, such as sensory loss and postural instability in elderly and disabled people or to help stroke patients and individuals with muscle and joint injuries in rehabilitation activities [64] . Noise-based techniques could potentially accelerate a patient's rehabilitation. In this regard, the ultimate realization of a noise-based device may be one that provides durable benefit that lasts long after the device is removed [76] .
Noise has detrimental effects on cognitive performance due to the competition for attentional resources between the distracting and the target stimuli. This has been observed for a wide variety of tasks and stimuli as well as in different participant populations [81] , [82] . However, recent empirical evidence suggests that noise can also improve central processing and cognitive performance. For example, auditory noise enhanced the speed of arithmetic computations [83] and recall on visual memory tasks [84] . Thus, adding a moderate level of noise to the input of the information processing system can increase its signal-to-noise output. On the other hand, adding too little or too much noise attenuates performance [82] . This is consistent with the phenomenon of SR. Noise exerted a positive effect on cognitive performance for patients with the attention deficit hyperactivity disorder, indicating that these subjects need more noise than controls for optimal cognitive performance [82] . The Moderate Brain Arousal model suggests that noise in the environment introduces internal noise into the neural system through the perceptual system. This noise induces SR in the neurotransmitter systems and makes noise beneficial for cognitive performance [81] . Similarly, a recent experiment showed that background noise had opposing effects on inattentive and attentive children. While it enhanced performance for the former group, background noise deteriorated performance for latter group. Background noise also reduced episodic memory differences between these two groups of school children. This suggests that cognitive performance can be moderated by external background white noise stimulation in a non-clinical group of inattentive participants [82] . However, one should be aware that these stochastic resonance effects were not always present [50] .
As described above, many healthy physiologic processes exhibit stochastic variations due to multiple regulatory influences operating over different time scales. These influences include biochemical pathways, opening and closing of ion chambers, feed-forward and feed-back loops, temperature fluctuations, circadian rhythms and environmental changes. Together, they produce pink noise in the output signal as is evident in healthy heart rate, blood pressure, respiratory rate, electroencephalographic potentials, or center-of-pressure time series [15] , [16] . These noise signals lose their 1/f characteristics with aging and disease due to the degradation of various control mechanisms and their interactions, becoming more white or Brownian. As a result, the organism loses resiliency or adaptive capacity [15] , [16] .
This has been demonstrated in the postural control system by examining the body's centerof-pressure (COP) excursions while standing on a force plate [85] . Under normal circumstances, the COP time series exhibits 1/f behaviour, characteristic of pink noise. However, with the loss of vision, sensation in the feet, or both, there is a progressive loss of complexity and long-range correlations in the data. As a result, the individual has more difficulty adapting to a superimposed cognitive task (e.g., counting backwards while standing) and postural sway increases [85] .
Similarly, anatomic structures lose fractal-like architecture with aging, leading to a loss of functionality. This is evident in degeneration and loss of connectivity of the bone trabecular network leading to osteoporosis and fractures; the breakdown of fractal-like alveoli in the lungs leading to emphysema; and the disruption of the collagen matrix in the dermis leading to skin fragility and hemorrhage. In addition, age-related diseases such as the Alzheimer's (e.g., [86] ) or Parkinson's (e.g., [87] ) diseases affect the stochastic variations of physiological variables. For example, the insole forces during the freezing of gait in patients with the Parkinson's disease have been shown to have stochastic behaviour similar to a Brownian process [87] . Furthermore, Parkinson's patients lose the noisy, fractal-like physiologic tremor of the normal motor control system and develop a highly periodic tremor, which is characteristic of their disease. Fortunately, there is evidence that noise can be restored in at least the postural control system by exploiting the phenomenon of SR. When subsensory vibratory white noise was applied to the soles of the feet in healthy elderly subjects while standing on a force plate, the fractal-like multiscale complexity of COP displacements increased to values similar to those seen in young subjects [65] . This intriguing finding supports the notion that noise is an important component of a healthy, and highly functional, postural control system.
Noise has also been shown to play role in the social sciences. For example, the psychic structure long known as the "self" is best conceptualized as a dynamical stochastic system [88] . Among the various topics addressed in this field, it has been found that models for opinion formation in a society exhibit a rich variety of nonlinear behavior, such as phase transitions and critical phenomena, stochastic resonance, chaos, and bistability [89] . In fact, the existence of SR in a model of opinion formation yields the appealing implication that there is an optimal noise level for a population to respond to an external "fashion" modulation. Lower noise intensities lead to the dominance of the majority's opinion, irrespective of external influences, while sufficiently stronger random fluctuations prevent the formation of a definite collective opinion [90] .
Since the recognition of noise at the beginning of the twentieth century, the prevalent view in most fields is that noise degrades system performance and most real-life events do not exhibit noise-like behavior. In this manuscript, we reviewed several biomedical fields where noise plays a constructive role and in some cases is necessary for a biomedical system to function properly. Such a constructive behavior is particularly obvious in systems that depend on the complex interactions of many different components operating on different time scales (i.e., nonlinear systems). Therefore, most of the research efforts have been geared towards:
• understanding the sources of stochastic fluctuations in biomedical systems and possible advantages and/or adverse consequences of these fluctuations on the systems;
• understanding why and how these systems have become robust in their noisy environments; and • how we can use noise to develop treatments and enhance human health.
Further development of noise-based devices or treatments in biomedicine depends on available computational and experimental tools that will answer questions about the the origins of noise in physiological systems and the mechanisms by which noise affects their function. On the computational side, we need to develop more sophisticated algorithms that are capable of simultaneously extracting important stochastic and deterministic variations from the system and handle huge amounts of data. Also, the software applications needed for the understanding of noise in biomedical/physiological systems are almost non-existent. Most computational investigations are carried out using custom-made functions or toolboxes via commercially available packages such as MATLAB (MathWorks, Natick, MA, USA) or SAS (SAS Institute, Cary, NC, USA). On the experimental side, we need to develop experiments and tools that can characterize the noise behavior in systems. The ultimate goal for these advances is to achieve full stochastic resolution over different scales and systems. However, a plan for wide dissemination of data acquired in these experiments should be embedded in these projects to accelerate advances in noise physiology. We anticipate that a limited number of laboratories will have necessary monetary, equipment and staff resources needed to carry some of these sophisticated experiments.
Noise is potentially a very powerful tool in physiology and medicine. We hope this paper will catalyze further research and applications of noise to improve human health and ameliorate diseases.
In this paper, we introduce Swift protocol. Swift was originally designed to be a replacement for the BitTorrent protocol and inherits some of the characteristics that have made BitTorrent successful, but was not intentionally designed according to the principles of information-centric networking (ICN) paradigm. It is, thus, until now a product of (unintentional) evolution towards ICN that we now seek to direct and accelerate, while retaining all the properties that make it work well on top of the existing network.
We find the ICN concept to be increasingly reflected in both the way Internet is being used and in how Internetbased services are being implemented today. In many cases, we find that the problems we struggle with in the current * Work by Victor Grishchenko was carried out while at the Technical University of Delft incarnation of the Internet are those that ICN design proposals seek to address.
For example, over 90% of today's Internet bandwidth [4] is effectively devoted to disseminating static multimedia content. In order to do so to an increasingly large and geographically diverse audience, various approaches are used. For example, for web-based content, Content Delivery Networks (CDNs) like Akamai use modified DNS servers that generate responses based on the topological/geographical location of the requester. These "tricks" are there to achieve on the current Internet the features that are at the core of ICN.
During the past years, a chain of new network architectures based on the information-centric paradigm (also content-centric, name-oriented, named-data) have been proposed, including CCN [17] , DONA [20] , NetInf [11] , secure naming by Wong et al [23] , and content-centric router by Arianfar et al [6] to address the limitations of IP, namely the inability to decouple data from storage, inefficient data dissemination, lack of support for middleboxes, ubiquitous availability of data, and security.
The historical conversation-centric end-to-end model, embodied in the TCP/IP stack, is based on message exchange between pairs of peers, typically servers and clients. On the other hand, ICN is a paradigm in which focus shifts away from the mechanics of moving bits between peers (endhosts). Instead, the focus in on the information itself, and the underlying network only a conduit for the information. In a sense, named-data network breaks with the end-to-end abstraction, as there are no ends and the entire network is considered a cloud, which both stores and serves data.
Similarly, we can see in the evolution of peer-to-peer (P2P) file-sharing technologies how they have adopted ways of managing content that more and more look like ICN. While BitTorrent [10] has always used a SHA-1 hash of the content data to identify that unique content item, it used to be that you also needed a location identifier (the address of the tracker through which the peers hosting the content can be located). However, the current incarnation of BitTorrent instead uses a shared global Distributed Hash Table (DHT) to locate peers using the aforementioned content hash as the key.
The historical Usenet discussion system [5] had all the key information-centric features: logical namespace and unique message identifiers, flood message propagation and caching. The git [3] revision control system represents version history of a project as a directed acyclic graph of revisions, where every revision is identified with SHA-1 hash of its contents; repositories push and pull content, thus forming a network of arbitrary topology.
This tendency towards information-centricity implies a strong demand for a generic named-data substrate that is not reflected yet in the de jure network architecture. Given this dissonance, some have proposed a networking revolution to dethrone the Internet Protocol (IP) in favor of a cleanslate redesign of the networking infrastructure.
While intellectually attractive, we do not consider such an approach realistic. Not only because it would require the expensive and disruptive wholesale replacement of the existing infrastructure, but more importantly because such a migration is unlikely to happen before the wholesale conversion of applications to information-centric analogues of the current application ecosystems, and that conversion is unlikely to happen until the required infrastructure is in place.
Having made this observation, it seems clear that evolution, not revolution, is the best way towards ICN, and by recognizing and helping this along, we can both make ICN happen sooner and ensure that the ICN approach will be one tested in both lab and real-world settings, and hence "the fittest".
We start with the necessary basic properties of any information centric architecture and determine which of them Swift already supports. Further, we determine which information centric primitives Swift does not provide and address them by leveraging existing technologies, such as DHTs to find peers and standard IP to route packets.
In this paper, we argue that our modular design addresses the gap between Internet usage and the underlying network, without requiring clean-slate redesigning of the architecture. In particular, Swift protocol supports most properties proposed in information-centric architectures like CCN [17] , DONA [20] , and NetInf [11] . First, Swift uses names -flat identifiers -to request content instead of end-point addresses; in addition, it segments named objects in uniquely identified chunks. Second, Swift employs perpacket integrity check, enabling any peer in the network to cache and relay content and verify the integrity of each piece. Third, Swift avoids transmitting additional metadata and is suitable for live/mutable data, by employing Merkle hashes [21] .
Moreover, Swift is a receiver-driven chunk-level transport protocol; the receiver may send concurrent requests for chunks to multiple peers in the network in order to enhance its content retrieval rate. To efficiently exploit available bandwidth, Swift employs a delay-based congestion control algorithm named LEDBAT [22] , and to address the issue of middleboxes Swift employs a NAT hole punching mechanism. For peer discovery, Swift can use centralized trackers or DHTs; in Section 7 we explain how Mainline DHT (MDHT) can be used to find peers offering the given data object in sub-second time periods.
The paper proceeds as follows. In Section 2 we introduce ICN related work and summarize system description. Section 3 discusses design properties and the resulting separation of transport and internetworking layers. Section 4 describes our variation of the Merkle hashing scheme and its extensions. In Section 5 we introduce a vocabulary of messages that constitutes our protocol. Section 6 describes our UDP-based implementation. Section 7 discusses the implications for peer discovery and packet routing. Section 8 concludes.
Most proposals on ICN architectures -evolutionary and clean-slate designs -aim to define the main building blocks of an information-centric network. In the CCN [17] design, content names have a hierarchical structure and are constructed according to the standard URI form. Content is requested using an interest packet which contains the name of the content. Every content router receiving the interest packet checks if the given packet is in its local cache and thus returns a corresponding data packet along the reverse path, otherwise, it forwards the interest to the correct interface using longest prefix matching. A similar approach for content retrieval is reflected in the PSIRP architecture [2] , albeit it uses flat instead of hierarchical names to address content.
Some recent ICN projects adopt flat, self-certifying, labels to name content. Initially employed in DONA's design [20] , flat names are used by the route-by-name protocol (devised on top of the IP layer) to request content. Similarly, efforts by Dannewitz et al [11] and Wong et al [23] explore secure naming schemes to ensure the data is persistent and not accessed by unauthorized users.
Self-certifying (flat) names have been criticized for their lack of scalability (cannot be aggregated), flexibility, and lack of security during the translation of flat names to humanreadable names. However, recent work by Ghodsi et al [13] argues that self-certifying names exhibit better security properties than human-readable names because 1) they can handle better denial-of-service attacks -the network knows the binding between the name and the key thus it can verify that a given object is associated with a given name, and 2) may scale better through explicit aggregation -using concatenations of the form A.B.C, where each letter is a name itself.
Despite differences in the naming scheme, the necessary mainstay of any name-oriented network architecture is to employ either cryptographic hashes or signatures in order to enable indiscriminate caching of data in the network and the possibility of its retrieval from any available peer. In Swift, we employ hashes -Merkle hashes -and argue that they are sufficient to perform any transport function. More specifically, Merkle hash trees [21] allow to identify and verify data, thus enabling any peer in the network to request, relay and store data. Furthermore, our variant of hash trees needs no supplementary transfer metadata, rendering transport into a thinner layer than usual.
Swift must be implemented by all inter-operating network peers, and its functioning involves cross-layer relaying of the data, known as internetworking (see Figure 1) . Hence, the required functionality needs to be as simple and formalized as possible. It follows naturally that any rich semantic data names or transfer metadata is unnecessary and should therefore not be part of transport.
We define a natural separation of Swift from the upper naming part which deals with problems inherently semantic and the lower internetworking layer. Leveraging existing deployed routing infrastructure and a simple hash-based naming mechanism, Swift retrieves pieces of content requested by the receiver from peers in the network -functionality that researchers propose to incorporate in any transport protocol for information-centric networks [9, 7] . The naming layer is out of scope, thus we make no specific assumptions.
In our design, content is identified by a single cryptographic hash that is the root hash in a Merkle hash tree, calculated recursively from the content (see details in the Merkle hash extension document [8] ). The ability to verify data against its name allows for storage in the network and retrieval of data from an arbitrary location. Second, as a (packet) network may need to check data integrity piece by piece, possible options boil down to either per-packet signatures, as in CCN, or Merkle hash trees. Differently from signatures, Merkle hash trees provide strict permanent identifiers of static data pieces, so we chose them as the foundation, later extending the approach to dynamic data (see Section 4).
The hashing scheme enables the entire informationcentric stack, illustrated in Figure 1 , in two ways. First, it allows for a perfect application-to-transport handover. Semantically-rich and application-dependent queries are eventually converted into requests to the transport layer for particular data pieces, precisely identified with hashes. Second, hashing enables information-centric internetworking, i.e. identification and relay of data pieces, data verification, and storage in the network.
As depicted in Figure 1 , Swift embeds a layer separation scheme very much reminiscent of TCP/IP. Namely, there is a relay internetworking layer that only deals with separate datagrams. On top of it, there is a somewhat more intelligent transport layer that deals with entire data streams, performing verification, caching, and storage.
Any peer running Swift may cache content. Technically, there is no difference between a peer and a cache -they run the same protocol; the conceptual difference lies in the intention: a cache "stores" content to further disseminate it but is not particularly interested in the given content. The caches may be regular peers or peers put in place by ISPs -who are interested in replicating "popular" content within their administrative domains and thus avoid transit traffic and costs to external domains. If operated by ISPs, such caches (interchangeably, peers) may manage the content they offer according to some basic rules, such as LRU or demand.
Discovering peers or caches may be done centrally through trackers or ISP-based trackers, or in a decentralized fashion through PEX or DHTs. We explain how discovering new
In Swift, data storage and data verification are highly interdependent and important in terms of security. For example, if a caching peer does not verify data integrity, it makes cache poisoning possible. While a final recipient does not accept (drops) incorrect data, an erroneous cache may form a clot in the network, preventing the correct data from passing through. Similarly, data verification requires storage in peers to some degree, as Merkle hash trees need accompanying uncle hash chains to be available in order to verify data pieces.
In a sense, hashes replace IP addresses as end-point identifiers. A receiver uses a root hash to "open" the connection to the network and retrieve the data. The receiver requests specific pieces of data using a novel method called bin numbers (see details in the RFC document [14] ) which allows the addressing of a binary interval of data using a single integer. This numbering mechanism reduces the amount of state that needs to be stored in each peer and minimizes the space required to denote intervals on the wire. Because the receiver directly addresses the data instead of a single end-point at a particular IP location, it has no control over which peer (replica) will respond; the receiver controls the reception of pieces based on local parameters.
We modified Merkle hash trees and focused on smooth operation of both vertical (application to transport) and horizontal (internetworking) handovers to ensure that no peer requires third parties to verify bindings between keys and names (as in CCN [17] ) or to retrieve additional metadata to perform their function. We ensure their operation is as simple and formalized, as possible. In Sec. 4.3, we extend our basic technique to the cases of live data streams and versioned data.
We developed a variant of the Merkle hash tree scheme [21] to satisfy three key requirements: (a) per-packet data integrity checks, (b) no additional metadata and (c) suitability for live/mutable data. The general concept is to start with the root hash only, then incrementally acquire data and hashes, while verifying every single step.
First, content is divided into 1KiB chunks named packets, except for the tail packet, which may have less than 1KiB of data. A cryptographic hash, such as SHA1, is then calculated on every packet. Second, a hash tree is defined over the complete [0, 2 63 ) byte range, which we consider to be a good approximation of infinity in relation to content size. The tree consists of aligned binary intervals called bins, i.e. [i2
Bins are nested, forming a strict binary tree (see Figure 2) . Each tree contains 2 64 bins of different sizes, including one void and one root bin; the base -the lowest level -of the tree is composed of 2 10 byte long bins. The base of the tree (the leaves) accommodates all the data chunks, starting from the left-most leaf. Normally, the base of the tree is wider than the number of chunks, thus the remaining empty leaves in the tree are assigned hash values of zero. In higher levels of the tree (above base), bins contain hashes which are calculated as a SHA1 hash of a concatenation of two -left and right -child (lower-level) hashes. This hashing process iterates until a hash value for the root bin is calculated, known as the root hash. Figure 2 illustrates an example where the file size is less than 8KiB long. Its [8192, 12288) empty bin has zero hash by definition, as do the rest of empty bins outside the [0, 8192) range. The root hash covers the entire [0, 2 63 ) range; this approach gives us a fixed point of reference when growing the hash tree down from the root.
The concept of peak hashes enables two cornerstone features: file size proving and unified processing of static data and live streams. In addition, they help avoid the usage of additional transmission metadata. Formally, peak hashes are hashes defined over filled bins, whose parent hashes are defined over incomplete (not filled) bins. A filled bin is a bin which does not extend past the end of the file, or, more precisely, contains no empty packets.
Practically, we use peaks to cover the data range with a logarithmic number of hashes, so each hash is defined over a "round" aligned 2 k interval. As an example, suppose a file is l = 7162 bytes long (see Figure 2) . That fits into seven packets ( 7162 1024 < 7), the tail packet being 1018 bytes long. For this particular file we will have three peaks, covering [0, 4096), [4096, 6144) and [6144, 7162) ranges (triangles depicted with double lines). The last range might also be written as [6144, 8192) because we round-up to 1KiB packet size.
The number of peak hashes can not exceed log 2 l 1024 . Practically, peak hashes provide us with more convenient "reference roots", as compared to the root hash which is 53 levels higher than the packets. More importantly, peak hashes allow a sender to quickly prove the file size to a recipient who only knows the root hash; otherwise, file size would have to be supplied as a separate metadata piece and thus separately verified, showing up in the protocol and in the interfaces.
In the case of live data streams, the root hash is undefined or, more precisely, transient, as long as new data keeps coming, filling new packets to the right. Hence a transfer has to be identified with a public key instead of a root hash. Keys are more difficult to deal with than hashes, as they have more degrees of freedom. For example, once a key is compromised, any party may rewrite a pre-existing stream.
Also, while a hash might be derived directly from the data, a signature can only be verified once known.
Because of such issues, we try to minimize key/signature usage by using the same peak hashes scheme as in the case of static data. Indeed, once a peak hash is defined, it never changes. Thus, we only need a logarithmic number of signatures to sign peak hashes. After that, we may deal with the same Merkle hash tree as before.
Signing the peak hashes only requires the sender to issue the newly formed peak hashes with their signatures attached. On the receiver side, the recipient will only have to check the signature of a new peak hash and whether it matches its child hashes. Such a calculation is incremental and local. Otherwise, if the root hash were to be signed instead, this would require constant re-verification of all the encompassing peak hashes.
Until this point, we assumed that the sender emits data in "round" 1KiB long packets. What if smaller portions of data need to be committed to the network? We do not equal, but we strongly associate our "packets" with linklayer "frames". Thus, once data is worth sending, before it fills a packet, then it also needs a hash and a signature.
In this section we describe the set of messages that constitute Swift protocol. In this section, we refer to it as a vocabulary which is instantiated as a transport protocol (as in Figure 1 ). No particular serialization, encapsulation schemes, or message exchange patterns are specified, beyond the very basic requirements.
A DAT A message simply carries pieces of data. A DAT A message must carry a bin of data. Specifically, each DAT A message contains the bin number of the piece and the piece itself. This way, uniform pieces or multiples of pieces can be processed, making it easier to check the data hash tree at once. A HASH message carries the necessary hashes that the receiver needs in order to verify the integrity of a piece. We employ the principle of atomic datagrams, which means that every piece of data must be verified once received and accepted, otherwise dropped. At this point, the sender must make sure the receiver has every hash needed to verify the incoming data immediately. Finally, it is possible to supply the recipient with parts of the hash tree incrementally, to allow for an amortized and local verification of data.
As we allow for the possibility of data retrieval from multiple peers in parallel, the vocabulary employs HIN T and HAV E messages. A HIN T (request) message indicates which pieces of data a receiver wants to retrieve, while a HAV E message conveys what pieces of data a sender has available. On incoming data, a receiver uses ACK messages to acknowledge the received pieces; acknowledgements follow the logic of hash trees, which means that data must be acknowledged in bins as well.
We define channels as a means to identify ongoing transfers, where each transfer is identified by either a hash or a public key. Channel identifiers are conveyed through the datagram headers.
As previously stated, Swift [1] protocol is implemented over UDP; the detailed design is described in an IETF draft [14] and an overview is outlined in a technical report [15] . The protocol is a direct implementation of the vocabulary (see Section 5), with some additions and extensions.
Messages are serialized as fixed-width fields starting with a single-byte message type field, followed by fixed-width payload fields, such as bin numbers, data, hashes and such. Messages are packed into UDP datagrams. Datagram processing is event-driven, fully implementing the atomic datagram concept. This means that every datagram is either immediately committed to storage or immediately dropped; there are no buffer-re-assembly mechanics.
The UDP implementation employs LEDBAT [22] congestion control algorithm, which allows streams to run virtually lossless under normal conditions. LEDBAT is a delaybased congestion control algorithm which increases/decreases the congestion window based on the estimated queuing delay. It uses an increased queuing delay as indicator of congestion and thus immediately reacts by backing off (decreasing the rate).
Queuing delay in LEDBAT is known as the one-way delay (label owd in Figure 3 ) and it is calculated as the difference of timestamped packets between the sender and the receiver. The receiver also maintains a minimum over all one-way delays -base delay -which indicates the amount of delay due to queuing. LEDBAT compares this estimated queuing delay against a fixed target delay value (line target in Figure 3) ; the difference determines if the congestion window should be increased or decreased. Figure 3 depicts how LEDBAT predicts congestion and avoids data losses for a given exchange between two peers (the figure only illustrates a preliminary test performed in a controlled environment with several peers spread across continents). In the testing scenario, one peer acts as a content provider -has the whole content -and other peers (requesters) are interested in the given content. The requesters retrieve the pieces, initially from the only content provider, and later on, they continue retrieving pieces from the participating requesters -who already obtained some pieces of the desired content.
Furthermore, Swift implements a unified mechanism of PEX and NAT hole punching functionality [12] . It uses two types of P EX messages -P EX REQ and P EX ADDto retrieve/exchange addresses among the peers, in a gossip fashion. However, P EX messages are transmitted in such a way that they facilitate the communication between peers that are located behind middleboxes: once a peer A introduces peer B to C, it should -within a period of 2 seconds -introduce peer C to B. This mechanism makes Swift agnostic to middleboxes.
To guarantee that a receiver can verify every packet, the sender has to prepend it with the missing hashes. In a network with no data loss, the receiver builds the hash tree incrementally, thus every packet of data needs one hash on average. More precisely, every even packet needs a hash for its sibling, every fourth also needs a hash for its uncle, every eighth also needs a hash for its parent's uncle, and so forth, thus the average is 1. In practice, some packets are lost, so a prudent sender over-provisions hashes to compensate for possible loss. Thus, the actual traffic overhead of hashes is somewhat above the perfect value of 2% (assuming 20 byte hashes for a 1024 byte packet).
The protocol needs to keep more state on the transfer progress, as data might arrive out of sequence -mostly because data is delivered from different peers in parallel. The state must also be communicated over the wire, using unreliable datagrams. We adopted a generic compressed-bitmap data structure named binmaps [16] , a hybrid of bitmap and a binary tree, which allows to track data at an arbitrary scale, starting from a single packet. Data is requested and acknowledged in bins; this provides the necessary compression and redundancy as continuous data pieces are acknowledged with a logarithmic number of messages.
In order to retrieve data associated to a root hash, Swift needs to discover peers. This peer discovery process is performed by requesting peers from a tracker.
Trackers are used in peer-to-peer systems to keep track of peers sharing a given piece of content. The tracker's interface is simple. Peers can request a list of peers for a given content identifier (a root hash in Swift, an info hash in BitTorrent). Peers also register itself in the tracker to be discovered by others.
Swift can use any tracking mechanism regardless of its particular implementation. Tracker mechanisms used in BitTorrent are prime candidates to be used due to their proven merits on large-scale deployments, but other implementations offering equivalent functionality may be used [24] .
BitTorrent's trackers can be centralized or DHT-based. In the first case, the URI of the tracker tracking a given piece of content is necessary. The DHT-based option, on the other hand, forms a global tracking system where all content is tracked, thus no tracker URI is needed.
We favor the DHT-based tracker mechanism due to the scalability of the DHT and the minimization of metadata for Swift (no tracker URI is needed, just a root hash) to retrieve the data. Scalability is well illustrated by Mainline DHT, the BitTorrent's largest DHT-based tracker on the Internet. Mainline DHT is supported by most of the popular BitTorrent clients, forming a DHT overlay of between 6 and 11 million nodes [19] 1 . It is difficult to estimate how many pieces of content Mainline DHT tracks at a given time, but given the size of the BitTorrent ecosystem, even conservative estimations would yield six-digit numbers.
Furthermore, recent measurements [18] have shown that Mainline DHT's response time is consistently low, which makes it suitable for latency-sensitive applications such as on-demand video streaming.
In this paper, we presented a peer-to-peer based transport protocol for content dissemination named Swift and argued that the protocol exhibits ICN properties that help close the gap between the way Internet applications are used today and the underlying infrastructure supporting such applications. Further, we explored ways Swift may embed additional ICN properties in its behavior by leveraging existing technologies and infrastructure, such as decentralized peer discovery mechanisms and standard IP routing.
We would like to thank Arno Bakker and Pehr Söderman for providing us with valuable feedback. The research leading to these results has received funding from the Seventh Framework Programme (FP7/2007-2013) under grant agreement No. 216217 (P2P-Next).
Analyzing an input-output relationship from samples is one of the central challenges in machine learning. The most common approach is regression, which estimates the conditional mean of output y given input x. However, just analyzing the conditional mean is not informative enough, when the conditional density p(y|x) possesses multimodality, asymmetry, and heteroskedasticity (i.e., input-dependent variance) as a function of output y. In such cases, it would be more appropriate to estimate the conditional density itself (see Figure 2 ). The most naive approach to conditional density estimation (CDE) would be -neighbor kernel density estimation ( -KDE) , which performs standard KDE along y only with nearby samples in the input domain. However, -KDE does not work well in high-dimensional problems because the number of nearby samples is too few. To avoid the small sample problem, KDE may be applied twice to estimate p(x, y) and p(x) separately and the estimated densities may be plugged into the decomposed form p(y|x) = p(x, y)/p(x) to estimate the conditional density. However, taking the ratio of two estimated densities significantly magnifies the estimation error and thus is not reliable. To overcome this problem, an approach to directly estimating the density ratio p(x, y)/p(x) without separate estimation of densities p(x, y) and p(x) has been explored (Sugiyama et al., 2010) . This method, called least-squares CDE (LSCDE), was proved to possess the optimal nonparametric learning rate in the mini-max sense, and its solution can be efficiently and analytically computed. Nevertheless, estimating conditional densities in high-dimensional problems is still challenging.
A natural idea to cope with the high dimensionality is to perform dimensionality reduction (DR) before CDE. Sufficient DR (Li, 1991; Cook & Ni, 2005 ) is a framework of supervised DR aimed at finding the subspace of input x that contains all information on output y, and a method based on conditional-covariance operators in reproducing kernel Hilbert spaces has been proposed (Fukumizu, Bach, & Jordan, 2009) . Although this method possesses superior theoretical properties, it is not easy to use in practice because no systematic model selection method is available for kernel parameters. To overcome this problem, an alternative sufficient DR method based on squared-loss mutual information (SMI) has been proposed recently (Suzuki & Sugiyama, 2013) . This method involves nonparametric estimation of SMI that is theoretically guaranteed to achieve the optimal estimation rate, and all tuning parameters can be systematically chosen in practice by cross-validation with respect to the SMI approximation error.
Given such state-of-the-art DR methods, performing DR before LSCDE would be a promising approach to improving the accuracy of CDE in highdimensional problems. However, such a two-step approach is not preferable because DR in the first step is performed without regard to CDE in the second step, and thus small errors incurred in the DR step can be significantly magnified in the CDE step.
In this letter, we propose a single-shot method that integrates DR and CDE. Our key idea is to formulate the sufficient DR problem in terms of the squared-loss conditional entropy (SCE), which includes the conditional density in its definition, and LSCDE is executed when DR is performed. Therefore, when DR is completed, the final conditional density estimator has already been obtained without an additional CDE step (see Figure 1 ). We demonstrate the usefulness of the proposed method, named least-squares conditional entropy (LSCE), through experiments on benchmark data sets, humanoid robot control simulations, and computer art. 
In this section, we describe our proposed method for conditional density estimation with dimensionality reduction.
be the input and output domains with dimensionality d x and d y , respectively, and let p(x, y) be a joint probability density on D x × D y . Assume that we are given n independent and identically distributed (i.i.d.) training samples from the joint density:
The goal is to estimate the conditional density p(y|x) from the samples. Our implicit assumption is that the input dimensionality d x is large, but its intrinsic dimensionality, denoted by d z , is rather small. More specifically, let W and
is an orthogonal matrix. Then we assume that x can be decomposed into the component z = W x and its perpendicular component z ⊥ = W ⊥ x so that y and x are conditionally independent given z: y ⊥ x|z.
(2.1)
This means that z is the relevant part of x, and the rest z ⊥ does not contain any information on y. The problem of finding W is called sufficient dimensionality reduction (Li, 1991; Cook & Ni, 2005) .
Let us consider a squared-loss variant of conditional entropy, squared-loss CE (SCE):
By expanding the squared term in equation 2.2, we obtain
Then we have the following theorem (its proof is given in appendix A), which forms the basis of our proposed method:
This theorem shows SCE(Y |Z) ≥ SCE(Y |X ), and the equality holds if and only if
This is equivalent to the conditional independence, equation 2.1, and therefore sufficient dimensionality reduction can be performed by minimizing SCE(Y |Z) with respect to W :
(2.5)
(R) denotes the Grassmann manifold, which is a set of orthogonal matrices without overlaps,
where I denotes the identity matrix and ∼ represents the equivalence relation: W and W are written as W ∼ W if their rows span the same subspace.
Since p(y|z) = p(z, y)/p(z), SCE(Y |Z) is equivalent to the negative Pearson divergence (Pearson, 1900) from p(z, y) to p(z), which is a member of the f-divergence class (Ali & Silvey, 1966; Csiszár, 1967) with the squaredloss function. Ordinary conditional entropy (CE), defined by
is the negative Kullback-Leibler divergence (Kullback & Leibler, 1951) from p(z, y) to p(z). Since the Kullback-Leibler divergence is also a member of the f-divergence class (with the log-loss function), CE and SCE have similar properties. Indeed, theorem 1 also holds for ordinary CE. However, the Pearson divergence is shown to be more robust against outliers (Basu, Harris, Hjort, & Jones, 1998; Sugiyama, Suzuki, & Kanamori, 2012) , since the log function, is very sharp near zero, is not included. Furthermore, as we show, SCE can be approximated analytically, and thus its derivative can also be easily computed. This is a critical property for developing a dimensionality-reduction method because we want to minimize SCE with respect to W , where the gradient is highly useful in devising an optimization algorithm. For this reason, we adopt SCE instead of CE below.
Since SCE(Y |Z) in equation 2.5 is unknown in practice, we approximate it using samples
(2.6)
If we set a = p(y|z), we have
If we multiply both sides of the above inequality with −p(z) and integrate over z and y, we have
where minimization with respect to b is now performed as a function of z and y. (For more general discussions on divergence bounding, see Keziou, 2003, and Nguyen, Wainwright, & Jordan, 2010) . Let us consider a linear-in-parameter model for b:
where α is a parameter vector and ϕ(z, y) is a vector of basis functions. If the expectations over densities p(z) and p(z, y) are approximated by sample averages and the 2 -regularizer λα α/2 (λ ≥ 0) is included, the above minimization problem yields
The solution α is analytically given by
which yields b(z, y) = α ϕ(z, y). Then, from equation 2.7, we obtain an approximator of SCE(Y |Z) analytically as
We call this method least-squares conditional entropy (LSCE).
The SCE approximator depends on the choice of models-i.e., the basis function ϕ(z, y) and the regularization parameter λ. Such a model can be objectively selected by cross-validation as follows:
ii. Evaluate the upper bound of SCE obtained by b (M, j) using the hold-out data S j :
where |S j | denotes the cardinality of S j . b. The average score is computed as
3. The model that minimizes the average score is chosen:
4. For the chosen model M, the LSCE solution b is computed from all samples S, and the approximator SCE(Y |Z) is computed.
In the experiments, we use K = 5.
Reduction with SCE. Now we solve the following optimization problem by gradient descent:
(2.9)
As shown in appendix B, the gradient of SCE(Y |Z = W X ) is given by
In the Euclidean space, the above gradient gives the steepest direction. However, on a manifold, the natural gradient (Amari, 1998) gives the steepest direction.
The natural gradient ∇ SCE(W ) at W is the projection of the ordinary
is equipped with the canonical metric W , W = 1 2 tr(W W ), the natural gradient is given as follows (Edelman, Arias, & Smith, 1998) :
Then the geodesic from W to the direction of the natural gradient ∇ SCE
where "exp" for a matrix denotes the matrix exponential and O d,d denotes the d × d zero matrix. Note that the derivative ∂ t W t at t = 0 coincides with the natural gradient ∇ SCE (see Edelman et al., 1998, for details) . Thus, line search along the geodesic in the natural gradient direction is equivalent to finding the minimizer from {W t |t ≥ 0}. Once W is updated, SCE is reestimated with the new W , and gradient descent is performed again. This entire procedure is repeated until W converges. When SCE is reestimated, performing cross-validation in every step is computationally expensive. In our implementation, we perform cross-validation only once every five gradient updates. Furthermore, to find a better local optimal solution, this gradient descent procedure is executed 20 times with randomly chosen initial solutions; the one achieving the smallest value of SCE is chosen.
Since the maximum of equation 2.6 is attained at b = a and a = p(y|z) in the current derivation, the optimal b(z, y) is actually the conditional density p(y|z) itself. Therefore, α ϕ(z, y) obtained by LSCE is a conditional density estimator. This implies that the upper-bound minimization procedure described in section 2.3 is equivalent to least-squares conditional density estimation (LSCDE) (Sugiyama et al., 2010) , which minimizes the squared error:
Then, in the same way as the original LSCDE, we may postprocess the solution α to make the conditional density estimator nonnegative and normalized as
where α l = max α l , 0 . Note that even if the solution is postprocessed as equation 2.10, the optimal estimation rate of the LSCDE solution is still maintained (Sugiyama et al., 2010) .
In practice, we use the following gaussian function as the kth basis:
where (u k , v k ) denotes the kth gaussian center located at (z k , y k ). When the sample size n is too large, we may use only a subset of samples as gaussian centers. σ denotes the gaussian bandwidth, which is chosen by cross-validation, as explained in section 2.4. We may use different bandwidths for z and y, but this will increase the computation time for model selection. In our implementation, we normalize each element of z and y to have the unit variance in advance and then use the common bandwidth for z and y.
A notable advantage of using the gaussian function is that the integral over y appeared in¯ (z) (see equation 2.8) can be computed analytically as
Similarly, the normalization term in equation 2.10 can also be computed analytically as
2.8 Discussion. We have proposed minimizing SCE for dimensionality reduction:
In previous work Suzuki and Sugiyama (2013) , squared-loss mutual information (SMI) was maximized for dimensionality reduction:
This shows that the essential difference is whether p(y) is included in the denominator of the density ratio. Thus, if p(y) is uniform, the proposed dimensionality-reduction method using SCE is reduced to the existing method using SMI. However, if p(y) is not uniform, the density ratio function p(z,y) p(z)p(y) included in SMI may be more fluctuated than p(z,y) p(z) included in SCE. Since a smoother function can be more accurately estimated from a small number of samples in general, the proposed method using SCE is expected to work better than the existing method using SMI. We will experimentally demonstrate this effect in section 3.
Sufficient dimension reduction based on the conditional density p(y|z) has also been studied in the statistics literature. The density-minimum average variance estimation (dMAVE) method (Xia, 2007) finds a dimensionreduction subspace using local linear regression for the conditional density in a semi-parametric manner. A similar approach has also been taken in the sliced regression for dimension reduction method (Wang & Xia, 2008) , where the cumulative conditional density is used instead of the conditional density. A Bayesian approach to sufficient dimension reduction called the Bayesian dimension reduction (BDR) method (Reich, Bondell, & Li, 2011) has been proposed recently. This method models the conditional density as a gaussian mixture model and obtains a dimension-reduction subspace through sampling from the learned prior distribution of low-dimensional input. These methods have been shown to work well for dimension reduction in real-world data sets, although they are applicable only to univariate output data where d y = 1.
In regression, learning with the squared loss is not robust against outliers (Huber, 1981) . However, density estimation (Basu et al., 1998) and density ratio estimation under the Pearson divergence are known to be robust against outliers. Thus, in the same sense, the proposed LSCE estimator would also be robust against outliers. We experimentally investigate the robustness in section 3.
In this section, we experimentally investigate the practical usefulness of the proposed method. We consider the following dimensionality-reduction schemes:
None: No dimensionality reduction is performed. dMAVE: The density-minimum average variance estimation method where dimension reduction is performed through local linear regression for the conditional density (Xia, 2007) . 1 BDR: The Bayesian dimension-reduction method where the conditional density is modeled by a gaussian mixture model and dimension reduction is performed by sampling from the prior distribution of low-dimensional input (Reich et al., 2011) . 2 LSMI: Dimension reduction is performed by maximizing an SMI approximator called least-squares MI (LSMI) using natural gradients over the Grassmann manifold (Suzuki & Sugiyama, 2013) . LSCE (proposed): Dimension reduction is performed by minimizing the proposed LSCE using natural gradients over the Grassmann manifold. True (reference): The "true" subspace is used (only for artificial data).
After dimension reduction, we execute the following conditional density estimators:
-KDE: -neighbor kernel density estimation, where is chosen by leastsquares cross-validation. LSCDE: Least-squares conditional density estimation (Sugiyama et al., 2010) .
Note that the proposed method, which is the combination of LSCE and LSCDE, does not explicitly require the post-LSCDE step because LSCDE is executed inside LSCE. Since the dMAVE and BDR methods are applicable only to univariate output, they are not included in experiments with multivariate output data.
3.1 Illustration. First, we illustrate the behavior of the plain LSCDE (None/LSCDE) and the proposed method (LSCE/LSCDE). The data sets illustrated in Figure 2 have d x = 5, d y = 1, and d z = 1. The first dimension of input x and output y of the samples is plotted in the graphs, and the other four dimensions of x are just standard normal noise. The results show that the plain LSCDE does not perform well due to the irrelevant noise dimensions of x, while the proposed method gives much better estimates.
Next, we compare the proposed method with the existing dimensionality-reduction methods on conditional density estimation by LSCDE in artificial data sets.
For d x = 5, d y = 1, x ∼ N (x|0, I 5 ), and ∼ N ( |0, 0.25 2 ), where N (·|μ, ) denotes the normal distribution with mean μ and covariance matrix , we consider the following artificial data sets: a. d z = 2 and y = (x (1) ) 2 + (x (2) ) 2 + . b. d z = 1 and y = x (2) + (x (2) ) 2 + (x (2) ) 3 + . c. d z = 1 and y = (x (1) ) 2 + with 0.85 probability, 2 − 4 with 0.15 probability. The first row of Figure 3 shows the dimensionality-reduction error between true W * and its estimate W for different sample size n, measured by
where · Frobenius denotes the Frobenius norm. All methods perform similarly for data set a, and the dMAVE and BDR methods outperform LSCE and LSMI when n = 50. In data set b, LSMI does not work well compared to other methods especially when n ≥ 250. To explain this behavior, we plot the histograms of {y} 400 i=1 in the left column of Figure 4 . They show that the profile of the histogram (a sample approximation of p(y)) in data set b is much sharper than that in data set a. As discussed in section 2.8, the density ratio (y) . For data set c we consider the situation where {y i } n i=1 contain outliers that are not related to x. The data profile of data set c in the right column of Figure 4 illustrates such a situation. The result on data set c shows that the proposed LSCE method is robust against outliers and gives the best subspace estimation accuracy, while the BDR method performs unreliably with large standard errors.
The right column of Figure 3 plots the conditional density estimation error between true p(y|x) and its estimate p(y|x), evaluated by the squared loss:
is a set of test samples that have not been used for training. We set n = 1000. For data sets a and c, all methods with dimension reduction perform equally well, which is much better than no dimension reduction (None/LSCDE) and is comparable to the method with the true subspace (True/LSCDE). For data set b, all methods except LSMI/LSCDE perform well overall and are comparable to the methods with the true subspace.
Next, we use the UCI benchmark data sets (Bache & Lichman, 2013) . We randomly select n samples from each data set for training, and the rest are used to measure the conditional density estimation error in the test phase. Since the dimensionality of the subspace d z is unknown, we chose it by cross-validation. More specifically, five-fold cross-validation is performed for each combination of the dimensionalityreduction and conditional-density estimation methods to choose subspace dimensionalities d z such that the conditional-density estimation error is minimized. Note that tuning parameters λ and σ are also chosen based on cross-validation for each method. Since the conditional-density estimation error is equivalent to SCE, choosing the subspace dimensionalities by the conditional-density estimation error in LSCE is equivalent to choosing subspace dimensionalities that give the minimum SCE value.
The results of univariate output benchmark data sets averaged over 10 runs are summarized in the subspace dimensionalities chosen by cross-validation averaged over 10 runs. It shows that all dimensionality-reduction methods reduce the input dimension significantly, especially for Yacht, Red Wine, and White Wine, where the best method always chooses d z = 1 in all runs.
The results of multivariate output Stock and Energy benchmark data sets are summarized in Table 3 , showing that the proposed LSCE/LSCDE method also works well for multivariate output data sets and significantly outperforms methods without dimensionality reduction. Table 4 describes the subspace dimensionalities selected by cross-validation, showing that LSMI/LSCDE tends to more aggressively reduce the dimensionality than LSCE/LSCDE.
We evaluate the performance of the proposed method on humanoid robot transition estimation. We use a simulator of the upper-body part of the humanoid robot CB-i (Cheng et al., 2007;  see Figure 5 ). The robot has nine controllable joints: shoulder pitch, shoulder roll, elbow pitch of the right arm, shoulder pitch, shoulder roll, elbow pitch of the left arm, waist yaw, torso roll, and torso pitch joints.
The posture of the robot is described by 18-dimensional real-valued state vector s, which corresponds to the angle and angular velocity of each joint in radians and radians per seconds, respectively. We can control the robot by sending the action command a to the system. The action command a is a nine-dimensional real-valued vector that corresponds to the target angle of each joint. When the robot is at state s and receives action a, the physical control system of the simulator calculates the amount of torque to be applied to each joint. These torques are calculated by the proportional-derivative (PD) controller as
where s i ,ṡ i , and a i denote the current angle, the current angular velocity, and the received target angle of the ith joint, respectively. K p i and K d i denote the position and velocity gains for the ith joint, respectively. We set K p i = 2000 and K d i = 100 for all joints except K p i = 200 and K d i = 10 for the elbow pitch joints. After the torques are applied to the joints, the physical control system updates the state of the robot to s .
In the experiment, we randomly choose the action vector a and simulate a noisy control system by adding a bimodal gaussian noise vector. More specifically, the action a i of the ith joint is first drawn from uniform distribution on [s i − 0.087, s i + 0.087]. The drawn action is then contaminated by gaussian noise with mean 0 and standard deviation 0.034 with probability 0.6 and gaussian noise with mean −0.087 and standard deviation 0.034 with probability 0.4. By repeatedly controlling the robot n times, we obtain the transition samples {(s j , a j , s j )} n j=1 . Our goal is to learn the (Sutton & Barto, 1998) . We consider three scenarios: using only two joints (right shoulder pitch and right elbow pitch), only four joints (in addition, right shoulder roll and waist yaw), and all nine joints. Thus, d x = 6 and d y = 4 for the two-joint case, d x = 12 and d y = 8 for the four-joint case, and d x = 27 and d y = 18 for the nine-joint case. We generate 500, 1000, and 1500 transition samples for the two-joint, four-joint, and nine-joint cases. We then randomly choose n = 100, 200, and 500 samples for training, and use the rest for evaluating the test error. The results are summarized also in Table 3 , showing that the proposed method performs well for all three cases. Table 4 describes the Figure 6 : Three actions of the brush, which is modeled as the footprint on a paper canvas. dimensionalities selected by cross-validation, showing that the humanoid robot's transition is highly redundant.
Finally, we consider the transition estimation problem in sumi-e style brush drawings for nonphotorealistic rendering (Xie, Hachiya, & Sugiyama, 2012) . Our aim is to learn the brush dynamics as state transition probability p(s |s, a) from the real artists' stroke-drawing samples.
From a video of real brushstrokes, we extract footprints and identify corresponding three-dimensional actions (see Figure 6 ). The state vector consists of six measurements: the angle of the velocity vector and the heading direction of the footprint relative to the medial axis of the drawing shape, the ratio of the offset distance from the center of the footprint to the nearest point on the medial axis over the radius of the footprint, the relative curvatures of the nearest current point and the next point on the medial axis, and the binary signal of the reverse driving or not. Thus, the state transition probability p(s |s, a) has nine-dimensional input and six-dimensional output. We collect 722 transition samples. We randomly choose n = 200, 250, and 300 for training and use the rest for testing.
The estimation results are summarized at the bottom of Tables 3 and 4 . These tables show that there exists a low-dimensional sufficient subspace and the proposed method can find it.
We proposed a new method for conditional-density estimation in highdimension problems. The key idea of the proposed method is to perform sufficient dimensionality reduction by minimizing the square-loss conditional entropy (SCE), which can be estimated by least-squares conditional-density estimation. Thus, dimensionality-reduction and conditional-density estimation are carried out simultaneously in an integrated manner.
We have shown that SCE and the squared-loss mutual information (SMI) are similar but different in that the output density is included in the denominator of the density ratio in SMI. This means that estimation of SMI is hard when the output density is fluctuated, while the proposed method using SCE does not suffer from this problem. The proposed method is also robust against outliers since minimization of the Pearson divergence automatically weighs down the effects of outlier points. Moreover, the proposed method is applicable to multivariate output data, which is not straightforward to handle in other dimensionality-reduction methods based on conditional probability density. The effectiveness of the proposed method was demonstrated through extensive experiments, including humanoid robot transition and computer art. Using ∂X −1 ∂ h k ∂W l,l = − 1 σ 2 n n i=1 ϕ k (z i , y i ) ((z (l) i − u (l) k )(x (l ) i −ũ (l ) k )).
Aviation meteorology is the most important aspect of supporting the safety and security of air traffic from extreme weather [1] . Weather conditions can cause or contribute to the aviation accidents included wind, visibility or ceiling, high-density altitude, turbulence, carburetor icing, updrafts or downdrafts, precipitation, icing, thunderstorms, wind shear, thermal lift, temperature (T) extremes, and lightning [2] . A weather forecaster is an actor who guarantees the efficiency and effectiveness of airport operational without affected by weather, so they have to observe all the weather parameters such as air temperature, winds, weather condition, and visibility. Furthermore, these parameters are analyzed in an isobar chart, streamline chart, and upper air chart to get an accurate weather forecast.
Based on Figure 1 , accidents by flight phase as a percentage of all accidents from 1998 to 2017 have dominantly occurred with approach (up to 20%) and landing (up to 50%). Next, it was followed by parking and taxi as a non-fatal hull loss, but it was also essential. Building and cargo in aerodrome may be received damage from the weather such as floods or strong surface winds. To minimize the negative risks, World Meteorological Organization (WMO) has arranged the rules with the use of Aerodrome Warning in Technical Regulations [4] , Volume II, Part I, 7.3. In Indonesia, The Agency for MeteorologyClimatology and Geophysics (BMKG) also compiled the detailed of Aerodrome Warning in PERKA BMKG No. 13 Tahun 2015 [5] . Aerodrome Warning is concise information about meteorological conditions that can affect aircraft and airport service facilities on land such as runway. Aerodrome Warning (AW) consists of weather conditions, wind direction and wind speed, and visibility with observing time and validity time of forecast. Weather conditions as though rainfall, tropical cyclones, thunderstorm, squall, hail, fog, volcanic ash, tsunami, smoke, and toxic chemistry gases should be reported if it occurred or will occur in AW format. Delay avoiding, cargo activities will be fluent and aircraft parking will exists in safety cone and remains sterile if AW is disseminated well [6] .
WIII AD WRNG 02 VALID 170420/170530 HVY TSRA WIND 28015KT MAX 25KT OBSAT 170400 NC= (1)
The code form above explains about Aerodrome Warning in Soekarno-Hatta Meteorological Station (WIII) number 2 with time validity on date 17 from 04:20 UTC until 05:30 UTC will occur heavy rain with thunderstorm with the average wind direction is from 280⁰ (South-West) and wind speed is 15 knot, and maximum wind speed up to 25 knot. The observation of AW was 04:00 UTC on date 17, with no change of phenomena intensity. Improvement in aerodrome warnings nowcasts need better predictions, thus a verification becomes its measurement. Weather forecast verification provides benefits, such as knowing the mistake which causes false prediction [7] . In cases, all stakeholders in the airport can prepare and have plans to mitigate undesirable activity disturbance.
Aerodrome warning archives from January to April 2019 are needed as the basic materials. A computer, an especially calculator, are used to calculate all formula in statistic verification. Automatic Weather Observation System (AWOS) data from January to April 2019 are collected in one folder including rainfall events, thunderstorm events, the peak of wind speed, and minimum visibility. Aerodrome warning and AWOS data are from Soekarno-Hatta Meteorological Station (07L) and Tanjungpinang Meteorological Station. 
Aerodrome warning as a nowcasting forecast and AWOS data as observation references are verified into 5 statistic parameters, such as Probability of Detection (POD), Bias, False Alarm Ratio (FAR), Threat Score (TS), and Heidke Skill Score (HSS). Value of Hits, False Alarms, Misses, and Correct Negatives be required for calculating those 5 parameters. Hits means the prediction and the observation occurred simultaneously, besides Correct Negatives means both of them do not occur. False Alarms shows that the forecast said "Yes", but in observation, it does not occur. Misses means that there is no prediction, but there happened extreme weather that passes the threshold.

(2)
Hits +False Alarms 
Hits +False Alarms +Misses (5) Probability events that can be detected by the Probability of detection (POD), is part of the incident what was observed occurred ("yes") and predicted. The value between 0 and 1, the best value when POD equals 1. The number of events predicted will occur ("yes"), but it does not occur as indicated by False Alarm Ratio (FAR). The value is between 0 and 1which FAR equals 0 is the best value.
Bias is a comparison of the average forecast towards the average observation and shows the frequency of a forecast event compared to observed events. Bias value can show how is the relationship between predictions occurring "yes" with "yes" observations, can be obtained by equation (1) . The value is between 0 and ∞, with bias equals 1, is the best value. Threat score (TS) can show a comparison predictions of the occurrence of "yes" with observations of events "yes", TS values range from 0 and 1, 0 indicates predictions without skill and 1 for the best predictions. HSS = Hits +Correct Negatives −(Expected Correct ) random N − (Expected Correct ) random
Verification will also be used Hiedke Skill Score (HSS) which can provide relative accuracy of forecasts against the chance of random. The HSS value interval is -∞ to 1. Value 1 shows the perfect forecasts.
Mean Absolute Error (MAE) measures the average magnitude of the errors in a set of forecasts, without considering their direction. It measures accuracy for continuous variables. Expressed in words, the MAE is the average over the verification sample of the absolute values of the differences between forecast and the corresponding observation. MAE is a linear score which means that all the individual differences are weighted equally in the average. Root Mean Squared Error (RMSE) measures the average magnitude of the error. The difference between forecast and corresponding observed values are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means RMSE is the most useful when large errors are particularly undesirable. MAE and RMSE can be used together to diagnose the variation in the errors in a set of forecasts. RMSE will always be larger or equal to MAE which there isthe difference between them especially for thevariance in the individual errors in the sample. If RMSE is similar to MAE, then all the errors are of the same magnitude. Both the MAE and RMSE can range from 0 to ∞. They are negatively-oriented scores which are lower values are better.
Rainfall variability in the Tanjungpinang was influenced by many factors. Weather patterns in Tanjungpinang were affected by its geographical location which was surrounded by the ocean so that the convection that occurs was more influenced by local factors [8] .
Climatologically, it will go through wet conditions which were the precipitation that occurs quite frequently from November to January and went through dry conditions with little precipitation in February. The Agency for Meteorology Climatology and Geophysics (BMKG) was intensively socialize the making of aerodrome warnings as a basis for early warning for extreme weather events such as heavy rain, low visibility, and strong winds which may occur around the runway or airport. BMKGled Tanjungpinang Meteorological Station to make an aerodrome warning for extreme weather conditions at Raja Haji Fisabilillah International Airport. Precipitation forecasting was one of the difficult parts to predict and was still being studied [9] . It could be done subjectively based on the forecaster point of view and objective by using a statistical or numerical methods. Figure 2 interpreted the aerodrome frequency warning chart produced bythe forecaster of Tanjungpinang Meteorological Station for extreme weather that occurred at the runway of Raja Haji Fisabilillah International Airport. It showed that aerodrome warning issued not routinely given when there was the potential for extreme weatherespecially when rain has occurred, which could affect the safety of airplane operation. Aerodrome warning for January and February was not made by forecasters of Tanjungpinang Meteorological Station during rain event that observed by AWOS or manual observation. Precipitation intensity that occurred in JanuaryandFebruary was mostly in the light intensity where some rainfall events were measured in rain gauge or AWOS as a trace of rain or no measurable accumulation. Rainfall frequency observed by AWOS in March and April also did not record actual rainfall events or measure it as a trace of rain (about 8.33%) a couple of times, but the rainfall 
Ad Warning Rainfall in AWOS Rainfall Observation Result AW frequency was smaller compared for January and February(about 53.84%). The frequency of aerodrome warnings in March and April ranged about 41.67% from the occurrence of rain events. It happened because of the rain that occurred is only in light category so the aerodrome warning was not made and disseminated to related parties. Aerodrome warning was verified by using the accuracy of statistical value predictions. The parameter values used to verify aerodrome warning issued by the Tanjungpinang Meteorological Station were presented in Table 3 . Aerodrome warning issued from March to April2019 was verified to find out the accuracy of the actual forecast towards extreme weather events recorded by AWOS. Verification was assessed only when aerodrome warning was issued and extreme weather conditions occurred, especially for the rainfall with light to heavy intensity followed by the potential for strong wind and thunderstorm. As for the dates which became the focus of verification in March 2019 that is 28 and 30 as well as in April 2019 that is on 10, 11, 12, 14, 20, 26, 29, and 30. The results of verification were presented in Table 4 .
Soekarno-Hatta Meteorological Station has been as a unit that disseminated Aerodrome Warning in Soekarno-Hatta International Airportincluded for 2 runways (07R/25L and 07L/25R) and has added a challenge to cover new runway (06/24). Figure 3 showed that in January and February, there were quite differences counted which recorded in Automatic Weather Observing System (AWOS) and Hellmann rain gauge paper as observation results. It was caused by rainfall pattern that enters to the aerodrome in other points of the runway. 
Ad Warning Rainfall in AWOS Rainfall Observation Result AW Furthermore, wind direction and speed was as the main factor of those conditions. 07L point in North runway would be touched if the wind moved from West. Figure 3also interpreted the highest number of aerodrome warning production was in January. Table 5 above indicated that the best score of POD was in Marchand also became the highest Bias value. FAR and TS amount among 4 months were not in big difference and they showed a good prediction. For HSS, it was still around fifty percent that meant the forecast was still in a good based on forecast relative accuracy. However, Soekarno-Hatta Meteorological Station in 4 months above was in critical season that was the rainy season. By analyzing all the values in Table 5 , weather forecasters in Soekarno-Hatta Meteorological Station were capable to provide Aerodrome Warning. 
Correct Negative January and February as the peak of the rainy season were needed to minimize False Alarm prediction. Figure 4 illustrated that most of the Hits and Correct Negative were dominating in all data. The bad result of prediction for misses in January was two times higher than February. False Alarm in February was still bigger.To more detail, March exists as the biggest amount of False Alarm ( Figure 5 ). But, Hits number was also in the biggest one. It meant that a weather forecaster wouldbe difficult to predict the time of rainfall occurrence in a transition season. Figure 6showed the number of Aerodrome Warning (AW) issued for wind parameters and visibility. The warning given was in the form of information on increasing wind speed significantly and visibility reduction which could affect aircraft operation. March was a month with the highest number of AW issued for both wind and visibility parameters, while February was a month with the lowest number of AW issued for wind parameters and visibility. Table 6showed the AW verification value issued by comparing AWOS data. Verification for wind and visibility parameters was done by statistical methods, namely MAE and RMSE. Verification was carried out in the AW issued from January to April2019. The verification value would be a good value if MAE and RMSE values were close to zero, so AW was able to Table 6 showed that AW could predict theextreme events specifically wind and visibility parameters. However, AW for wind parameter that issued in February 2019 was not able to predict correctly and showed a large verification value. 
Quantum physics allows us a perfect randomness, so most of all quantum information-theoretic primitives try to offer an unconditional security under the randomness. For examples, quantum key distribution protocols such as BB84 [1] and B92 [2] highly depend on a random measurements for given classified non-orthogonal quantum states.
Instead of the random measurement on non-orthogonal states, we can consider a direct randomization of quantum states through a quantum channel. This randomizing procedures are efficiently accomplished via the private quantum channels (PQC) or quantum one-time pads [3] . In the paper we are interest to some schemes for approximate encryptions (no perfect) and we make an attempt to reducing some classical communication resources. We would like to call the randomizing procedures or maps as random unitary channels (RUC) in terms of quantum channels. There are several methods for the approximate randomizing quantum states, for examples, [4, 5, 8] : We here adapt the procedure of Hayden et al. [4] .
Many applications of RUC in quantum protocols (See e.g., [4, 6, 7] .) are started from the approximate version of PQC. Here we will propose new approximate quantum state sharing (AQSS) scheme, which uses two approximate PQCs (APQC) and reduces the classical pre-shared secrets about one-half as compared with a perfect protocol. Actually our protocol could be including the (well-known) quantum secret sharing protocols [9, 10] , because a quantum state itself is able to operate special quantum tasks, though those are impossible in the classical power. Imagine that if there is a quantum computer only activated under a bipartite quantum state (or quantum key), then our AQSS protocol will give a efficient and secure solution for the quantum key. These approximate quantum state sharing protocols may offer us more opportunities as compared with the quantum secret sharing.
Let's take account of the pre-shared secrets for the approximate quantum state sharing protocols under RUC-based PQC roughly. Assume that a sender Charlie prepares a quantum state ϕ AB (two-qudit) and transmits the state through two independent RUCs, then two distant agents Alice and Bob will receive some output state of including high entropy. For the state ϕ AB the perfect randomization protocol will require exactly the amount of 4 log d-unitary matrices (Pauli matrices). On the other hand, the construction of Hayden et al. [4] for our AQSS scheme implies that only 2 log d + o(log d)-unitaries sufficient. In other words, the perfect quantum state sharing protocol needs to 2l bits of pre-shared secret information, while the AQSS protocol demands about l bits of information. Note that the works in [5, 8] will give a similar result for l bits bound.
We will prove the information-theoretic security of the AQSS scheme in two kinds of eavesdropping: an interior and exterior attackers. The proof of having higher entropy condition for the exterior attacks is not easy fact, so we split the input state ϕ AB to separable and entangled cases. As a result, the von Neumann entropy in both cases can be chosen sufficiently larger, and a leakage information will be arbitrarily small. Finally the authors show that our bipartite AQSS scheme naturally can be generalized to an one-sender and multiparty-receivers schemes.
In section II we introduce the definition of random unitary channels, and briefly mention about special property known as the destruction of quantum states on a product random unitary channel. We present our AQSS protocol based on two approximate PQCs in section III, and investigate the security of AQSS of considering two attacks: an exterior and interior strategies. we finally conclude our results in section IV. 
Now let us define the random unitary channel, and then construct an approximate private quantum channels. For all density matrices ϕ ∈ B(C d ), a completely positive trace-preserving map N :
where the trace norm is defined by
This definition directly induces the notion of random unitary channels. That is, for every ϕ, a quantum channel N :
is ε-randomizing, where the unitary operators U i ∈ U(d), and the probability p i 's are all positives with i p i = 1.
(The notation B(C d ) denotes the set of bounded linear operators from C d to itself and
Note that the parameter n is the number of Kraus operation elements for RUC, so it corresponds to the dimension of arbitrary environment.
For the approximate constructions of RUC, it was known that for all ε > 0 there exist random unitary channels in sufficiently larger dimension d, such that n can be taken to be O(d log d/ε 2 ) in [4] and O(d/ε 2 ) in [12] where U i 's are chosen randomly according to the Haar measure. We here fix the number n of having exactly n = 150d ε 2 , the Theorem 1 in [12] .
As mentioned in the Introduction, most intuitive application of the random unitary channel is the approximate private quantum channel [4] , which is a modification of the perfect private quantum channel [3] via RUC. The RUCbased APQC is the main tool of constructing the proposed AQSS protocol.
The security of PQC is preserved by the argument of the accessible information in which the leakage information is less than ε. Although small information is leaked to exterior attackers, Bob's decoding state is almost equal to Alice's original state ϕ. The FIG. 1 describes the total procedure of APQC.
In the next section we use two one-way independent PQCs between a sender Charlie and a receiver Alice, and the sender Charlie and another receiver Bob. Let's define two RUCs, from the definition of (Eq. (2)), such that
where we fix the probability as an equally weighted probabilities p i = 1 nA and p j = 1 nB for all i, j, and assume that the number of n A is equal to n B , i.e., n A = n B = 150d/ε 2 . For an approximate state sharing of any bipartite quantum state, above two channels play an important role in the approximate quantum state sharing scheme. 
where a security parameter ε be a positive less than 1. The relation above asserts that all encoding states are information-theoretically secure. Unfortunately, for any entangled states proving the bound is not a simple task. Note that the argument for the (efficient) randomization is related to a destruction of correlations in quantum states [4, 11] . The following section gives the AQSS protocol and the security of the protocol. The last of the section, we briefly describe a multiparty AQSS scheme.
Let us assume that Charlie-Alice and Charlie-Bob have independent two APQCs, and Charlie wants to sharing a bipartite quantum state ϕ AB securely between Alice and Bob.
The protocol for a bipartite quantum state sharing is simple ( See FIG. 2 ):
(i) The sender Charlie selects a quantum state ϕ AB and transmits the state through the channel N A ⊗ N B to the receivers Alice and Bob.
(ii) Distant two parties Alice and Bob just hold the state N A ⊗ N B (ϕ AB ) they received.
(iii) When Alice and Bob want to reveal the original state ϕ AB , they must cooperate in a single location. They perform the inverse unitary operations under the locally shared keys.
The security of the AQSS protocol is divided two cases of an exterior and interior attacks. Actually the security is based on information-theoretic assumption, which means that the intercepted states must have the higher von Neumann entropy. Thus any attackers cannot obtain sufficient information for the original states.
First, let us consider an attack accomplished by an exterior Eve. Assume that Eve intercepts the state N A ⊗ N B (ϕ AB ). We here claim that
as d goes to infinity. We don't know the accurate description for the state N A ⊗ N B (ϕ AB ) for all inputs, so we will divide the state ϕ AB into the separable and entangled one and investigate the behavior each other. If product state is given, it is possible to infer the inequality Eq. (4) easily. By using the triangle inequality with respect to the trace norm for the two RUCs, if
, a separable state is given, then
where the inequalities Eq. (6) and Eq. (7) come from the norm convexity and the triangle inequality, respectively [4] . Thus any separable inputs for the product channel are very close to the maximally mixed state
For the separable input cases, there is another bound that depends on the dimension parameter d and n: We can prove that the expectation value for the difference between the channel output and the maximally mixed state (with respect to the trace norm) is very close, that is,
where E {Ui,j } denotes the total expectation value of
and {U j } nB j=1 for the independent RUCs N A and N B , respectively. The Appendix in this paper states that the inequality Eq. (8) is non-trivial and obtained precisely by exploiting the relation between the trace norm and the Hilbert-Schmidt norm. As mentioned above, let's take
This implies that Eve's attack is impossible in principle. What can we do for an entangled input state? Though a direct proof could be impossible, there is an evidence for the statement, the Eq. (5). The Theorem III.3 in [4] states that, for a positive operator-valued measure (POVM) {L i } which is implemented using local operation and classical communication (LOCC), i p i − q i 1 ≤ ε, where Bob cannot obtain any information for ϕ A without Charlie-Alice's key information. Symmetrically Alice's attack is useless. In other words, the Charlie's aim of sharing a quantum state ϕ AB between Alice and Bob will be securely accomplished.
At least above-mentioned two attacks (exterior and interior eavesdropping) cannot break the security of the proposed AQSS protocol. so the cooperation between Alice and Bob always restores the original state approximately.
In the proposed scenarios, the perfect protocol for quantum state sharing requires exactly d 4 unitary operators, while our protocol only needs to total 22500d 2 /ε 4 unitaries for sufficiently larger d. This fact directly means that some pre-shared key bits are reduced by factor 2, since the AQSS is needed 2 log d − 4 log ε + O(1) secret bits, but the perfect QSS is required 4 log d bits. For any state ϕ AB ∈ B(C d 2 ), and for any channel N AB (for an ε > 0 is arbitrary), let's consider a relation like that
Then, it is sufficient to construct the perfect QSS (ε = 0) with d 4 Pauli operators for the channel N AB in the sense of PQC [4, 8] . In the case of our approximate QSS, the product channel of two RUCs (N AB = N A ⊗ N B ) just consume of half secret bits, so we say that it is efficient in weak sense (though small information is always leaking).
Without loss of generality, a direct extension of the bipartite quantum state sharing protocol (Eq. (8)) gives the security of a multiparty approximate quantum state sharing (MAQSS). Assume that a sender Charlie (C) prepares an m-qudit ϕ A1A2···Am . If they initially have shared PQCs between C-A 1 , C-A 2 and so on, then, for any ε > 0,
The above Eq. (12) implies that any exterior attacks will be failed. Furthermore all interior attacks (including group conspiracy) will be frustrated to obtain the whole state without others secrets, it has similar reason to the two receivers protocol. Let's look at the cost of secret bits for the MAQSS scheme. Roughly speaking, the perfect scheme requires 2m log d secret bits, but MAQSS only m log d + o(log d)-bits sufficient.
We studied that the approximate quantum state sharing schemes are efficient from the classical information cost of view and those are robust to the two kinds of attacks. The proposed AQSS protocol basically depends on an approximate private quantum channel, which is constructed via two independent random unitary channels. Although the protocol leaks small information corresponding to the security parameter ε, the scheme preserves its informationtheoretic security, and so the AQSS and MAQSS schemes can be interpreted as some high-efficiency state sharing protocols for any bipartite and multipartite quantum states.
The Direct Simulation Monte Carlo (DSMC) method is a computational tool for simulating flows in which effects at the molecular scale become significant [1] . The Boltzmann equation, which is appropriate for modeling these rarefied flows, is extremely difficult to solve numerically due to its high dimensionality and the complexity of the collision term. DSMC provides a particle based alternative for obtaining realistic numerical solutions. In DSMC the movement and collision behavior of a large number of representative "simulation particles" within the flow field are decoupled over a time step which is a small fraction of the local mean collision time. The computational domain itself is divided into either a structured or unstructured grid of cells which are then used to select particles for collisions on a probabilistic basis and also are used for sampling the macroscopic flow properties. The method has been shown to provide a solution to the Boltzmann equation when the number of simulated particles is large enough [2] . The sizes of DSMC cells have to be much smaller than the local mean free path for a meaningful simulation in general.
Since its introduction, the Direct Simulation Monte Carlo (DSMC) [1] has become the standard method for simulating rarefied gas dynamics. It is generally very computationally intensive, especially in the near-continuum (collision-dominated) regime. The general wisdom for accelerating the DMSC computation is to parallelize the code using the MPI protocol running on clusters with large numbers of processors such as PDSC by Wu et al. [3] . Such implementations rely upon the Multiple Instructions on Multiple Data (MIMD) parallelization philosophy and parallel efficiency over massive numbers of nodes is not optimal. Recently, Graphics Processing Units (GPUs) have become an alternative platform for parallelization, employing a Single Instruction on Multiple Data sets (SIMD) FIGURE 1. A flowchart describing the application of DSMC to GPU-accelerated computation.
parallelization philosophy. The resulting parallelization is much more efficient at the cost of flexibility -as a result, the computational time of several scientific computations, especially those which are optimally applied to vectorized computation strategies, have been demonstrated to reduce significantly. The application of GPU computation also has significant advantages in lower power consumption and significantly reduced equipment costs.
Experience showed that the higher the locality of the numerical scheme/algorithm, the higher the speedup is. However, there seems no successful previous study applying GPUs to accelerate the DSMC computation, which employs a (generally) highly local algorithm. The DSMC method, which is a particle-based method developed by Bird [1] , is described in Figure 1 . Following initialization, the DSMC method generally involves:
 Moving all the simulated particles, which includes treatment of boundary conditions,  Indexing all simulation particles (sorting particles into cells),  Performing collisions between particles -outcomes of collisions are stochastic in nature, hence the use of the term "Monte Carlo" in the DSMC name,  sampling the molecules within cells to obtain the macroscopic quantities.
In this study, an all-device (GPU) computational approach is adopted, which includes particle moving, indexing, colliding between particles and sampling. This required some changes of the original DSMC method in order to allow efficient all-device computation. Figure 1 shows the flowchart of DSMC computation using a single-GPU. During the initialization stage, input data is loaded into memory and initial states are determined on the host (CPU). This information (including particle and computational cell information) is transferred to the GPU device global memory. Following this, the unsteady phase of the DSMC simulation is performed -particle moving, indexing, particle selection and collisions and sampling are executed on GPU. During particle movement, each particle is tracked by a thread, with each thread tracking N p /N thread +1 particles, where N p is total simulated particles and N thread is number of threads employed by the GPU device. Each thread reads/writes particle data to/from the global memory of the GPU device [4] . The particle indexing phase of the computation is similar to Bird's DSMC implementation [1] . We use a function contained within the Software Development Kit (SDK) of CUDA, scanLargeArray, to scan through data elements of large arrays contained within global memory. This function is used to efficiently perform particle indexing. During the collision phase, a different parallelization philosophy employed -all particle collisions within a cell are handled by a single thread, allowing efficient recollection of data since all data is coalesced. During the sampling phase, shared memory of GPU [4] is used to store properties of all particles in each cell. After all particles within the same cell are sampled, we copy the sampled data from the (faster) shared to (larger) global memory. When our simulation is nearing completion (i.e. the flow has reached steady state and the sampled data is sufficient to remove undesired statistical scatter) we move the sampled data from device (GPU) global memory to CPU memory. Finally, calculation of the macroscopic properties is performed by the host and the data is written to file for further analysis.
In this study, we verify our DSMC implementation and demonstrate significant speedup of DSMC computations using a single GPU device through the simulation of several two dimensional benchmark problems: namely, (i) supersonic flow over a (fixed temperature) horizontal flat plate, and (ii) a supersonic lid-driven cavity problem. Approximately 50 particles per cell for all benchmark test cases are maintained throughout the simulations. The DSMC computation employs a VHS collision model [1] for each case. All benchmark simulations are performed on the latest high end computation equipment: single CPU computations employ an Intel Xeon X5472 CPU (3.0 GHz, 12 MB Cache) while GPU computations employ an Nvidia Tesla C1060 (240 microprocessors @ 1.4 GHz, 4 GB DDR3 global memory) hosted by the same Intel Xeon CPU employed for the single CPU test cases.
This benchmark involves the two dimensional supersonic flow over a flat plate. Ideal argon (γ=5/3) with temperature 300 K is initially assumed to be moving with Mach Number M = 4.4 over a diffusely reflecting flat plate of fixed temperature 500 K. The length of the flat plate is L = 0.9m, with the initial flow-field density based on the Knudsen number (computed with characteristic length based on the plate length). The initial simulation conditions are summarized in Table 1 .
Following the initialization, the simulation is allowed to progress in an unsteady fashion until a steady solution is reached, after which samples are taken to eliminate statistical scatter. Figure 2a shows the resulting contours of temperature for Case III. The results show that the GPU code can reproduce the data simulated by the serial (CPU) code with allowances for statistical scatter. Figure 2b shows the speedup obtained using GPU as compared to using a single core of the Intel Xeon X5472 CPU for each case. We demonstrate a decrease in computational time of 3~10 times when the GPU device specified is employed for the simulation. Table 2 summarizes the computational time and speedup of each component of all cases using both CPU and GPU. We observe that the speedup using GPU computing increases with reduced rarefaction of the flow. This is also justified since (i) application of a GPU device requires significant overhead due to data transfer and device initialization, and (ii) the general ratio of memory bound communications to device computations is reduced for larger flow problems, resulting in more time spent in computation than in communication. In the current implementation, the sampling phase of the simulation performs best due to the ideal parallelization using the device shared memory. 
The second test case is a two-dimensional supersonic lid driven cavity problem. Here the simulation domain is a square cavity (1x1m) with diffusely reflecting walls of fixed temperature (300K). All walls are stationary except the upper wall which is moving (with positive velocity) at Mach Number M = 2. The gas (ideal argon, γ=5/3) is initially at rest with a temperature of 300 K and various density depending on the governing Knudsen number, computed with a characteristic length equal to the box width. These initial conditions are summarized in Table 3 .
Following initialization, the simulation is progressed in time until the flow is steady and samples are taken to reduce the statistical scatter. Figure 3a show the temperature contour of Case II. It is worth noting that the region in the central region of the lid-driven cavity is rarefied, with a low density and relatively high temperature (as also shown in Figure 3 ). The GPU results are almost identical to the equivalent CPU results which again validate the CUDA GPU implementation. Differences between the results are (probably) able to be explained by differences between the random numbers employed by the GPU and CPU solvers and general levels of statistical scatter. The random numbers employed by the GPU DSMC implementation are drawn from pre-computed (fixed-length) arrays to improve the efficiency of random number use on the GPU device. Figure 3b shows the speedup using GPU is about 3~9 times as compared to that using a single core of the CPU described above. All computational timings are summarized in the Table 4 for reference. The ratio of communication to computation, a critical factor in GPU efficiency, is minimal for larger DSMC problems. Hence, the speedup using GPU computing is shown to increase with reducing rarefaction of the flow (i.e. increasing collision dominance). The sampling phase of the DSMC simulation is shown to be the most efficient phase of the computation when applied to GPU computation due to the low number of communications required. 
Presented here is the application of DSMC to a novel (entire-device based) GPU acceleration. In the implementation discussed, the CPU is only employed during the initialization and concluding phases of the simulation -the main processes employed during the DSMC simulation are performed on the GPU device while the CPU remains idle. The resulting computations demonstrate a speedup of 3~10 times, depending on problem size, resulting from the lack of communication-bound processes. Efficient use of various memory (caches) on the GPU device for different phases of the DSMC simulation also allow high levels of parallelization. The GPU-DSMC code has been verified through comparison against a conventional serial DSMC computation, with results showing excellent comparison in a fraction of the time. The GPU device used for the benchmark problems (Nvidia Tesla C1060) is commonly available for a fraction of the price associated with a conventional computer cluster required to match its performance.

Light detection and ranging (LIDAR) is an application of lasers used to discover information about a distant object, usually some form of distance information. LIDAR data are usually collected via airplane, with the airplane traversing a subject area of interest and collecting data about this area through the use of laser pulses. These laser pulses are emitted from the airplane fuselage from a laser scanner device, and the round trip travel time from this emission and the returning light is measured. This time delay is then used to determine the elevation of the ground in this particular area. LIDAR equipment is usually paired with a global positioning system (GPS) in order to accurately pinpoint the x and y coordinates of a specific data measurement.
The way in which LIDAR data is collected is a very detailed one that needs to be explained more thoroughly in order to appreciate it fully. This information as released by the National Oceanic and Atmospheric Administration (NOAA) details in depth about how this data is so intricately extracted. Already discussed is the use of laser pulses, which scan the area of interest up to 5000 pulses per second.
The laser type used is generally the neodymium-doped yttrium aluminum garnet (Nd:YAG) laser with a wavelength around 1064 nanometers, placing the laser in the infrared range of light. This pulsed laser is emitted when the population inversion in the resonator has reached its maximum, achieved by what is known as Q-switching, which places a switch inside the resonator which releases the stored pulse when the maximum amount of atoms are in an excited state.
Of major interest is the mechanism which allows this technology to perform its task of collecting LIDAR data. Two mirrors, one a 45-degree angled folding mirror and the other a moving mirror, combine to direct the laser pulses towards the ground. The laser pulses encounter the folding mirror first, with the reflection directed towards the moving mirror below. The reflection from the moving mirror directs the laser pulses towards the ground, with an overall coverage angle of 30 degrees. Since these aircraft fly at heights of around 700 meters, the 30 degree angle allows an overall circular coverage area with a diameter of approximately 350 meters. Therefore, in order for the aircraft to cover a broad area of land, multiple paths are usually flown, resulting in some overlap between coverage areas, as illustrated in Figure 1 
With the LIDAR data collected, the real question becomes apparent: how is LIDAR data processed? LIDAR data in combination with a GPS allow for each measured elevation to be paired with its location, effectively creating a three-dimensional coordinate for each measured elevation. The x and y coordinates are usually latitudes and longitudes derived from the GPS system, while the z coordinate is the measured elevation using the LIDAR collection device, measured usually in either meters or feet. An ASCII file with each line containing a measurement point (x,y,z) having each coordinate separated by either a tab or comma is common when working with LIDAR data. The precision on this height measurement is usually in the range of 10-25 centimeters, allowing for a very accurate measurement of ground elevation.
LIDAR data is currently being used in a diverse array of applications, ranging from the study of seismology to traffic control analysis. LIDAR is used in seismology to detect faults, with one such example relating to a particular fault in Seattle. LIDAR allowed the detection of the Seattle fault from an earthquake that occurred over 1000 years ago, effectively painting a picture of the surface in the area by penetrating through tree canopies, allowing for a view of what is now known as the Seattle fault. LIDAR data is also used in traffic law enforcement, replacing radar as a speed detector in police laser systems. Other applications of LIDAR relate to its use in adaptive cruise control systems in cars and its use as a height measuring tool in the forestry industry.
One of the main uses of LIDAR data and an important idea in the development and identification of building footprints pertains to its ability to create topographic maps of areas. LIDAR data is able to be manipulated and processed in such a way to allow a visual representation of an area based on the time delay measurements translated into heights along with the paired GPS coordinates. The final result of such processing techniques on this data is known as a digital elevation model (DEM).
While the creation of DEMs is not directly considered here, it is important to note its use as it is the main scientific and image processing based application of LIDAR data. In the figure below, a DEM of Mount Saint Helens is shown as an example of this particular LIDAR application performed by LIDAR mapping company EarthData [9]. Colors gravitating towards the red end of the spectrum represent areas of higher elevation, whereas colors present on the blue end of the spectrum represent areas of lower elevation. This particular DEM is shown in three dimensions in order to enhance the view and appearance of the elevation data generated using LIDAR. LIDAR intensity images are also an important visualization of LIDAR data, allowing irregularly spaced LIDAR data to be represented as a digital grayscale image of varying intensities, with higher intensities representing higher elevations, and lower intensities representing lower intensities. This particular creation of grayscale intensity images will prove extremely important in the classification of particular areas of interest, in this case, the classification of buildings.
LIDAR intensity images can be examined using image processing techniques. These techniques allow the visual representation of LIDAR data to be identified, classified, and separated into areas of interest to be analyzed. One such classification involves the identification of buildings, with this identification proving important in various real life applications, including the study of urban population and the development of ground plans. The identification of buildings can also be extended into the development of three-dimensional building models.
Previous work has successfully identified building footprints through the use of filtering, region growing, and dominant direction estimation [1] . However, the use of dominant direction estimation as a means of cleaning building footprints assumes a particular relationship between perpendicular/parallel building edges and oblique building edges. Also, a visual comparison to aerial imagery was used to detect omission and commission errors. With a refined definition of what constitutes a building boundary along with a new method for confirming building boundaries through the use of a satellite image region growing algorithm, this work provides a new view of building footprint detection.
One of the major applications of LIDAR technology has been the ability to use the data for feature extraction and identification. Building footprint extraction is just one of the many feature extraction opportunities provided by airborne LIDAR data. The goal is to determine the location of buildings in the area of interest, effectively separating buildings by determining their boundary. There are many ways to accomplish this task, with commonalities along the way in each. One such method of extracting building footprints will be described in great detail here.
Before any analysis can be done on LIDAR data, the data must become useful in a way that allows understanding and interaction. Once the area of interest has been determined, the LIDAR XYZ files need to be interpreted in a way to allow such an understanding to be possible. A visual depiction of LIDAR data becomes a necessary means for understanding and future analysis, and the use of image processing provides such a means to the researcher. The LIDAR data will be visualized in a digital image, with the x and y values representing pixel coordinates of the image, and the elevation value z representing the intensity value of the grayscale image. This pre-processing stage sets up the ability for any application of LIDAR data and will help to extract building footprints.
First, the LIDAR data XYZ files are read into a three-dimensional array, with each coordinate (x,y,z) being read into a row of the array. The goal is to create an image using the LIDAR data points that maps each coordinate to a pixel or group of pixels. LIDAR data by its very nature has a randomness to its data points, which prohibits a one-to-one mapping of each LIDAR point to a pixel, as some areas have such a dense coverage but others are sparsely sampled. Upon reading the data into the array, the number of points read is determined and stored in order to find the minimum x and y values of the LIDAR data. The horizontal size of the image is determined by the user, and the vertical size is derived based on this horizontal size by manipulation of the maximum and minimum coordinates, as indicated in the following equation.
Basically, the user controls how large of an image the LIDAR data will be processed into, but only to an extent. The LIDAR data itself and its range determines the height to prohibit too small or too large an overall image size chosen. However, this can result in some information loss because of the lack of a one-to-one mapping as stated above. Also, each pixel could contain more than one LIDAR coordinate point, in which case the smallest elevation value is chosen as the pixel value at that location. This would seem to grid the LIDAR data into an image reasonably, but not all pixels have a corresponding LIDAR data point and elevation value. In this case, a value is used as a place holder if there is no corresponding LIDAR point for that pixel.
Once the above step is performed, the image is processed further by interpolation of pixel values. The image is interpolated in order to replace the place holder points described above with a minimum elevation value in a window. A window size w is specified for this interpolation with a value of 3. Each pixel value is scanned in a search for the place holder value, indicating that there is no data for this specific location in the area of interest. When a pixel of this value is found, the pixel values in the window are scanned. If less than 25% of the pixel values in the window contain the place holder value, the minimum pixel value in the window is determined and assigned to the pixel in question. However, if more than 25% of the pixel values in this window also contain the place holder value, then the pixel value in question remains the place holder value, but the window size is increased by two. This process continues up to a maximum window size specified by the user, in this case the maximum window size being 15. This progressive window operation allows stray place holder pixels to be replaced by surrounding values in order to have a more complete image, but place holder pixels that do not get replaced through this interpolation remain as such and are represented in the resulting pre-processed grayscale image as the highest intensity possible, resulting in a white color. The resulting image after interpolation is shown in Figure 3 below. This image represents a subset of Lenoir County, North Carolina. The pre-processed image above is of size 1201x401, obtained through the interpolation methods discussed above. Upon examination of this final pre-processed image, a few traits are immediately noticeable. First, the portions of the image that had no LIDAR data points corresponding to the actual pixel coordinate have a white intensity. Also, the overall intensity differences, ranging from light grey to a grayscale intensity that is close to black in color, indicate elevation differences of the LIDAR data coverage area. The lower intensities (dark grey / black) represent very low-lying areas relative to the rest of the coverage area, indicating that these points are most likely ground points. The higher intensities (lighter grey) indicate that the elevation is higher in those locations and could therefore correspond to buildings, but further detail will be necessary to make such a determination. This pre-processed image represents the starting point for all further analysis, as various image processing techniques will be employed in order to accomplish the task of identifying building footprints.
With the pre-processed image obtained, and the overall goal being to determine the locations of buildings and their boundaries, a method must be determined to perform such a separation of the building pixels from the rest of the image. However, one cannot simply separate the building objects from the rest of the image immediately. First, the pixels representing ground elevations must be separated from the pixels representing non-ground elevations. Non-ground elevations refer to any pixel whose intensity has a value greater than a predefined threshold for height, regardless of whether or not the point corresponds to a building, vegetation, or other object that is higher off the ground than this predefined threshold.
There are many methods that have been employed in order to accomplish this task of separating non-ground points from ground points. One such way was formulated by Vosselman, who identified non-ground points and separated them from ground points by analyzing the slope between a given pixel and the eight neighbors of the pixel. Vosselman determined the slope from a given pixel to each of its neighbors, and then found the overall maximum slope. If the maximum slope was less than a predefined threshold, then the point was labeled a ground point; otherwise, the point was considered a non-ground point.
The major pitfall with separating non-ground points from ground points arises in the derivation of a best-fit filtering threshold. Zhang [2] has identified the following two general problems with separating ground points from non-ground points: commission errors and omission errors. Commission errors refer to mistakenly identifying non-ground points as ground points, while omission errors refer to mistakenly including ground points as non-ground points. If one selects the best-fit threshold for the filtering method of choice, then these errors will be minimized, and the resulting separation will be as accurate as possible. Zhang proposed what is known as the progressive morphological filter to mathematically remove non-ground points from a processed grayscale image like the one in Figure 3 above.
A morphological filter is a filter that utilizes a combination of dilations and erosions in order to dilate and/or erode features of an image. When dealing with a grayscale image as in the preprocessed image of Figure 3 , these dilation and erosion operations actually become maximum and minimum operations. The progressive morphological filter combines the use of maximum filters and minimum filters in order to detect the non-ground elevations by effectively replacing all of these values with ground pixel values, removing the non-ground pixels from the image.
In order to implement progressive morphological filter, an initial window size w is chosen (3 in this case). A minimum filter of size w x w (3x3) was performed on the pre-processed image of Figure 3 . This minimum filter operates on the original image by sweeping the window over the entire image, and for each given pixel, replacing the pixel's value with the minimum pixel value included in the window neighborhood. This effectively removes any non-ground object smaller than the window size because it changes the appropriate pixel value to the minimum value in the region. This minimum filter is immediately followed by a maximum filter of the same window size, effectively repairing any damage done to non-ground objects that are larger than the window size by replacing the pixel value with the maximum value in the window area. This combination of a minimum filter following by a maximum filter is known as an opening operation of the morphological filter.
Once this initial opening operation is performed, the difference between the original image and the new image filtered using the opening operation is obtained. This image should therefore have pixel intensity values of zero for all ground pixels since the ground pixels were not removed in the opening operation performed. However, this image should have pixel values relating to the non-ground values in the original image, as the difference image was obtained by subtracting each intensity value of the filtered image from the original image. This difference image now becomes extremely important in the implementation of the progressive morphological filter as it relates directly the best-fit threshold described above. The height threshold was predefined to be 0.1 for this morphological operation. However, the best-fit threshold was determined to be dependent on the window size in the following way:
Therefore, the actual filtering operation occurs by comparing each difference image pixel value with this best-fit threshold value. If the given difference image pixel value is greater than this best-fit threshold value, then the corresponding original pixel value is replaced with the pixel value of the image obtained after the opening operation. If the given difference image pixel value is less than the best-fit threshold value, then the corresponding original pixel value remains the same. This process repeats every pixel of the image is traversed.
Upon initial viewing, one would think that the operation performed in equation (2) would simply set the best-fit threshold to a value of 0.3 since the window size above was chosen to be three. However, this is not the case, as this morphological operation is progressive in nature. The progressive nature of the morphological filter refers to the fact that the window size progresses to a higher value after each performance of this algorithm. A maximum window size is set by the user initially. A window step size is also set initially by the user, which indicates how much the window will increase after each traversal of the algorithm. In this particular case, a maximum window size of 40 and a window step size of 10 were chosen. The following table shows the progressive increasing nature of the window size for the morphological filtering operation. This table above shows that the initial window size was set by the user at a value of 3. The opening operation was performed with a window size of 3x3, and then the difference image values were compared to the best-fit threshold as described above. The window step size was then added to the initial window size to achieve a new window size of 13x13 for the opening operation. This process is repeated until the maximum window size of 40 was achieved. The best-fit threshold also progressively changes with respect to the window size according to equation (2) above. The grayscale image obtained after removal of all non-ground objects through the implementation of the progressive morphological filter is given in Figure 4 below. In comparison to the original pre-processed image in Figure 3 , one can see that the areas of higher intensity are now completely removed in Figure 4 . This proves that the progressive morphological filter performed well in separating ground pixel values from non-ground pixel values.
With the non-ground pixel values successfully separated from the ground pixel values and identified, the algorithm to identify building areas and their boundaries becomes the focus. The algorithm proposed to accomplish the task of separating building areas from other non-building non-ground objects is the region growing algorithm. The region growing process will iteratively separate the non-ground pixels into distinct regions of interest which will allow a successful building footprint extraction.
Region growing is an image processing concept that allows an image to be separated into areas (regions) that have a predefined characteristic. This process starts from a single pixel that satisfies a given condition, effectively becoming the first pixel of the region. Then, the eight neighbors of the pixel are examined against the same condition that included the first pixel in the region. If any of these pixels also satisfy the criteria given for acceptance, these pixels are also considered part of the region and indicated as such. Then, the eight neighbors of each of these added pixels are examined, with the process continuing in this way until no new pixels are added to the region. Figure 5 below shows the beginning of the region growing process, the choice of a seed point that meets the specific condition under testing. The seed pixel is the first pixel in the scanned image containing a "true" output to the testing condition. The region will then grow outward from this pixel by testing each of the pixels eight neighbors individually, adding said pixels to the region if the condition is satisfied. Figure 6 below illustrates the region growing process after multiple iterations of the condition check. As can easily be seen, the highlighted area in the above figure indicates that these particular pixels satisfied the condition for inclusion and therefore are now members of the region. This process continues until no new neighboring pixels can be added to the region, creating an effective boundary for the region in question.
With the basics of the region growing process now explained, the detailed region-growing algorithm applied to the specific Lenoir County area of North Carolina is performed. The progressive morphological filter has effectively separated the ground and non-ground pixels by creating an output image that has removed all non-ground pixels and replaced them with average ground pixel values.
With this initial separation complete, the non-ground pixels must be further segmented into consolidated regions of interest, as each non-ground pixel should belong to an area of nonground pixels, representing anything from a car to a building to an area of vegetation. Zhang [2] referred to two different types of non-ground pixels as inside points and boundary points. An inside point is a non-ground pixel whose eight neighbors are also non-ground pixels. A boundary point is defined as a non-ground pixel with at least one of its eight neighbors being a ground pixel. Therefore, all non-ground pixels can be segmented further into two distinct subsets of pixels, classified based on whether the pixel is an inside point or a boundary point. This revelation allows the first region growing segmentation to take place. The desired outcome is a set of regions of inside points, along with a set of regions of boundary points corresponding to these inside point regions.
The region growing process for segmenting regions of inside points is the first task after the progressive morphological filtering operation is performed. First, a testing condition must be established which will allow for a new region to begin from an initial seed point with properties that satisfy the condition. A height (pixel intensity value) threshold is chosen as this initial condition, given a value of 10, and this threshold will be the first testing condition upon which a region is initiated. This threshold is variable, allowing the algorithm to run with any threshold with value greater than zero desired. The image to be operated on is the difference image, which now contains intensity values near zero for ground pixels and intensity values equal to the height of non-ground objects for all pixels that are not considered part of the ground area. The difference image is scanned, starting from the upper left corner, until a pixel with intensity value greater than the threshold is found. This initial conditioning does not identify the point as an inside point, so the pixel cannot yet be added to a region.
Once a pixel satisfying the initial threshold condition is found, further conditioning is performed to determine whether or not this pixel is an inside point. Of note is the fact that each pixel will be identified as "belonging to a region" or "not belonging to a region" by a labeling scheme within the algorithm itself. Initially, all pixels will contain a label of "not belonging to a region."
The second condition tests whether the pixel belongs to a region already by scanning the pixel's label for this information, represented in the algorithm as a Boolean '1' (belongs to a region) or '0' (does not belong to any region). If the pixel does not belong to any region as of yet, the second condition is satisfied and further conditioning is performed. The third and final condition does not test the pixel in question itself, but the pixel's eight neighbors. The third condition checks whether all of the eight neighbors satisfy the initial condition of having an intensity value greater than the threshold. This is accomplished by performing an AND operation of each neighbor condition, resulting in a satisfied condition only when all pixels are greater than the threshold. If all of the eight neighboring pixels have a value greater than this threshold, then the pixel in question is indeed an inside point. Therefore, the pixel is added to the region as the seed point and labeled as described above as "belonging to a region" to avoid placing the same pixel in multiple regions or in the same region more than once. The pixel is added to the region in the algorithm itself by utilizing linked lists, with each region being identified by its own linked list. This allows for each region to have varying size along with the possibility of adding pixels at a random rate based on condition testing. If the above process fails to produce a pixel that satisfies the conditions of an inside point, the difference image scan continues until the first inside point is found, making it the initial seed point for the region.
With the seed point for the inside point region determined, the region must then be grown. Since a seed inside point was found, the scanning of the difference image stops. The location of the scan is saved so that when the region is completely grown, the scan can begin again from the same location, looking for another region to grow. As shown in Figure 5 above, the region growing process grows outward from the seed point, so the conditioning moves on to the eight neighbors of the seed point.
First, the upper-left neighbor of the seed point is identified for testing. Since the seed point itself was labeled as an inside point, by definition this already indicates that this upper-left neighbor passes the initial threshold test. Therefore, the upper-left neighbor is tested based on whether or not it already belongs to a region. If it already belongs to a region, then the testing of this pixel fails, and the upper neighbor of the seed point is identified for testing. However, if the upper-left neighbor is not already included in a region, then further testing is performed to determine whether this neighbor is also an inside point of the region in question. This further testing involves the eight neighbors of this upper-left neighbor. If all of the eight neighbors of the upper-left neighbor have intensity values greater than the threshold, then the upper-left neighbor is considered an inside point, labeled, and added to the region. This process continues, with each neighbor of the original seed point being checked for its possibility of belonging to the region.
Once all of the eight neighbors of the original seed point are tested, the linked list which contains the pixels in the region will now have a maximum of nine pixel members. The region growing implementation does not stop here, however, as some regions may have more than nine total pixels in the region. Assuming all of the eight neighbors of the original seed point have been added to the region, the region contains nine pixel members, and the next available pixels to undergo testing for inclusion in the region are the neighbors of these eight neighbors. Therefore, the linked list's link is incremented in order to get to the second pixel that was added to the region. Then, the algorithm tests all of its eight neighbors in the same way as the original seed point was tested, and if any of its neighbors meet the conditions for inclusion, these pixels are added to the list. This process continues until the end of the linked list is reached, successfully exhausting all possible pixel values to be included in the region, effectively creating a region of labeled pixels with a defined boundary.
With the end of the region determined, the difference image scan continues from the same pixel in which it left off, checking for the next seed point for a new region to begin. The image is scanned until the next pixel with intensity greater than the threshold and with property of not already belonging to a region is found. If a pixel value is found to successfully meet this condition, another region is grown as described above. This region growing process for inside points continues until the entire difference image is scanned, effectively segmenting the nonground pixels into regions of inside points with well defined boundaries.
With the inside points completely separated into regions, the determination of boundary points becomes the important priority. These inside point regions are incomplete in the sense that along their boundaries, there are still non-ground pixels that are not members of the region. This occurs due to the definition of an inside point, which states that all eight of its neighbors must be non-ground points as well. With this in mind, what if one or more of these eight neighbors already defined as non-ground points have neighbors that are ground points? These pixels are also part of the non-ground region of interest, just not part of the specific inside point regions. These pixels are the boundary points for the inside point regions, and they need to be separated as such in order to identify every non-ground point as either an inside point or a boundary point.
In order to determine these boundary point regions, a new scan of the difference image is performed after each inside point region is determined. The initial condition for inclusion in a boundary point region is threefold: the pixel must not already be included in an inside point region, the pixel must have an intensity value greater than the threshold, and the pixel must have at least one of its neighbors be an inside point. Once the seed point for the boundary point region is found, further conditioning is performed similar to the conditioning described above for determining inclusion in the inside point regions. If at least one of the eight neighbors of this new seed point is a ground point, and at least one of the eight neighbors of this new seed point is labeled as belonging to an inside point region, then the pixel is labeled as belonging to a boundary point region and added to the region as the initial seed point. The key element to this conditioning allowing the boundary point regions to be separated according to their relevance to other inside point regions is the fact that at least one of the eight neighbors already belongs to a ground point region. Otherwise, without this condition there would be no way to separate each boundary point region, resulting in one large region of boundary points. However, as will be described in the post-processing stages of the building footprint algorithm, the boundary point regions must be separated here as such.
The following table shows the pseudo code for the region growing separation algorithm, resulting in distinct regions of inside points and boundary points. Once this algorithm is performed, the non-ground pixels are successfully separated into regions of inside points and regions of boundary points. Figure 7 below shows the inside point regions, identified by the pixels that are shaded white in the image. In a direct comparison with Figure 3 , Figure 7 illustrates the algorithms ability to identify nonground pixels effectively. Moreover, it also illustrates the ability of the algorithm to identify regions of interest based upon a defined threshold that can be preset by the user. This allows for a height threshold specification for inclusion, allowing for the elimination of other non-ground objects that are not necessarily buildings, objects such as cars, shrubs, and other vegetation. The result of the figure above is an illustration of the automated determination of building ground plans and outlines found using the region growing algorithm for the segmentation of boundary points. The figure clearly defines non-ground regions, including those of smaller building areas. It can be noticed in this image as well that the effects of small distinct nonground pixels are not included in the region identification, having successfully been removed as not belonging to a building region. Further comparison of these two figures with the original pre-processed image will be performed after a discussion of a more advanced region growing algorithm implementation next.
The above algorithm successfully resolved the non-ground pixels into two distinct types, boundary points and inside points. The region growing process allowed these two distinct types of pixels to be represented as grown region areas with clearly defined boundaries. However, in order to reduce inclusion errors present in any region growing process, further segmentation of the inside point regions is necessary. The process involved in this further segmentation employs the use of a plane-fitting technique to separate inside point regions into smaller regions representing building surfaces. The overall goal is to segment each inside point region into smaller regions based on the formation of best-fit planes of pixels and their neighbors, resulting in a representation of planar surfaces within each inside point region.
The plane-fitting process used is a least-squares method of determining a best-fit plane from a given number of three-dimensional points. The algorithm determines a plane that best fits nine distinct three-dimensional points, in this case an inside point pixel and its eight neighboring pixels. The solution provides the least squares solution to the following equation:
with I(x,y) representing the intensity of the pixel located at coordinate (x,y) and A,B,C representing parameters of the best-fit plane determined from the nine pixels in question. The goal of this least squares solution is to minimize the sum of the squared errors between the pixel intensity values and the resulting intensity value when using the plane equation (3).
Given a set of pixels
under consideration, a function is defined as follows: . This condition results in the following system of three simultaneous equations which can be solved for the bestfit plane of the given nine pixels:
Equation (5) The above matrix equation is solved for the parameters A, B, and C respectively which defines the best-fit plane of the nine pixels in question. With these parameters determined, a plane-fitted intensity value can be found for any of the pixels in question. This plane-fitted intensity value can then be used in comparison with the actual intensity value observed at the pixel in question in order to create a condition for a region growing procedure that will allow segmentation into surface regions.
In order for this condition to be used in the region growing process, an initial seed point must be determined. Therefore, an additional detail is added to the region growing processes above -the calculation of the ABC parameters for each inside point pixel determined in the inside point segmentation process. Therefore, when a pixel meets all criteria for inclusion in an inside point region, a best-fit plane for the pixel and its eight neighbors is calculated. Along with these ABC parameters for the plane equation, the minimum sum of squared errors (SSE) is also calculated for each inside point according to the following equation:
with x i and y i representing the coordinate of the pixel in question and I actual (x i ,y i ) representing the actual intensity value at the pixel in question. This equation represents the sum of squared error calculation, determined by the square of the difference between the calculated best-fit intensity value and the actual pixel intensity value at the specific coordinate being analyzed.
This SSE calculation allows an initial seed point to be determined for the surface growing algorithm. Once an inside point region is completely grown, all best-fit plane parameters and the corresponding SSE for any inside point and its eight neighbors has been determined using the matrix equation (6). The linked list that houses the inside point region pixels and this best-fit plane data is now sorted in ascending order according to the SSE, effectively determining the minimum SSE value for the inside point region in question. The inside point with the minimum SSE determined through the plane-fitting algorithm is added to the surface region. The reason for this minimum SSE being the condition for initial inclusion in the surface algorithm is that the inside point with the smallest SSE value has the most accurate best-fit plane associated with it and its eight neighbors. This allows a fairly accurate plane representation, and therefore the best chance for a smooth surface to be determined from pixels in a particular area, for example a face on a roof of a building. The goal is to grow surfaces in this particular algorithm, so the condition must allow for the best possible surfaces to be grown, which is accomplished by selecting the inside point with this minimum SSE value.
With the initial seed point selected, the conditional testing proceeds to the eight neighbors of this seed point. The testing first checks if the neighbor is an inside point, using the same conditions described above in the general region growing algorithm. If the neighbor is an inside point, then additional surface testing is performed. Using the ABC parameters of the initial seed point and the specific x and y coordinates of the neighbor in question, a calculated intensity value is determined using the plane equation (3). This calculated intensity value is then compared to the actual intensity value at this point, and the absolute value of the difference between the two values is calculated. If this difference is greater than a specific Δh threshold (usually 15-30 cm according to Zhang [1] ), then the pixel in question has an intensity that is not within the desired range and therefore is not a close enough fit to be included in the surface. However, if the calculated height difference is less than this Δh threshold, then the pixel is a close enough fit to the growing surface and is included in the surface region.
This testing continues until all eight neighbors have been tested for inclusion in this particular surface region. Once the eight neighbors have been completely checked, the pointer to the linked list containing the surface region values is incremented, and further testing is performed on the eight neighbors of this point. This process continues until the surface region has been fully traversed and no further growing can be performed, effectively ending the surface in question.
However, testing does not stop here. Every point within the inside point region must be added to a surface, and thus far, only the initial surface determined from the minimum SSE inside point has been accounted for. Therefore, the linked list pointer to the inside point region is incremented, finding the second smallest SSE value. If this point does not already belong to a surface, then this inside point becomes the new initial seed point, and surface growing begins again in the same fashion. This process continues until the end of the inside point region is reached, completing the traversal of the entire inside point region.
The fact that every inside point must belong to a surface results in surfaces of various sizes. If, for instance, an inside point on the outer edge of an inside point region becomes the seed point, the SSE may be the largest without regard to the plane-fitting algorithm's calculations on the "smoothness" of a group of pixels, and the surface grown would be extremely small because the surrounding pixels may not lie within the defined height threshold, possibly resulting in a surface of only one pixel if no other points fit the plane for that seed point. The process is rather complicated in this sense but works as desired for breaking up the larger inside point regions into sub-regions with conditions that define well defined surfaces. This further segmentation results in many more surfaces than inside point regions, separated by their relative fit to the region in general.
For the test area in Lenoir County, North Carolina shown in figure 3 , there were 218 inside point regions found and 1655 surfaces determined from these points. This much greater number of surfaces shows that the conditioning performed in this surface growing algorithm vigorously separate the inside point regions, resulting in a much finer conditioning and therefore datasets that share very similar properties which can be used to refine the building footprint algorithm drastically.
With the non-ground points now completely segmented into surfaces, the next step is simply an adjustment phase, hoping to help eliminate any errors that may have become prevalent in the previous steps. Because the segmentation above relies on thresholds in order to separate pixels and determine building footprints from these separated pixels, the quality of the building footprints and the amount of errors present in them are all determined by how closely the thresholds chosen are to the optimum threshold. After much testing, the values for the thresholds were chosen as indicated above, with a summary of the threshold values given in Table 3 below. All of the above threshold values have been explained in the sections above relative to their use in the algorithms they are associated with. However, there is another threshold mentioned here that has not been discussed yet, which relates to the merging of building surfaces which allows the final building footprints to be determined. In order to eliminate small patches of vegetation, a minimum building area must be determined. This minimum area threshold is set to 60 square meters, encompassing an area of three pixels on average, so any inside point regions having less than three pixels are neglected in the building separation process.
The building footprints for the particular area of Lenoir County, North Carolina considered thus far have been determined using the region growing algorithm. Inside points and boundary points were identified, and the resulting images have been displayed. However, the building footprints that have been identified are noisy since LIDAR data is inherently irregularly spaced. Therefore, some simplification of these noisy footprints must be performed. In the boundary point figure above, the edges of identified buildings have a zigzag quality and need to be simplified into a line that represents the true building boundary. Many simplification algorithms exist, but the Douglas-Peucker algorithm was implemented to simplify the noisy building footprints.
One such technique to develop a cleaner building footprint is the Douglas-Peucker algorithm, an algorithm developed by cartographers D.H. Douglas and T.K. Peucker in [7] . The DouglasPeucker algorithm allows for a simplification of any polyline by a recursive threshold technique.
The Douglas-Peucker algorithm operates on the boundary point regions identified by the conditioning described above. An initial simplification guess begins the process. For each boundary point region, the initial guess contains the first boundary pixel in the region and the last boundary pixel in the region connected together to form a line segment. The algorithm then determines whether the initial guess of a line segment is a good enough simplification for the polyline in question. This determination is performed based on a user-defined distance threshold.
Assuming the boundary point regions represent buildings, a line segment should never be a good enough simplification in this case. Therefore, further processing is necessary. The entire boundary point region is scanned, and the distance from each boundary pixel to the initial guess line segment is calculated. If the boundary point in question is within the outer limits of the initial guess line segment, a perpendicular distance is calculated. If the boundary point in question is on either side of the outer limits of the initial guess line segment, then the distance from either the left endpoint of the initial guess (the boundary point is on the left side of the initial guess line segment) or the right endpoint of the initial guess (the boundary point is on the right side of the initial guess line segment) is calculated. This distance calculation is measured in pixel units, with each side of a pixel considered to have unit length.
Once all boundary point pixels in the region have had distances from the initial guess line segment calculated, the boundary point pixel at which the maximum distance from the initial guess line segment is identified. This maximum distance value is compared to the distance threshold provided by the user, and if the maximum distance is less than this threshold value, then the initial guess line segment was an accurate enough simplification, and the new boundary point region contains only two points, the endpoints of the initial guess line segment. However, if the maximum distance is greater than the threshold value, then the initial guess line segment was not accurate enough, and further processing must be performed.
Assuming the initial guess line segment was not accurate enough, a new guess is determined. The pixel at which the distance threshold was exceeded becomes one of the endpoints of two new line segments representing the new guess. The first point of the boundary point region is connected to this maximum distance pixel to form a line segment, and this maximum distance pixel is connected to the last of the boundary point region to form another line segment. Each line segment is now assumed to be a new initial guess, and the process continues recursively until all boundary point calculated distances are less than the user-defined threshold. The figure below illustrates the process described above. The boundary points that remained after the Douglas-Peucker algorithm was performed are colored white above. The building footprint simplifications are obtained by connecting each consecutive Douglas boundary point together as line segments. While most buildings are identified fairly well, some of the smaller buildings were unsuccessfully simplified. This problem is due to the order that the boundary point regions were identified in. This problem is considered and eliminated in the next chapter containing improvements to the above algorithms.
With the algorithms completed and the building footprints successfully extracted, ways to possibly improve results were considered. It was noted that the possible improvements could exist when the boundary point regions were initially identified. The resulting footprints were extremely noisy, and therefore simplification seemed to be a difficult process. Therefore, a new way of defining boundary points was considered, and a new method of growing the boundary point regions was developed.
Boundary points were initially identified as any pixel in the difference image which had a value greater than a height threshold that also had at least one neighboring pixel identified as a ground pixel. This definition is somewhat incomplete, considering that any boundary point should also have a neighboring pixel identified as an inside point. Therefore, the definition of what constitutes a boundary point was made more strict, requiring the intensity of the difference image to be greater than the given height threshold, at least one neighboring pixel identified as a ground pixel, and at least one neighboring pixel identified as an inside point. It was believed that this extra condition would obviously result in fewer pixels to pass the boundary pixel test, and therefore result in less overall boundary pixels and a better overall simplification of the building footprints and a less noisy one as well. The figure below shows the identification of boundary footprints with this new definition. The above image with boundary pixels colored white shows a definite improvement in the reduction of noisy edges from the original boundary point figure identified in Chapter 3. A visual comparison between specific buildings in the two images will be given in the next chapter.
While there is a noticeable improvement in the identification of boundary points via the new definition, the footprints are still noisy enough to require further processing improvements. The Douglas-Peucker algorithm result illustrated above was not an acceptable simplification in all cases. It was determined that the Douglas-Peucker algorithm is extremely order intensive, in the sense that the order that the boundary points were placed into regions was extremely important in the determining a successful resulting simplification.
Boundary points were initially sorted through by scanning the difference image from the left portion of the image to the right part of the image in a top-down manner. The entire difference image was scanned for each boundary point region, a fairly inefficient process. Also, because of the left-to-right, top-down identification of the boundary point regions, the order of the boundary points could become scattered and not representative of buildings. Initially, this did not seem to be causing a problem, but once the Douglas-Peucker algorithm was implemented on these unordered boundary point regions, the result was clear. Because the Douglas-Peucker algorithm only checks distances of points to the initial guess on points that exist between the endpoints of the initial guess, ordering is extremely important. The Douglas-Peucker algorithm approximates the polyline obtained by connecting every boundary point in the order that they are added to the region.
Therefore, the goal was set to develop a sorting algorithm that would order boundary points in a way that, if each boundary point in the region were connected to the adjacent boundary points in the region, the result would be exactly as shown in the boundary point region figure above, that is they would represent the buildings that are to be simplified. The sorting was performed by using a variation on the region growing algorithm. First, the definition condition of a boundary point must be satisfied. Once a pixel satisfied this condition, the pixel was labeled as a boundary point and added to the region.
The next step is key to the development of a successful sort. The region growing algorithm described above would check every neighbor and add any pixels that satisfied the condition to the region. However, this could in some cases add more than neighbor to the region if more than one neighbor satisfied the condition. This would cause a problem in the ordering, as it is desired for the sort to represent a point-to-point connection of the boundaries of buildings. If more than one neighbor were added to the boundary point region for satisfying the boundary condition, then the corners of buildings would not contain a point-to-point connection. Therefore, once a seed point for the boundary point region was determined, the region growing algorithm begins. However, if two or more neighbors satisfy the boundary condition, only the first point is added to the list, and the region growing process continues in whatever direction this point was from the seed point. All boundary points obtained via the new definition by the original left-to-right, topdown scan are contained in this new region growing scan.
Once the sorting was complete and all boundary points were identified, the Douglas-Peucker algorithm was run on this new boundary point data. The image below shows the result of the Douglas-Peucker algorithm with this new modified region growing algorithm sort. The resulting image shows that the Douglas-Peucker algorithm with this modified sorting mechanism improves upon the previous top-down sorting run. A comparison of particular problem areas will be shown in the next chapter.
The resulting images above show that the algorithms for identifying building footprints successfully identify buildings. However, it was desired that there be data available to use in the future for possible comparison purposes, that is the ability to compare the obtained LIDAR building footprints with some other form of data to determine how accurate the LIDAR building footprints were.
In [1] Zhang performs a visual comparison to aerial photos by overlaying the obtained footprints over the aerial image to determine how accurate the LIDAR building footprints were. Contrary to Zhang, this particular satellite comparison algorithm attempts to provide a tool to compare the obtained LIDAR footprints to satellite imagery automatically.
The following image shown in Figure 13 was obtained by performing a screen capture of a satellite image of Lenoir County using Yahoo Maps. The image was resized to match the LIDAR data image as closely as possible (the LIDAR image has size 1201x401 and the satellite image has size 1203x420). The goal is to perform a region growing algorithm on the above satellite image. The condition to include a point in a particular region is the Euclidean distance between the associated RGB color intensities. This color region growing algorithm will provide regions that have similar color within a user-defined color threshold. The area of each region along with the average values of the x and y coordinates of the pixels in each region are calculated. The areas of the inside point and boundary point regions along with their average x and y coordinates were calculated with the region growing approach above. The claim is that with this data, buildings can be separated from vegetation based on color characteristics, which could prove helpful in removing false building identifications that could occur in dense forest areas. Also, if some areas and averages obtained from the inside/boundary region growing algorithm are similar to the areas and averages obtained from this color region growing algorithm, this algorithm could provide confirmation that the LIDAR building footprints were obtained accurately without visual inspection.
Because all points must be grown into corresponding color regions, there is no initial condition for selection of a seed point. The seed point is selected on the condition that it has not already been placed in a region, and it is then added to a region, and its color is identified according to its red, green, and blue components. Then, a condition is placed on its eight neighbors according to the following equation: 
A user-defined color threshold is provided. If the above calculated distance is less than the color threshold, then the pixel is considered close enough in color to the seed point and added to the region. This process continues until all 8 neighbors of the original seed point have been tested. Once this process is complete, the neighbors of the neighbors that were just added to the color region are tested based on their distance away from the seed point color. This process continues until all neighbors fail the color threshold test, effectively ending the region. Then, another region is started by selecting a point that has not yet been included in a region, and the process continues until every point has been assigned to a region.
The threshold provided to the algorithm for testing was based upon the maximum value of D color . The satellite image was normalized so that the minimum possible color value was 0 and the maximum possible color value was 1. Therefore, the maximum color difference would result in a D color value of approximately 1.7. The user-defined threshold therefore is limited to a value between 0 and 1.7 for this particular algorithm. The algorithm successfully provides the area of each region along with x coordinate and y coordinate averages for each region. This data is available for comparison purposes to the LIDAR obtained building regions.

In Chapter 4, the results of changing the original approach to the region growing algorithm were presented, and it was shown that there was visual confirmation of improvement in the identification of building footprints. However, a side-by-side comparison will further show the effect the above changes have had on the building footprint results.
The figure below shows an enlarged section of the Lenoir County study area, comparing the old boundary point definition to the new boundary point definition. This building footprint comparison clearly shows that redefining what a boundary point really is results in a less noisy and simplified footprint that more accurately represents the actual building boundaries. The extreme jagged nature of the footprint on the left is replaced by a fairly smooth representation on the right, considering that LIDAR data is irregularly spaced and therefore noisy to begin with.
Yet another comparison figure below shows a problematic area in the original Douglas-Peucker algorithm and its improved result. 
The goal of extracting building footprints from a LIDAR dataset was completed successfully.
Redefining what constitutes a building boundary point helped to improve the Douglas-Peucker algorithm's identification of building corners and improved the simplification process. The additional ability to grow regions based on the RGB color attributes in a satellite image allows for confirmation or disconfirmation of building footprints. This ability to automatically prove or disprove the validity of the extraction of building footprints improves upon simple visual inspection of aerial or satellite imagery for such purposes.
Even though the algorithms above successfully identify building footprints and provide a means for comparison with satellite imagery, future work is available. Actual comparisons could be performed by sorting the color regions and inside/boundary regions according to areas and attempting to find similarities between them, confirming building locations and possibly removing dense forestry areas misidentified as buildings. Also, with building footprints successfully identified, a next step would be to perform some three-dimensional modeling using the building footprints as a guide to the type of building model and the locations of these models.
Thanks to a combination of experimental assays and computational studies, knowledge about protein function has been steadily accumulating in public databases, where it is commonly described through the Gene Ontology 1 (GO). On the one hand, hypothesis-driven research has traditionally led to the thorough characterization of one or few proteins at a time. On the other hand, high-throughput technologies have opened the way to very large-scale exploratory surveys to study biological processes, identify binding partners, or establish subcellular locations. Meanwhile, some homology-based approaches for annotation transfers have developed enough to produce fairly confident results. The GO consortium, for instance, makes wide use of a semi-automated tool for phylogenetic analysis and functional inference 2 , and of mappings between protein domain families to GO terms that are valid for all their members 3 . Despite these multi-pronged efforts, however, a substantial fraction of deposited sequences still have no functional annotation at all, and the remaining ones usually lack assignments for at least one GO domain. When available, this information may not be at the finest level of detail possible, not only because of the way some electronically inferred annotations are generated, but also because of the varying levels of resolution characterizing experimental results 4, 5 . Finally, nature can still spring surprises: protein moonlighting demonstrates that novel functions can still await discovery even for well-researched proteins 6 . One way to fill in some of these gaps employs machine learning to examine diverse biological data types separately or in combination, and to provide functional hypotheses that complement homology-based annotation transfers [7] [8] [9] . In particular, over the years several supervised methods have been devised for function prediction from amino acid sequences, which are easier to collect than structural data or genome-wide measurements of gene expression or protein-protein interactions. GOStruct 10 and FANN-GO 11 , for instance, make GO term assignments by analysing the patterns of BLAST 12 E-values to experimentally characterized proteins using structured Support Vector Machines (SVM) and multioutput neural networks, respectively. Given the computational complexity of training classifiers with multiple correlated outputs, it is difficult to learn the relationship between the input features and the whole GO; the proponents have therefore adopted workarounds such as reducing the number of output terms and ensemble modelling. Rather than tackling this complex structured learning problem, other researchers have tested with success the possibility of converting it into a set of simpler binary classification tasks. This approach has recently allowed our group to train GO term-specific neural networks from features describing the results of profile-profile comparisons 13 . Alignment-derived features, such as similarity scores, sequence coverage and E-values, can help learn which sequence similarity patterns correlate with the conservation of individual annotations, thus allowing more effective control on homology-based annotation transfers. Complementary efforts have investigated the usefulness of biophysical attributes to make homology-free inferences, under the assumption that proteins with similar functions would have similar biological features despite the lack of significant sequence similarities. For example, the occurrence of signal peptides gives useful hints about protein subcellular location, and also limits the number of their molecular functions and of the biological processes they partake. The idea was first implemented in ProtFun, which is based on neural networks trained for the functional classification of protein sequences from similarities in amino acid composition, and content of signal peptides, trans-membrane helices, post-translationally modified residues as well as other biological features 14, 15 . The observation that the length and position of intrinsically disordered protein regions strongly correlates with some molecular activities and biological processes led to an expanded set of sequence-derived features, which FFPred scans through a library of GO term-specific SVMs to annotate protein chains 16, 17 . A more recent study has confirmed the effectiveness of this feature-based approach with the use of random forests for supervised learning 18 . In this paper, we describe the latest FFPred release, which updates the previous one with an extended vocabulary spanning all three GO domains, reflecting the increasing attention in cellular component annotations, as evidenced from recent experiments in the Critical Assessment of Functional Annotation initiative. We evaluate FFPred 3 prediction accuracy using two complementary approaches and describe its improvements over the previous version. Finally, we show how its predictions can help get a glimpse into the effects of alternative splicing on human protein function. The results show patterns of functional conservation and variation consistent with the presence or absence of particular biophysical attributes and with general biological knowledge.
Summary of tool updates. Thanks to the continued growth of annotation databases, the latest FFPred release features a GO term vocabulary, which spans all three GO domains for the first time and is almost twice the size of that in the previous update. Supplementary Data file 1 lists the 868 GO terms, for which a dedicated SVM is available along with the classification accuracy estimated from the validation experiments following the training procedures. The new release makes still use of SVMs, which are known to successfully handle imbalanced classification tasks-typical in computational biology-where it is extremely important to allow for error control and avoid overfitting to known observations. Subcellular localization prediction has been the focus of many previous studies, which mostly focused on the well-known compartments of eukaryotic cells-such as nucleus, cytosol, endoplasmic reticulum, Golgi apparatus, mitochondrion and other organelles. The newly added cellular component terms in FFPred 3 also include some of the numerous macromolecular complexes found in them. The extensions to the other two sub-ontologies provide more specific descriptions for functional categories previously covered, and they reflect the increasing body of knowledge in areas such as organelle localization, immune system and reproductive processes, response to stimuli and chromosome segregation. A small fraction of molecular function and biological process terms have been removed (Fig. 1a,b) , because they no longer occur in curated databases-mostly after the GO consortium made them obsolete. The majority of functional categories that have been retained can be predicted with negligible changes in expected accuracy-though some exceptions exist. As a consequence of the extended knowledge about human protein function since the last update, the patterns of biophysical attrbutes linked to terms such as sulfur compound metabolic process (GO:0006790), neurotrophin TRK receptor signaling pathway (GO:0048011), growth factor activity (GO:0008083) and protein kinase binding (GO:0019901) can be more easily identified and modelled. For other functions, such as calcium ion transport (GO:0006816), single organismal cell-cell adhesion (GO:0016337), ATPase activity (GO:0016887), and nuclease activity (GO:0004518), SVM performance has dropped, suggesting that their relationships to sequence-derived features are more complex than previously appreciated (Fig. 1c,d) .
The tool is designed with a focus on the function of human proteins, and so annotations curated for other organisms are never used for training. To learn effectively the relationship between biophysical attributes and GO terms, sufficiently large numbers of positive instances are needed, thus limiting the specificity of the functional categories that can be currently predicted. While this feature may not be desirable for all applications, its benefits to overcome some well-known limitations of homology-based annotation transfers have already been reported 15, 17 . Interestingly, previous work showed that the tool can also help annotate protein function for other eukaryotic organisms. The updated tool is publicly available on the web at http://bioinf.cs.ucl.ac.uk/ffpred.
Performance evaluation. The accuracy estimates in Supplementary Data file 1 are GO term-specific and point out the usefulness of FFPred 3 to prioritize human genes for downstream experimental screening when homology offers little or no help. To complement this analysis and gauge how well protein function as a whole can be predicted for such difficult cases, a timed experiment similar to the Critical Assessment of Functional Annotation challenge was conducted, by training a separate SVM library using the public databases released in November 2013. The resulting 597 classifiers were then used to assign GO terms to human proteins with no experimentally verified biological roles at that time, and their accuracy was finally measured against the UniProtKB-GOA data as of March 2016. For comparison purposes under difficult working conditions with limited or completely missing homology information, additional predictions were generated by a baseline method (Naïve), which ranks GO terms by prevalence in UniProtKB-GOA, and by a sequence similarity-based approach (BLAST), which can transfer annotations only from distantly related and experimentally characterized proteins as detailed in Methods. Other machine-learning based tools for GO term prediction from patterns of biological features could not be included in the study: ProtFun 15 has not been updated in a very long time and only covers a handful of currently valid GO terms, whereas ProFET 18 requires training from scratch classifiers for all GO categories of interest.
The precision-recall plots in Fig. 2 and the data in Table 1 provide graphical and numerical reports on the evaluation results for the three separate GO domains, according to standard practice in the field. At high levels of recall (i.e. above roughly 40% for molecular function and 20% for the other two sub-ontologies), FFPred 3 predictions achieve higher precision values than the baseline approaches do, and the maximum F-scores in Table 1 clearly back up this observation. However, the highest scoring predictions made by BLAST for subcellular locations and by Naïve for all sub-ontologies attain higher precision than the corresponding ones by FFPred 3. This result surprisingly suggests that these less sophisticated approaches are more useful than FFPred 3, when only a handful of assays can be run on each protein. Or are they?
It is widely accepted that an obvious pitfall of precision-recall analysis is the total disregard of how informative predictions are. The most confident GO term assignments made by Naïve for each test protein-GO:0043226 (binding), GO:0005488 (organelle) and GO:0009987 (cellular process)-are far from useful in cutting down the options for the design of experiments, indeed. Nonetheless, their very shallow nature guarantees that they will be eventually confirmed for most, if not all, proteins. Furthermore, comparing the precision values achieved by different methods and plotted against the same level of recall could be more ambiguous than it looks at first sight. If the recall is less than 1.0, the predictors are evaluated on non-identical sets of target proteins, which can even be disjoint. Another confounding aspect is the number of GO term predictions above a given decision threshold made for individual proteins: predictors based on high-throughput functional data aim at high recall and generally produce longer lists of assignments than those generated by methods based on homology transfers, which tend to achieve higher precision. Finally, correctly assigning the term t to distinct proteins p and q can pose prediction challenges of diverse nature, depending on how many proteins are annotated with t, and on how closely p and q follow the patterns of features used to build the classifiers-e.g. sequence similarity, domain architecture, biological attributes, gene expression and so on. Therefore, it is useful to look at method performance from a different angle, by considering both the accuracy and the informativeness of equal numbers of high scoring predictions for each target and sub-ontology-thus reducing the above biases and yielding results that can be interpreted more clearly and more easily by non-specialists, too.
The top row panels in Fig. 3 summarize prediction quality in terms of F 1 measure and the underlying precision and recall values are plotted in Figure S1 . It is quite clear that FFPred 3 is superior to both Naïve and BLAST across all three GO domains, because it achieves higher recall than the other predictors do, in combination with intermediate values of precision. The data also clearly confirm the expectation that Naïve predictions Table 1 . Performance comparison between FFPred 3 and the baseline prediction methods. For each method, the table reports the total numbers of true positives (TP), false positives (FP) and false negatives (FN) each method achieves at the decision threshold that maximises the F 1 score for each GO domain. NP is the number of proteins with at least one prediction with a confidence score greater than or equal to the corresponding threshold value, which is used to calculate the average precision of each method according to equation (4) in the main text. The average recall is calculated using equation (5) using the number of proteins with annotations in the GO domain under consideration, which can be found in the section "Methods". The latter two values are used to locate the full triangles in the precision-recall space shown in Fig. 2 . generally are highly precise, but not deep enough in the GO graph to outperform the other approaches in terms of recall. The results for the CC sub-ontology are an interesting exception: the low numbers of false negatives most likely arise from the relatively shorter distances between nodes associated with experimental annotations and nodes associated with the most frequent terms in UniProtKB-GOA. The plots also clearly illustrate the limits of homology-based transfers in such challenging situations. When the evolutionary distances from previously annotated proteins are large, only the most general functional aspects are retained (e.g. catalytic or transporter activity), while the finer details diverge (e.g. the nature of the substrates and the chemistry of the reactions), thus resulting in high numbers of both false positives and false negatives, and ultimately affecting negatively precision, recall and F-measure values.
As mentioned above, the design and implementation of FFPred 3 produced a list of GO terms with varying levels of detail, so it could be questioned how informative its predictions are and how helpful they can be to experimenters. In Fig. 3 , the plots in the bottom row show the average amount of useful information the highest scoring predictions would actually provide. For this purpose, the analysis only considers true positive predictions, which are not regarded as equally valuable as in the standard precision-recall analysis, however. They are rather weighted according to their information content, which estimates their specificity and informativeness from their occurrence in the UniProtKB/SwissProt database -so that more frequent functional categories are down-weighted, and vice versa. The plots undoubtedly prove that FFPred 3 correct predictions are consistently more specific than those generated by BLAST, which in turn are more specific than those made by Naïve. Therefore, despite the relatively low levels of term specificity, FFPred 3 can give useful hints to drive the experimental characterization of proteins, when routes alternative to homology transfers are needed. Table S1 gives some clear examples of how well FFPred 3 top-ranked predictions compare with the validated GO term assignments, which some proteins with no prior experimental functional data have recently acquired.
Insights into the functional consequences of alternative splicing in humans. Experimentally supported functional information for individual splice variants is generally scarce-only a handful of isoform-level GO term annotations have been reviewed and included in public databases. Even when some isoforms encoded by the same gene have been assayed, the data are still largely incomplete, because the experiments are usually focussed on a particular functional aspect. Within this active area of research, FFPred 3 and similar methods for protein function prediction have the opportunity to help investigate the functional ramifications of alternative splicing. Indeed, very often comparative sequence analysis can only suggest that the relatively small sequence changes between splice isoforms cause more or less pronounced structural and functional differences. In other words, this approach is typically unable to put forward more detailed testable hypotheses. This opens up the possibility that alternative splicing products may not encode biochemically active molecules, but rather constitute a reservoir for natural selection [19] [20] [21] -a conjecture that is also hard to verify. Notwithstanding, experimental evidence shows that the functional divergence between alternative splice variants can vary from subtle modulations of biochemical activities to completely antagonistic regulatory roles 22 . It is therefore interesting to investigate: i) which functional aspects tend to be more robust to splicing, and consequently conserved across splice variants of the same gene; and ii) whether canonical isoforms tend to be enriched in functions that are different from those over-represented in their alternative variants-see Methods for further details on the conservation and primarity scores.
To examine these patterns, a large-scale survey was carried out on 9,214 human proteins and their recorded splice variants using FFPred 3, under the assumption that eventually they all fulfil a physiological role in the cell. The analysis was restricted to the GO term predictions compatible with the manually curated assignments existing in UniProtKB/SwissProt, as to reduce the effects of spurious results on the biological interpretation. The summary data in Supplementary Data file 2 indicate that the GO terms used in this study display varying levels of conservation across sets of alternatively spliced transcripts, even though it is difficult to assess the statistical significance of the observed differences. Only five predicted (and admittedly broad) functions appear to be consistently assigned to all the variants of a gene, and very few of them are highly conserved, when the focus is on the most reliably predicted GO terms-i.e. the SVM Matthews correlation coefficient value is in the top 50% of the distribution recorded for the corresponding sub-ontology. For instance, only six of such terms annotate all isoforms of a gene in 90% or more of the cases examined. Therefore, despite the use of a consolidated set of predictions, the findings support the expectation that alternative splicing plays a role in diversifying the cellular functional repertoire. Support for this theory is strengthened by the differential associations of individual biological roles with canonical or alternative splice isoforms -as gauged by the GO term primarity scores. The Supplementary Data file 3 indicate that there are many more GO categories preferentially associated with principal variants than with alternative ones, partly because these analyses are restricted to predicted functions in line with available annotations in UniProtKB/SwissProt. Nevertheless, the GO terms with high primarity scores tend to represent more constitutive cellular functions, and those with negative scores appear to be mostly associated with larger sets of alternatively spliced genes or to be induced by changes in the environment or in the cellular conditions. As mentioned above, it is difficult to draw statistically sound conclusions from this initial study: identifying the canonical isoform of each gene is still an open question, and here a rather simple and pragmatic approach was taken just like in previous studies.
To emphasize the unique advantages that analyzing biological features can offer, Fig. 4 gives some insight into their relationship with some of the most conserved functions in each GO domain-see Methods for more details. The heatmap allows to link the over-and under-representation of specific biophysical attributes with the conservation of particular functional aspects. Similarly, Figs 5 and 6 show the extent of positive or negative correlation between sequence-derived feature groups and the GO terms that are preferentially associated with principal or alternative splice variants, respectively. The results generally reflect well-established trends between functional categories and the occurrence or lack of intrinsically disordered residues, transmembrane helices and signal peptides, and these interpretable patterns of association also apply to extended lists of GO terms, which are either expected to be predicted with lower confidence or to be less conserved ( Figures S2, S3 and S4 ).
The figures above provide a general overview across the whole human isoform proteome; however, the online server allows to study how alternative splicing is likely to preserve or abolish individual functions, by providing a detailed graphical view of the biological features detected in the input sequences. The following showcases how functional conservation and variation are consistent with the presence or absence of particular biophysical attributes and, most importantly, with independent biological knowledge.
Protein intrinsic disorder has long been linked to binding activities and regulatory processes in the light of both experimental and computational investigations [23] [24] [25] , and its enrichment in DNA binding proteins has a two-fold explanation. Basic leucine zipper (bZIP) and AT hook domains-both well known examples of disordered regions-are frequently found in many transcription factors and regulators, and some are conserved in their splice isoforms, too. The proto-oncogene c-Fos (UniProt accession P01100) and the high mobility group protein HMGI-C (UniProt accession P52926) include one bZIP and three AT hook motifs, respectively, which are all conserved across their known splice isoforms. Most often, however, DNA binding proteins usually include additional disordered segments that are not directly involved in DNA binding, but rather in the establishment of transient and highly specific protein-protein interactions for transactivation purposes. These regions are either maintained upon splicing-like the C-terminal domain of c-Fos-or swapped with other disordered segments to rewire cellular and signaling networks 26 . Signal peptides and transmembrane helices provide useful hints about protein subcellular localization and transmembrane transporter activities. They are unsurprisingly over and under-represented accordingly in those splice isoforms that need to retain the corresponding roles. The main and alternative isoforms of both the calcium-transporting ATPase type 2C member 1 (UniProt accession P98194) and of the 5-hydroxytryptamine receptor 3E (UniProt accession A5X5Y0) clearly illustrate this point. Alternative splicing hardly affects the transmembrane segments of these channels-only the isoform P98194-2 loses one helix-therefore they still localize in the membrane, and likely act as transporters of possibly different molecules.
Some associations-such as those between beta strands and several functional categories-may not look blatantly obvious, but brief scrutiny reveals their consistency with known biological facts. Nucleotides such as FAD, NAD and NADP are commonly bound by β α β super-secondary structure motifs, which usually occur in tandem in the Rossman fold where they can form relatively large beta sheets. Mitochondrial glutathione reductase (UniProt accession P00390) has five known isoforms that all preserve the nucleotide binding site, for instance, thus suggesting that the sequence differences do not impact this functional aspect, but something else. It is known that the isoform P00390-1 is indeed found in the mithocondrion, while isoform P00390-2 is cytoplasmatic, for instance. The enrichment of residues in beta strands in isoforms at the cell periphery is also easily explained by the abundance of immunoglobulin-like (Ig-like) domains, which fold into a beta sandwich structure and are involved in a wide range of functions such as cell surface recognition, immune response and muscle structural organization. Both the mucosal addressin cell adhesion molecule 1(UniProt accession Q13477) and the leukocyte Ig-like receptor subfamily A member 5 (UniProt accession A6NI73) exemplify well this over-representation. Both proteins include a signal peptide followed by two Ig-like domains, one transmembrane helix and a C-terminal cytosolic region. All recorded splicing events cause the removal or replacement of sequence regions outside the signal peptide and the core of the Ig-like domains, thus proving that the alternative variants are still secreted. Based on these examples, we would expect that this updated version of FFPred 3 will assist experimentalists narrow down the number of assays to functionally characterize individual variants of their own interest. In turn, those efforts will definitely stimulate further bio-curation work to interpret this information and make it available in machine-readable format. Initial computational studies have been carried out to advance this area of functional genomics using gene expression profile data 27, 28 ; their integration with other complementary sources of biological information that are tissue and condition-specific will undoubtedly be the focus of many more investigations in the near future. , and the following is a brief overview of the procedure, which is also graphically summarised in Supplementary Figure S5 . Candidate functional classes were identified based on the availability of sufficiently large and confident positive and negative instances, which were split into training (70%) and validation (30%) data. The training subset was then encoded through 258 sequence-derived features covering a range of 14 different functional and structural aspects; the resulting vectors were fed into SVM-Light 32 to perform feature selection and parameter optimization. Based on the number of training instances available for each function, the number of folds k ranges between 3 and 5, within the constraint that the partitions are equally sized. Feature selection was performed using a backward elimination approach, which involves first using all feature groups to estimate classification accuracy, and then iteratively testing if the removal of each feature group improves it. At each step, a grid search of the SVM hyper-parameter space was conducted with k-fold cross-validation to estimate SVM performance using the highest average Matthews correlation coefficient (MCC)
where TP is the number of proteins correctly labelled as positives (true positives); TN is the number of proteins correctly labelled as negatives (true negatives); FP is the number of misclassified negative cases (false positives); and FN is the number of misclassified positive instances (false negatives). These parameters were used to build a binary classifier from all training examples, the performance of which was tested against the proteins in the unseen validation set. Only GO terms corresponding to predictors achieving MCC ≥ 0.05 were retained, and for them FFPred 3 makes predictions with SVMs trained on the joint training and validation sets to make the most of available annotations. Platt scaling 33 is applied to estimate the posterior probability that the input protein performs the function associated with a SVM given the raw output score.
Datasets and procedures for performance evaluation. Only for the purpose of estimating prediction accuracy, an intermediate version of the SVM library was trained using the GO OBO flat file released on 2013-11-05, the UniProt-GOA gene association file for human submitted to the GO Consortium on 2013-10-28, UniProtKB and UniRef90 release 2013_10. The training procedures outlined above produced a vocabulary consisting of 400 terms in the biological process (BP) domain, 108 in the molecular function (MF) domain, and 89 in the cellular component (CC) domain, which allowed to make predictions for all human protein sequences released as targets of the second Critical Assessment of Functional Annotation challenge 34 . The benchmark set was collated from the UniProt-GOA gene association file, by selecting those human proteins that received GO term assignments supported by evidence code EXP, IDA, IMP, IGI, IEP, TAS or IC between 2014-01-20 (end of the CAFA2 prediction stage) and 2016-03-14 (the database release date). Annotations to the term "protein binding" (GO:0005515) were discarded because they convey limited functional information unless the context is quoted (e.g. where and when the activity takes place and the requirement or absence of other molecules), and because these qualifiers are neglected by current function prediction evaluation protocols. This resulted in 3,881 annotations for 1,365 proteins in total-602 MF annotations for 454 proteins, 1,802 BP annotations for 661 proteins, and 1,477 CC annotations for 991 proteins.
Prediction accuracy was measured separately for each GO domain by precision-recall analysis as in similar studies following the lead of the CAFA experiments 34, 35 . For each protein x in the benchmark set and decision threshold v, the set of predicted terms P x,v was built by collecting all terms with confidence scores greater than or equal to v and their ancestors in GO linked by "is a" relationships and different from the root; the set of reference terms R x was generated in a similar way by up-propagating the validated annotations for x. These sets were used to calculate the number of true positives tp x,v , false positives fp x,v and false negatives fn x,v respectively as the sizes of the intersection P x,v ∩ R x , of the set difference P x,v \R x and of the set difference R x \P x,v . These data were combined into precision
Scientific RepoRts | 6:31865 | DOI: 10.1038/srep31865
and then averaged across the test set using the formulas
where m is the number of target proteins in the GO domain at hand and n is the number of those with at least one prediction scoring at least v. Finally, the average F-measure for the threshold v was calculated as
that is by taking the harmonic mean of p v and r v .
A complementary evaluation of function prediction quality was carried out on the top-ranked predictions for each target t and GO domain d. To this end, after ranking based on confidence scores, the initial predictions were trimmed to the same length l ∈ {1, 2, 3, 4, 5, n t,d }, where n t,d is the number of experimental annotations for t in the sub-ontology d. To handle ties in confidence scores, first 1,000 prediction lists of the desired length l were randomly sampled without replacement for each protein. Then, the average values of precision, recall, F-measure were calculated for each list of top l predictions; finally the average of such statistics over all replicates were analysed.
Along with the above statistics, the average sum of true positive information content was also calculated from all replicates. The information content of a GO term t was estimated in a Bayesian framework as proposed by Clark and Radivojac 36 using the equation
where P(t) represents the set of parent nodes of t, and the function N(·) returns for any set of GO terms the number of human proteins annotated in UniProtKB-SwissProt with evidence code EXP, IDA, IMP, IGI, IEP, TAS or IC. The Supplementary Data file 4 includes the complete sets of reference annotations and of predictions used in these performance comparison experiments.
Baseline function prediction methods. Naïve predictions were generated based on the frequency of the GO term annotations for human sequences recorded in UniProt-GOA as of 2013-10-28. To this end, initial counts were obtained for all GO terms except "protein binding" (GO:0005515) supported by the evidence codes EXP, IDA, IPI, IMP, IGI, IEP, IC and TAS. The data were then propagated following "is a" links in the GO released on 2013-11-05, and finally scaled between 0 and 1 for each domain separately, by dividing the final counts by the number of occurrences of the root node and rounding the result to three decimals like FFPred does. The resulting 6,504 pairs of GO terms (469 for CC, 1,268 for MF and 4,767 for BP) and scores were used to annotate all proteins in the benchmark set.
BLAST predictions were obtained by first collecting all BLAST 12 hits in the UniRef90 31 sequence database released in October 2013 with an E-value greater than 1e-03. Then the annotations in UniProtKB release 2013_10 supported by evidence code EXP, IDA, IPI, IMP, IGI, IEP, IC and TAS were transferred to the target sequences. GO term confidence scores were calculated by dividing the local alignment sequence identity by 100. When multiple BLAST hits were annotated with the same function, the highest score was retained.
Annotation and functional analysis of human splice variants. The sequences of the human isoform proteome and the classification between main and alternative splice variants were obtained from the release 2015_03 of UniProtKB/SwissProt and the accompanying "varsplic" file. Individual isoforms were discarded if a) their amino acid sequence is unknown; or b) it is shorter than 15 amino acids; or c) it is longer than 1500 amino acids, or d) it includes non-standard amino acid symbols; or e) it is recorded in a separate database entry due to substantial differences from the canonical sequence. When these filters led to the exclusion of main variants, associated alternative sequences were removed from the dataset as well. This initial screening yielded 28,310 splice variants for 9,267 UniProtKB/SwissProt entries.
FFPred 3 was run to make isoform-specific GO term predictions, which were then screened for consistency with the UniProtKB/SwissProt data. Only functional classes that were either explicitly assigned by the curators or implied by the GO data released on 2015-02-27 were retained. Removal of principal isoforms at this stage also led to the elimination of all related alternative variants, hence producing a final dataset P as consisting of 28,142 sequences for 9,214 UniProtKB/SwissProt entries.
Patterns of conservation and variation were analysed for all GO terms predicted to the splice isoforms of at least 20 distinct UniProtKB/SwissProt entries. For each functional class G, the survey aimed at quantifying its tendency to be conserved upon splicing, as well as its preference for principal rather than alternative splice variants. The average conservation of G across splice variants of the same gene was measured as the ratio between the number of UniProtKB/SwissProt entries where G was assigned to all isoforms, and the number of database records where it was predicted for at least one isoform. The primarity of G-that is its enrichment among main isoforms rather than alternative variants-was taken as where m G and a G are respectively the numbers of main and alternative isoforms annotated with G, while n = 9214 is the number of genes in the dataset, and m = 28142 is the total number of splice variants. Therefore, δ G > 0 if G is preferentially found among canonical isoform predictions; δ G < 0 if G is assigned more often to alternative variants than to main ones; and δ G = 0 if G is equally associated with the two sets of protein products.
To investigate further and interpret the conservation of each GO term g in the light of current biological knowledge, the biological attributes associated with the set of canonical and splicing variants annotated with g (V g ) were compared with those previously observed in the positive training set (T g ) of the corresponding SVM. In particular, for each sequence-derived feature f, the median value m g,f,T observed during the training process was compared to m g,f,V -the median value in V g -by first mapping the latter to the lowest percentile p g f T , , seen in T g and then by calculating
Therefore, E g,f = 0 if the two median values are identical, E g,f > 0 if on average f takes higher values in V g than in T g , while E g,f < 0 if f typically has lower values in V g than T g . Similarly, the association between a feature f and a functional class g that is over-represented in either set of canonical or alternative protein isoforms was estimated using Pearson's correlation coefficient between the values f takes on V g and the correspondin g SVM output scores.
In science policy it was assumed into the 1990s that society can benefit most from a science which pursues research at a high level. Correspondingly, indicators were (and are) used in scientometrics, such as citation counts, which measure the impact of research on science itself. Since the 1990s a trend can be observed in science policy no longer to assume that society benefits from a science pursued at a high level (Bornmann, 2012 (Bornmann, , 2013 . It is now expected that the benefit for society be demonstrated. Thus, for example, organizations which support research (such as, for example, the US National Science Foundation) now expect that supported projects lead to an outcome which is of interest not solely to science. For these organizations the consequence for the peer review procedure is that not only the possible scientific yield of the project has to be assessed, but also the returns for other sections of society.
These days, scientific work is not assessed solely on the basis of the peer review procedure, but also with indicators. A good example of these quantitative assessments is university ranking (Hazelkorn, 2011) . The most important indicators in this connection (not only with university ranking) are bibliometric indicators based on publications and their citations (Vinkler, 2010) . The impact of research is generally measured with citations. Since the impact of one publication on another publication is measured here, citations measure the impact of research on research itself. Citations allow a determination as to whether research (for example in institutions or countries) is being pursued at the highest level on average or not. But citations cannot be used to measure the impact of research on other sections of society. This is why scientometrics has taken up the wish in science policy to measure the impact of research beyond the confines of science, and is seeking new possibilities for impact measurement (Bornmann, 2014) . With societal impact assessments the (1) social, (2) cultural, (3) environmental and (4) economic returns (impact and effects) from results (research output) or products (research outcome) of publicly funded research are measured (Bornmann, 2013) . Currently the most favored procedure for measuring societal impact involves case studies, which, however, are seen as too time-consuming and therefore less practicable.
An attractive possibility for measuring societal impact is seen in altmetrics (short for alternative metrics) (Mohammadi & Thelwall, 2014) . "Altmetrics refers to data sources, tools, and metrics (other than citations) that provide potentially relevant information on the impact of scientific outputs (e.g., the number of times a publication has been tweeted, shared on Facebook, or read in Mendeley). Altmetrics opens the door to a broader interpretation of the concept of impact and to more diverse forms of impact analysis" (Waltman & Costas, 2014, p. 433 ). An overview of various altmetrics may be obtained from Priem and Hemminger (2010) . Twitter (www.twitter.com), for example, is the best known microblogging application. This application allows the user to post short messages (tweets) of up to 140
characters. "These tweets can be categorized, shared, sent directly to other users and linked to websites or scientific papers … Currently there are more than 200 million active Twitter users who post over 400 million tweets per day" (Darling, Shiffman, Côté, & Drew, 2013) . Priem and Costello (2010) define tweets as Twitter citations if they contain a direct or indirect link to a peer-reviewed scholarly article. These Twitter citations can be counted and assessed as an alternative metric for papers.
There are already a number of studies concerning altmetrics. An overview of these studies can be found in Bar-Ilan, Shema, and Thelwall (2014) , Haustein (2014), and Priem (2014) . Many of these studies have measured the correlation between citations and altmetrics.
Since the correlations were often at a moderate level, the results are difficult to interpret: Both metrics seem to measure something similar but not identical. The studies published so far cannot yet provide a satisfactory answer to the question whether altmetrics is appropriate for the measurement of societal impact or not. That is the reason for this investigation of the question.
In January 2002, a new type of peer-review system has been launched, in which about 5000 Faculty members are asked "to identify, evaluate and comment on the most interesting papers they read for themselves each month -regardless of the journal in which they appear" (Wets, Weedon, & Velterop, 2003, p. 251) . What is known as the Faculty of 1000 (F1000) peer review system is accordingly not an ex-ante assessment of manuscripts provided for publication in a journal, but an ex-post assessment of papers which have already been published in journals. The Faculty members also attach tags to the papers indicating their relevance for science (e.g. "new finding"), but which can also serve other purposes. One example of the tags which the members can attach is "good for teaching". Papers can be marked in this way if they represent a key paper in a field, are well written, provide a good overview of a topic, and/or are well suited as literature for students. Papers marked with this tag can be expected to have an impact beyond science itself (that means societal impact), unlike papers without this tag. If altmetrics indicate a greater impact for papers with this tag than those without, this would suggest that altmetrics measure societal impact.
This study is essentially based on a dataset with papers (and their evaluations and tags from Faculty members) extracted from F1000 (see also Mohammadi & Thelwall, 2013) . This dataset was extended with further data -bibliometric (e.g. citation counts) and altmetric (e.g.
). There follows in the next sections a comparison of altmetric counts with citation counts, to investigate the differences between the two metrics in relation to tags and recommendations.

F1000 is a post-publication peer review system of the biomedical literature (papers from medical and biological journals). This service is part of the Science Navigation Group, a group of independent companies that publish and develop information services for the professional biomedical community and the consumer market. F1000 Biology was launched in 2002 and F1000 Medicine in 2006. The two services were merged in 2009 today constitute the F1000 database. Papers for F1000 are selected by a peer-nominated global "Faculty" of leading scientists and clinicians who then rate them and explain their importance (F1000, 2012) . This means that only a restricted set of papers from the medical and biological journals covered is reviewed, and most of the papers are actually not (Kreiman & Maunsell, 2011; Wouters & Costas, 2012) .
The Faculty nowadays numbers more than 5,000 experts worldwide, assisted by 5,000
associates, which are organized into more than 40 subjects (which are further subdivided into over 300 sections). On average, 1,500 new recommendations are contributed by the Faculty each month (F1000, 2012) . Faculty members can choose and evaluate any paper that interests them; however, "the great majority pick papers published within the past month, including advance online papers, meaning that users can be made aware of important papers rapidly" (Wets, et al., 2003, p. 254) . Although many papers published in popular and high-profile journals (e.g. Nature, New England Journal of Medicine, Science) are evaluated, 85% of the papers selected come from specialized or less well-known journals (Wouters & Costas, 2012) .
"Less than 18 months since Faculty of 1000 was launched, the reaction from scientists has been such that two-thirds of top institutions worldwide already subscribe, and it was the recipient of the Association of Learned and Professional Society Publishers (ALPSP) award for Publishing Innovation in 2002 (http://www.alpsp.org/about.htm)" (Wets, et al., 2003, p. 249 ). The F1000 data base is regarded as a significant aid for scientists seeking the most relevant papers in their subject area: "The aim of Faculty of 1000 is not to provide an evaluation for all papers, as this would simply exacerbate the 'noise', but to take advantage of electronic developments to create the optimal human filter for effectively reducing the noise," (Wets, et al., 2003, p. 253) .
The papers selected for F1000 are rated by the members as "Good," "Very good" or "Exceptional" which is equivalent to scores of 1, 2, or 3, respectively. In many cases a paper is assessed not just by one member but by several. The FFa (F1000 Article Factor), given as a total score in the F1000 database, is calculated from the different recommendations for a publication. Besides making recommendations, Faculty members also tag publications with classifications, as for example (see http://f1000.com/prime/my/about/evaluating):
investigates the effects of an intervention (but neither randomized nor controlled) in human subjects. The classifications, recommendations and bibliographic information for publications form the fully searchable F1000 database containing more than 100,000 records (end of 2013). Overall, the F1000 database is regarded simply as an aid for scientists to receive pointers to the most relevant papers in their subject area, but also as an important tool for research evaluation purposes. So, for example, Wouters and Costas (2012) write that "the data and indicators provided by F1000 are without doubt rich and valuable, and the tool has a strong potential for research evaluation, being in fact a good complement to alternative metrics for research assessments at different levels (papers, individuals, journals, etc.)" (p.
14).
In January 2014, F1000 provided me with data on all recommendations (and classifications) made and the bibliographic information for the corresponding papers in their system (n=149,227 records (Bornmann, in press ). In addition, Altmetric can only reliably provide data for papers published after 2011. For this reason the dataset is reduced in what follows -where altmetric data is statistically evaluated -to papers from the period after 2011.
Since Altmetric could not add altmetric data for all the papers, but only for 69%, the question arises how the remaining papers should be treated in the statistical evaluation. One could perhaps argue that these papers should be set to zero counts for all altmetrics.
Apparently, not even a mention is available for these papers in any of the social media platforms. On the other hand, the dataset from Altmetric should have at least a mention for all papers under F1000 since the data to which Altmetric attaches altmetric data originate from F1000, and F1000 recommendations are also evaluated by Altmetric. But since this is not the case in the current dataset, the 31% of the papers for which Altmetric was not able to supply any altmetric data were recorded as missing and excluded from the statistical analysis. As the following analyses shows, only a few papers published after 2011 are affected by this problem -that is those papers which were used in the current study for the analysis of alternative metrics. the fraction of papers with 0 counts is also provided. As the results show, the counts in the altmetrics are generally low. For example, the papers in the dataset have an average of 0.46 blogs mentioning a paper with a minimum of 0 and a maximum of 111. An important reason for the generally low averages in the altmetrics is the large fraction with 0 counts: For almost all altmetrics in the table, (significantly) more than two thirds of the papers show zero counts.
Since the total altmetric counts and the unique tweeters mentioning papers (Twitter counts) are the only altmetrics with significantly higher average counts and significantly lower share of 0 counts than with the other altmetrics, these are the only ones included in the following statistical analysis.
On the one hand, the analysis of Twitter counts in this study has the further advantage that the data evaluated originates only from one service (which represents the standard in the area of microblogging). This facilitates the collection of data for Altmetric and ensures the reliability of the counts. Blogs, for example, do not have this advantage: "While most other Web 2.0 applications are closely identified with a few 'name-brand' services (for instance, Twitter for microblogging and delicious for social bookmarking), blogging is not" (Priem & Hemminger, 2010) . Blogs are distributed over the whole Web, and there is no standard service aggregating these blogs. On the other hand, Twitter is particularly in use by people who operate outside the area of science: Although Twitter is one of the most often used social media platforms, it is generally assumed that only few scientists actually tweet (Darling, et al., 2013; Mahrt, Weller, & Peters, 2012) .
The statistical software package Stata 13.1 (http://www.stata.com/) is used for this study; in particular, the Stata commands nbreg, margins, and coefplot are used.
A series of regression models has been estimated. The outcome variables (number of citations, number of tweeds, number of total altmetric counts) in the models are count variables. They indicate "how many times something has happened" (Long & Freese, 2006, p. 350) . The Poisson distribution is often used to model information on counts. However, this distribution rarely fits in the statistical analysis of bibliometric and altmetric data, due to overdispersion. "That is, the [Poisson] model underfits the amount of dispersion in the outcome" (Long & Freese, 2006, p. 372) . Since the standard model to account for overdispersion is the negative binomial (Hausman, Hall, & Griliches, 1984) , negative binomial regression models are calculated in the present study (Hilbe, 2007) .
The violation of the assumption of independent observations by including several different items of information about the same paper (such as several F1000 recommendation scores or several subject categories associated with a paper) is considered by using the cluster option in Stata (StataCorp., 2013) . This option specifies that the information items are independent across papers but are not necessarily independent within the same paper (Hosmer & Lemeshow, 2000, section 8.3 ).
The publication years of the papers were included in the models predicting different counts (e.g. citations) as exposure time (Long & Freese, 2006, pp. 370-372) . The exposure option provided in Stata takes into account the time that a paper is available for citations or other mentions (e.g. in Twitter).
In this study, adjusted predictions are used to make the results easy to understand and interpret. Such predictions are referred to as margins, predictive margins, or adjusted predictions (Bornmann & Williams, 2013; Williams, 2012; Williams & Bornmann, in preparation) . The predictions allow a determination of the meaning of the empirical results which goes beyond the statistical significance test. Whereas the regression models illustrate which effects are statistically significant and what the direction of the effects is, adjusted predictions can provide us a practical feel for the substantive significance of the findings. Table 2 shows the distribution of the tags over the records in the dataset (in which papers appear more than once) and total tag mentions ("total" line). It is very clear that the tags are applied very differently: Whereas, for example, "new finding" makes up about half of the tag mentions, for "review" it is only about 2%. In order to be able to make a reliable statement about the validity of the altmetrics, the following statistical analysis does not include all tags, but only those with more than 5% of mentions or allocated to more than 10% of records. What expectations are there in the current study in relation to the connection between altmetrics counts or citation counts and the categorization of papers with the five selected tags (which are described in further detail in section 2.1)? In connection with "new finding", "confirmation" and "interesting hypothesis", it is expected that the citation counts for such papers would be higher for those where a Faculty member has used this tag than for those where this did not happen. Since these tags particularly relate to aspects which are relevant in a scientific context, it would not be expected that the altmetric tags show this difference between tagged and untagged papers. In contrast to this, we could expect that papers tagged with "good for teaching" would (also) be interesting for a group of people outside science or research. These are papers which are well written, provide an overview of a topic and are well suited for teaching. Therefore, a higher altmetrics count would be expected for papers with this tag than for papers without it. The "technical advance" tag is used on papers that present a new technique or tool (whether that's a lab technique/tool or a clinical one) that make an advance on an existing technique. The tag can be used both for research papers and outside,

i.e. clinical or fieldwork. Thus, a similar effect of this tag on altmetric or citation counts would be expected in the statistical analysis.
In order to ascertain how total altmetric counts, Twitter counts, and citation counts differ with differently tagged papers, three regression models were calculated with the three counts as dependent variables and the tags as independent variables (see Table 3 ). Each model includes the individual recommendation scores of the Faculty members alongside the tags.
This enables us to ascertain the influence of the tags on the different counts, controlling for the effect of the recommendations. Since the recommendations reflect the quality of the papers, the results of the tags are adjusted for the quality of the papers. In other words: the different results for the tags can hardly be traced back to the differing quality of the papers. As Table 3 shows, the three models involve papers from different years: The models with altmetrics as dependent variables can only take into account papers published after 2011 (see above). The model with citation counts as dependent variable only involves papers published before 2011. Since the citation window for the papers extends from the publication year to the end of 2012 in this study, the citation window for papers published after 2011 is too narrow to measure the citation impact reliably (Wang, 2013) . The inclusion of papers from before 2011 leads, however, to a shortage of records tagged with "good for teaching" (0.1%, n=181) (see Table 3 ). The "good for teaching" tag is relatively new for F1000Prime; it was introduced only in 2011. Therefore, it cannot be included in the analysis of the citations. The results of the regression models are shown in Table 4 . These are the test statistics depend on the models with all independent variables, they are calculated for the different tags under control of the recommendation scores (and adjusted for quality). In all the models in Table 4 , a statistically significant result is seen for the recommendation scores of the Faculty members. Since the coefficients have a positive sign, higher total altmetric counts, Twitter counts, and citation counts are to be expected with better scores. Thus the quality of the papers does not only play an important role for the citation impact, but also for the altmetric counts. The relation between the different recommendation scores and the predicted numbers of counts is presented in Figure 1 : It is very clear that citation counts in particular separate the differently evaluated papers.
In the two models for the altmetrics (models 1 and 2), the coefficient for "good for teaching" is statistically significant. Correspondingly, Figure 2 and Figure 3 show higher predicted numbers of counts for papers where this tag is set, than for those papers where this was not the case. For example, we can expect a paper with this tag to have around seven
Twitter citations more than one without -if the paper is rated as "very good" by Faculty members and has no other tags. These results for "good for teaching" correspond to the expectations (see above) and indicate that altmetric data (and especially tweets) can indicate papers which are of interest outside of science.
Unfortunately, the "good for teaching" tag could not be included in the model for the citation counts (see above). Therefore, there is a lack of results which could be included in a comparison. Model 3 for the citation counts provides two statistically significant results (see Table 4 ): Citations are particularly to be expected if a paper presents original data, models or hypotheses (tag: "new finding") or introduces a new practical/ theoretical technique (tag:
"technical advance"). Whereas the results for "new finding" correspond with the expectations (see above), the results for "technical advance" can clarify the unspecific expectations formulated above: Papers tagged with "technical advance" seem to involve techniques with relevance for research rather than for areas outside research. For both tags, Figure 4 shows a clear citation impact advantage for papers with this tag than for those without.
The results in Figure 4 also show that confirmatory results (tag: "confirmation") and interesting hypotheses (tag: "hypothesis") can hardly be associated with higher or lower citation counts (against the expectation). 
If altmetric data is to be used for the measurement of societal impact in the evaluation of research, the question arises of its normalization (Torres-Salinas, Cabezas-Clavijo, & Jimenez-Contreras, 2013). With citation counts, there is a consensus in the bibliometric community that the impact of papers should be normalized in relation to the subject category (the field) and the publication year (the time) (Bornmann, Leydesdorff, & Wang, 2013) . Is this also necessary for the Twitter counts and total altmetric counts investigated here? The following statistical analysis will focus on the question of taking into account the subject categories, since, for the papers in this study, only the publication year is available and not the publication month or day. Unlike citations which arise only a long time after the appearance of a paper, altmetric data generally appears relatively quickly (Priem, Taraborelli, Groth, & Neylon, 2010; Rodgers & Barbrow, 2013) . The temporal aspect in the normalization of altmetric data can therefore only be clarified with data on the month or day level. Other empirical studies have already indicated subject area differences with altmetric data. Thus, for example Loach (2014) shows from Twitter counts in the Altmetric database "that medical articles receive a disproportionate amount of online attention. In fact, 60% of tracked tweets from the last week pointed to articles from journals publishing Medical and Health Science research. Interestingly, 63% of these were directed to articles from journals tagged as relating to Clinical Medicine or Public Health specifically." An important disadvantage of the studies which have so far appeared on subject area difference is that the quality of the papers is not controlled in the analyses of the subject area differences. Therefore, it is not known whether the differences between the subject areas depend on aspects specific to the subject or quality differences between the papers. Thus, medical papers could receive more online attention just because these papers are generally of a higher quality than papers from other subject categories. The quality of the papers should therefore be controlled in the analysis of subject area differences.
In the current study, WoS subject categories are used to determine subject area differences in the counts. Most bibliometric studies use these categories, which, however, are not applied on the level of individual papers, but on the level of journals: A set of journals is combined in a subject category by Thomson Reuters. Table 5 shows the distribution of the papers over the subject categories published after 2011 in the dataset. Since the evaluation of the altmetric data could only include papers after 2011, the table refers to this part of the data.
As the table shows, around 14% of the category classifications relate to "multidisciplinary sciences" -that corresponds to around 20% of the papers. This journal set includes the two multi-disciplinary journals Nature and Science. Around 13% of the papers in the dataset were published in a journal belonging to the category "cell biology". The subject categories in Table 5 are included as independent variables in two negative binomial regression models, where one includes the total altmetrics counts and the other the Twitter counts as dependent variable. With the help of this model, the predicted numbers of counts could be determined for the individual subject categories, where the quality of the papers is controlled for by the individual recommendation scores (which are included as mean scores per paper in the model alongside subject categories). The model also takes into account that the papers appeared in different publication years and have different numbers of subject categories. The results of the regression models will not be presented in
table form in what follows, since the tables are very extensive given the large number of different subject categories. But the predicted numbers of counts with 95% confidence intervals for the individual subject categories are presented as the results of the models (see Figure 5 and Figure 6 ). Figure 5 . Predicted numbers of total altmetric counts with 95% confidence intervals. The graphic is based on 13,278 papers published after 2011 with 18,254 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, in which the quality of the papers is controlled by the individual recommendation scores of the Faculty members. Predicted number of count Figure 6 . Predicted numbers of Twitter counts with 95% confidence intervals. The graphic is based on 13,278 papers published after 2011 with 18,254 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, where the quality of the papers is controlled for with the individual recommendation scores of the Faculty members.
In order to determine whether the predicted numbers of counts for the subject categories with the altmetric data follows a similar (or different) pattern to that with the citation counts, Figure 7 shows the predicted numbers of citation counts with 95% confidence intervals. As with the evaluations described in section 3.2, these predicted numbers arise from a negative binomial regression model based on papers from the time period before (and not after) 2011. Even if the analysis underlying Figure 7 only took into account subject categories with more than 100 instances in the dataset (similarly to Figure 5 and Figure 6 ), the subject Predicted number of count categories in Figure 7 do not coincide with those shown in Figure 5 and Figure 6 . The reason for the discrepancies lies in the different publication years involved. Figure 7 . Predicted numbers of citation counts with 95% confidence intervals. The graphic is based on 42,858 papers published before 2011 with 60,468 subject category instances (only subject categories with more than 100 instances). The results arise from a negative binomial regression model, where the quality of the papers is controlled for with the individual recommendation scores of the Faculty members.
As the results in the three figures show, the predicted numbers of counts for the altmetric data on the one hand is very different from that for the bibliometric data, on the other. With the altmetric data (total altmetric counts and Twitter counts), the predicted Predicted number of count number of counts is relatively low for almost all subject categories. Only for "biology", "ecology", "evolutionary biology", "multidisciplinary sciences" and especially for "medicine, general & internal" are they higher. Particularly in the journals of the subject category "medicine, general & internal" an especially large number of contributions seem to be published which are not only of scientific interest.
The predicted numbers of citation counts shown in Figure 7 , shows a different pattern from the predicted numbers of altmetrics data. In Figure 7 there are quite a few subject categories which stand out with relatively high counts (and many categories with hardly any), but the individual subject categories are distributed over a large bandwidth of different predicted numbers of counts. This difference between the altmetric data and citations in the distribution over the predicted numbers of counts is visualized in Figure 8 . Box plots are used to represent the distribution of the counts which are visualized in Figure 5 , Figure 6 and Figure 7 . In Figure 8 it can clearly be seen that the predicted numbers of citation counts are distributed over a greater area than the predicted numbers of total altmetric counts and Twitter counts. Correspondingly, the citation counts show a significantly greater standard deviation than the altmetric data (see Figure 8 ). Twitter count (n=45) Citation count (n=58) Figure 8 . Distribution of the predicted number of total altmetrics counts, Twitter counts and citation counts. Whereas the standard deviations for the total altmetrics counts and Twitter counts are std=11.3 and std=9.5, for the citation counts it is std=23.5.
The results for the differences in the distribution of the predicted numbers of counts between the altmetric and the bibliometric data indicate that the subject categories have a different meaning in this area. Whereas the evaluation of the bibliometric data indicates different citation practices in the fields (which should be taken into account with a normalization), the evaluation of the altmetric data gives the impression that only papers from a few specific subject areas receive a larger number of mentions. With the altmetric data, it does not therefore appear a matter of different habits in the mentioning of papers between the fields, but of a particularly large (or small) interest among people outside science for papers from a few specific areas (or for the bulk of scientific papers). Therefore, a normalization of the counts on the level of subject categories (journal sets) is not regarded as reasonable.
Can altmetric data be validly used for the measurement of societal impact? The current study has sought to answer this question with a comprehensive dataset from very disparate sources (F1000, Altmetric, and an in-house database based on WoS). In the F1000 peer review system, experts attach particular tags to papers which indicate whether a paper could be of interest for science or rather for other segments of society. In this study, these tags were
used in an attempt to analyze the validity of altmetric data. A "good for teaching" tag indicates that a paper could be of interest outside the science. If papers with this tag receive more altmetric counts than those without, this would be an indication of the validity of measuring societal impact with altmetric data. Conversely, papers with tags for specifically scientific aspects (such as "new finding" or "hypothesis") should show no effect on the altmetric counts. For contrast with the altmetric data results, this study analyzed citation counts.
First of all, the results of the regression model in relation to all counts (bibliometric and altmetric) show a correlation with the quality of the papers: With better recommendation scores of the Faculty members, the higher the counts are. For example, for recommendation scores "good", "very good", and "exceptional" the corresponding predicted probabilities of citations are 69, 113, and 178. The effect of the recommendation scores occurs -as expected -more strongly with the citation counts than with the altmetric data, and is in agreement with the results of Bornmann (in press). In the study of Bornmann (in press), it is shown that the recommendations of the Faculty members are correlated with field-und time-normalized citation impact scores. The further results of the regression models show substantial differences between altmetrics counts and citation counts (see Bar-Ilan, 2012 ). With regard to a possible societal impact measurement with altmetrics, the results of the present study indicate that with altmetric data impact measurement beyond the science seems possible:
Papers with the tag "good for teaching" do really achieve higher altmetric counts than papers without this tag -if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented ("new finding"). Although the tag "good for teaching" could not be included in the model with the citation counts (so the contrasting comparison was absent), no (statistically) significant effect was demonstrated for the tag "new finding" in the models with the altmetric data.
The results of this study possibly indicate that papers tailored for a readership outside the area of research or science lead to societal impact. This result is in agreement with the proposal of Bornmann and Marx (2014) . To produce societal impact, the authors suggest that scientists write assessment reports summarizing the status of the research on a certain subject and representing knowledge which is available for society to access. An assessment report should be couched in generally understandable terms so that readers who are not familiar with the subject area or the scientific discipline can make sense of it. In the view of Bornmann and Marx (2014) , these reports could be seen as part of the secondary literature of science, which has up to now drawn on review journals, monographs, handbooks and textbooks (primary literature is made up of the publications of the original research literature). With the help of these assessment reports it should be possible to reach people from other segments of society (besides science) and to achieve a correspondingly high impact that would then have an effect on the altmetric data.
If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. Bibliometric data -citations -are normalized for subject area and time. This study has therefore taken a second analytic step involving a possible subject area normalization of altmetric data. In this analysis too, additional citation data was considered to be able to determine common factors and differences in the results. In contrast to the predicted numbers of citation counts, where the subject categories each showed very different clustering, the predicted numbers of altmetric counts (total Altmetric counts and Twitter counts) showed very few subject categories producing high levels of clustering (and the great bulk of the categories low clustering): "biology", "ecology", "evolutionary biology", "multidisciplinary sciences" and especially "medicine, general & internal".
In comparison with the other subject categories (which obtained relatively low counts), these categories are of the sort which appeal to a wider audience public. This wider audience is generally especially interested in topics like ecology and evolution, as well as research results from (internal) medicine (or particular diseases). In addition, there is a special interest in contributions from the best-known scientific journals Nature, Science, and
Proceedings of the National Academy of Sciences (PNAS), which publish research from all disciplines. There are obviously -as the results of this study show -particular topics in the biomedical area which are of especially great interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters' journal sets, a normalization of altmetric data (especially Twitter) should not be based on the level of subject categories, but on the level of topics: Thus, for example, Twitter's homepage includes a current list of trending topics as a main feature. "These terms reflect the topics that are being discussed most at that moment on the site's fast-flowing stream of tweets. In order to avoid topics that are popular regularly (e.g., good morning or good night on certain times of the day), Twitter focuses on topics that are being discussed much more than usual, that is, topics that recently experienced an increase of use, therefore trending" (Zubiaga, Spina, Martínez, & Fresno, 2014) . The comparison of Twitter citations of papers published on a particular topic would then show a greater or lesser interest in papers on this topic. A normalization of Twitter citations could then be performed on the level of papers on a topic.
In relation to the measurement of societal impact, the results of this study are promising: Altmetric data (Twitter counts) seem able to indicate papers which produce societal impact. However, it is not clear which kind of impact is measured: Does it measure social, cultural, environmental and/ or economic impact? With evaluating citations in university text books (impact on education), patents (impact on industry) and clinical guidelines (impact on clinical praxis), there are already some approved instruments available for the reliable societal impact measurements which could be complemented by altmetrics.
In a bid to measure the influence of research on industry, Narin, Hamilton, and Olivastro (1997) studied the frequency with which scientific publications were cited in US patents. They evaluated 400,000 US patents issued between 1987 and 1994. Their results
show that the knowledge flow from US science to US industry tripled in these years. Grant (1999) and Lewison and Sullivan (2008) pursued a similar objective to Narin, et al. (1997) with their evaluation of clinical guidelines: how does knowledge flow from clinical research to clinical practice? The pilot study by Grant (1999) examined three guidelines and was able to ascertain that they contained citations of a total of 284 publications (which can be categorised by author, research institution, country, etc.). For Grant (1999) , the study results demonstrate the usefulness of his approach to tracing the flow of knowledge from research funding into clinical practice.
As most of the former empirical studies on altmetrics have pointed out, we need further studies (including a broad range of altmetrics) dealing with the question of the specific impacts of altmetrics. For this, datasets are required which contain information about the importance of individual publications outside the area of science. This information should be produced by experts (and thus be reliable and valid). Unfortunately, the F1000 dataset does not contain this information. It would be particularly interesting to have information on the importance of publications for very specific segments of society (such as the economy, politics or culture). With this data, one could determine which specific altmetric impact one could measure in which segment of society.
It is well acknowledged that HCI research has a significant role to play in understanding how digital technology can facilitate and support new forms of civic engagement. Over the last five years, we have seen a wealth of work where technology has been used as a means for collecting community opinion [28, 45] to support community activists and community organisations to gather data [47] and facilitate discussion around political decision making [11] .
These emerging landscapes for HCI research typically require extensive working with and within communities [13, 44] , and often come laden with ideals around supporting new forms of democracy and participation in civic life. Furthermore, it involves placing greater emphasis not on just designing systems to collect public opinion, but to design systems for citizens, civic groups and local government to collect public opinion from others.
In this paper, we build on this prior work by detailing our experiences of collaborating with community organisations who used our 'Viewpoint' situated consultation technologies. We discuss fieldwork from two collaborative projects where our voting devices have been deployed to collect opinion on specific issues at different stages of campaigns and participatory governance exercises. Rather than focusing primarily on an evaluation of Viewpoint 'in use', we highlight the various trade-offs and decisions made before, and the making sense and use of collected data following deployment of the devices. Following [13, 14, 15] we highlight some of the human work that goes into planning and overseeing the use of consultation technologies for community organisations, and the ways in which the research team guided and influenced this process. Our reflections on this fieldwork highlight specific issues related to: the forming of the right questions to be posed on the devices; the identification of and gaining access to the right locations for promoting engagement and discussion; and the difficulties community organisations face in using and responding to the data and insights collected through novel consultation technologies.
Our contributions to the developing HCI discourse surrounding civics technology are two-fold. First, through rich ethnographic insights we highlight stakeholder (researcher and community partner) influence and responsibilities in deployments of community consultation technologies. Second, based on our two case studies, we highlight challenges and opportunities for HCI researchers working with communities and civic organisations, while problematising the perceived neutrality of community consultation technologies in contexts where only a privileged few set the questions, situate the devices and have access to the data.
The field of HCI has for many years dealt with issues to do with civic action, engagement and participation. A huge amount of work within the CSCW and CHI communities has examined how social media services are appropriated for civic discourse [11] , information sharing [42] , activism [30] , protest [46] , and action [25, 39] . Alongside studying the role of technology in relation to issues of civic importance, there has been increased attention paid to conducting in-the-wild studies of systems in community and civic contexts. Going back over 15 years, projects such as Civic Nexus [34] and CiVicinity [5] have highlighted the benefits of closely collaborating with communities to create Participatory design and systems that connect local actors and transform practices in voluntary and community sector organisations. More recently, a number of studies have explored how technology can support new forms of community engagement and participation in local decisionmaking. Much of this work has focused upon the evaluation of situated displays in public places to engage citizens in voting, consultation, and other forms of sharing and contributing to such processes (e.g. [8, 20, 22, 24, 43] ). For example, ongoing work in Oulu, Finland, has articulated the value of interactive public displays in engaging members of the public in commentating and giving feedback on planning proposals [23] . Taking a different approach, in the Bespoke project, Taylor et al. [45] deployed their Viewpoint technology as a simple means for local government representatives to set questions for community members to respond to. The ambition here was to promote wider participation, and a sense of increased efficacy, for community residents. Koeman et al. [28] took an approach to distributing voting boxes at multiple locations around communities. Again, like Taylor et al. [45] , they harness lightweight forms of engagement to promote participation in opinion sharing-however, they took a further step in visualising the results on a location-by-location basis, as well as in a 'neutral' ground, which promoted wider discussion around the contrasts and divisions within the community itself.
For a long time, the participatory design and community informatics literature-along with wider participatory research scholarship-has discussed, articulated and debated the challenges involved in working with community organisations and facilitating new practices and processes [2, 6, 34] . Issues such as these are becoming of increasing importance to the HCI scholarship on civic and community technology. This is especially so, given that HCI researchers are no longer just deploying technologies for opinion gathering and consultation-rather, in many respects, they are aiming to support others in developing such practices. This is particularly recognisable in Vlachokyriakos et al's [47] work on PosterVote, where the ambition was to build platforms to be appropriated and deployed by activists, rather than deploying it and evaluating it on their behalf. Beyond the technical and design features of the system under study, PosterVote raised questions related to the governance and ownership of the data collected and the influence of activists groups on the way people voted. In their work on crowdsourced cycling data, LeDantec et al. [12] noted issues of the provenance, legibility and meaningfulness of data generated by publics to those making planning decisions. Taylor et al. [43] -also raising issues of who owns and accesses communitygenerated data-note the ways in which residents make data meanigful by placing it into context. They also note the important role the research team played as a percieved neutral party to support dialogue and sensemaking around community-generated data, as well as providing the necessary skills and expertise to install and maintain devices and related infrastructures and archives. This is echoed by [44] who discuss the critical importance of building relationships with local residents and lead community members through the duration of projects and ensure skills and infrastructure are in place to sustain endeavours beyond the completion of the research project. In a similar vein, Hosio et al. [27] discuss the percieved value of situated displays in civic and community contexts, highlighting the range of additional costs and burdens they bring to the local government organisations who use them.
These examples in different ways pose questions about the responsibilities of different stakeholders in civic technology contexts where decision-making is a primary concern. They also raise issues related to the role of the researcher in these contexts, and whether they have a responsibility to not just to provide new tools with which to consult but also help organisations and individuals develop the skills, resources, capacity and practices to use these in a meaningful and sustainable manner. We build on the above by discussing our experiences of conducting field trials of distributed, multi-site community consultation technologies with two communty organisations. These deployments were intended to be led by our partners, as we will highlight, however, our community partners faced a number of conceptual and practical challenges in planning, overseeing and making sense of the insights from these deployments. Through our discussion of these projects we will highlight the ways in which the research team played an important role in carefully guiding and, at times, explicitly directing and managing parts of these deployments.
Our work built on the prior work of Taylor et al [45] and their original Viewpoint system. In the following, we provide an overview of this original work, followed by how our projects and version of the technology builds upon it.
The original Viewpoint technology was developed as part of the Bespoke project. The overarching project explored issues to do with community cohesion and political disengagement in a small city in North West England. Viewpoint allowed local councillors and community As noted in [45] , the choice of a simple interface situated in a public space proved to be successful in gathering high quantities of feedback. Across a two-month deployment, eight different polls received an average of over two hundred votes each, an order of magnitude higher than original expectations. However, Viewpoint was less successful in creating the kind of positive feedback loop that had been intended. Community members remained sceptical of whether any change would occur, and few meaningful responses or promises of action were given by the local government collaborators. Having to work closely with councillors to help them formulate a question that was capable of being answered through a binary choice, the rapid turnover on questions, and the lack of actionable issues with a burning need for input made this difficult to achieve. As since discussed extensively by Harding et al. [26] , this exposed a failure to integrate with existing council processes that might have created avenues for change. This was compounded by the way that Viewpoint placed the agenda firmly in the hands of those in positions of power, with community members acting as passive respondents.
Findings from the original Viewpoint suggest a number of future possibilities that we chose to take forward. First, the use of short-term, targeted deployments would allow the device to be deployed only when specific input was needed and actionable. This might also take advantage of the novelty effects that had been observed. Second, situated voting technologies might be more closely tied into existing practices. This could mean integrating with council feedback schemes, but it may also mean putting Viewpoint in the hands of community organisations who are already engaged with local authorities. For the most part, this repositioning only requires a change in how the device is used. However, to better support new deployment contexts, we redesigned Viewpoint with a focus on flexibility and portability. The redesigned device ( Figure 1 ) allowed greater flexibility in how questions could be presented and responded to. It made use of a physical rotary control rather than buttons to allow voters to respond through multiplechoice answers or points on a sliding scale. It also supported voice and video input if required, along with a touch-screen display to be enabled as and when deemed appropriate. Additionally, the devices were made considerably smaller, with the intention that they might be more easily moved between different locations, and a 3G modem was added as a backup in situations where Wi-Fi could not be provided.
We deployed the second generation of Viewpoint in two case studies, where the technology was used by community groups to elicit feedback on issues related to local planning and transportation developments. In the following sections we provide an overview of these case study contexts.
Our first case study involved working with the local chapter of an international movement that champions sustainable communities. When we approached them, the group were beginning to collect evidence to support the pedestrianisation of Acorn Road, a small shopping street that formed the centre of their neighbourhood. The group felt that the street was overly congested, making it dangerous for local pedestrians and cyclists who they felt most used the street, and that many of the cars could easily be re-routed to create a more pleasant environment.
The campaigners used the Viewpoint boxes to contribute to their ongoing consultations with people on Acorn Road. They developed two questions (discussed in the Findings) that were displayed on the devices for two weeks each, during the summer and during the autumn (to collect data from the student population). Three devices were deployed in the community: two in local supermarkets, with a third inside the neighbourhood's library. Simultaneously, the group carried out a street survey in which they stopped passers-by to ask a number of questions, one of which related to means of travel, and a traffic observation survey. Across the entire deployment, shoppers placed 2,040 votes in total. By contrast, the group's past attempts to collect feedback online had returned only a few dozen results.
Our second study was conducted in a small coastal town in Northern England comprised of approximately 6,000 residents. The town attracts large numbers visitors in the summer season and is currently experiencing major redevelopment, particularly in its harbour area. Beside this regeneration, which is primarily funded by the local Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA government, there is significant private investment. The development within the town is primarily driven by a Local Development Trust (henceforth 'the Trust'). The Trust was set up two decades ago as a response to the perceived lack of opportunities for development and employment in the town. It is responsible for attracting funding grants for community projects. One requirement the Trust has is to carry out public consultation as part of its applications to receive grants, along with further consultation to then allocate the budgets associated with community projects.
Significantly, there is a feeling amongst many residents in the town that regeneration is often focused on tourism, excluding the needs of those who live there. This is further compounded by high-levels of unemployment and closure of local industry over the last several decades. As such, this has led to a lack of trust that local and central government will act in their interests. These issues were echoed by the Trust, which has noted a substantial decline in levels of engagement in recent years which they put down to a feeling of disfranchisement from some residents. This is problematised further by a feeling that the consultations they perform tend to attract the same group of people every time, and the setting of consultation events at a fixed time and place leaves many people unable to attend.
These were important motivators for the Trust in using Viewpoint devices in their application scoping and project allocation processes. They developed three questions (again, discussed in the Findings) that were displayed on four boxes during the summer of 2015. Four devices were deployed in different locations around the town in areas where they expected to capture feedback from local people who would normally not engage in their consultations. Across the deployment of the Viewpoint devices, people registered 699 responses in total, with a significant number of these votes coming from devices located in parts of the town rarely engaged with by the Trust.
Initial contact with each group came via community engagement activities conducted by our research lab. The research team then met with each of the organisations to discuss their projects, modes of engagement and consultation, and the challenges they faced.
During the Acorn Road deployments, the devices and the progress of the campaign were monitored by both a researcher and the campaign group, although the primary source of data collected was automated interaction logs. The researcher and campaign group maintained regular contact through the deployment. For Ambit, we expanded on this approach to place more focus on in-situ observations. Semi-structured interviews were conducted with four 'custodians' (people working in locations where devices were deployed). These interviews were audiorecorded and focused on the use of the device by others as perceived by the custodians. These were supplemented by observations where the researcher would 'hang around' [48] and note interactions with the device and events that occur in these spaces. The researcher talked with users of the devices to ask them about why they participated and their response to the ongoing results presented on Viewpoint.
Overall, 84 hours of fieldwork observations were conducted and 22 conversations documented.
Each trial ended with a semi-structured interview with the representatives of each community partner, as well as device custodians for Ambit, where we discussed consultation results and how they may use the data gathered. The data collected (transcriptions of interviews and field notes) was then used as a corpus for thematic analysis [3, 9] . Data coding was driven by questions related to how choices and decisions that impacted on the consultation process were made during the projects. Coding of data was shared between the first and second authors and checked by the third author. Codes were then clustered into the themes presented in this paper.
Our analysis generated five themes. We organise our themes to present a comparative narrative of how the two field trials played out over time.
A critical aspect of our engagements across each study was working with the community organisations to establish the types of questions to be posed on the devices. Learning from our prior projects with the older Viewpoint boxes, each of our case studies was tied in one form or another to pre-existing campaigns and consultation processes. However, the stages within these processes at which the Viewpoints were used were rather different-this shaped the discussions around what questions may be asked, and how they should be asked and responded to. As such, what was initially assumed to be a simple endeavour in setting questions to ask the public, became complex decisionmaking processes in their own right.
In Acorn Road, the choice of overriding topic was determined by the pre-existing campaign. As noted, the ambitions behind the campaign were to advocate for the pedestrianisation of a local street. Initially the organisation considered using Viewpoint to directly ask residents whether they thought the street should be pedestrianised, with the intention that this could be presented to the council like a petition. However, prior to deployment concerns were raised about this direct line of questioning:
" This was a concern raised all the more by the fact that local businesses-who were opposed to pedestrianisation-were envisaged as being the most likely deployment locations, so some sensitivity was required. Instead, the group developed a new set of questions and possible responses that, as they articulated it, were more "objective" (Acorn Road campaigner). Two questions were asked: "What has been your main means of travel today?" followed by "How far do you live from here?" These questions sought to ascertain what percentage of the street's users were travelling by car and what proportion were local and might reasonably travel by foot or bicycle instead. The questions themselves were carefully chosen and reflected the many stakeholders that existed in this community issue. The decision to collect "objective" data about usage of the street rather subjective opinions about its future meant that data was felt to be unbiased (or, at least, less biased), and was envisaged as being taken more seriously by the complex range of actors involved in future decision making.
A similarly complex set of trade-offs around question setting occurred in the Ambit project. The Ambit project differed significantly from Acorn Road in that it occurred at a much earlier stage in a consultation process. As such, there was not a specific campaign for the organisation facilitating the trial to push-rather the Viewpoint devices were to be used as part of a scoping exercise for future project development. The main ambition here was to capture views from people about the locations and places in the town they felt needed investment. This initial broad consultation process would identify specific locations in the area to be targeted in a more focused piece of consultation work. Like Acorn Road, however, the language used in the questions posed was very carefully considered. While the Trust did have money for projects, they "did not want to raise expectations" (Trust representative) that this scoping process would lead to money being committed before current projects were complete. Initial questions suggested for the devices included words such as "funding", "investment" and "projects"-however, these were iterated to instead focus on the locations in the town people wished to "change" or where they "like to visit". These changes to how the questions were posed made them much more ambiguousat the same time it allowed the Trust to distance themselves from acting on the results of the consultation, should they feel unable to commit to working in the locations most identified in the responses.
Across both deployments the community groups had an agenda that in some way they wanted to obfuscate. As researchers, we had to find ways to fulfil the broad goals of each consultation, yet also encourage flexibility against some of the more conservative plans for using the technologies made by the community organisation. This continued when designing how citizens would respond to the questions posed on Viewpoint, which we discuss next.
While in both studies the same system was used, the process of providing responses to questions was very different. The way in which people were invited to respond represented the types of questions our collaborators defined and, again, the stage in their consultation process they found themselves to be in. In Acorn Road, the campaign group were very clear about the form of data they required and, as noted, had a strong desire for it to look objective. It needed to carefully compliment previous surveys they had conducted, and the data was to be used in a public report handed to the local council. The campaigners had a preference for collecting detailed information using a multistage questionnaire presented on Viewpoint. Ultimately, they had to be convinced by us that this was at odds with the design intent underpinning Viewpoint-that it offered lightweight and quick engagement-and that based on previous work it was unlikely people would stand and complete a longer questionnaire in a public place. This demonstrated a tension between the insights gained from our previous research experience and their desire for it to fit in with familiar frameworks of data.
The process of determining the form of response was rather more complex in Ambit, perhaps in part due to the scoping nature of the Trust's exercise. Initially, much like the campaign group, the organisation envisaged appropriating Viewpoint in relation to traditional consultation methods. They imagined using the box as an "interactive questionnaire" (Trust representative) where passers-by could switch through different questions and respond to them through scaled answers. However, when iterating imagined answers to these questions it was thought such an approach would lead to a very restrictive set of responses. Through a series of meetings, we came to an agreement that the Viewpoint boxes should represent the local area cartographically, where respondents could simply touch those parts of the town that correspond with their response to the question. Again, however, this process was not as Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA simple as may appear-there was anxiety among the Trust that capturing just a location on a map was "not enough" and that they needed to "know more" (Trust representative). There was a desire to capture additional comments from people who provided their response to the question. Ideas suggested at this stage included supporting video or audio feedback via the cameras and microphones built into the system. These were discounted by the Trust however due to perceived privacy concerns.
The issues encountered in both of the projects around defining the ways people responded to consultation questions highlighted issues around the legibility of even relatively simple civic technologies to community groups. Furthermore, it highlights the significant agency the research team had in advising and, in some respects, pushing ideas around what the technology was for and how it would best work. This is an issue we return to in more detail in the following section.
Through the two projects, the Viewpoint boxes were deployed across seven different locations for at least twoweeks at a time. Strategic selection of locations was important in both of the projects. Prior to each of the projects, it was assumed that good places for locating these boxes would be busy places. For the campaign group there was a concern for making sure the Viewpoint boxes were placed in carefully chosen locations in and around Acorn Road where they captured "a lot of footfall" but also a broad audience of passers-by. Two of the devices were located in local supermarkets-one a branch of the country's largest retailer and the other a newly opened branch of an upmarket chain. Both shops were heavily trafficked and the location of the devices, both just past the check-outs (see Figure 1) , helped to secure a large number of votes. In the Ambit project, the Trust wished to locate one of the boxes in a newly opened seafood centre, where again it was assumed they "would get a lot of people coming" (Trust representative). Indeed, these assumptions were confirmed as the seafood centre did capture the largest number of responses overall. The other locations where the devices were deployed blurred notions of public and private. In both projects, local libraries were used as locations for a device. Libraries were chosen as a legitimate space for the type of consultation processes both groups were engaging in-indeed, historically libraries in the UK often act as venues where redevelopment plans are displayed. Unlike shops and busy tourist destinations, libraries are also almost exclusively used by local residents, which has implications in terms of demographics that can be reached. In both projects, libraries were the locations with the fewest responses, yet they were appreciated for being able to reach parts of the community busier locations might not.
Another location used in the Ambit project was a local pub. Like libraries, community pubs are almost exclusively used by local residents, and they are places that have a unique social mix, considered to contribute to social capital and a healthy community [36] . Pubs are places void of institutional influence where citizens share information, often through vernacular, rather than formal interactions, but this sharing is a by-product of the focal activity of socialisation [17, 18] . While this may appear to be an unorthodox location for these devices, it was chosen due to being embedded in a different part of the town to the other devices in Ambit. We also assumed the chatter, gossip and complaints that may occur in such spaces might be usefully harnessed for the purposes of the consultation. During the deployment, the pub received the second highest quantity of responses-this was despite being deployed for the shortest period of time. More significantly, however, votes made at the pub were dramatically different to the three other locations in Ambit. In response to the 'place you would change' question, places where factories had closed down and job losses occurred were dominant. Our interviews and observations highlighted how bringing a Viewpoint to this location engendered conversations-and that the space was formed of regulars and hangers-around meant discussions were deeper and more heated than the impromptu and fleeting conversations seen at the other places the boxes were located.
It's worth noting that in all cases gaining access to the 'right' locations was not a simple affair. In the Ambit project a key criteria for the Trust was to elicit feedback from people who were not the "usual suspects". One of the core benefits seen in using the Viewpoint boxes was that they could be situated around the town in different locations, perhaps where dwellers and passers-by might not be those who would normally interact with the Trust. However, while the Trust desired to connect with the wider community, they were limited in brokering locations where the devices could be located. Indeed, early in the project, suggested locations were primarily based on their existing social networks. While in many cases the proprietors of these venues were happy to engage, the locations were not fitting with the stated aims of the consultation.
Again, the research team found themselves conflicted in the Ambit collaboration-should we let the Trust continue in their planned process of consultation, or should we push back and direct them to using other locations? It became clear at this stage that our collaborators simply did not have the social capital within the town with which to access certain places. What then followed involved the lead author taking ownership of a small number of boxes as a seemingly neutral party, in an attempt to engage new stakeholders across the town in the project. This process of wider engagement was, in itself, an incredibly time consuming and intensive process. It literally involved the researcher walking from one end of the town, visiting venues, and if appropriate spending time in each, before approaching staff or managers to explain the project, often with positive and supportive reactions.
Overall, the locating of the devices for the Acorn Road project was a simpler process-the campaign group used their existing links with local supermarkets and the local council to have the boxes installed. However, the ease of gaining initial consent masks a variety of issue that needed to be taken into account: health and safety regulations, pressure from managers regarding the appearance of a store, and local politics. As previously discussed, the choice of location even influenced the questions that could be asked, as it became increasingly important not to alienate traders who were also hosting the devices. These were issues experienced in Ambit as well, where despite the initial good will reported above, sometimes practical and infrastructural problems in specific shops, cafes and pubs meant Viewpoints could not be installed.
A further set of issues encountered during both studies related to the expectations our collaborators set around the value and validity of the data they would be collecting, and how they were able to use this going forward. In the case of the Acorn Road deployment, there was a very clear trajectory for the organisation from the collection of the data through to presenting it to the local authority alongside data collected by traditional methods. The Viewpoint devices were unique in capturing data over longer periods of time, while expending less human resource from the campaign. That the types of data collected were comparable to one another was of huge value here. On its own the data from Viewpoint wasn't seen as entirely "legitimate", but as one feature in a set of tools it was seen to be "very useful" (Acorn Road campaigner) in gathering a wider picture of the issue at hand. This was aided all the more that many of the results were "as expected":
In particular, the campaign group felt the devices validated their intuition that more people travelled via public transport to the local area, and that the majority of shoppers were likely to be students:
This is not to say that the trial did not process some unexpected results. One of the results that did surprise the campaigners was that despite a significant number of people travelling by car, the vast majority travelled from less than three miles away: In making sense of the data, the campaign group were not only forced to readdress their assumptions, but also change the focus of their campaign-softening their plans for full pedestrianisation, to a semi-pedestrianised space that still acknowledged the need for some parking and other access.
Compared to the Acorn Road campaign, the Viewpoint data in Ambit was somewhat more challenging for the Trust to make sense of. This was, in part, a result of the more ambiguous framing of the questions and the form of response determined at the start of the project. In this case, contributions on Viewpoints located where there would likely be many tourists were interpreted, perhaps unsurprisingly, as highlighting places that were primarily visitor attractions. Locations identified on boxes in primarily residential areas-such as the pub-instead were contextualised in the history of the town and ongoing concerns around employment opportunities. The data was used to construct a narrative about the different priorities people who used Viewpoint would have. These narratives were often based on ideas of who was living in specific parts of the town, what their imagined concerns and aspirations were and why they would choose specific places to change. Because the Viewpoints had not captured the provenance of these responses, the Trust's representatives made sense of these through relying on their own prior assumptions and knowledge.
While much of the data was used to reinforce what was already 'expected' by our partners-for Acorn Road judgements were made about the mode of travel of shoppers; for Ambit this was much more determined by the Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA imagined social class and background of the people responding in a particular place-it did leave space for ambiguity and interpretation, and even the groups reassessing their interpretations.
Throughout both of the studies, the Viewpoint deployments caused a significant amount of discussion and debate. This was somewhat a novelty effect-across many of the periods spent observing Viewpoint at a distance, members of the public would be seen staring at it, asking questions of staff in venues about what it was, what it was doing here, and how to use it. The leader of the campaign group noted that "it stimulated discussion, for sure" and that people "asked what's going to be done". All of the custodians interviewed at the end of the Ambit project reported how conversations that were born from curiosity typically then opened a debate about specific issues around the places displayed on the devices. In some cases these individuals saw it as a catalyst to discuss wider issues and in turn contest future decisions: 
Custodians of the devices also explained how people that may not normally be engaged in such processes were seemingly empowered by the boxes. For instance, our custodian in the public house explained how many people she knew well, but rarely discussed politics with, were suddenly inspired to comment on recent redevelopments based on the presence of the Viewpoint box. She remarked in surprise that older, opinionated, but "technophobic" regulars were seen to participate in giving their response. Another one of the locations hosting a device in Ambit regularly runs training sessions and courses for young people. The assumptions of the custodians in this space was that such a group would be reluctant to get involved in activities beyond their own training: "But they actually took
Clearly, the hope of promoting discussion and gathering data, from our collaborators perspectives, was to inform either their own decision making, or to put pressure on and influence the decisions of others. In Acorn Road, the Viewpoint consultation was a key step in a much longer process. Combined results of the Viewpoint survey and a street survey asking similar questions were presented to the city council as a report. The group's recommendation was softened based on the results, from full pedestrianisation of the street to a one-way street with shared space for cars, bikes and pedestrians. What followed was a period of consultation and protest lasting three years. After proposals were unveiled, local businesses complained about the loss of parking spaces, leading the council to launch a consultation on two different proposals. After the more radical proposal was chosen, concerns were raised about the validity of the vote and who was able to contributeconcerns that are echoed in our own findings around Viewpoint, which on its own lacked legitimacy. Interestingly, part of this consultation took place through the city's recently-launched online consultation platform, as well as through more traditional means. Two further petitions-one from each side of the argument-led to a reopened consultation and a revised plan that retained more parking spaces. At the time of writing, work is just beginning on the site, three years after the Viewpoint data was collected. Despite Viewpoint's role being dwarfed by the scale of the process, it is notable that there were clear points where Viewpoint and other technologies empowered citizens to drive or shift the agenda. There was similar hope that the Ambit deployment would also lead to some decision being made-even if this was just to determine more focused consultations in the next round. However, the discussions promoted around Viewpoint were in many respects born from a suspicion of the local council, or of those running the consultation. For example, early in the deployment one of the custodians commented on how some people were suspicious of places "missing" from the maps on the devices:
"People asked where places were, they were concerned about as they could not identify them on the map, so they had these preconceived ideas already [of deception and mistrust] and saw this as an opportunity to say something." (Ambit custodian 4)
Fears were raised that places that were missing had already been determined as not worthy of changing. Furthermore, as time went on, the data being captured by the Viewpoint boxes themselves appeared to reaffirm these divisions within the town: different Viewpoint sites captured very different impressions of what were important places to change; these would then be viewable and made visible to people interacting with the devices; this, in turn, promoted even further discourse around mistrust and division in the town. Critically, the Trust themselves-and not just local governors-were open for critique. They were described as a "very closed group", and indistinguishable from other institutions of power in the town: "the same people's on the development trust's as is on the harbour commissioners; they've all got their finger in little pies." (Ambit custodian 4). The lack of visibility with what was happening to the data they collect, coupled with the feeling some groups are excluded from any development has left people concerned about whose interests are being served.
The community groups in our studies were sometimes discussed as an extension of the civic authority, warranting the same distrust. Despite the studies being designed to support consultation, there were many unexpected results around broader issues of division and decision-making that were equally, if not more, interesting.
Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA
Our two case studies highlight some of the challenges for researchers and community organisations engaging in the use and deployment of situated consultation technologies. Some challenges were practical and technical in naturesuch as gaining access to appropriate locations that had the required space and infrastructure for the boxes to work, but most were conceptual, social and political. In the following sections we examine some of these issues. We draw out a number of key reflections on our experiences with Viewpoint and ask questions for the HCI and civic technology communities going forward.
One issue that was apparent in both of the case studies was the important role the researcher plays as an agent and as part of the infrastructure of civic technology deployments.
There is very often a tendency for the voice of the researcher to be 'written out' in the aid of objectivity, a criticism that HCI scholarship has faced in recent years [1, 7] . In our case it would be impossible not to acknowledge how critical the research team were in shaping the work conducted. We acted as critical friends to bounce ideas off-helping our collaborators to think through the questions they wished to pose, the places they wished to pose them in, and supporting them in understanding the particular affordances offered by the boxes. On other occasions we were more direct in our guidance. This was, we felt, to ensure that they maximised the potential of the technology. However, with it we also invoked a particular stance on what we saw as the 'right' and 'wrong' way to motivate people to participate in local decision making.
The participatory research literature highlights the importance of capacity building [29] and the negotiation of power and control in community research contexts [10] . In our examples we could see how a mutual exchange of skills and expertise informally supported the development of reflective practices around setting questions and inviting responses. Control over this had to be continually negotiated however, and a balance had to be found between offering our expertise and not taking ownership of the consultation. Often community partners have more at stake and more to lose than the research team, so in our case it was not surprising they resisted some of the ideas we brought. At different moments the social capital of the researchers or the community groups was more appropriate, and understanding this dynamic was an important element in maintaining positive and successful partnerships. Greater honesty and critical reflection on these issues is therefore needed, not just in civic technologies research, but in a broader range of participatory projects in HCI where such issues might arise.
The rhetoric underpinning much of the civic technology and digital democracy literature is that digital systems can support new relationships between citizens and states (e.g. [21, 41] ), and provide new mechanisms for decision-making (e.g. [19, 31, 37, 38, 40] ). In many respects it was this rhetoric that motivated our collaborators' use of the Viewpoints. In using the devices, both hoped to come to some agreement about the issues that faced them, to determine what 'should' be done about a busy road, or where the focus 'should' be of future community projects. However, in practice the results of both case studies raised more questions than answers. In the case of Ambit there were some occasions where people at different sites found commonalities with the views seemingly expressed elsewhere. Primarily however Viewpoint provided a platform for community members to express their concerns around the ongoing regeneration of the town, and a recent history of political dissatisfaction and economic disadvantage. The maps on the devices made visible social divisions and perceived and actual inequalities within this small community. In the Acorn Road trial the questions raised by the system were perhaps less divisive but equally as complex to deal withthey required the campaigners to re-evaluate their own perceptions of the problem at hand, to soften their political stance, and to find common solutions to the very different challenges and positions of a myriad of stakeholders.
Viewpoint was somewhat predicated on the idea that technology can provide lower barriers of entry to having a say and thus support the conditions for democratic processes to occur. This simplistic view ignores how such interventions fit into the much wider, complex network of processes and actors of varying degrees of power and influence at play. It also ignores pre-existing issues around trust between different parties, an issue that has been argued to be oft-discounted in digital voting and consultation literature [33] . As noted by Harding et al. [26] , this is not just mistrust of decision-makers and authorities by certain groups of citizens, but also mistrust from certain decisionmakers as to the legitimacy and value of contributions in certain formats or from specific groups of people. In our case it was clear that Viewpoint in some cases reaffirmed these issues of mistrust. Perhaps revealing these issues could be productively channelled in the long-term, but only if technologies like Viewpoint are designed in ways to account for this bigger picture and embedded as an actor in a carefully designed process of decision-making.
As with prior work [12, 43, 47] our studies also raised questions around who owned the data generated by the Viewpoint devices, and the subtle ways ownership and power over the data were deployed by our partnering organisations. Burgess [4] warns that the ongoing appropriation of deliberative engagement by institutional authorities can often serve to legitimise policy decisions set independently of public participation. It would be unfair to claim that this was the case in our studies-there was great will and desire from both of our partners to reach into new parts of the community, to consult a wide number of people, and to use the insights gathered in a meaningful and honest manner. However, because both of the projects had Civic Tech, Participation and Society #chi4good, CHI 2016, San Jose, CA, USA very specific agendas involved it was hard for our collaborators not to let these shape the ways in which the consultations with the devices happened.
One observation here is that our desire to seek case studies where there were the results would the "actioned" (at least envisioned to be) meant our partners very carefully thought through the types of questions they would ask. While in part this was a process of ensuring they would ask good quality questions, as we saw it also involved them thinking through the potentially negative consequences of asking the wrong questions. In Ambit, the wrong question was one that would be seen to commit the Trust to spend project funds on a particular site in town; for the campaign group it was to make known their own values and opinions from the consultation. It could be argued that by choosing not to expose their ultimate intentions, the campaign group prevented citizens and local businesses from making their voices heard. We can imagine that these other stakeholders might want to collect their own data if they are in opposition to those conducting the polls, or to verify the data being collected; indeed, this was a desire and even an expectation expressed by the custodians of our devices in the Ambit project.
That the systems were deployed in clear decision-making processes also raised further questions around power, the use of citizen voice, and potentially on efficacy. Where the first generation Viewpoint device was designed with inbuilt mechanisms to support accountability and a sense of efficacy, the timescales and number of actors involved in these actionable contexts make this impossible. The slow timescales of decision-making processes and eventual outcomes are juxtaposed sharply against the quick, straightforward, lightweight interactions afforded by civic technologies, leaving a gap between engagement and action that may cause citizens to question 'what' is happening with the data. Fundamentally the length of these consultation processes means the rhetoric around feedback and voter efficacy become highly problematic.
In this paper we have reported on our experiences of working with community organisations that used our distributed, situated technologies as platforms for consulting their local communities. We have highlighted the diverse ways the Viewpoints promoted discussion and debate, facilitated the making of decisions, and exposed mistrust and contestation in the places they were deployed. In some cases this was promoted by the fact the boxes were installed in highly public locations, seen and engaged with, by a large number of people; it was also because they were distributed in a range of other locations as well, where existing practices of sociality and conviviality could be harnessed further. This is in keeping with recent work on similar systems (e.g. [28] ) that highlights the potential of these technologies to create a buzz and dialogue around local matters of concern. It also overlaps with scholarship on deliberative democracy [16, 32] , which states that political discourse are acts of everyday talk, and we should take the processes of decision-making to those sites where such talk occurs in civil society [35] .
However, it's critical to note that while we (the researchers) captured the buzz around the deployments, our collaborating community partners did not. The design of Viewpoint was such that it followed a simple framework of participation in the aid of lowering the barrier to entry. In doing so it purposely designed out the collection of "noise", privileging the idea that participation in local matters of concern can be captured at the press of a button or the tap of a map. If it were not for the performance of fieldwork around our systems then the richness and detail of conversations would be missed. While this may appear to be a moot point, it's a critical one in a context where the ambition is to create platforms that enable people to ask questions of others. In this context, it is critical to understand 'why' people say what they do, and to capture the wider discourse the questions posed provoke. Such additional layers of study would be practically difficult for our collaborators to conduct. Further, the perceived lack of objectivity of our collaborators from some of those being consulted may, in some respects, have made any such attempts meaningless. In this regard, the perceived neutrality of the researchers [43] eased people into sharing their views in a more candid manner.
One might argue that there were opportunities in our work to design in the capturing of such 'noise'. We could have invited people to give video or audio responses to questions. But beyond the privacy concerns bound up in this, there is a more poignant concern that such interactions lose the richness and discursive, dialogic element of debate.
In future work, we should perhaps look to designing systems that adapt to the conversations already taking place in society, rather than asking citizens to adapt to artificial interactions to express their views. With this comes an appreciation that it is the researcher's duty to proactively capture and convey this richness, so that community organisations-and the authorities and institutions to which they lobby-can acknowledge and use them appropriately in their decision-making processes.
We'd like to thank our community partners and the Viewpoint 'custodians' for giving their time to these projects. 

The recent widespread adoption of social network sites (SNS; boyd & Ellison, 2007) influences communication behavior in a variety of contexts, including political participation (Smith & Rainie, 2008) , identity construction (Liu, 2007) , collegiate teacher-student relationships (Mazer, Murphy, & Simonds, 2007) , and adolescent friendships (Lenhart & Madden, 2007) . Though users appropriate these sites for varied purposes, the maintenance of networked interpersonal relationships is their central attraction and function (Donath, 2007; Ellison, Steinfeld, & Lampe, 2007; Tufekci, 2008) . Accordingly, such sites are now receiving attention from interpersonal communication researchers, though a theoretical understanding of how SNS may contribute to relational closeness remains in infancy (Baym & Ledbetter, 2009) . Of the hundreds of SNS available on the Internet, Facebook is one of the most popular across a variety of demographic categories (Boyd & Ellison, 2007) . This article explores motivations toward self-disclosure and social connection as distinct yet related predictors of Facebook use within specific relationships. Though any number of specific interpersonal communication motivations might merit research attention, Facebook itself explicitly calls attention to these motivations in the site's slogan, prominently featured on the opening page: "Facebook helps you connect and share with the people in your life" (Facebook.com, 2009, emphasis added) . Facebook creator Zuckerberg (2008) acknowledged that the site's features are designed with these two motivations in mind. Separately, Ledbetter (2009b) identifies self-disclosure and social connection as fundamental motivations that foster online interpersonal communication more generally. Given decades of debate regarding interpersonal outcomes associated with online communication (for a review, see Walther & Parks, 2002) and that interpersonal communication scholars identify relational closeness as an outcome of practical and theoretical interest (Vangelisti & Caughlin, 1997) , the chief goal of this study is to test a theoretical model that elaborates how these two motivations might contribute to Facebook communication behavior (within specific interpersonal relationships) and, in turn, how such communication is associated with relational closeness.
Online communication's integration with offline social networks is seen clearly in the recent emergence of social network sites (SNSs), or "web-based services that allow individuals to (1) construct a public or semi-public profile within a bounded system, (2) articulate a list of other users with whom they share a connection, and (3) view and traverse their list of connections and those made by others within the system" (boyd & Ellison, 2007, p. 211) . Though Facebook originated in 2004 as an SNS exclusively for college student use, the site soon opened to corporate networks in early 2006 and then to the general public by the end of that year (boyd & Ellison, 2007) . As of this writing, Facebook remains one of the most popular SNSs across a variety of demographic categories (Hargittai, 2007 ). Yet before further considering the nature of interpersonal relationships on Facebook, we must address the ambiguous nature of the term friend when discussing SNS communication (boyd & Ellison, 2007) . Though colloquial and 30 Communication Research 38(1) affective components. These affective/cognitive orientations, in turn, influence behavior toward the attitude object. Given the diverse manifestations of online communication, some may question whether research can speak meaningfully about an attitude toward online communication as a whole. Without denying the value of examining attitudes toward specific technologies, a robust research tradition examines trait-like orientations toward tech nology at a more abstract level, identifying constructs such as online communication apprehension (Scott & Timmerman, 2005) , generalized problematic Internet use (Caplan, 2003) , and information reception apprehension from technology sources (Wheeless, Eddleman-Spears, Magness, & Preiss, 2005 ) that significantly predict technology use and related outcomes. This investigation follows this tradition, with the hope that such knowledge will help build theory that explains both current and future communication technologies (Sawhney, 2007) .
With this theoretical background in mind, Ledbetter (2009b) validates attitude toward online self-disclosure (OSD) and attitude toward online social connection (OSC) as two fundamental orientations influencing media-use patterns in interpersonal relationships, with similar concepts echoing in related lines of research (e.g., "disposition toward social grooming and privacy concerns, " Tufekci, 2008, p. 561) . Specifically, Ledbetter argues that these orientations address an individual's attitude toward the medium itself, which then influences both the formation and interpretation of online messages. That previous research recognizes both self-disclosure (Acquisti & Gross, 2006; Mazer et al., 2007) and social connection (Donath, 2007; Ellison et al., 2007) as core SNS behaviors further supports this line of argumentation; also, that Facebook's basic site structure aims to gratify both of these attitudinal orientations (Zuckerberg, 2008) further merits considering theoretical links between these motivations, communication behavior, and subsequent relational outcomes. We will review each of these orientations in turn.
OSD. Mazer and his colleagues (2007) provided perhaps the earliest peer-reviewed article on Facebook self-disclosure. Conceptualizing self-disclosure as "any message about the self that a person communicates to another" (Wheeless & Grotz, 1976, p. 47 ), Mazer and his colleagues identify several Facebook features that foster self-disclosure: "users post personal information such as pictures, hobbies, and messages to communicate with fellow students and instructors as well as friends and family" (p. 2). Building from Mazer and his colleagues' work and Petronio's (2002) treatment of self-disclosure as coordinating boundaries around private information, Walther and his colleagues (Walther, Van Der Heide, Kim, Westerman, & Tong, 2008) noted that self-disclosure occurs alongside information about the self provided by other users (e.g., through "wall" posts or comments on status messages). Wright and his colleagues (Wright, Craig, Cunningham, Igiel, & Ploeger, 2008) further validate the importance of self-disclosure behavior via Facebook, finding that breadth and depth of self-disclosure is associated with increased interdependence and predictability. Thus, as Facebook's own slogan claims, the site is indeed a location where users share information about the self with a proscribed set of others.
Communication researchers have long recognized the role of self-disclosure in healthy relational development (Petronio, 2002) , and Mazer et al. (2007) likewise report that Facebook self-disclosure can enhance the quality of teacher-student relationships. However, evidence from other studies of online communication suggests that generalized attraction to OSD may be associated with negative psychological and relational outcomes. Online communication scholars have long considered the antecedents and outcomes of identity formation and self presentation enacted via OSD (O'Sullivan, 2000; Turkle, 1995) , with several studies reporting that communicators often self-disclose more online than they do when face to face (Ho & McLeod, 2008; Joinson, 2001; Postmes, Spears, & Lea, 1998) . McKenna, Green, and Gleason (2002) focused on self-disclosure in online-only relationships, arguing that lack of social competence may account for heightened selfdisclosure online because those with poor social skills may prefer the greater control over communication behavior that online contexts afford:
Logically, those individuals . . . who have the social skills needed to communicate themselves well and effectively have little need to express their true selves or "Real Me" over the Internet. The rest of us should be glad that the Internet exists. . . . Thus we would expect people who are lonely or are socially anxious in traditional, faceto-face interaction settings to be likely to feel better able to express their true self over the Internet and so to develop close and meaningful relationships there. (p. 12) Thus, they argue that motivation to self-disclose online may produce beneficial relational outcomes, as online communication may provide the socially anxious with opportunities to build social skills and meaningful relationships (see also Valkenburg & Peter, 2008) .
Like McKenna et al. (2002) , Caplan (2003) agreed that poor social skills are associated with a preference for online communication (and particularly online self disclosure). Caplan (2007) identified lack of communication competence as a theoretical motivator, arguing that those with high social anxiety prefer online communication's "greater control over self presentation" and "less perceived social risk, than in traditional FtF communication" (p. 235 ). Yet Caplan (2003) challenged the claim that such use generates positive outcomes, demonstrating that preference for online communication is associated with depression, loneliness, and other negative psychosocial outcomes. Though Caplan's (2002) research initially focused on online and offline social life as separate social spheres, his recent research identifies communication competence as a more general influence on online communication behavior; in other words, Caplan (2007) does not theorize or test whether relational medium of origin or degree of multimodality influences online communication frequency. Relatedly, Spitzberg's (2006) overview of communication competence in online contexts concludes that loneliness and depression are related to online communication use in complex ways. Following these lines of theoretical development, Ledbetter (2009b) directly tested the association between OSD and generalized communication competence, finding a significant moderate inverse association between the two constructs.
To summarize, self-disclosure is an important Facebook communication behavior, and thus we would expect those with high OSD to use it more. Moreover, Caplan's (2003 Caplan's ( , 2007 OSC. In contrast to OSD, we argue that maintaining existing social connections (i.e., OSC) is a relationally healthier motivation for using online communication. Ledbetter (2009b) reports that both OSC and OSD exhibit similar patterns of association with online communication behavior, yet differ in their association with generalized communication competence: Though OSD is inversely associated with communication competence, OSC yields a positive association of nearly equivalent magnitude. This may suggest that communicatively competent people do not seek online communication because they wish to avoid discomfort attendant with face-to-face communication, but rather because they perceive online communication as a useful method for sustaining preexisting weak and strong social ties (Haythornthwaite, 2005) .
Other research supports our assertion that OSC is associated with positive relational outcomes. When countering claims that online communication (i.e., more generally than just SNS use) produces negative relational outcomes (Kraut et al., 1998; Nie et al., 2002) , scholars frequently provide empirical evidence demonstrating beneficial outcomes for the strength of both local and long distance social ties (Baym, Zhang, & Lin, 2004; QuanHaase, Wellman, Witte, & Hampton, 2002) . That SNSs likewise maintain social networks may sound tautological; nevertheless, recent research elaborates mechanisms via which SNSs foster such connections. For example, Ellison and her colleagues (2007) reported that Facebook social connections develop several types of social capital, Stern and Taylor (2007) reported that college students use Facebook to maintain social connections developed on campus and with old friends, and Baym and Ledbetter (2009) suggested that shared interests may motivate the formation of some SNS relationships.
In addition to Ledbetter (2009b) , other empirical evidence suggests that internal attitudinal factors influence attraction to online communication as a space for building social connections. Both Donath (2007) and Tufekci (2008) conceptualized SNS use as analogous to social grooming among primates (Dunbar, 1998) , advancing the claim that resources devoted to regular, brief contacts facilitate relational ties with other individuals in a social network. Tufekci noted that this desire for social grooming varies in magnitude across individuals, with some people valuing such behaviors and others considering them unnecessary; in Tufekci's study, those who generally desire social grooming were also more likely to use an SNS. Donath claims that this motivation arises from the nature of SNSs as "more temporally efficient and cognitively effective" for the purpose of "maintaining ties" (p. 231). Donath noted that this increased efficiency may facilitate formation of social supernets or social networks that are larger than those sustainable through other communication media. This line of theoretical development resonates with Parks' (2006) recent argument that all dyadic relationships are intimately constituted in webs of network ties, with individuals sustaining ties using several communication media (Sawhney, 2007; Walther & Parks, 2002) . Taken as a whole, then, this research indicates that many people use SNSs because they wish to maintain existing social ties and that this motivation, in contrast to OSD, is associated with positive relational and psychosocial outcomes. What remains less clear, however, is the extent to which OSC is associated with offline communication between Facebook Friends. Some research suggests that those who engage in social networking behavior when online are also likely to do so when communicating offline (Quan-Haase et al., 2002; Tufekci, 2008 We also expect a significantly positive association between Facebook communication frequency and offline communication frequency. Though media multiplexity theory (Haythornthwaite, 2005) suggests that tie strength is a moderator (i.e., such that strong ties communicate across many media whereas weak ties use fewer media), most studies find a significantly positive association between offline and online frequency with a specific relational partner (Baym et al., 2004; Ramirez & Broneck, 2009 
Thus far, we have considered relational outcomes associated with Facebook communication but have not specified these in testable hypotheses. In this investigation, relational closeness is our chief outcome of interest, as Vangelisti and Caughlin (1997) noted that relational closeness is a variable of interest in a wide variety of relationship types (including friendship, family, and romantic relationships). Though we acknowledge that closeness is not the only possible relational outcome worthy of investigation, it is also worth acknowledging that close relationships are important sources of social support (Burleson & MacGeorge, 2002) and that ongoing closeness promotes relational longevity (Ledbetter, Griffin, & Sparks, 2007) . Closeness has also received attention as an outcome variable associated with several forms of online communication behavior across diverse samples, including online relational maintenance (among U.S. college students; Ledbetter, 2009a) , duration of Internet use (among Israeli adolescents; Mesch & Talmud, 2006) , and both frequency of online communication and depth of online self-disclosure (among Dutch adolescents; Valkenburg & Peter, 2007) . Though scholars have not devoted as much attention to closeness across SNSs, Baym and Ledbetter (2009) reported that though relational quality (a variable conceptually similar to closeness) with SNS Friends tends to be low, frequency of SNS contact between Friends is positively associated with relational quality (even after controlling for contact across other media). In this study, we conceptualize 34
closeness as a subjective experience of intimacy, emotional affinity, and psychological bonding with another person (see Aron, Mashek, & Aron, 2004) ; given the foregoing literature, we predict that frequency of Facebook communication will uniquely and positively predict Friend closeness. Our conceptualization of closeness bears strong resemblance to Haythornthwaite's approach to strong and weak social ties in her theory of media multiplexity. Strong social ties include relationships such as those with friends, romantic partners, and family members; such relationships exhibit behavior that reflects emotionality, interdependence, and intimacy (i.e., a high level of closeness). By contrast, weak ties are "casual contacts" that are more loosely connected to an individual's social network and are not characterized by intimacy (Haythornthwaite, 2005, p. 128) . According to media multiplexity theory, the number of different communication media that dyad members use is strongly associated with whether a tie is weak or strong. Specifically, strong ties employ several media types, but weak ties use only one or two media. As Baym and Ledbetter (2009) As we argued earlier, previous research and theory (e.g., Caplan, 2007) suggests that online communication motivated by OSD is associated with negative outcomes. Nevertheless, some interpersonal communication theory (Altman & Taylor, 1973) and empirical research (Laurenceau, Barrett, & Pietromonaco, 1998) suggests that self-disclosure is positively associated with relational closeness. Thus, the computer-mediated communication and traditional interpersonal communication literatures offer divergent predictions regarding this association. Thus, we advance a research question:
Research Question 1: Does OSD indirectly (i.e., mediated via Facebook and offline communication constructs) predict relational closeness (with specific Facebook Friends)?
Though earlier online research characterizes online communication as reducing a sense of social connection, work countering this claim demonstrates that those who build social connections offline also tend to do so online and, consequently, experience positive relational outcomes (e.g., Quan-Haase et al., 2002 That extant literature suggests divergent outcomes from OSC and OSD implies that these motivations are inversely associated with each other. However, previous research reports a positive association between the two constructs (Ledbetter, 2009b) ; as such, it is theoretically unclear what outcomes arise from an individual who possesses high levels of both motivations. Following Caplan (2007) , one might speculate that problematic Internet use driven by OSD would reduce beneficial outcomes from increased social connections. Alternatively, following theoretical arguments that online social ties may enhance the social skills of the lonely and socially anxious (McKenna et al., 2002; Valkenburg & Peter, 2008) , it could stand to reason that OSC is associated with positive social outcomes regardless of an individual's level of OSD. In any case, the extant literature at least suggests the possibility of a meaningful interaction effect between these two constructs on online communication and relational closeness, though the available evidence does not permit a prediction of the nature of this association in advance. Thus, 

In order to capture a diverse sample of Facebook users, we recruited participants via three approaches. First, with the consent of the computing services department at a large Midwestern university, a random sample was drawn from the list of all students enrolled in undergraduate courses. Second, other participants were recruited through announcements on the Facebook pages of various members of the research team. Third, we posted a call for participants on the listserv of a professional organization interested in technology and communication. After discarding participants that indicated no Facebook usage (n = 27), these sampling techniques resulted in a group of 325 participants (75 men, 250 women) with 226 (69.5%) identifying themselves as undergraduate students. Participants' age ranged from 18 to 59 years (M = 23.4, SD = 6.0), and most participants (90.5%) reported their ethnic identity as White.

Recruitment procedures directed participants to a Web link containing an informed consent form, and upon acceptance, participants were taken to the secure online questionnaire. If the participants were Facebook users, the questionnaire instructed them to open their Facebook account in a separate window and load their profile. At the time of data collection (early 2008), Facebook profiles included a box at the left side of the screen that displayed Friends selected from a person's primary network. Although Facebook has not publicly discussed the algorithm behind Friend selection for this window, tests of the feature at the time seemed to indicate that Friend selection was at least pseudorandom (although it is worth noting that Facebook's recent site redesign seems to have altered this algorithm since data collection). This method of Friend selection was designed to move beyond the practice of participant friend selection common in friendship research (e.g., Johnson, Wittenberg, Villagran, Mazur, & Villagran, 2003; Ledbetter, 2009a) . The survey directed participants to complete several measures based on the first Friend who appeared in this box. At the end of the survey, participants had the option of entering their e-mail addresses for a chance to win one of four US$20 gift certificates from Amazon.com. These e-mail addresses were removed from the data set before analysis.
Online communication attitude. The self-disclosure and social connection subscales of Ledbetter's (2009b) generalized measure of online communication attitude assessed OSD and OSC, respectively. The self-disclosure subscale contains 7 items: "I feel less nervous when sharing personal information online"; "I feel like I can be more open when I am communicating online"; "I feel like I can sometimes be more personal during Internet conversations"; "When online, I feel more comfortable disclosing personal information to a member of the opposite sex"; "I feel less shy when I am communicating online"; "I feel less embarrassed sharing personal information with another person online"; and "It is easier to disclose personal information online." The social connection subscale contains 6 items: (Wheeless et al., 2005) . Participants responded on a 7-point Likert-type scale with response options ranging from 1 (strongly disagree) to 7 (strongly agree). Cronbach's alpha reliability was acceptable for both the OSD (.92) and OSC (.87) dimensions. Friend demographic information. Participants reported basic demographic information about the randomly chosen Friend. Most reported that their Friend was a member of the participant's sex (n = 193, 59.4%), though others reported on cross-sex relationships (n = 132, 40.6%). Age of the Friend ranged from 17 to 60 (M = 22.8, SD = 5.2), with length of relationship ranging from 1 month to 43 years (M = 4.3 years, SD = 5.2). Most participants reported that their Facebook Friend was, indeed, a friend (n = 204, 62.8%) or an acquaintance (n = 73, 22.5%), though a small number reported on a romantic partner (n = 11, 3.4%), a family member (n = 6, 1.8%) or did not specify the type of relationship (n = 31, 9.5%). Though most participants reported on local relationships (n = 221, 68.0%), some reported on long-distance relationships (n = 104, 32.0%).
Facebook communication. Informed by Lenhart and Madden's (2007) description of the methods of communication possible within Facebook, a 6-point Likert-type scale assessed frequency of Facebook communication with the Friend. This measure contains 7 items: "I write on my friend's wall," "I send my friend a private message," "I communicate with the friend in a Facebook group," "I 'poke' my friend," "I comment on one of my friend's photographs," "I comment on one of my friend's notes," and "I communicate with the friend through an application on Facebook." Participants responded on a 6-point Likert-type scale with response options ranging from 0 (never) to 5 (very frequently). Following Baym and Ledbetter's (2009) evidence that communication frequency on another SNS (Last.fm) exhibits unidimensional structure, we submitted all items to an exploratory factor analysis using the principal components extraction method with varimax (i.e., orthogonal) rotation. Using the criterion of eigenvalue >1.0 produced a unidimensional solution with all items loading above 0.60 (McCroskey & Young, 1979) . The 7 items also demonstrated strong internal reliability (a = .87), and thus were averaged to form a single measure of Facebook communication frequency with the Friend.
Offline communication. Several theorists in the field of computer-mediated communication urge examination of online communication alongside offline communication media (Baym et al., 2004; Sawhney, 2007; Walther & Parks, 2002) . Following this line of theoretical development, Ledbetter (2009b) factor-analyzed media use via a 6-point Likerttype scale structure (0 = never to 6 = very frequently) adopted from Scott and Timmerman (2005) , finding that face-to-face and telephone communication load onto the same factor of offline media use. We used the same instrument in this study to measure frequency of offline communication with the Friend, with an additional item measuring cellular-phone text messaging. These 3 items demonstrated good internal reliability (a = .85), and thus were treated as separate manifest indicators of a single latent construct in the confirmatory and structural models.
Relational closeness. Vangelisti and Caughlin's (1997) 7-item measure assessed relational closeness with the Facebook Friend. Sample items include the following: "How often do you talk about personal things with this person?" and "How close are you to this person?" Participants responded on a 7-point Likert-type scale with response options ranging from 1 (not at all) to 7 (very much). The measure demonstrated strong internal reliability (a = .93).
All hypotheses and research questions were addressed via structural equation modeling (SEM) using the LISREL 8.80 for Windows software package. Two chief advantages of SEM are holistic assessment of an a priori specified model, which is clearly advantageous for the model specified in this study ( Figure 1 ); in addition, SEM corrects for error variance and thus more accurately identifies parameters of interest. We assessed model fit using four frequently reported fit indices: (1) model chi-square, (2) the root mean square error of approximation (RMSEA), (3) the non-normed fit index (NNFI), and (4) the comparative fit index (CFI; Kline, 2005) . For the RMSEA statistic, lower values indicate better model fit, with 0.08 the traditional threshold for acceptable fit (and 0.05 for close fit). For the NNFI and CFI statistics, better fitting models achieve higher values, with 0.90 and 0.95 as traditional thresholds for acceptable and close model fit, respectively (Kline, 2005) .
As shown in Figure 1 , the hypothesized model contained 6 latent constructs: (1) attitude toward online self-disclosure (i.e., OSD), (2) attitude toward online social connection (i.e., OSC), (3) an interaction term for OSD and OSC, (4) Facebook communication frequency, (5) offline communication frequency, and (6) relational closeness. The OSD, OSC, Facebook communication, and relational closeness constructs were identified by creating three parcels ("aggregate-level [indicators] comprised of the sum (or average) of two or more items, responses, or behaviors"; Little, Cunningham, Shahar, & Widaman, 2002, p. 152) per latent construct. Given the unidimensional nature of these constructs, items were assigned to parcels by thirds (e.g., for the 6-item OSD measure, the first parcel contained Items 1 and 4, the second parcel contained Items 2 and 5, and the third parcel contained Items 3 and 6). Offline communication was identified by single-item indicators of face-to-face, telephone, and text messaging communication. The interaction effect was modeled by creating an orthogonalized interaction term, a method that more effectively removes multicollinearity than Baron and Kenny's (1986) method of mean-centering predictors prior to computing the interaction term. As described by Little, Card, Bovaird, Preacher, and Crandall (2007) , this necessitates forming a series of nine product terms between the mean-centered parcels for each construct (i.e., all possible multiplicative interactions between one of the three OSD parcels and one of the three OSC parcels). These product terms were then regressed onto the firstorder parcels, and their unstandardized residuals were saved. These unstandardized residuals were then combined into three parcels such that each interaction-term parcel contains only one instance of each of the first-order parcels (see Marsh et al., 2007) , resulting in indicators that are entirely orthogonal to the first-order indicators (e.g., Soliz & Harwood, 2006) . Table 1 presents the correlation matrix between the continuous study variables at the manifest level of measurement. Before latent variable analyses, an EM (expectation-maximization) algorithm imputed the trivial amount of missing data (less than 1%) in the data set (Vriens & Melton, 2002) . Consistent with standard two-step procedures for SEM (Kline, 2005) , confirmatory factor analysis (CFA) first evaluated the fit between the manifest indicators and their respective latent constructs. To evaluate potential covariates, a series of three metric invariance tests (Little, 1997) compared (1) male and female participants, (2) undergraduatestudent status (i.e., undergraduate versus nonundergraduate participants), and (3) local and long-distance friends. Specifically, this procedure invokes a sequential series of model constraints that evaluate equality of indicator loadings (i.e., weak metric invariance), equality of indicator means (i.e., strong metric invariance), and homogeneity of the variance/covariance matrix among latent constructs. These tests indicated both weak and strong metric invariance Thus, any apparent differences between groups are likely due to chance variation, and thus all groups should be analyzed in a single structural model (Ledbetter, 2009a) To further probe the nature of the association between the two components of online communication attitude and Facebook communication, the interaction effect was decomposed using the method described by Cohen, Cohen, West, and Aiken (2003) . To do this, we recomputed the structural model as a mean and covariance structures (MACS) model. As Kline noted, standard SEM lacks a mean structure (i.e., all latent variables are assumed to be standardized with a mean of 0), and thus information about means is lost. A mean structure is added to a structural model "by regressing exogenous or endogenous variables on a constant that equals 1.0" (2005, p. 287) . From the standpoint of regression analysis, this essentially adds intercept terms to both the manifest and latent variables. By identifying the model and mean structure via the contrast coding method described by Little, Slegers, and Card (2005) , we obtained intercepts and predicted values that reflect the original measurement metric of the manifest indicators, thus aiding interpretation of the interaction effect decomposition. Using these values to generate linear regression equations, we plotted the relationship between OSC and Facebook communication at three different levels of OSD (i.e., at the minimum value of 1, at the latent mean value of 3.63, and at the maximum value of 7). Figure 3 presents results of this decomposition.

Though OSC positively predicts Facebook communication when OSD is low, increased levels of OSD weaken the strength of this association. Specifically, OSC significantly predicts Facebook communication at both the minimum, B = 0.29 (95% CI = 0.15-0.44), b = .39 (95% CI = 0.20-0.58, p < .01), and mean, B = 0.17 (95% CI = 0.09-0.26), b = .23 (95% CI = 0.11-0.34, p < .01), levels of OSD, but the association is nonsignificant at a maximum OSD score: B = 0.02 (95% CI = -0.15 to 0.19) and b = .02 (95% CI = -0.20 to 0.25, p > .05). Examination of the graph indicates that the regression lines converge at an OSC value between the minimum and the mean. Solving the regression equations for this point of convergence reveals that it occurs when an individual's OSC score is 1.88. In other words, when an individual's OSC is slightly below a mean response of 2 (i.e., disagree), that individual's Facebook communication with a specific Friend will tend to be 0.77 (i.e., slightly below a mean response of 1, or very rarely) regardless of that individual's level of OSD. Taken as a whole, these results suggest that OSD has a moderate inverse association with Facebook communication when OSC is high, and OSC has a moderate positive association with Facebook communication when OSD is low. At low levels of OSC or high levels of OSD, the effect of the other independent variable becomes much weaker. 42
The initially hypothesized model also predicted that both offline and Facebook communication are positively associated with relational closeness. When controlling for the significantly positive covariance between these two latent constructs, Y = .72 (95% CI = 0.65-0.79, p < .01), both offline communication, B = 1.30 (95% CI = 1.02-1.57) and b = .70 (95% CI = 0.55-0.85, p < .01), and Facebook communication, B = 0.33 (95% CI = 0.13-0.53) and β = .18 (95% CI = 0.07-0.30, p < .01), emerged as separate predictors of relational closeness. Offline communication appeared to be a much stronger predictor than Facebook communication, and thus we formally tested the significance of this difference by creating a nested model with the relevant regression paths constrained to equality; this produced a significant decline in model fit, Dc 2 (1) = 24.28, p < .01, demonstrating that offline communication is indeed a stronger predictor of closeness.
In addition to direct effects on relational closeness, the model also leaves the possibility that online communication attitude (i.e., OSC and OSD) indirectly predicts relational closeness via Facebook communication. This possibility was tested via Preacher and Hayes's (2004) procedure for generating robust estimates of unstandardized regression weights with nonparametric bootstrapping, a technique in which "cases from the original data file are randomly selected with replacement to generate other data sets, usually with the same number of cases as the original" (Kline, 2005, p. 42) . After computing the structural model across these data sets, the unstandardized regression weight is defined as the mean of the products of the indirect path's component parameter estimates; statistical significance is then determined by (a) sorting these estimates in ascending order, and (b) when a = .05 and k represents the number of bootstrapped samples, obtaining the values that appear at .025 × k and .975 × k in the ordered list. These represent the boundaries of the confidence interval; if this interval does not contain zero, then the bootstrapped estimate is statistically significant. As bootstrapping does not assume normal distribution of unstandardized regression weights, the boundaries of the confidence interval are not necessarily symmetrical around the estimate. Standardized regression weight estimates were obtained via the covariance matrix of latent constructs from computation of a model based on bootstrapped estimates of the covariance matrix of manifest indicators.
Bootstrap analyses revealed that OSC, B = 0.08 (95% CI = 0.01-0.15) and b = .04 (95% CI = 0.01-0.08, p < .05); OSD, B = -0.04 (95% CI = -0.10 to -0.001) and b = -.02 (95% CI = -0.06 to -0.001, p < .05); and the interaction effect, B = -0.03 (95% CI = -0.07 to -0.001) and b = -.02 (95% CI = -0.04 to -0.001, p < .05), significantly and indirectly predicted relational closeness. As the contrast coding method of identification is not amenable to bootstrapping in LISREL, we could not decompose the interaction effect in the metric of the original variables. Rather, we conducted decomposition using information from bootstrapped models with latent construct variance fixed to 1.0, thus expressing the interaction effect in terms of construct standard deviations. The pattern of results from this decomposition was almost identical to the decomposition for Facebook communication (Figure 3) , such that OSC is a significant positive predictor of relational closeness at low, that is, two standard deviations below the mean, B = 0.14 (95% CI = 0.04-0.23) and b = .07 (95% CI = 0.02-0.13, p < .01), and mean, B = 0.08 (95% CI = 0.01-0.15) and b = .04 (95% CI = 0.01-0.07, p < .05), levels of OSD but not when OSD is high, that is, two standard deviations above the mean, B = 0.01 (95% CI = -0.08-0.11) and b = .01 (95% CI = -0.04-0.06, p > .05). Together, the direct and indirect effects explained 70.7% of the variance in relational closeness.
The overarching goal of this investigation was to test a theoretical model whereby trait-like attitudes toward online communication predict Facebook and offline communication, with these constructs then predicting relational closeness. Results generally supported the hypothesized model, with the exception of the speculated paths between online communication attitude and offline communication. More important, OSD functioned somewhat differently than predicted by some previous online communication research (e.g., Caplan, 2007; McKenna et al., 2002) , not only yielding an inverse main association with Facebook communication but also reducing the positive contribution of OSC to this dependent variable and, indirectly, to relational closeness. Taken as a whole, these results support media multiplexity theory (Haythornthwaite, 2005) yet suggest that the theoretical expectation that social anxiety fosters online communication (Caplan, 2007) 
One of Facebook's core functions is building connections within a social network (Zuckerberg, 2008) , and, as expected (Hypothesis 3), those who use online communication for that purpose (i.e., possess high OSC) are more likely to communicate with their Facebook Friends (Tufekci, 2008) . As the OSC variable addresses orientation toward a preexisting social network rather than just a dyad, this calls attention to the need to understand broader network-level forces when examining dyadic relationships. In other words, traditional interpersonal communication theory considers closeness as an outcome of dyad-and individual-level variables, whereas OSC is an individual-level variable that may bespeak group-and network-level realities. Though network forces no doubt operate offline as well (Parks, 2006) , Donath (2007) argues that SNSs facilitate creation of social supernets, or social networks, "with many more ties than is feasible without socially assistive tools" (p. 231); this may only augment group-and network-level effects on specific dyads. The social-relationships model (Kenny, Kashy, & Cook, 2006) permits statistical isolation of individual, dyadic, and group effects and thus may be an invaluable tool for identifying which effects are truly individual, truly unique to the dyad, or truly a reflection of broader social forces.
Similarly, drawing from previous research and theory indicating that social anxiety produces attraction to OSD (e.g., Caplan, 2007; McKenna et al., 2002) , we predicted that OSD would positively predict Facebook communication with a specific Friend (Hypothesis 1). Instead, OSD inversely predicted Facebook communication in the final model. This differs both from the positive zero-order association with SNS use reported in Ledbetter (2009b) and the nonsignificant zero-order association reported here (see Table 1 ). This suggests that, when examined in the context of a structural model that controls for the shared variance between Facebook and offline communication, OSD may not foster Facebook use as it does other forms of online communication. Interpreting this unexpected finding requires reconsidering the theoretical mechanisms that underlie the association between preference for OSD and online communication use. One approach is to consider the role of moderating variables. McKenna et al. examined how individuals selfdisclose within completely online relationships, whereas most Facebook friendships exist between individuals who also know each other offline ; thus, considering the moderating influence of a relationship's medium of origin (as well as current degree of multimodality) in future research might further explain this unexpected finding.
Alternatively (yet not necessarily in contradiction), as Caplan notes, the logic of the expectation that OSD positively predicts communication frequency rests in the communicator's desire to manage self-presentation and identity:
In almost all social interactions, people are motivated to engage in strategic self presentation and identity management and to avoid making undesired impressions on others. Social anxiety arises from the desire to create a positive impression of one's self in others along with a lack of self-presentational confidence. Most importantly . . . the self-presentational theory of social anxiety posits that, in order to increase their perceived self-presentational efficacy, socially anxious individuals are highly motivated to seek low-risk communicative encounters. (p. 235) Traditional forms of online communication (e.g., e-mail) provide such low-risk encounters, as the private and/or asynchronous nature of the communication medium permits almost complete control over self-presentation (Walther, 1996) . Yet Donath (2007 ) argued (and Tong, Van Der Heide, Langwell, & Walther, 2008 , empirically confirm) that users of an SNS are evaluated, in part, in terms of the nature of their social connections with others; thus, it stands to reason that identity management partially lies within the control of social network members (and outside the control of the individual). Recent empirical evidence supports this theoretical claim, finding that wall posts written by friends and the physical attractiveness of those friends influences perception of a Facebook profile's owner . Walther and his colleagues explained these findings in terms of the information's level of warrant, or "degree to which that information is perceived to be immune to manipulation from the target to whom the information pertains" (p. 32); a wall post by a Facebook Friend is an example of such high-warrant information. Because the information target cannot favorably alter that information, Walther, Van Der Heide, Hamel, and Shulman (2009) argue that others perceive that information as more trustworthy than low-warrant information. Recent empirical evidence supports this expectation. Taken together with Caplan's findings (2007) and results of the current study, it is possible that those who are socially anxious may prefer traditional forms of online communication because they wish to control their own self-presentation by avoiding high-warrant information. As Facebook's site design encourages proliferation of high-warrant information (Zuckerberg, 2006) , those with high OSD may avoid it in favor of other low-warrant forms of online communication.
This line of argument is further supported by decomposition of the interaction effect between OSD and OSC on Facebook communication (Research Question 1). Though OSC is a positive predictor of Facebook communication when OSD is low, this association is nonsignificant at high levels of OSD. That is, high levels of OSD tend to weaken the association between OSC and Facebook communication. As noted in the theoretical warrant, such a finding supports Caplan's argument that OSD is socially debilitating, perhaps reducing beneficial outcomes that might otherwise accrue from the desire to maintain preexisting relationships online. In other words, if preference for OSD does reflect a desire for greater control over self-presentation (Caplan, 2007) , such a motivation may override a person's desire to build online social connections. Following Walther and his colleagues' (2008) recent research, perhaps those who possess both high OSC and high OSD seek out forms of online communication that do not provide high-warrant information. In terms of theoretical development, this suggests that social anxiety is not necessarily associated with online communication as a whole but rather encourages use of media that lack high-warrant information, online or otherwise. Testing this theoretical claim via experiment is a clear direction for future research.
The expectations that self-disclosure (Hypothesis 2), social connection (Hypothesis 4), and the interaction between them (Research Question 3) would predict offline 46
communication were not supported in the final model. Though previous research reports significant zero-order associations between these constructs and face-to-face communication frequency (Ledbetter, 2009b) , a significant association did not emerge when modeling offline communication as a latent construct and controlling for variance shared with Facebook communication. Perhaps other structural/contextual variables (such as temporal ability to synchronize schedules for offline contact; Ling & Yttri, 2002) influence frequency of offline communication with specific Facebook Friends, and thus the hypothesized associations did not emerge.
As predicted by media multiplexity theory (Haythornthwaite, 2005) , both offline communication (Hypothesis 6) and Facebook communication (Hypothesis 7) positively predicted relational closeness. This replicates the pattern of results obtained by Baym and Ledbetter's (2009) study of Last.fm, a music-oriented SNS, suggesting that the predictions of media multiplexity theory apply across many types of SNSs. What remains unanswered is whether use of multiple SNSs with the same friend also additively contributes to relational outcomes; indeed, we are not aware of any study that examines SNS use as a multimodal phenomenon. But if researchers cannot fully understand online communication use apart from patterns of offline communication behavior (Baym et al., 2004 , and as our final model indicates), then one might expect that continually examining single SNSs in isolation may yield an incomplete theoretical picture of their role in interpersonal relationships. The measures employed here offer at least some of the tools necessary for such future research. It is important to note that our Facebook communication scale was developed and used in the present study before some recent alterations were made to Facebook's status message feature; when these data were collected, the status-message feature forced users to include the word "is" (e.g., "John is tired" was possible, whereas "John stayed up too late last night" was not), and the feature did not include the ability to attach direct comments to a status message. Even though the instrument demonstrated strong internal reliability and unidimensional structure (as did Baym & Ledbetter's measure of Last.fm communication), scholars should consider the status message as a possible scale item in future investigations, as well as any other new forms of communication developed on such a continuously evolving website.
Both dimensions of online communication attitude and the interaction effect between them produced significant indirect effects on relational closeness. OSD inversely predicted relational closeness (thus answering Research Question 1), and OSC emerged as a positive predictor (Hypothesis 8). This pattern of results supports our chief contention that OSC is a healthy, communicatively competent motivation for using online communication; however, motivation arising from OSD is associated with negative relational outcomes (Ledbetter, 2009b) . As such, this investigation provides empirical evidence consistent with the theoretical expectation that attraction to OSD produces not only negative psychosocial outcomes but also negative relational outcomes (Caplan, 2003) . These results are also consistent with the finding that Facebook communication better supports and facilitates the concept of bridging (versus bonding) of social capital . As with decomposition of the interaction effect's association with Facebook communication, OSD and OSC interact in such a way that high OSD reduces the positive indirect association between OSC and relational closeness to nonsignificance. This is consistent with media multiplexity theory (Haythornthwaite, 2005) as, to the extent that high OSD reduces OSC's association with Facebook communication, the theory suggests that losses in closeness would occur unless dyad members compensate with the addition of another medium. Such an interpretation also follows Haythornthwaite's finding that different social networks enact different hierarchies of media use.
The direct and indirect effects in the model explained a large amount of the variance in relational closeness (approximately 71%). Along with other recent empirical evidence (Baym & Ledbetter, 2009; Ledbetter, 2009c) , this suggests that media multiplexity is a parsimonious yet robust account of how media use is associated with strength of a relational tie. In turn, this further supports the importance of studying individual attitudinal factors that may foster or inhibit use of particular communication media. On a more practical level, these results refute some popular claims that SNSs reduce relational closeness (Henry, 2007; Tilsner, 2008) , as Facebook communication positively predicted relational closeness even when controlling for the contribution of offline communication. However, this finding must be interpreted in light of the significantly stronger association between offline communication and relational closeness (perhaps reflecting that relational maintenance is more temporally efficient via media with multiple nonverbal cues; Walther, 1996) .
Of course, any study must be interpreted within the limitations imposed by the research design. Though it is tempting to make causal inferences from analytic methods that model endogenous and exogenous variables, the cross-sectional nature of the data necessitates caution. Future longitudinal research might test the extent to which closeness predicts communication frequency or vice versa. Though a particular strength of the study is the inclusion of data beyond a college-student sample and establishment of metric invariance across groups, the sample is relatively homogeneous regarding racial and ethnic identity. Future research may consider cultural dimensions such as individualism and collectivism that have demonstrated associations with online communication in previous research (Lee & Choi, 2005; Zhang, Lowry, Zhou, & Fu, 2007) . Our sample also contained more women than men, even though our recruitment procedures were not sex specific in any respect. We do not possess an explanation for why more women completed our questionnaire, and nonsignificant metric invariance tests suggest this probably does not influence study results greatly. The study also explained a relatively small amount of variance in Facebook communication; this is perhaps to be expected when global trait-like constructs predict variables located within specific relational contexts. Future research might address this by more explicitly examining the degree of variance that exists within individual SNS networks; dyadic data analyses (Kenny et al., 2006) may also yield higher effect sizes by accounting for the attitudes of both friends.
It is also worth noting that this investigation did not directly measure participant social anxiety. Though multiple studies establish preference for OSD as positively associated with social anxiety and related constructs (Caplan, 2007; Ho & McLeod, 2008; Kelly & Keaten, 2007; McKenna et al., 2002; Morahan-Martin & Schumacher, 2003; Valkenburg & Peter, 2008 ) and thus warrants use as an interpretive heuristic in this investigation, it remains possible that other forces foster a positive attitude toward OSD, such as the desire to create a sense of relational immediacy (e.g., in teacher-student relationships; Mazer et al., 2007) or finding others who share rare or stigmatized conditions (Walther & Boyd, 2002) . To the extent that social anxiety is not perfectly associated with OSD, it remains possible that OSD is positively associated with relational outcomes if shared variance with social anxiety were controlled. Of course, verifying this speculation requires further empirical investigation.
Given their widespread proliferation and adoption, especially among younger users (Lenhart & Madden, 2007) , it stands to reason that SNSs will remain an important medium for maintaining social connections. The existence of these sites raises important questions regarding individual traits that might influence online communication frequency and the integration of dyads into larger social structures (Parks, 2006) . These results inform these broader projects by further identifying attitude toward online self-disclosure and social connection as two such traits that may produce divergent effects on both media use and, to some degree, subsequent outcomes in interpersonal relationships.
In this paper we describe a very simple, distributed mutual exclusion protocol by which a process can gain the right to execute for a fixed time interval A without interference from other processes. Our protocol is directly inspired by backoff protocols for multiple access channels; the collision detection protocol of Ethernet is the most well-known example. In Ethernet, a process wishing to send on an (apparently) empty channel simply does so. If it detects that its send collided with another process', it "backs off" for a random delay and tries again later. Here we use similar principles to derive a mutual exclusion algorithm for synchronous message-passing systems that is deterministically safe and that ensures entry to the "critical section'.' with probability one.
The performance of our protocol can generally be characterized in terms of amortized system response time [ 5 ] .
Bell Labs, Lucent Technologies 600 Mountain Ave., Murray Hill, NJ 07974 USA reiter @ research.bel1-labs.com
The amortized system response time is the mean delay that each o f t processes incurs before entering the critical section, assuming that all t (and no others) start contending at the same time. We prove an upper bound on the expected amortized system response time of O ( A t ) , thereby showing that our protocol is adaptive in that the amortized system response time is independent of the maximum number of processes that might contend. In addition, in the case of no contention, the delay a process incurs before entering the critical section is merely one round-trip message delay on the network, and thus is independent of A.
Fault tolerance is a feature of our protocol. We present our protocol in a system model with distinct clients and servers, motivated by the system in which we have implemented it, described later. Clients, which contend for mutual exclusion, may crash without affecting the protocol. In particular, since a client is granted exclusion for a fixed time period A-and there is no designated "unlock" operation that a client must perform-a client's failure after it succeeds in gaining exclusion does not preclude other clients from subsequently gaining exclusion after the A time period expires. Moreover, our protocol masks the arbitrary (Byzantine) failure of a threshold number of servers.
We use this mutual exclusion protocol to develop a protocol by which operations on a replicated object can be serially ordered. Despite the fact that this ordering protocol is deterministically safe even in an asynchronous systemand our mutual exclusion protocol is not-the mutual exclusion protocol is key to ensuring that operations are ordered and complete (with probability one) once the system stabilizes. Our ordering protocol orders arbitrarily many operations on the object as soon as a single contender gains access to the critical section.
Aside from always-safe operation ordering, we have found our mutual exclusion protocol useful for other tasks within the system that motivated it, called Fleet [ 161. Fleet supports highly available, shared data for clients using an infrastructure of servers that may suffer arbitrary (Byzantine) failures. In order to detect the presence of faulty servers, statistical fault detection algorithms mine for evidence of faulty servers in the responses they return [I] . Since detection is most accurate when data is accessed sequentially, Fleet attempts to serialize data accesses, and we employ the mutual exclusion protocol described here for this purpose. Our mutual exclusion protocol has the useful property that it remains probabilistically live even during periods of instability (asynchrony) in the system.' So, while fault detection may suffer during periods of instabiliity, the nonblocking properties of the Fleet data access protocols are never compromised.
The rest of this paper is structured as follows. We review related work in Section 2 and more precisely state our system model in Section 3. In Section 4 we describe our mutual exclusion protocol, and we outline certain optimizations to it in Section 5. We then develop our ordering protocol based upon it in Section 6. A proof of correctness for our ordering protocol can be found in Section 7.
In Singhal's taxonomy [21], the mutual exclusion protocol we present is a "Maekawa-type" protocol, following [12] . In this class of protocols, a process pi requests permission to enter the critical section from a set Qi of processes, such that Qi f l Qj # 8 for all i, j . Each process in a request set Qi grants exclusive access to the critical section until it is exited, and pi is allowed to enter the critical section only if all processes in Qi grant pi access. Due to the intersection property of request sets and exclusive locking at each process in a request set, only one process can be in the critical section at any time. Also due to these properties, however, Maekawa-type algorithms are generally prone to deadlock and consequently require extra messages to 'detect and recover from deadlocks. Deadlock-free Maekawa-type protocols, such as [ 10,201, have been proposed by strengthening the constraints on request sets so that for all i , j , eitherpi E Qj o r p j E Qi [21]. However, in our context, this strengthening is not possible because the clients requesting mutual exclusion are distinct from the servers that comprise the request sets. Clients cannot be added to reque., qt sets because they are transient and because the population of clients that might contend is not known a priori. The protocol that we present here works with Maekawa's original (weaker) intersection property Qi n Q j # 8 in the fault model addressed in [12]. At the same time, our protocol is not prone to deadlock.
'Formally, probabilistic liveness during periods of instability holds only if the scheduling adversary is nonadaptive. That is, for any execution, the scheduler chooses the distribution from which message delays will be drawn before the protocol execution begins; it cannot change this distribution in response to events in the execution. We omit further discussion of this issue here, except to note that in practice, this is an assumption we are willing to adopt for Fleet.
As discussed in Section I , we evaluate our protocol based on the amortized system response time that it achieves. This measure was introduced in the context of shared-memory mutual exclusion algorithms [ 5 ] , where there are examples boasting amortized system response times of O ( t ) or even O(1) (e.g., [3, 51). An alternative to using our protocol is to employ one of these algorithms, using a distributed protocol to emulate each shared variable it uses (e.g., [ 151). While the resulting algorithm would have superior amortized system response time (asymptotically), the performance in practice would be far worse than our prolocol in the main case we care about-i.e., contentionfree performance-due to the overheads of the variable emulation protocols. This also holds for backoff-style mutual exclusion algorithms explored for the shared-memory setting (e.g., [ 2 ] , which assumes even stronger objects than shared variables).
The manner in which we build upon our mutual exclusion protocol to order operations on data objects in an asynchronous system is similar to work of Fetzer and Cristian on consensus in the tinied asynchronous model [6], and the works of Lamport on Paxos [ 1 11 and of Keidar and Dolev on extended3-phase commit (E3PC) [8] . These works compose a (not necessarily safe) mutual exclusion protocol with a commit protocol to derive solutions to problems equivalent to our ordering problem. Paxos and E3PC, while building on mutual exclusion protocols, do not propose mutual exclusion implementations of their own. Fetzer and Cristian employ a mutual exclusion protocol that rotates the preference for access to the critical section among the possible contenders in sequence, but enables the next preferred contender to be bypassed without delay if that contender is unavailable. As such, its mutual exclusion primitive is also adaptive in the sense above. In order to achieve this, however, the: protocol relies on clock synchronization among the participating servers. Clock synchronization is not required by our protocol. More generally, however, our work contributes relative to all the above by providing a new and efficient mutual exclusion primitive, and by admitting arbitrary (Byzantine) server failures in our ordering protocol.
Our system model divides the set of processes into clients and servers. We assume a fixed, known set I / of n servers and an arbitrary, finite, but unknown number of clients. The protocol of a process is described in terms of event handlers that are triggered by the arrival of events such as the receipt of a message, the tick of a local clock, or input from a calling application. Once triggered, we assume that Ihe event handler runs to completion without delay.
Processes that obey their protocol specifications and receive (and handle) infinitely many events in an infinite run are called correct, Other processes are calledfuully. Up to a threshold b of servers may fail, and may do so arbitrarily (Byzantine failures); i.e., the event handlers of a faulty server may not conform to their specifications. While any number of clients may fail, clients are assumed to fail only by crashing, i.e., simply by no longer receiving events. This restriction of client failures to crashes may seem unrealistic when servers are presumed to fail arbitrarily. However, typically little can be done to protect an application from Byzantine clients, as such clients can always corrupt an object's data by submitting requests with incorrect content. One way of dealing with this in practice is to allow the creator of an object to prohibit untrusted clients from modifying objects using access control mechanisms that remain in force at correct servers provided that b or fewer servers fail (even arbitrarily). Thus, our assumption of client crashes in practice reduces to an assumption about the clients trusted to modify that object by its creator.
We assume that the local clock of each correct process ticks at the same rate as real time, so that a process can accurately measure the passage of a chosen, real-time timeout period. Since the timeout periods involved in our protocols would be very short in practice, this is a reasonable assumption. We do not assume that clocks at processes are synchronized.
Processes communicate by message passing. We assume that communication channels provide at-most-once message transmission: if pl and p2 are correct, then p2 receives any message m from pl at most once, and then only if pl sent m to pa. (Obviously, we further assume that p l never sends the same message twice to p 2 , which it can implement by, e.g., including a unique sequence number in the message.) There is a globally known constant 6. We say that a run is stable ut real time T if for each correct client, there exists a quorum of correct servers such that any message sent at time T' 2 T between that client and a server in that quorum arrives by time T' + 6. The definition of quorum that we use will be given in Section 4. A run is synchronous if it is stable at the time the run begins. Though processes communicate by message passing, we present our protocols in terms of remote operation invocations on servers, for simplicity of presentation. In synchronous runs, we also neglect the processing time of such remote invokations and assume that they complete instantaneously. We will return to explicit message passing events when necessary to prove correctness.
In this section we present our mutual exclusion protocol by which clients can contend for the opportunity to run for A time units without interference by other clients. More precisely, there is an operation contend that a client can invoke. When the invocation returns at the client, the client then has A time units in which to execute in isolation. After A time units pass, however, another client's contend operation may return, As discussed in Section 1, in addition to requiring mutual exclusion, we will also be concerned with the system response time.
The idea of the protocol is for clients to access servers simply to find out whether other clients are simultaneously contending. In order to provide mutual exclusion, every pair of clients must access 2b + 1 correct servers in common. If a client detects that another client is contending, it backs off for a random delay, chosen from a distribution that adapts to the number of contending clients. Intuitively, clients thus delay an amount of time proportional to the number of simultaneously contending clients, while eventually, when they sufficiently space their contentions, each succeeds. More precisely, the protocol is probabilistically live in a synchronous system, i.e., with probability 1 some client's contend operation returns.
The requirement that any two clients access at least 2b+ 1 common correct servers can be satisfied if each client queries the servers according to a special variant of masking quorum systems [ In this figure, ' ' 11' ' denotes concurrent invocation of statements, and " d t~ S" denotes the selection of an element of set S uniformly at random and assignment of that element to d. At a high level, the protocol executes as follows.
When presented with a request from a client, the server re- where s is a "retry value" that records the number of times the client has previously queried servers in this contend operation. That is, clients employ an exponential backoff strategy: the expected duration of a client's delay is proportional to twice its delay during its last retry.
The correctness of this protocol is proved easily in the following lemma: As discussed previously, t h e m e a s u r e o f quality on which w e focus for our mutual exclusion protocol is amortized system response time. T h e following l e m m a implies that the expected amortized s y s t e m response t i m e i s O(At). In t h e mutual exclusion protocol as presented in Figure 1 a n d analyzed in Lemma 2, client backoff w a s exponential a s a function of t h e n u m b e r o f retries in its contend operation.
Even though exponential backoff yields O(At) amortized system response time, analysis of backoff strategies in the context of multiple access channels shows that it performs less well in other measures than various polynomial backoff strategies (e.g ., [ 7 ] ) . While this analysis does not apply to our case directly, we expect that similar properties hold in our setting, and thus in practice it may be preferable to experiment with other backoff strategies.
In this section we sketch several possible improvements and optimizations to the mutual exclusion protocol. The implementation of the proposed ideas and the assessment of their practical implications are the subject of our ongoing work.
Avoiding backoff by breaking symmetry If the application is such that one client c repeatedly contends with little delay between contentions, then we can improve c's response time if c does not back off between consecutive try attempts. The backoff protocol will adequately space the other client's retries, and c's asymmetric strategy will enable it to gain mutual exclusion quickly.
In this optimization, each server maintains an internal data structure, called delayed reply fist, where it records IDS of the clients whose try requests arrive while the server is locked. As soon as the server's status becomes FREE, it goes through the records in the delayed reply list and sends FREE to the client with the lowest ID and LOCKED to everyone else. This optimization may allow the lowest ranking contending client a smooth entry to the critical section, without backoff.
It is possible to make the protocol safe even during instability periods if clients disregard those replies to their try() requests that arrive after 26 time units. However, in practice, this optimization can negatively affect the throughput of the applications whose implementation does not require the underlying mutex to be safe (e.g., the operation ordering presented in Section 6).
Parameterized contend In order to allow for better adaptation to changing system conditions and to application needs, it is possible to make A a parameter of the contend operation (and, consequently, of the try request) instead of being a system-wide constant. Both safety and the expected delay become parameterized by the actual A's employed.
As discussed in Section 1, one of the main applications for the mutual exclusion protocol of Section 4 is a protocol for serializing operations on replicas of an object in a distributed system. In order to perform an operation o on the replicated object, a client application submits the operation for execution. The properties that our ordering protocol satisfies are the following:
Order There is a well-defined sequence in which submitted operations are applied, and the result of each operation that returns is consistent with that sequence.
Liveness If a run is eventually stable, then every operation submitted by a correct client is performed with probability one, and if performed, its result is returned to the client.
Due to the Order and Liveness properties, our protocol emulates state machine replication [ 191. Among others, our implementation supports the following distinct features: First, the ordering responsibilities are delegated to the clients, which are not Byzantine by assumption. This way, we need not employ digital signatures or signaturelike cryptographic constructions, thus improving the performance and scalability of the protocol. Second, our protocol makes progress by updating only quorums of replicas, which helps to achieve better load balancing and enhances scalability. Third, our protocol supports nondeterministic operations, since each operation is applied at a client and the resulting object state is then copied back to servers.
Some modern protocols for implementing state machine replication in Byzantine environments (e.g., [ 17, 9, 41) assume a less restricted failure model by allowing arbitrary client failures. In these solutions, clients do not actively participate in the protocol, but serve merely as users that inject new operations into the server universe and collect responses. While this approach prevents Byzantine clients from interfering with the ordering protocol, it does not prevent attacks in which faulty clients corrupt object's data by submitting operations with arbitrary parameter values. Thus, in practice, the added value of providing protection against Byzantine client failures in terms of the system security guarantees is outweighed by the performance and scalability gain resulting from delegating ordering responsibilities to the clients.
The detailed client and server programs are shown in Figure 2 and Figure 3 respectively. The client program for submit(o) consists of two threads executed concurrently. The first thread, described in lines 2.3-7, simply submits the operation o to the servers for execution and awaits responses. The second thread, lines 2.8-32, invokes operations to create a new state and commits states in a serial order; we call this the ordering thread. I f f and g are: functions, then f1g denotes a function such that (flg)(o) == g ( o ) if g(o) # I and f ( o ) otherwise; see line 3.24. The following subsections contain details about operations, states, and ranks that are essential to understanding the ordering thread.
Our protocol works by applying an operation to ii state to produce a new state and a return result. A client submits an operation o to be performed by invoking submit(o). For simplicity of presentation, we assume that the same operation is never submitted by two distinct clients or twice by the same client. In practice, enforcing such uniqueness of operations can be implemented by each client labeling each of its operations with the client's identifier and a sequence number.
A state, denoted by (T (possibly with subscripts and/or superscripts), is an abstract data type that has the following interfaces:
a.version is an integer-valued field. It denotes the "version'' of the state. This field can be set by the protocol manipulating the state. A state's interfaces are assumed to satisfy the following properties. First, a.reflects(o) = t r u e iff doOp(o) vias invoked on some previous state. In practice, this can be implemented by recording within the state the highest operation sequence number already performed for each client. Second, if (T is the result of applying operations (via do0p) to a prior instance (T' such that d.reflects(o) = false, g.reflects(o) = true, and cT.version = d.version + -1, then a.response(o) is defined and returns the result for operation o. Note that by this assumption, a.response(o) can be eliminated ("garbage collected") when o.version is incremented. In this way, the size of (T can be limited.
Aside from the instance of garbage collection just mentioned, we do not further elaborate on garbage collection here. The primary data structures that grow in our protocol as presented in Figures 2 and 3 are (i) the record of which client operations have been performed (to compute u.reflects(o)) and (ii) a response function maintained at each server that records the response for each client operation (see lines 3.34,24). In practice, eliminating unnecessary data from these structures can be achieved, for example, by propagating information among servers in the background (e.g., using the techniques of [ 131) to convey when information about a given operation can be purged from the system. Other optimizations are possible, e.g., that trade off passing complete states versus update suffixes.
Each client executes the ordering thread of our protocol with an associated integer called its rank. We assume that no (WO clients ever adopt the same rank, which can be ensured, e.g., if each client's rank is formed with its identifier in the low-order bits. When invoking an operation on a server U in our protocol, a client always sends its current rank: as an argument to the invocation; this rank is denoted by r in u.get(r), u.propose(a, T ) and u.commit(a, r ) invocations. A server responds to only the highest-ranked client that has contacted it. In particular, if a server U is contacted by a client with a lower rank than another client to which it has already responded, then it throws a RankException that notifies the client of the higher rank under which another client contacted it. In order to get U to respond to it, the client will have to abort its current protocol execution, adjust its rank, and try again (starting at line 2.8).
The precise criteria that dictate when a client aborts its protocol run to adjust its rank are important to the liveness of our protocol. On the one hand, if a client aborts its protocol run based upon receiving a single RankException. then the client risks being aborted by a faulty server who in fact was not contacted by a higher ranking client. On the other hand, if the client requires b + 1 RankExceptions in order to abort, then the client may not abort even though b correct servers have been contacted by a higher-ranking client and thus will refuse to return responses to this client.
Our solution to this issue therefore mandates that the quorum system Q we employ satisfy the following property: For every B1, B2 s U with lBll = (Bzl = b, there exists Q E Q such that Q n (B1 U Bz) = 0. This restriction enables the client to complete its protocol run using quorums provided up to b correct servers respond with RankException. Consequently, whereas original masking quorum systems existed as long as n > 4b [ 141, this stronger constraint limits their existence to systems in which n > 6b (see Corollary 4.4 in [ 141). When a client is forced to adjust its rank due to receiving b + 1 RankExceptions, it does so by choosing a value larger than the maximum of all ranks reported by those RankExceptions.
We note that an alternative approach would be for clients to digitally sign (e.g., [IS] ) their ranks using a key available only to clients allowed to access the object (or a subset of them designated to execute the ordering protocol). When a scrver throws a RankException to a client, it passes the highest rank under which any client has contacted it, includ-1) submit (0):
2) waiting t true; contend(); ] until (pending = 0); a'.version t a'.version + 1;
ing the digital signature on that rank from that client. The client receiving the RankException can verify the validity of the rank by verifying the digital signature on it. In this implementation, a client can abort its protocol run based on a single RankException with which the client receives a validly signed rank, since a faulty server cannot forge signatures. This approach imposes overheads in terms of key management and computation, and we therefore opt against it. In particular, digital signatures tend to be relatively intensive to compute and verify. While for a small number r of clients, digital signatures can be emulated using message authentication codes, this approach does not scale well.
At a high level, the ordering thread of the protocol at a client works by first contending for mutual exclusion,, using the protocol of Section 4 (line 2.9). Once this contend returns, the protocol executes similarly to a 3-phase c:ommit protocol. It first invokes get on each server U in some quorum Qg" to obtain the states last committed to U (0:) and last proposed to U ((TE"); the rank proposer, of the: client who proposed a : " ; and the current set pending, of pending operations submitted to U . The client then computes the following values: a' is set to be the state with the highest version number that has been committed to some correct server ((i.e., at least b + 1 servers) in Qget (lines 2.14-15). 0 upc is set to be the state proposed to some correct server (Le., at least b + l servers) in Qg" by the h:ighestranking set of proposers (lines 2.16,33-43) .
completed is set to be the highest version number of all states that the responses from the servers iin Qget reveal to be committed at a full quorum. In part.icular, if b + 1 servers report proposed states atC with version numbers larger than U , then a state with version v must be committed at a full quorum (line 2.17).
'The client chooses which state (T to propose and commit to quorums based on these values. If oc has a version number larger than completed, then it will propose and com-. mit a' to ensure that c f gets committed to a full quorum (line 2.19). Its second choice will be to propose and commit the proposed state upc if its version number is larger than completed (line 2.21). Otherwise, it creates a new state by applying operations to a' (lines 2.23-24,44-5 1), and proposes and commits that state.
The protocol ensures that each newly proposed object state (T' is derived from the state CT that has been most recently committed by applying operations in the pending sets of correct servers to (T (line 2.24). This is guaranteed as follows. If 0 has been committed to a full quorum, then (T' = U at each correct server in that quorum. This implies that any client that succeeds in invoking get at a full quorum evaluates (T' to (T, and applies any pending operations to it. If, on the other hand, (T has not been committed at a full quorum, then it is possible for clients to evaluate ac to a prior state. However, since U must be proposed to a full quorum before it is committed, any client that invokes get on a full quorum evaluates nPC to 0. The client will therefore complete the commitment of a (bypassing (T' since completed 2 aC.version) and then continue by applying new operations to a to derive a'.
Kank is used to break ties between clients that attempt to propose different states simultaneously. Suppose that p and q each invoke get on a quorum of servers and obtain cc := a as above. 
In this section we prove that Order and Liveness are satisfied by our protocol. Let M denote a finite set of methods that for any object state U , a rank r and an operation o, consists of get(.), propose(a, r ) , commit(a, r ) and submit(o).
We assume that any method p E M can be invoked at a server u at most once throughout the execution. In practice such a requirement can be easily enforced using unique method identifiers composed of client identifier and the sequence number. Let p.rnnk be the rank with which method p is invoked. We consider the following system events: For a client p , let p.send(u, p ) be the client event that sends the method invocation p E M to server U, and let p.ret(u.p, p ) , be the client event triggered by the reply of the server U with return value p to a previously sent method p.
A server event is a computation performed upon receiving g e t , propose, commit, or submit invocations from a client. The event that occurs at a server U as a result of the invocation of a p E M is denoted u.p. The code executed by a correct server upon reception of such an invocation is the code of the corresponding server method (see Figure 3) . This code executes to completion (return) atomically, with the exception of submit ; submit executes atomically until the sleep command, and its return constitutes a separate event. A faulty server can perform arbitrary computation steps upon reception of client invocations.
We model the system execution as a countable set H of events partially ordered by -+ relation induced by the natural order of the method invocations and returns. We define the causal cone of an event e in H , denoted ccone(e, H ) , to be the subset of H such that Ve' E H , e' E ccone(e, H ) iff e' -+ e.
If a client p invokes method p on every server U E S C U in H , then we will unite all p.send (u,p) 
For the following lemma, we introduce the following definition. For any p.propose(a,r), we define its closest complete propose to be p'.propose(a', r') such that p.propose(u,r); and (iii) there does not exist a comp".propose(u", r " ) -+ p.propose(a, r ) . Note that any p.propose(a, r ) , other than the propose of a' at system initialization, has a closest complete propose, and that its closest complete propose is unique by Corollary 3.
(i) p'.propose(u', r') is complete; (ii) p'.propose(u', r') + H plete p".propose (a", r") (a2, r g ) ) . That is, we suppose the result holds for any q'.propsse(a', T ' ) E ccone(q.propose(a2, T Z ) ) , and we prove the result for q.propose(a2, T Z ) . Let p.propose(C, T ) be the first complete propose invocation in the causal chain leading to p.propose(a1, T I ) such that 5.version = u1.version. By the induction hypothesis for 4.2, C = a l .
According to the protocol, the value of a2 is computed based on the values of ( 0 ; ; ar, proposeru, pending,) returned by each server U in some quorum ~g~~(~z ) in response to g.get(r2) invocation. Furthemore, if U is correct, then the value of each a:, a,P' and proposer,, is determined by some (not necessarily complete) propose invocation p'.propose(a', T ' ) E ccone(q.get(r2)). By applying results of Lemma 3, Corollary 3 and the induction hypothesis, we conclude the following: If T' 3 f , then the closest complete propose ofp'.propose(a', T ' ) is eitherp.propose (5, F ) or the one that causally follows p.propose(6, T ) . Therefore, either U' = (TI, or U' is a state that extends U I with some previously submitted operations. Otherwise, if T' < F , then the closest complete propose of p'.propose(u', T ' ) causally precedes p.propose (5, F ) and therefore, a'.version 5 5.version.
Once we know the possible values of U : , u u p C and proposer, as returned by q.get(rz), and given that any two quorums intersect by at least 2b + 1 servers, we derive that the value of U 2 computed in lines 2.14-24 satisfies the lemma results.0 Theorem 1 (Order) There is a well-deJinfid sequelwe in which submitted operations are applied and the result of each operation that returns is consistent with that sequence. Theorem 2 (Liveness) If a run is eventually stable, then every operation submitted by a correct client is petjhrmed with probability one, and $performed, its result is rerurned to the client.
Proof :(Sketch) Once the system is stable, eventually some (correct client q returns from its invocation of contend with probability m e . This client executes for sufficiently long (if A is chosen adequately) in isolation of other clients. It either commits an existing state a to a full quorum or else extends U with operations in pending and proposes and commits the new state U' at a full quorum.0
Taxonomies and, in general, networks of words connected with transitive relations are extremely important knowledge repositories for a variety of applications in natural language processing (NLP) and knowledge representation (KR). In NLP, taxonomies such as WordNet [17] are widely used in intermediate tasks such as word sense disambiguation (e.g. [1] ) and selectional preference induction (e.g., [25] ) as well as in final applications such as question answering (e.g., [4] ) and textual entailment recognition (e.g. [5] ). In KR, taxonomies as well as other word networks are the bulk of domain ontologies.
To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29] ). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21] . These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26] ). The task is generally seen as a classification (e.g., [22, 27] ) or a clustering (e.g., [3] ) problem. This allows the use of machine learning models.
Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27] , are represented. These feature spaces are used to determine whether or not new word pairs coming from the text collection have to be included in existing knowledge repositories. Decision models are learnt * DISP University Rome "Tor Vergata" using existing knowledge repositories and then applied to new words or word pairs. Generally, these models use as features all the possible and relevant generalized contexts where words or word pairs can appear. For example, possible features in the word pair classification problem are "is a" and "as well as". Given the nature of the problem, these feature spaces can then be huge as they include all potential relevant features for a particular relation among words. Relevant features are not known in advance. Yet, large feature spaces can have negative effects on machine learning models such as increasing the computational load and introducing redundant or noisy features. Feature selection is the solution (see [11] ).
In this paper, we want to study how to improve performances of taxonomy learning methods by using feature selection. We focus on the probabilistic taxonomy learning model introduced by [27] as it uses existing taxonomies exploiting the transitivity of the isa relation. Leveraging on the particular model, we propose a novel way of using singular value decomposition (SVD) as unsupervised model for feature selection. In a nutshell, given the probabilistic model for taxonomy learning, we use SVD as a way to compute the pseudoinverse matrix needed in logistic regression. We will analyze if our method for using unsupervised feature selection positively affect performances.
Before staring, in Sec. 2 we will shortly review methods for taxonomy learning and for feature selection. We motivate our choice of working within the probabilistic setting. In Sec. 3, as SVD is the core of our method, we will then introduce SVD as unsupervised feature selection model. In Sec. 4 we then describe how we introduced SVD as natural feature selector in the probabilistic taxonomy learning model introduced by [27] . To describe how we use SVD as natural feature selector, we will shortly review the logistic regression used to compute the taxonomy learning model. We will describe our experiments in Sec. 5. Finally, in Sec. 6, we will draw some conclusions and describe our future work.
Extracting knowledge bases from texts is one of the major goal of NLP and KR. These methods can give an important boost to knowledge-based systems. In this section we want to shortly analyze some of these methods in order to motivate our choice to work within an existing probabilistic model for learning taxonomies. We also review the more traditional models for super-vised and unsupervised feature selection.
The models for automatically extracting structured knowledge, such as taxonomies, from texts use variants of the distributional hypothesis [12] exploit some induced lexical-syntactic patterns (originally used in [26] ).
The distributional hypothesis is widely used in many approaches for taxonomy induction from texts. For example, it is used in [3] for populating lattices, i.e. graphs of a particular class, of formal concepts.
Lexical syntactic patterns are also a source of relevant information for deciding whether or not a particular relation holds between two words. This approach has been widely used for detecting hypernymy relations such as in [13, 18] , for other ontological relations such as in [21] , or for more generic relations such as in [24, 28] . These learning models generally use the hypothesis that two words are related according to a particular relation if these often appear in specific text fragments.
Despite the wide range of models for taxonomy learning, only very few exploit the structure of existing taxonomies. The task is seen as building taxonomies from scratch. In [3] , for example, lattices and the related taxonomies are the target. Yet, existing taxonomies may be used to drive the process of building new taxonomies. In [19] , WordNet [17] and WordNet glosses are used to drive the construction of domain specific ontologies. In [22] , taxonomies are augmented exploiting their structure. Inserting a new word in the network is seen as a classification problem. The target classes are the nodes of the existing hierarchy. The distributional description of the word as well as the existing taxonomy structure is used to make the decision. This model is purely distributional. In [27] , a probabilistic model exploiting existing taxonomies is introduced. This model is purely based on lexicalsyntactical patterns. Also in this case, the insertion of a new word in the hierarchy is seen as a binary classification problem. Yet, the classification decision is taken over a pair of words, i.e., a word and its possible generalization. The probabilistic classifier should decide if this pair belongs or not to the taxonomy.
The probabilistic taxonomy learning models has at least two advantages with respect to the other models. The first advantage is that it coherently uses existing taxonomies in the expansion phase. Both existing and new information is modeled in the same probabilistic way. The second advantage is that classification problem is binary, i.e., a word pair belongs or not to the taxonomy. This allows to build a unique binary classifier. This is not the case for models such as the one of [22] , where we need a multi-class classifier or a set of binary classifiers. For these two reasons, we are using the probabilistic taxonomy learning setting for our study.
Yet, in applications involving texts such as taxonomy learning, machine learning models are exposed to huge feature spaces. This has not always positive effects. A first important problem is that huge feature spaces require large computational and storage resources for applying machine learning models. A second problem is that more features not always result in better accuracies of learnt classification models. Many features can be noise. Feature selection, i.e., the reduction of the feature space offered to machine learners, is seen as a solution (see [11] ).
There is a wide range of feature selection models that can be classified in two main families: supervised and unsupervised. Supervised models directly exploit the class of the instances for determining if a feature is relevant or not. The idea is to select features that are highly correlated with final target classes. Information theoretic ranking criteria such as mutual information and information gain are often used (see [8] ). Unsupervised models are instead used when the information on classes of instances is not available at the training time or it is inapplicable such as in information retrieval. Straightforward and simple models for unsupervised feature selection can be derived from information retrieval weighting schemes, e.g., term frequency times inverse document frequency (tf * idf ). In this case, relevant features are respectively those appearing more often or those more selective, i.e., appearing in fewer instances.
Feature selection models are also widely used in taxonomy learning. For example, attribute selection for building lattices of concepts in [3] is done applying specific thresholds on specific information measures on the attributes extracted from corpora. This models uses conditional probabilities, point-wise mutual information, and a selectional-preference-like measure as the one introduced in [25] .
A very important way of unsupervised feature selection is the application of the SVD. As this is the bulk of our methodology we will review how SVD can be used for this purpose. SVD has been largely used in information retrieval for reducing the dimension of the document vector space [7] . SVD, originally, is a decomposition of a rectangular matrix. Given a generic rectangular n × m matrix A, its singular value decomposition is A = U ΣV T where U is a matrix n × r, V T is a r × m and Σ is a diagonal matrix r × r. The diagonal elements of the Σ are the singular values such as δ 1 ≥ δ 2 ≥ ... ≥ δ r > 0 where r is the rank of the matrix A. For the decomposition, SVD exploits the linear combination of rows and columns of A.
There are different ways of using SVD as unsupervised feature reduction. An interesting way is to exploit its approximated computations, i.e. :
where k is smaller than the rank r. The computation algorithm [10] allows to stop at a given k different from the real rank r. The property of the singular values, i.e., δ 1 ≥ δ 2 ≥ ... ≥ δ r > 0, guarantees that the first k are bigger than the discarded ones. There is a direct relation between the informativeness of the i-th new dimension and the singular value δ i . High singular values correspond to dimensions of the new space where examples have more variability whereas low singular values determine dimensions where examples have a smaller variability (see [15] ). These latter dimensions can be then hardly used as efficient features in learning. The possibility of computing approximated versions of matrices gives a powerful method for feature selection and filtering as we can decide in advance how many features or, better, linear combination of original features we want to use.
In this section we will firstly introduce the probabilistic model (Sec. 4.1) and, then, we will describe how SVD is used as feature selector in the logistic regression that estimates the probabilities of the model (Sec. 4.2). To describe this part we need to go in depth into the definition of the logistic regression and some ways of computing it.
In the probabilistic formulation [27] , the task of learning taxonomies from a corpus is seen as a maximum likelihood problem. The taxonomy is seen as a set T of assertions R over pairs R i,j . If R i,j is in T , i is a concept and j is one of its generalization (i.e., the direct or the indirect generalization). For example, R dog,animal ∈ T describes that dog is an animal according to the taxonomy T .
The main probabilities are then: (1) the prior probability P (R i,j ∈ T ) of an assertion R i,j to belong to the taxonomy T and (2) the posterior probability P (R i,j ∈ T | − → e i,j ) of an assertion R i,j to belong to the taxonomy T given a set of evidences − → e i,j derived from the corpus. These evidences are derived from the contexts where the pair (i, j) is found in the corpus. The vector − → e i,j is a feature vector associated with a pair (i, j). For example, a feature may describe how many times i and j are seen in patterns like "i as j" or "i is a j". These among many other features are indicators of an is-a relation between i and j (see [13] ). Given a set of evidences E over all the relevant word pairs, the probabilistic taxonomy learning task is defined as the problem of finding a taxonomy T that maximizes the probability of having the evidences E, i.e.:
In [27] , this maximization problem is solved with a local search. What is maximized at each step is the ratio between the likelihood P (E|T ) and the likelihood P (E|T ) where T = T ∪ N and N are the relations added at each step. This ratio is called multiplicative change ∆(N ) and is defined as follows ∆(N ) = P (E|T )/P (E|T ). The main innovation of the model in [27] is the possibility of adding at each step the best relation N = {R i,j } as well as R i,j with all the relations induced from R i,j , i.e., N = {R i,j } ∪ I(R i,j ) where I(R i,j ) are the relations induced using the existing taxonomy and R i,j . Given the taxonomy T and the relation R i,j , the
We will experiment with our feature selection methodology in two different models:
flat: at each iteration step, a single relation is added, i.e. R i,j = arg max Ri,j ∆(R i,j ) inductive: at each iteration step, a set of relations is added, i.e. I( R i,j ) where R i,j = arg max Ri,j ∆(I(R i,j )).
The last important fact is that it is possible to demonstrate that
where k is a constant (see [27] ) that will be neglected in the maximization process. This last equation gives the possibility of using the logistic regression as it is. In the next sections we will see how SVD and the related feature selection can be used to compute the odds.
We here show that the odds(R i,j ) in eq. 2 can be computed with logistic regression (Sec. 4.2.1). We then describe how we can compute logistic regression using a particular pseudo-inverse matrix (Sec. 4.2.2). Finally, we show that approximated pseudo-inverse matrices can be computed using SVD (Sec. 4.2.3).
Logistic Regression [6] is a particular type of statistical model for relating responses Y to linear combinations of predictor variables X. It is a specific kind of Generalized Linear Model (see [20] ) where its function is the logit function and the dependent variable Y is a binary or dichotomic variable which has a Bernoulli distribution. The dependent variable Y takes value 0 or 1. The probability that Y has value 1 is function of the regressors x = (1, x 1 , ..., x k ).
The probabilistic taxonomy learner model introduced in the previous section falls in the category of probabilistic models where the logistic regression can be applied as R i,j ∈ T is the binary dependent variable and − → e i,j is the vector of its regressors. In the rest of the section we will see how the odds, i.e., the multiplicative change, can be computed. We start from formally describing the Logistic Regression Model. Given the two stochastic variables Y and X, we can define as p the probability of Y to be 1 given that X=x, i.e.p = P (Y = 1|X = x) The distribution of the variable Y is a Bernoulli distribution. Given the definition of the logit(p) as logit(p) = ln 
where β 0 , β 1 , ..., β k are called regression coefficients of the variables x 1 , ..., x k respectively. It is obviously trivial to determine the odds(R i,j ) related to the multiplicative change of the probabilistic taxonomy model. The odds, the ratio between the positive and the negative event, can be determined as follows:
The remaining problem is how to estimate the regression coefficients. This estimation is done using the maximal likelihood estimation to prepare a set of linear equations using the above logit definition and, then, solving a linear problem. This will give us the possibility of introducing the necessity of determining a pseudo-inverse matrix where we will use the singular value decomposition and its natural possibility of performing feature selection. Once we have the regression coefficients, we have the possibility of estimating a probability P (R i,j ∈ T | − → e i,j ) given any configuration of the values of the regressors − → e i,j , i.e., the observed values of the features. Let assume we have a multiset O of observations extracted from Y × E where Y ∈ {0, 1} and we know that some of them are positive observations (i.e., Y = 1) and some of them are negative observations (i.e., Y = 0). For each pair, the relative configuration − → e l ∈ E appears at least once in O and can be determined using the maximal likelihood estimation P (Y = 1| − → e l ). Then, from the equation of the logit (Eq. 3), we have a linear equation system, i.e.:
where Q is a matrix that includes a constant column of 1, necessary for the β 0 of the linear combination of the values of the regression. Moreover it includes the set of evidences, i.e. Q = (1, − → e 1 ... − → e m ).
The set of equations in Eq. 5 are a particular case multiple linear regression [2] . As Q is a rectangular and singular matrix, the system (Eq.5) has no solution. This problem can be solved by the Moore-Penrose pseudoinverse Q + [23] . Then, we determine the re-
We finally reached the point where it is possible to explain our idea that is naturally using singular value decomposition (SVD) as feature selection in a probabilistic taxonomy learner. In previous sections we described how the probabilities of the taxonomy learner can be estimated using logistic regressions and we concluded that a way to determine the regression coefficients β is computing the Moore-Penrose pseudoinverse Q + . It is possible to compute the MoorePenrose pseudoinverse using the SVD in the following way [23] . Given an SVD decomposition of the matrix Q = U ΣV T the pseudo-inverse matrix is:
The diagonal matrix Σ + is a matrix r × r obtained calculating the reciprocals of the singular value of Σ.
We have now our opportunity of using SVD as natural feature selector as we can compute different approximations of the pseudo-inverse matrix. The algorithm for computing SVD is iterative (Sec. 3). The firstly derived dimensions are those with higher singular value. We can then decide how many dimensions we want to use. The first k dimensions are more informative than the k + 1. We can consider different k in order to obtain different SVD as approximations of the original matrix (Eq. 1). We can define different approximations of the inverse matrix Q + as Q + k , i.e.:
In this section, we want to empirically explore whether our use of SVD feature selection positively affects performances of the probabilistic taxonomy learner. The best way of determining how a taxonomy learner is performing is to see if it can replicate an existing "taxonomy". We will experiment with the attempt of replicating a portion of WordNet [17] . In the experiments, we will address two issues: determining to what extent SVD feature selection affect performances of the taxonomy learner and determining if, for the probabilistic taxonomy learner, SVD is better than other simpler models for supervised and unsupervised feature selection. We will explore the effects on both the flat and the inductive probabilistic taxonomy learner.
In the rest of the section we will describe: the experimental set-up (Sec. 5.1) and the results of the experiments in term of performance (Sec. 5.2).
To completely define the experiments we need to describe some issues: how we defined the taxonomy to replicate, which corpus we have used to extract evidences for pairs of words, which feature space we used, and, finally, the feature selection models we compared against. As target taxonomy we selected a portion of WordNet 2 [17] . Namely, we started from the 44 concrete nouns divided in 3 classes: animal, artifact, and vegetable. For each word w, we selected the synset s w that is compliant with the class it belongs to. We then obtained a set S of synsets. We then expanded the set to S adding the siblings (i.e., the coordinate terms) for each synset in S. The set S contains 265 coordinate terms plus the 44 original concrete nouns. For each element in S we collected its hypernyms, obtaining the set H. We then removed from the set H the 4 topmosts: entity, unit, object, and whole. The set H contains 77 hypernyms. For the purpose of the experiments we both derived from the previous sets a taxonomy T and produced a set of negative examples T . The two sets have been obtained as follows. The taxonomy T is the portion of WordNet implied by O = H ∪ S , i.e. T contains all the (s, h) ∈ O × O that are in WordNet and T contains all the (s, h) ∈ O × O that are not in WordNet. We have 5108 positive pairs in T and 52892 negative pairs in T . 2 We used the version 3.0
We then produced two experimental settings: a natural and an artificial one. In the natural setting we used only positive pairs in the training set. This is the natural situation when augmenting existing taxonomies. Only positive word pairs can be derived from existing taxonomies. Yet, negative pairs cannot. In the artificial setting we used both positive and negative examples.
To obtain the training and the testing sets, we randomly divided the set T ∪ T in two parts T tr and T ts , respectively, of 70% and 30% of the original T ∪ T .
As corpus we used ukWaC [9] . This is a web extracted corpus of about 2700000 web pages containing more than 2 billion words. The corpus contains documents of different topics such as web, computers, education, public sphere, etc.. It has been largely demonstrated that the web documents are good models for natural language [14] .
As the focus of the paper is the analysis of the effect of the SVD feature selection, we used as feature spaces both n-grams and bag-of-words. Out of the T ∪ T , we selected only those pairs that appeared at a distance of at most 3 tokens. Using this 3 tokens, we generated two spaces: (1) bag-of-word and (2) the bigram space that contains bigrams and monograms. For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in the context of features for learning a taxonomy.
Finally, we want define the feature selection models we compared against. As unsupervised feature selection models we used the term frequency times the inverse document frequency (tf*idf ). Instances − → e have the role of the documents. As supervised feature selection models we used the mutual information (mi). For all the feature selection models, we selected the first k features. Finally, we used a manual feature selection model based on the Heart's patterns [13] . In this model that we call manual, we used as features only the classical Hearst's patterns.
In the first set of experiments we want to focus on the issue whether or not performances of the proba-bilistic taxonomy learner is positively affected by the proposed feature selection model based on the singular value decomposition. We then determined the performance with respect to different values of k. This latter represents the number of surviving dimensions where the pseudo-inverse is computed. The features of this experiment are unigrams derived from a 3-sizedwindow. Punctuation has been considered. Figures 1 plots the accuracy of the probabilistic learner with respect to the size of the feature set, i.e. the number k of single values considered for computing the pseudoinverse matrix. To determine if the effect of the feature selection is preserved during the iteration of the local search algorithm, we report curves at different sizes of the set of added pairs. Curves are reported for both the flat model and the inductive model. The flat algorithm adds one pair at each iteration. Then, we reported curves for 40 and 80 added pairs. The curves show that accuracy doesn't increase after a dimension of k=400. For the inductive model we report the accuracies for around 40, 80, 130 added pairs. The optimal dimension of the feature space seems to be around 500 as after that value performances decrease or stay stable. SVD feature selection has then a positive effect for both the flat and the inductive probabilistic taxonomy learners. This has beneficial effects both on the performances and on the computation time.
In the second set of experiments we want to determine whether or not SVD feature selection for the probabilistic taxonomy learner behaves better than other feature selection models. We then fixed k to 600 both for the SVD selection model and for the other feature selection models. In this experiments, the original feature space is the bigram space. Figure 2 shows results. Curves report accuracies of the different models after n added pairs. In the natural setting, we compared our model against the tf * idf and the manual feature selection. Our SVD model outperforms both models of feature selection. The same happened against mutual information (M I) in the artificial setting. Our SVD way of selecting features seems to be very effective.
We presented a model to naturally introduce SVD feature selection in a probabilistic taxonomy learner. The method is effective as allows the designing of better probabilistic taxonomy learners. We still need to explore whether or not the positive effect of SVD feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in [27] .
Technological measures to mitigate climatic change include greenhouse gas (GHG) emission reductions and climate geoengineering options. Among these measures, solar radiation management (SRM) technologies such as placing sunshades in space and injecting sulfur aerosol into the stratosphere have been evaluated as having relatively large potential to contribute to the mitigation of climate change (The Royal Society, 2009 ).
However, while earlier studies dealing with strategies of climate change mitigation have focused on deriving optimal dynamic paths of the GHG emissions, especially carbon dioxide (CO 2 ), few have additionally considered the timing and scale of implementing SRM options. Though a pioneering study by Wigley (2006) shows plausible trajectories of the combination of CO 2 emissions reduction and SRM by stratospheric aerosol injection in the future, it lacks deep discussion of economics and risk management.
The present study aims at drawing desirable scenarios based on those combined points of view by using an integrated assessment model of climate and economy. For discussing the combination of CO 2 emissions reduction and SRM, the study pays special attention to the so-called "termination problem," i.e., the risk of adverse effects to climatic condition accompanied with a rapid global warming if the use of the SRM option is terminated for any reason after its implementation.

The 2007 version of the DICE model known as an integrated assessment model of climate change, DICE-2007 (Nordhaus, 2008 , is modified to deal explicitly with SRM options. The DICE model is available for public use through its developer's Web page and has served as the basis of most other economic models of climate change. The model is a nonlinear programming model that integrates a neoclassical macroeconomic growth model with the following three models: an emissions model that computes the amount of CO 2 emissions caused by economic production and the cost of mitigating the emissions, a climate model that simulates the flow and stock of CO 2 in the air and ocean and their impact on the changes in global mean atmospheric temperature, and a damage model that estimates the damage cost caused by a given rise in air temperature. The objective function is the total discounted sum of a representative individual's instantaneous utility stream. It is a one-region model that covers the entire world and derives the optimal dynamic paths of macro investment and CO 2 reduction rate. The total period of time is divided into 60 time periods, the first of which comprises the ten years centered on 2005.
Since radiative forcing that determines the greenhouse effect is controllable only by atmospheric CO 2 concentration in the DICE model, this study modifies the model to include SRM options as a factor controlling radiative forcing, as applied earlier in Kosugi (2010) . The two most important points of the modification are described as follows.
(i) Either placing sunshades in space or injecting aerosols into stratosphere is considered to be applicable. The balance of flow and stock of the sunshading materials is modeled; the service life of the materials, i.e., the period in which the materials stay in the area effective for SRM, is taken into account When we define the variables ) (t S and ) (t G as the mass stock of sun-shading materials accumulated in space or the stratosphere (Mt) and the mass flow of the materials lifted into space or the stratosphere (Mt/yr.), respectively, at time period t , and the parameter S δ as the depreciation rate of the sunshading materials accumulated in space or the stratosphere (yr.
-1 ), the balance of flow and stock of sunshades in space is modeled as:
noticing that a time period consists of ten years in the DICE model. Given the short staying period of injected aerosol in the stratosphere of a few years at the longest, the model for it is as follows:
(1')
(ii) The decrease in radiative forcing by implementing an option is assumed to be proportional to the up-mass stock of the sun-shading material. Letting ) (t F and ) (t F EX be total radiative forcing and its exogenous part due to non-CO 2 GHGs (W/m 2 relative to 1900) and ) (t M AT the mass of carbon in the atmosphere (GtC), this is modeled as:
where η and m denote the parameters connecting radiative forcing with temperature (°C/W/m 2 ) and the sunshade mass-effectiveness coefficient, i.e., the mass of the stock of sun-shading materials required to offset the increase in radiative forcing due to a doubling of the atmospheric CO 2 concentration (Mt/2×CO 2 ), respectively.
By using the calculated radiative forcing, the air temperature is estimated through the following simple climate model as in the original DICE model: (4) where variables ) (t T AT and ) (t T LO represent the global mean surface temperature and the temperature of the ocean depths (°C relative to 1900), respectively. Other modifications include: (iii) the cost of installing the sun-shading materials is subtracted from consumption; (iv) CO 2 emissions induced by installing the sun-shading materials are taken into account; (v) constraints to avoid an air temperature drop are imposed; the global mean air temperature is kept at no less than its 1900 value in the whole period and the rate of temperature decrease doesn't exceed 0.2 °C per decade; and (vi) the CO 2 mitigating trend is assumed to be continued; the rate of CO 2 mitigation is constrained not to decline with an elapse of time.
Among the variety of parameters in the model, the parameters used in the original DICE model were set to be the same as the reference values applied in the DICE-2007. Table 1 (a) shows a major set of extractions from those parameter settings.
The parameters introduced to incorporate SRM options in the model are set based on a survey of literature data (Hertzfeld, et al., 2005; Lenton and Vaughan, 2009; McClellan et al., 2010; Pearson, et al., 2006) as shown in Table 1 (b). Figure 1 shows the trajectory of the global mean air temperature calculated by using the modified DICE model described above. The figures hereafter show the results up to 2125 out of the whole time period calculated in the model. As seen from Figure 1 , the optimal path of SRM deployment follows the maximum allowable implementation starting from 2045 or 2015 if the space-sunshade installation or the stratospheric aerosol injection is applicable, respectively. This result implies that depending largely on an SRM option can be a more cost-effective measure for mitigating climatic change than facilitating CO 2 emissions reduction. In this case, as shown in Figure 2 (see "w/o temp. limit" in the figure) the global industrial CO 2 emission is allowed to rise steadily.
However, in the case of such a large dependency on SRM for mitigating climate change, we would be faced with the problem described below should the implementation of SRM be terminated. Space, w/o temp. limit Stratosph., w/o temp. limit Space, w/ temp. limit Stratosph., w/ temp. limit The broken lines in Figure 1 indicate the temperature increases after SRM termination at the respective time periods. More specifically, it shows the calculated global mean air temperature rise hypothesizing that the values of all the variables, e.g., CO 2 emissions, are the same as those calculated earlier through the model while no new sun-shading materials are placed into space or the stratosphere after each of the time periods. The abrupt rise in air temperature after the SRM termination is called the "termination problem," which has been described as one of the most serious risks concerning the use of SRM (Brovkin, et al., 2009 ).

For the safer use of SRM options, we need to avoid the risk of abrupt warming, which would occur in a situation where SRM implementation is terminated. The causes of termination could include unsuccessful continuous multilateral political negotiations regarding SRM or the unexpected revelation of a major adverse side effect of the SRM. Although such an occurrence is itself unforeseeable, the extent of the adverse effect brought about by the SRM termination can be estimated, and it is possible to control the use of SRM to keep the damage from unforeseen discontinuation at a certain allowable level.
Given the climate control recommendation by WBGU (2003) to constrain the rise in global average air temperature below 2 °C and the per-decade rate of temperature rise within 0.2 °C, a guideline for SRM use is derived such that the above condition holds even if SRM is terminated at any time.
The above guideline can be implemented in the model by introducing the following formulae. Let ) , ( t t S ′ be the group of variables representing the virtual dynamic path of the mass stock of sun-shading materials accumulated in space or the stratosphere (Mt) assuming an SRM termination at time t′ . For t t ′ < , clearly
while for T t t < ≤ ′ , setting the value of ) (t G to null in Eqs. (1) 
while for
, consistently with Eqs. (3) and (4), 
These two constraints should be applied for all t and t′ ; however, incorporating Eq. (10) for 3 < t makes the model infeasible, i.e., the rise in global mean air temperature in the next decade will inevitably be above 0.2 °C. We therefore apply Eq. (10) for 3 ≥ t . The total numbers of variables and constraints become 13 and 20 times, respectively, as many as those of the model before the extension. The computation time to find the utility maximizing solution is 41 seconds for the extended model when space-sunshades are assumed to be available as an SRM option, which is 27 seconds longer than the preextension when the model is solved by GAMS/ CONOPT3 (Brooke et al., 1992; Drud, 1994) with a PC based on the Intel(R) Core(TM) 2 Duo CPU P9300, 2.26GHz with 1.93 GB RAM.
The global mean air temperature calculated through the extended model is shown as the solid line in Figure 3 . Compared with Figure 1 , this figure suggests a moderate use of SRM, especially in the case of stratospheric aerosol injection, to lower the air temperature when we adopt the guideline introduced above. As in Figure 1 , the broken lines in Figure 3 indicate the trajectory of the temperature after an unexpected SRM termination at the respective time periods; we can confirm that, when the use of SRM is moderated to reflect the guideline of limiting the temperature rise that would occur by SRM termination, abrupt warming by SRM use termination is avoided. Figure 2 includes the optimal paths of the industrial CO 2 emissions when the constraint on the limit of temperature rise in case of SRM termination is adopted (see "w/ temp. limit") together with those without the limit of temperature rise explained in Section 2.3. The results imply that reducing CO 2 emissions is expected to play a more important role in mitigating climate change when we adopt the guideline of limiting temperature rise. Specifically, the amount of industrial CO 2 emissions should be kept at around the present level in the former half of this century and is expected to be reduced rapidly afterward, reaching only 20% of the 2005 levels by 2085. Figure 4 shows the calculated atmospheric CO 2 concentration, which steadily increases in this century and reaches 700 ppmv a century hence if the guideline of limiting the temperature rise in case of SRM termination is not adopted. With the limit of temperature rise in such a case, on the other hand, the increase in CO 2 concentration is expected to be mitigated to peak at 490 ppmv by 2075; afterward the concentration decreases to below 450 ppmv after 2125.
To observe the desirable combination of CO 2 emissions reduction and SRM for contributing to mitigating climate change derived under the guideline of limiting temperature rise in case of SRM termination, the decrease in radiative forcing by use of each measure to mitigate climate change, i.e., the difference from the radiative forcing compared to the case where no climate mitigation policy is implemented, is illustrated in Figure 5 assuming that stratospheric aerosol injection is usable as an SRM option.
CO 2 emissions reduction contributes more to lessening radiative forcing than SRM throughout the time periods addressed by the model, and the Space, w/o temp. limit Stratosph., w/o temp. limit Space, w/ temp. limit Stratosph., w/ temp. limit contribution of emissions reduction becomes much greater as time passes. Though we omit a figure corresponding to the case of using space-based sunshades instead of stratospheric aerosol injection, a similar tendency is observed for this case. 
SRM geoengineering is expected to be a lower-cost option of climate control compared to CO 2 emissions reduction, and may considerably contribute to the cost-effectiveness of global climatic change mitigation. However, this option is accompanied by the risk of rapid global warming if the implementation of SRM is unexpectedly terminated for any reason. As a guideline for the use of SRM to avoid the risk, this study suggests that the adverse effect should be controlled within an acceptable range in case of unexpected SRM termination at any time after its implementation. We incorporated the guideline into the integrated climate-economy model DICE by extending the model and quantitatively showed the contributions of CO 2 emissions reduction and SRM recommended to prevent global warming.
The extension of the model brings increases in the numbers of variables and constraint equations, resulting in a longer computation time to solve the model. The model is still solved within a minute using a PC because it incorporates a very simplified climate module; if we further extend the model to deal with geographic distribution of climate change, the computation time is estimated to increase, which may impose a barrier to practical evaluation.
Finally, it should be emphasized that there are some risks with the use of SRM other than those considered in the present modeling study. The quantitative results obtained from this study should be interpreted as the economic potential of SRM use assuming that such risks are low. If we needed to regard these risks as considerably high, more restrained use of SRM would be recommended.
Independent component analysis (ICA) is a computational and statistical technique with applications in areas ranging from signal processing to machine learning and more. Formally, if S is an n-dimensional random vector with independent coordinates and A ∈ R n×n is invertible, then the ICA problem is to estimate A given access to i.i.d. samples of the mixed signals X = AS. We say that X is generated by an ICA model X = AS. The recovery of A (the mixing matrix ) is possible only up to scaling and permutation of the columns. Moreover, for the recovery to be possible, the distributions of the random variables S i must not be Gaussian (except possibly one of them). Since its inception in the eighties (see [CJ10] for historical remarks), ICA has been thoroughly studied and a vast literature exists (e.g. [HKO01, CJ10] ). The theory is well-developed and practical algorithms-e.g., FastICA [Hyv99] , JADE [CS93] -are now available along with implementations, e.g. [CAS + ]. However, to our knowledge, rigorous complexity analyses of these assume that the fourth moment of each component is finite: E(S 4 i ) < ∞. If at least one of the independent components does not satisfy this assumption we will say that the input is in the heavy-tailed regime. Many ICA algorithms first preprocess the data to convert the given ICA model into another one where the mixing matrix A has orthogonal columns; this step is often called whitening. We will instead call it orthogonalization, as this describes more precisely the desired outcome. Traditional whitening is a second order method that may not make sense in the heavy-tailed regime. In this regime, it is not clear how the existing algorithms would perform, because they depend on empirical estimation of various statistics of the data such as the covariance matrix or the fourth cumulant tensor, which diverge in general for heavy-tailed data. For example, for the covariance matrix in the mean-0 case this is done by taking the empirical average (1/N )
where the {x(i)} are i.i.d. samples of X. ICA in the heavy-tailed regime is of considerable interest, directly (e.g., [Kid01b, Kid01a, SYM01, CB04, CB05, SAML + 05, WKZ09, JEK01, CS07, BC99]) and indirectly (e.g., [BG10, GTG09, WOH02] ) and has applications in speech and finance. We also mention an informal connection with robust statistics: Algorithms solving heavy-tailed ICA might work by focusing on samples in a small (but high probability) region to get reliable statistics about the data and avoid the instability of the tail. Thus, if the data has outliers, the outliers are less likely to affect such an algorithm.
Recent theoretical work [AGNR15] proposed a polynomial time algorithm for ICA that works in the regime where each component S i has finite (1 + γ)-moment for γ > 0. This algorithm follows the two phases of several ICA algorithms: (i) Orthogonalize the independent components. The purpose of this step is to apply an affine transformation to the samples from X so that the resulting samples correspond to an ICA model where the unknown matrix A has orthogonal columns. (ii) Learn the matrix with orthogonal columns. Each of these two phases required new techniques: (1) Orthogonalization via uniform distribution in the centroid body. The input is assumed to be samples from an ICA model X = AS where each S i is symmetrically distributed (w.l.o.g, see Sec. 2) and has at least (1 + γ)-moments. The goal is to construct an orthogonalization matrix B so that BA has orthogonal columns. In [AGNR15] , the inverse of the square root of the covariance matrix of the uniform distribution in the centroid body is one such matrix. (2) Gaussian damping. The previous step allows one to assume that the mixing matrix A is orthogonal. The modified second step is: If X has density ρ X (t) for t ∈ R n , then the algorithm constructs another ICA model X R = AS R where X R has pdf proportional to ρ X (t) exp(− t 2 2 /R 2 ), where R > 0 is a parameter chosen by the algorithm. This explains the term Gaussian damping. This achieves two goals: (1) All moments of X R and S R are finite. (2) The product structure of is retained. This follows from two facts: A has orthogonal columns, and the Gaussian has independent components in any orthonormal basis. Because of these properties, the model can be solved by traditional ICA algorithms.
The algorithm in [AGNR15] is theoretically efficient but impractical. Their orthogonalization uses the ellipsoid algorithm for linear programming, which is not practical. It is not clear how to replace their use of the ellipsoid algorithm by practical linear programming tools, as their algorithm only has oracle access to a sort of dual and not an explicit linear program. Moreover, their orthogonalization technique uses samples uniformly distributed in the centroid body, generated by a random walk. This is computationally efficient in theory but, to the best of our knowledge, only efficient in practice for moderately low dimension.
Our contributions. Our contributions are experimental and theoretical. We provide a new and practical ICA algorithm, HTICA, building upon the previous theoretical work in [AGNR15] . HTICA works as follows: (1) Compute an orthogonalization matrix B. (2) Pre-multiply samples by B to get an orthogonal model. (3) Damp the data, run an existing ICA algorithm. For step (1), we propose two theoretically sound and practically efficient ways below, orthogonalization via centroid body scaling and orthogonalization via covariance. Our algorithm is simpler and more efficient, but needs a more technical justification than the method in [AGNR15] . We demonstrate the effectiveness of HTICA on both synthetic and real-world data.
Orthogonalization via centroid body scaling. We propose a more practical orthogonalization matrix than the one from [AGNR15] (orthogonalization via the uniform distribution in the centroid body, mentioned before). First, consider the centroid body of random vector X, denoted ΓX (this is really a function of the distribution of X; formal definition in Sec. 2). For intuition, it is helpful to think of the centroid body as an ellipsoid whose axes are aligned with the independent components of X. The centroid body is in general not an ellipsoid, but it has certain symmetries aligned with the independent components. Let random vector Y be a scaling of X along every ray so that points at infinity are mapped to the boundary of ΓX, the origin is mapped to itself and the scaling interpolates smoothly. One such scaling is obtained in the following way: It is helpful to consider how far a point is in its ray with respect to the boundary of ΓX. This is given by the Minkoswki functional of ΓX, denoted p : R n → R, which maps the boundary of ΓX to 1 and interpolates linearly along every ray. We can then achieve the desired scaling by first mapping a given point to the boundary point on its ray (the mapping x → x/p(x)) and then using the function tanh, which maps [0, ∞) to [0, 1] with tanh(0) = 0 and lim x→∞ tanh(x) = 1 to determine the final scale along the ray, namely, tanh p(x). More formally, our scaling is the following: Let Y be tanh p(X) p(X) X. We show in Sec. 4.1 that B = Cov(Y ) −1/2 is an orthogonalization matrix when Cov(Y ) is invertible. In order to make this practical, one needs a practical estimator of the Minkowski functional of ΓX from a sample of X. In Sec. 4.1 and 5, we describe such an algorithm and provide a theoretical justification, including finite sample estimates. The proposed algorithm is much simpler and practical than the one described in [AGNR15] . In particular, it avoids the use of the ellipsoid algorithm by the use of a closed-form linear programming representation of the centroid body (Prop. 10, Lemma 11) and new approximation guarantees between the empirical (sample estimate) and true centroid body of a heavy-tailed distribution. In Sec. 4.1, we discuss our practical implementation and show results where orthogonalization via centroid body scaling produces results with smaller error. Orthogonalization via covariance. Previously, (e.g., in [CB04] ), the empirical covariance matrix was used for whitening in the heavy-tailed regime and, surprisingly, worked well in some situations. Unfortunately, the understanding of this was quite limited . We give a theoretical explanation for this phenomenon in a fairly general heavy-tailed regime: Covariance-based orthogonalization works well when each component S i has finite (1 + γ)-moment, where γ > 0. We also study this algorithm in experimental settings. As we will see, while orthogonalization via covariance improves over previous algorithms, in general orthogonalization via centroid body has better performance because it has better numerical stability; but there are some situations where orthogonalization via covariance matrix is better.
Empirical Study. We perform experiments on both synthetic and real data to see the effect of heavy-tails on ICA.
In the synthetic data setting, we generate samples from a fixed heavy-tailed distribution and study how well the algorithm can recover a random mixing matrix (Sec. 3).
To study the algorithm with real data, we use recordings of human speech provided by [Don09] . This involves a room with different arrangements of microphones, and six humans speaking independently. The speakers are recorded individually, so we can artificially mix them and have access to a ground truth. We study the statistical properties of the data, observing that it does indeed behave as if the underlying processes are heavy-tailed. The performance of our algorithm shows improvement over using FastICA on its own.
Heavy-tailed distributions arise in a wide range of applications (e.g., [Nol15] ). They are characterized by the slow decay of their tails. Examples of heavy-tailed distributions include the Pareto and log-normal distributions.
We denote the pdf of random variable Z by ρ Z . We will assume that our distributions are symmetric, that is ρ(x) = ρ(−x) for x ∈ R. As observed in [AGNR15] , this is without loss of generality for our purposes. This follows from the fact that if X = AS is an ICA model, and if we let X = AS be an i.i.d. copy of the same model, then X − X = A(S − S ) is an ICA model with components of S − S having symmetric pdfs. One further needs to check that if the components of S are away from Gaussians then the same holds for S − S ; see [AGNR15] . We formulate our algorithms for the symmetric case; the general case immediately reduces to the symmetric case.
For K ⊆ R n , K denotes the set of points that are at distance at most from K. The set K − is all points for which an -ball around them is still contained in K. The n-dimensional p ball is denoted as B n p . An important related family of distributions is that of stable distributions (e.g., [Nol15] ). In general, the density of a stable distribution has no closed form, but is fully defined by four real-valued parameters. Some stable distributions do admit a closed form, such as the Cauchy and Gaussian distributions. For us the most important parameter is α ∈ (0, 2], known as the stability parameter; we will think of the other three parameters as being fixed to constants.
We use the notation poly(·) to indicate a function which is asymptotically upper bounded by a polynomial expression of the given variables.
If α = 2, the distribution is Gaussian (the only non-heavy-tailed stable distribution), and if α = 1, it is the Cauchy distribution.
Definition 1 (Centroid body). Let X ∈ R n be a random vector with finite first moment, that is, for all u ∈ R n we have E(| u, X |) < ∞. The centroid body of X is the compact convex set, denoted ΓX, whose support function is h ΓX (u) = E(| u, X |). For a probability measure P, we define ΓP, the centroid body of P, as the centroid body of any random vector distributed according to P.
Note that for the centroid body to be well-defined, the mean of the data must be finite. This excludes, for instance, the Cauchy distribution from consideration in the present work.
In this section, we show experimentally that heavy-tailed data poses a significant challenge for current ICA algorithms, and compare them with HTICA in different settings. We observe some clear situations where heavy-tails seriously affect the standard ICA algorithms, and that these problems are frequently avoided by using the heavy-tailed ICA framework. In some cases, HTICA does not help much, but maintains the same performance of plain FastICA.
To generate the synthetic data, we create a simple heavy-tailed density function f η (x) proportional to (|x| + 1.5) −η , which is symmetric, and for η > 1, f η is the density of a distribution which has finite k < η − 1 moment. The signal S is generated with each S i independently distributed from f ηi . The mixing matrix A ∈ R n×n is generated with each coordinate i.i.d. N (0, 1), columns normalized to unit length. To compare the quality of recovery, the columns of the estimated mixing matrix,Ã are permuted to align with the closest matching column of A, via the Hungarian algorithm. We use the Frobenius norm to measure the error, but all experiments were also performed using the well-known Amari index [ACY + 96]; the results have similar behavior and are not presented here.
Focusing on the third step above, where the mixing matrix already has orthogonal columns, ICA algorithms already suffer dramatically from the presence of heavy-tailed data. As proposed in [AGNR15] , Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l 2 -norm) to data from a related ICA model X R = AS R , where R > 0 is a parameter to be chosen. The independent components of S R have finite moments of all orders and so the existing algorithms can estimate A.
Using samples of X, we construct the damped random variable X R , with pdf ρ X R (x) ∝ ρ X (x) exp(− x 2 /R 2 ). To normalize the right hand side, we can estimate
If x is a realization of X R , then s = A −1 x is a realization of the random variable S R and we have that S R has pdf ρ S R (s) = ρ X R (x). To generate samples from this distribution, we use rejection sampling on samples from ρ X . When performing the damping, we binary search over R so that about 25% of the samples are rejected. For more details about the technical requirements for choosing R, see [AGNR15] . Figure 1 shows that, when A is already a perfectly orthogonal matrix, but where S may have heavy-tailed coordinates, several standard ICA algorithms perform better after damping the data. In fact, without damping, some do not appear to converge to a correct solution. We compare ICA with and without damping in this case: (1) FastICA using the fourth cumulant ("FastICA -pow3"), (2) FastICA using log cosh ("FastICAtanh"), (3) JADE, and (4) Second Order Joint Diagonalization as in, e.g., [Car89] . The Frobenius error of the recovered mixing matrix with the 'pow3' and 'tanh' contrast functions, on 10-dimensional data, averaged over ten trials. The mixing matrix A is random with unit norm columns, not orthogonal. In the left and middle figures, the distribution has η = (6, . . . , 6, 2.1, 2.1) while in the right figure, η = (2.1, . . . , 2.1) (see Section 3.2 for a discussion).
We now present the results of HTICA using different orthogonalization techniques: (1) Orthogonalization via covariance (Section 4.2 (2) Orthogonalization via the centroid body (Section 4.1) (3) the ground truth, directly inverting the mixing matrix (oracle), and (4) No orthogonalization, and also no damping (for comparison with plain FastICA) (identity). The "mixed" regime in the left and middle of Figure 2 (where some signals are not heavy-tailed) demonstrates a very dramatic contrast between different orthogonalization methods, even when only two heavy-tailed signals are present.
In the experiment with different methods of orthogonalization it was observed that when all exponents are the same or very close, orthogonalization via covariance performs better than orthogonalization via centroid and the true mixing matrix as seen in Figure 2 . A partial explanation is that, given the results in Figure 1 , we know that equal exponents favor FastICA without damping and orthogonalization (identity in Figure 2 ). The line showing the performance with no orthogonalization and no damping ("identity") behaves somewhat erratically, most likely due the presence of the heavy-tailed samples. Additionally, damping and the choice of parameter R is sensitive to scaling. A scaled-up distribution will be somewhat hurt because fewer samples The data was sampled with parameter η = (6, 6, 6, 6, 6, 6, 6, 6, 2.1, 2.1).
will survive damping.
While the above study on synthetic data provides interesting situations where heavy-tails can cause problems for ICA, we provide some results here which use real-world data, specifically human speech. To study the performance of HTICA on voice data, we first examine whether the data is heavy-tailed. The motivation to use speech data comes from observations by the signal processing community (e.g. [Kid00] ) that speech data can be modeled by α-stable distributions. For an α-stable distribution, with α ∈ (0, 2), only the moments of order less than α will be finite. We present here some results on a data set of human speech according to the standard cocktail party model, from [Don09] . The physical setup of the experiments (the human speakers and microphones) is shown in Figure 3 . To estimate whether the data is heavy-tailed, as in [Kid00], we estimate parameter α of a best-fit α-stable distribution. This estimate is in Figure 4 for one of the data sets collected. We can see that the estimated α is clearly in the heavy-tailed regime for some signals.
Using data from [Don09] , we perform the same experiment as in Section 3.2: generate a random mixing matrix with unit length columns, mix the data, and try to recover the mixing matrix. Although the mixing is synthetic, the setting makes the resulting mixed signals same as real. Specifically, the experiment was conducted in a room with chairs, carpet, plasterboard walls, and windows on one side. There was natural noise including vents, computers, florescent lights, and traffic noise through the windows. Figure 4 demonstrates that HTICA (orthogonalizing with centroid body scaling, Section 4.1) applied to speech data yields some noticeable improvement in the recovery of the mixing matrix, primarily in that it is less susceptible to data that causes FastICA to have large error "spikes." Moreover, in many cases, running only FastICA on the mixed data failed to even recover all of the speech signals, while HTICA succeeded. In these cases, we had to re-start FastICA until it recovered all the signals.
4 New approach to orthogonalization and a new analysis of empirical covariance
As noted above, the technique in [AGNR15] , while being provably efficient and correct, suffers from practical implementation issues. Here we discuss two alternatives: orthogonalization by centroid body scaling and orthogonalization by using the empirical covariance. The former, orthogonalization via centroid body scaling, uses the samples already present in the algorithm rather than relying on a random walk to draw samples which are approximately uniform in the algorithm's approximation of the centroid body (as is done in [AGNR15] ). This removes the dependence on random walks and the ellipsoid algorithm; instead, we use samples that are distributed according to the original heavy-tailed distribution but non-linearly scaled to lie inside the centroid body. We prove in Lemma 3 that the covariance of this subset of samples is enough to orthogonalize the mixing matrix A. Secondly, we prove that one can, in fact, "forget" that the data is heavy tailed and orthogonalize by using the empirical covariance of the data, even though it diverges, and that this is enough to orthogonalize the mixing matrix A. However, as observed in experimental results, in general this has a downside compared to orthogonalization via centroid body in that it could cause numerical instability during the "second" phase of ICA as the data obtained is less well-conditioned. This is illustrated directly in the table in Figure 4 containing the singular value and condition number of the mixing matrix BA in the approximately orthogonal ICA model.
In [AGNR15] , another orthogonalization procedure, namely orthogonalization via the uniform distribution in the centroid body is theoretically proven to work. Their procedure does not suffer from the numerical instabilities and composes well with the second phase of ICA algorithms. An impractical aspect of that procedure is that it needs samples from the uniform distribution in the centroid body. We described orthogonalization via centroid body in Section 1, except for the estimation of p(x), the Minkowski functional of the centroid body. The complete procedure is stated in Subroutine 1.
We now explain how to estimate the Minkowski functional. The Minkowski functional was informally described in Section 1. The Minkowski functional of ΓX is formally defined by p(x) := inf{t > 0 : x ∈ tΓX}. Our estimation of p(x) is based on an explicit linear program (LP) (10) that gives the Minkowski functional of the centroid body of a finite sample of X exactly and then arguing that a sample estimate is close to the actual value for ΓX. For clarity of exposition, we only analyze formally a special case of LP (10) that decides membership in the centroid body of a finite sample of X (LP (9)) and approximate membership in ΓX. This analysis is in Section 5. Accuracy guarantees for the approximation of the Minkowski functional follow from this analysis.
). Let U be a family of n-dimensional product distributions. LetŪ be the closure of U under invertible linear transformations. Let Q(P) be an n-dimensional distribution defined as a function of P ∈Ū . Assume that U and Q satisfy:
1. For all P ∈ U , Q(P) is absolutely symmetric.

of ICA model X = AS so each S i is symmetric with (1 + γ) moments. Output: Matrix B approximate orthogonalizer of A 1: for i = 1 : N do,
Let λ * be the optimal value of (10) with q = X (i) .
3. For any P ∈Ū , Cov(Q(P)) is positive definite.
Then for any symmetric ICA model X = AS with P S ∈ U we have Cov(Q(P X )) −1/2 is an orthogonalizer of X.
Lemma 3. Let X be a random vector drawn from an ICA model X = AS such that for all i we have
Proof. We will be applying Lemma 2. Let U denote the set of absolutely symmetric product distributions P W over R n such that E|W i | = 1 for all i. For P V ∈Ū , let Q(P V ) be equal to the distribution obtained by scaling
For all P W ∈ U , W i is symmetric and E|W i | = 1 which implies that αW , that is, Q(P W ) is absolutely symmetric. Let P V ∈Ū . Then Q(P V ) is equal to the distribution of αV . For any invertible linear transformation T and measurable set M, we have
. Thus Q is linear equivariant. Let P ∈Ū . Then there exist A and P W ∈ U such that P = AP W . We get
) is a diagonal matrix with elements E(α 2 W 2 i ) which are non-zero because we assume E|W i | = 1. This implies that Cov(Q(P)) is positive definite and thus by Lemma 2, Cov(Y ) −1/2 is an orthogonalizer of X.
Here we show the somewhat surprising fact that orthogonalization of heavy-tailed signals is sometimes possible by using the "standard" approach: inverting the empirical covariance matrix. The advantage here, is that it is computationally very simple, specifically that having heavy-tailed data incurs very little computational penalty on the process of orthogonalization alone. It's standard to use covariance matrix for whitening when the second moments of all independent components exist [HKO01] : Given samples from the ICA model X = AS, we compute the empirical covariance matrixΣ which tends to the true covariance matrix as we take more samples and set B =Σ −1/2 . Then one can show that BA is a rotation matrix, and thus by pre-multiplying the data by B we obtain an ICA model Y = BX = (BA)S, where the mixing matrix BA is a rotation matrix, and this model is then amenable to various algorithms. In the heavy-tailed regime where the second moment does not exist for some of the components, there is no true covariance matrix and the empirical covariance diverges as we take more samples. However, for any fixed number of samples one can still compute the empirical covariance matrix. In previous work (e.g., [CB04] ), the empirical covariance matrix was used for whitening in the heavy-tailed regime with good empirical performance; [CB04] also provided some theoretical analysis to explain this surprising performance. However, their work (both experimental and theoretical) was limited to some very special cases (e.g., only one of the components is heavy-tailed, or there are only two components both with stable distributions without finite second moment).
We will show that the above procedure (namely pre-multiplying the data by B :=Σ −1/2 ) "works" under considerably more general conditions, namely if (1 + γ)-moment exists for γ > 0 for each independent component S i . By "works" we mean that instead of whitening the data (that is BA is rotation matrix) it does something slightly weaker but still just as good for the purpose of applying ICA algorithms in the next phase. It orthogonalizes the data, that is now BA is close to a matrix whose columns are orthogonal. In other words, (BA)
T (BA) is close to a diagonal matrix (in a sense made precise in Theorem 5). Let X be a real-valued symmetric random variable such that E(|X| 1+γ ) ≤ M for some M > 1 and 0 < γ < 1. The following lemma from [AGNR15] says that the empirical average of the absolute value of X converges to the expectation of |X|. The proof, which we omit, follows an argument similar to the proof of the Chebyshev's inequality. LetẼ N [|X|] be the empirical average obtained from N independent samples X (1) , . . . , X (N ) , i.e., (
Lemma 4. Let ∈ (0, 1). With the notation above, for N ≥
Theorem 5 (Orthogonalization via covariance matrix). Let X be given by ICA model X = AS. Assume that there exist t, p, M > 0 and γ ∈ (0, 1) such that for all i we have
} for all i with probability 1 − δ when N ≥ poly(n, M, 1/p, 1/t, 1/ , 1/δ).
Proof idea. For i = j we have E(S i S j ) = 0 (due to our symmetry assumption on S) and E(
. Now by our assumption that (1 + γ)-moments exist, Lemma 4 is applicable and implies that empirical averageẼS i tends to the true average ES i as we increase the number of samples. The true average is 0 because of our assumption of symmetry (alternatively, we could just assume that the X i and hence S i have been centered). The diagonal entries of L are bounded away from 0: This is clear when the second moment is finite, and follows easily by hypothesis (c) when it is not. Finally, one shows that if in L the diagonal entries highly dominate the off-diagonal entries, then the same is true of
and so by Lemma 4, for i = j,
when N ≥ ( γ . Next, we aim to bound D 2 which can be done by writing
where
. Consider the random variable 1(s
≥ N p and use a Chernoff bound to see
and when k∈[N ] 1(s
Then with probability at least 1 − n exp(−N p/8),
all entries of D −1 are at least t 2 p/2. Using this, if N ≥ N 1 := (8/p) ln(3n/δ) then D 2 ≤ 2/pt 2 with probability at least 1 − δ/3.
Similarly, suppose that D 2 ≤ 2/pt 2 and choose 1 = min{
pt 2 } and
2 ≤ t 4 p 2 /8 with probability at least 1 − δ/3. Invoking (7), when N ≥ max{N 1 , N 2 }, we have
with probability at least 1 − 2δ/3. Finally, we upper bound 1/d i for a fixed i by using Markov's inequality:
so that 1/d i ≤ N 4 for all i with probability at least 1 − δ/3 when N ≥ N 3 := n/3δ. Therefore, when
for all i with overall probability at least 1 − δ.
We used the following technical result.
Lemma 6. Let · be a matrix norm such that AB ≤ A B . Let matrices C, E ∈ R n×n be such that C −1 E 2 ≤ 1, and letC = C + E. Then
This implies that if
2 ), then
In Theorem 5, the diagonal entries are lower bounded, which avoids some degeneracy, but they could still grow quite large because of the heavy tails. This is a real drawback of orthogonalization via covariance. HTICA, using the more sophisticated orthogonalization via centroid body scaling does not have this problem. We can see this in the right table of Figure 4 , where the condition number of "centroid" is much smaller than the condition number of "covariance."
5 Membership oracle for the centroid body, without polarity
We will see now how to implement an -weak membership oracle for ΓX directly, without using polarity. We start with an informal description of the algorithm and its correctness.
The algorithm implementing the oracle (Subroutine 2) is the following: Let q ∈ R n be a query point. Let X 1 , . . . , X N be a sample of random vector X. Given the sample, let Y be uniformly distributed in {X 1 , . . . , X N }. Output YES if q ∈ ΓY , else output NO.
Idea of the correctness of the algorithm: If q is not in (ΓX) , then there is a hyperplane separating q from (ΓX) . Let {x : a T x = b} be the hyperplane, satisfying a = 1, a T q > b and a T x ≤ b for every x ∈ (ΓX) . Thus, we have h (ΓX) (a) ≤ b and h ΓX (a) ≤ b − . We have
when N is large enough with probability at least 1 − δ over the sample X 1 , . . . , X N . In particular, h ΓY (a) ≤ b, which implies q / ∈ ΓY and the algorithm outputs NO, with probability at least 1 − δ.
If q is in (ΓX) − , let y = q + q ∈ ΓX. We will prove the following claim: Informal claim (Lemma 13): For p ∈ ΓX, for large enough N and with probability at least 1 − δ there is z ∈ ΓY so that z − p ≤ /10.
This claim applied to p = y to get z, convexity of ΓY and the fact that ΓY contains B σ min (A)B n 2 (Lemma 9) imply that q ∈ conv(B ∪ {z}) ⊆ ΓY and the algorithm outputs YES. We will prove the claim now. Let p ∈ ΓX. By the dual characterization of the centroid body (Proposition 10), there exists a function λ :
By Lemma 4 and a union bound over every coordinate we get P( p − z ≥ ) ≤ δ for N large enough.
Lemma 7 ([AGNR15]). Let S = (S 1 , . . . , S n ) ∈ R n be an absolutely symmetrically distributed random vector such that E(
). Let X be a random vector on R n . Let A : R n → R n be an invertible linear transformation. Then Γ(AX) = A(ΓX).
Lemma 9. Let S = (S 1 , . . . , S n ) ∈ R n be an absolutely symmetrically distributed random vector such that
Proof. From Lemma 7 we know ±e i ∈ ΓS. It is enough to apply Lemma 13 to ±e i with = / √ n and δ = δ /(2n). This gives, for any
Proposition 10 (Dual characterization of centroid body). Let X be a n-dimensional random vector with finite first moment, that is, for all u ∈ R n we have E(| u, X |) < ∞. Then
Proof. Let K denote the rhs of the conclusion.We will show that K is a non-empty, closed convex set and show that h K = h ΓX , which implies (8).
By definition, K is a non-empty bounded convex set. To see that it is closed, let (y k ) k be a sequence in K such that y k → y ∈ R n . Let λ k be the function associated to y k ∈ K according to the definition of K. Let P X be the distribution of X. We have λ k L ∞ (P X ) ≤ 1 and, passing to a subsequence k j ,
Thus, we have y = lim j y kj = lim j E((λ kj (X)X) = E(λ(X)X) and K is closed.
To conclude, we compute h K and see that it is the same as the definition of h ΓX . In the following equations λ ranges over functions such that λ : R n → R is Borel-measurable and −1 ≤ λ ≤ 1.
and setting λ * (x) = sgn x, θ ,
Lemma 11 (LP). Let X be a random vector uniformly distributed in {x
2. Point q ∈ ΓX iff there is a solution λ ∈ R N to the following linear feasibility problem:
3. Let λ * be the optimal value of (always feasible) linear program
with λ * = ∞ if the linear program is unbounded. Then the Minkowski functional of ΓX at q is 1/λ * .
1. This is proven in [McM71] . It is also a special case of Proposition 10. We include an argument here for completeness.
. We compute h K to see it is the same as h ΓX in the definition of ΓX (Definition 1). As K and ΓX are non-empty compact convex sets, this implies K = ΓX. We have
2. This follows immediately from part 1.
3. This follows from part 1 and the definition of Minkowski functional.
Input: Query point q ∈ R n , samples from symmetric ICA model X = AS, bounds s M ≥ σ max (A), s m ≤ σ min (A), closeness parameter , failure probability δ. Output: ( , δ)-weak membership decision for q ∈ ΓX.
1:
Check the feasibility of linear program (9). If feasible, output YES, otherwise output NO.
Proposition 12 (Correctness of Subroutine 2). Let X = AS be given by an ICA model such that for all i we have E(|S i | 1+γ ) ≤ M < ∞, S i is symmetrically distributed and normalized so that E|S i | = 1. Then, given a query point q ∈ R n , > 0, δ > 0, s M ≥ σ max (A), and s m ≤ σ min (A), Subroutine 2 is an -weak membership oracle for q and ΓX with probability 1 − δ using time and sample complexity poly(n, M, 1/s m , s M , 1/ , 1/δ).
Proof. Let Y be uniformly random in (
. There are two cases corresponding to the guarantees of the oracle:
• Case q / ∈ (ΓX) . Then there is a hyperplane separating q from (ΓX) . Let {x ∈ R n : a T x = b} be the separating hyperplane, parameterized so that a ∈ R n , b ∈ R, a = 1, a T q > b and a T x ≤ b for every x ∈ (ΓX) . 
we have
In particular, with probability at least 1 − δ we have h ΓY (a) ≤ b, which implies q / ∈ ΓY and, by Lemma 11, Subroutine 2 outputs NO.
• Case q ∈ (ΓX) − . Let y = q + q = q(1 + q ). Let α = 1 + q . Then y ∈ ΓX. Invoke Lemma 13 for i.i.d. sample (x (i) ) N i=1 of X with p = y and equal to some 1 > 0 to be fixed later to conclude y ∈ (ΓY ) 1 . That is, there exist z ∈ ΓY such that z − y ≤ 1 .
Let w = z/α. Given (12) and the relationships y = αq and z = αw, we have w − q ≤ z − y ≤ 1 . To conclude, remember that q ∈ (ΓX) − . Therefore q + ≤ √ nσ max (A) (from Lemma 7 and equivariance of the centroid body, Lemma 8). This implies
The claim follows.
Lemma 13. Let X be a n-dimensional random vector such that for all coordinates i we have E(|X i | 1+γ ) ≤ M < ∞. Let p ∈ ΓX. Let (X Proof. By Proposition 10, there exists a measurable function λ : R n → R, −1 ≤ λ ≤ 1 such that p = E(Xλ(X)). Let
By Proposition 10, z ∈ ΓY . We have E X (i) (X (i) λ(X (i) )) = p and, for every coordinate j,
By Lemma 4 and for any fixed coordinate j we have, over the choice of (X (i) )
whenever N ≥ (8M √ n/ ) 1 2 + 1 γ . A union bound over n choices of j gives: 
Infinite sequences of symbols are of paramount importance in a wide range of fields, ranging from formal languages to pure mathematics and physics. A landmark was the discovery in 1912 by Axel Thue, founding father of formal language theory, of the famous sequence 0110 1001 1001 0110 1001 0110 · · · .Thue was interested in infinite words which avoid certain patterns, like squares ww or cubes www, when w is a non-empty word. Indeed, the sequence shown above, called the Thue-Morse sequence, is cube-free. It is perhaps the most natural cube-free infinite word. A common way to transform infinite sequences is by using finite state transducers. These transducers are deterministic finite automata with input letters and output words for each transition; an example is shown in Figure 1 . Usually we omit the words "finite state" and refer to transducers. A transducer maps infinite sequences to infinite sequences by reading the input sequence letter by letter. Each of these transitions produces an output word, and the sequence formed by concatenating each of these output words in the order they were produced is the output sequence. In particular, since this transducer runs for infinite time to read its entire input, this model of transduction does not have final states. A transducer is called k-uniform if each step produces k-letter words. For example, Mealy machines are 1-uniform transducers. A transducer is non-erasing if each step produces a non-empty word; this condition is prominent in this paper.
Although transducers are a natural machine model, hardly anything is known about their capabilities of transforming infinite sequences. To state the issues more clearly, let us write x y if there is a transducer taking y to x. This transducibility gives rise to a partial order of stream degrees [6] that is analogous to, but more fine-grained than, recursiontheoretic orderings such as Turing reducibility ≤ T and many-one reducibility ≤ m . We find it surprising that so little is known about . As of now, the structure of this order is vastly unexplored territory with many open questions. To answer these questions, we need a better understanding of transducers.
The main things that are known at this point concern two particularly well-known sets of streams, namely the morphic and automatic sequences. Morphic sequences are obtained as the limit of iterating a morphism on a starting word (and perhaps applying a coding to the limit word). Automatic sequences have a number of independent characterizations (see [1] ); we shall not repeat these here. There are two seminal closure results concerning the transduction of morphic and automatic sequences:
(1) The class of morphic sequences is closed under transduction (Dekking [4] ).
(2) For all k, the class of k-automatic sequences is closed under uniform transduction (Cobham [3] ).
The restriction in (2) to uniform transducers is shown by the following example.
Let w ∈ { 0, 1 } ω be defined by w(n) = 1 if n is a power of 2 and w(n) = 0 otherwise. This sequence is 2-automatic. Let h be the morphism 0 → 0 and 1 → 01. Taking the image of w under h, that is h(w), yields a sequence that is no longer automatic (but still morphic). Here is a sketch that h(w) is not 2-automatic. Note that the i th digit in h(w) is 1 iff i = 2 n + n for some n. Suppose that M is a finite-state machine with the property that reading in each number i in binary yields the i th digit of h(w). Let N be large enough so that the binary representation of 2 N + N has a run of zeroes longer than the number of states in N. Then by pumping, N must accept a number which is not of the form 2 n + n.
In this paper, we do not attack the central problems concerning the stream degrees. Instead, we are interested in a closure result for non-erasing transductions. Our interest comes from the following easy observation: This motivates the question: how powerful is non-erasing transduction?
The main result of this paper is stated in terms of the notion of α-substitutivity. This condition is defined in Definition 8 below, and the definition uses the eigenvalues of matrices naturally associated with morphisms on finite alphabets. Indeed, the core of our work is a collection of results on eigenvalues of these matrices. We prove that the set of α-substitutive words is closed under non-erasing finite state transduction. We follow Allouche and Shallit [1] in obtaining transducts of a given morphic sequence w by annotating an iteration morphism, and then taking a morphic image of the annotated limit sequence. For the first part of this transformation, we show that a morphism and its annotation have the same eigenvalues with non-negative eigenvectors. For the second part, we revisit the proof given in Allouche and Shallit [1] of Dekking's theorem that morphic images of morphic sequences are morphic. We simplify the construction in the proof to make it amenable for an analysis of the eigenvalues of the resulting morphism.
Durand [5] proved that if w is an α-substitutive sequence and h is a non-erasing morphism, then h(w) is α k -substitutive for some k ∈ N. We strengthen this result in two directions. First, we show that k may be taken to be 1; hence h(w) is α k -substitutive for every k ∈ N. Second, we show that Durand's result also holds for non-erasing transductions.
We recall some of the main concepts that we use in the paper. For a thorough introduction to morphic sequences, automatic sequences and finite state transducers, we refer to [1, 8] .
We are concerned with infinite sequences Σ ω over a finite alphabet Σ. We write Σ * for the set of finite words, Σ + for the finite, non-empty words, Σ ω for the infinite words, and Σ ∞ = Σ * ∪ Σ ω for all finite or infinite words over Σ.
A morphism is a map h : Σ → Γ * . This map extends by concatenation to h : Σ * → Γ * , and we do not distinguish the two notationally. Notice also that
An erased letter (with respect to h) is some a ∈ Σ such that h(a) = ε. A morphism h : Σ * → Γ * is called erasing if has an erased letter. A morphism is k-uniform (for k ∈ N) if |h(a)| = k for all a ∈ Σ. A coding is a 1-uniform morphism c : Σ → Γ.
A morphic sequence is obtained by iterating a morphism, and applying a coding to the limit word.
+ be a word, h : Σ → Σ * a morphism, and c : Σ → Γ a coding. If the limit h ω (s) = lim n→∞ h n (s) exists and is infinite, then h ω (s) is a pure morphic sequence, and c(h ω (s)) a morphic sequence.
If h(x 1 ) = x 1 z for some z ∈ Σ + , then we say that h is prolongable on x 1 . In this case,
is a pure morphic sequence. If additionally, the morphism h is k-uniform, then c(h
Example 4. A well-known example of a purely morphic word is the Thue-Morse sequence. This sequence can be obtained as the limit of iterating the morphism 0 → 01, 1 → 10 on the starting word 0. The first iterations are 0 → 01 → 0110 → 01101001 → 0110100110010110 → · · · , and they converge, in the limit, to the Thue-Morse sequence. As the morphism h is 2-uniform, the sequence is also 2-automatic.
An example of a purely morphic word which is not automatic is provided by the Fibonacci substitution a → ab, b → a. Starting with a, the fixed point is abaababaabaababaababaabaababaabaababaaba · · · .
Definition 6. For a ∈ Σ and w ∈ Σ * we write |w| a for the number of occurrences of a in w. Let h be a morphism over Σ. The incidence matrix of h is the matrix M h = (m i, j ) i∈Σ, j∈Σ where m i, j = |h( j)| i is the number of occurrences of the letter i in the word h( j).
Theorem 7 (Perron-Frobenius). Every non-negative square matrix M has a real eigenvalue α ≥ 0 that is greater than or equal to the absolute value of any other eigenvalue of M and the corresponding eigenvector is non-negative. We refer to α as the dominating eigenvalue of M.
The dominating eigenvalue of a morphism h is the dominating eigenvalue of M h . An infinite sequence w ∈ Σ ω over a finite alphabet Σ is said to be α-substitutive (α ∈ R)
if there exist a morphism h : Σ → Σ * with dominating eigenvalue α, a coding c : Σ → Σ and a letter a ∈ Σ such that (i) w = c(h ω (a)), and (ii) every letter of Σ occurs in h ω (a).
Remark. Let us remark on the importance of the condition (ii) in Definition 8. Without this condition every α-substitutive sequence w ∈ Σ ω would also be β-substitutive for every β > α that is the dominating eigenvalue of a non-negative integer matrix.
This can be seen as follows. Let h : Σ → Σ * be a morphism with dominating eigenvalue α. Let a ∈ Σ such that w = h ω (a) exists, is infinite and contains all letters from Σ. Then w is α-substitutive. Now let β > α be the dominating eigenvalue of a non-negative integer matrix. Then there exists an alphabet Γ (disjoint from Σ, Γ ∪ Σ = ∅) and a morphism g : Γ → Γ * with dominating eigenvalue β.
ω (a) = w and the dominating eigenvalue of z is β.
Two complex numbers x, y are called multiplicatively independent if for all k, ∈ Z it holds that x k = y implies k = = 0. We shall use the following version of Cobham's theorem due to Durand [5] . Theorem 9. Let α and β be multiplicatively independent Perron numbers. If a sequence w is both α-substitutive and β-substitutive, then w is eventually periodic.
Example 11. The transducer (Σ, ∆, Q, q 0 , δ, λ) shown in Figure 1 can be defined as follows: Σ = ∆ = { 0, 1 }, Q = { q 0 , q 1 , q 2 } with q 0 the initial state, and the transition function δ and output function λ are given by:
We use transducers to transform infinite words. The transducer reads the input word letter by letter, and the transformation result is the concatenation of the output words encountered along the edges.
Definition 12. Let M = (Σ, ∆, Q, q 0 , δ, λ) be a transducer. We extend the state transition function δ from letters Σ to finite words Σ * as follows: δ(q, ε) = q and δ(q, aw) = δ(δ(q, a), w) for q ∈ Q, a ∈ Σ, w ∈ Σ * . The output function λ is extended to the set of all words Σ ∞ = Σ ω ∪ Σ * by the following definition: λ(q, ε) = ε and λ(q, aw) = λ(q, a) λ(δ(q, a), w) for q ∈ Q, a ∈ Σ, w ∈ Σ ∞ . We introduce δ(w) and λ(w) as shorthand for δ(q 0 , w) and λ(q 0 , w), respectively. Moreover, we define M(w) = λ(w), the output of M on w ∈ Σ ω . In this way, we think of M as a function from (finite or infinite) words on its input alphabet to infinite words on its output alphabet
ω and y ∈ ∆ ω , we write y x if for some transducer M, we have M(x) = y.
Notice that every morphism is computable by a transducer (with one state). In particular, every coding is computable by a transducer.
Definition 13. Let M = (Σ, ∆, Q, q 0 , δ, λ) and N = (Σ , ∆ , Q , q 0 , δ , λ ) be transducers, and assume that Σ = ∆. We define the composition N • M to be the transducer
Here δ and λ are the extensions of the transition and output functions of N to Σ * , respectively. Proposition 14. Concerning the composition relation on transducers and on finite and infinite words:
Definition 15. Let h : Σ * → Σ * be morphisms, and let Γ ⊆ Σ be a set of letters. We call a letter a ∈ Σ (i) dead if h n (a) ∈ Γ * for all n ≥ 0, (ii) near dead if a Γ, and for all n > 0, h n (a) consists of dead letters,
with respect to h and Γ. We say that the morphism h respects Γ if every letter a ∈ Σ is either dead, near dead, resilient, or resurrecting. (Note that all of these definitions are with respect to some fixed h and Γ.)
Lemma 16. Let g : Σ * → Σ * be a morphism, and let Γ ⊆ Σ. Then g r respects Γ for some natural number r > 0.
Proof. See Lemma 7.7.3 in Allouche and Shallit [1] .
Definition 17. For a set of letters Γ ⊆ Σ and a word w ∈ Σ ∞ , we write γ Γ (w) for the word obtained from w by erasing all occurrences of letters in Γ.
Definition 18. Let g : Σ * → Σ * be a morphism, and Γ ⊆ Σ a set of letters. We construct an alphabet ∆, a morphism ξ : ∆ * → ∆ * and a coding ρ : ∆ → Σ as follows. We refer to ∆, ξ, ρ as the morphic system associated with the erasure of Γ from g ω .
Let r ∈ N >0 be minimal such that g r respects Γ (r exists by Lemma 16). Let D be the set of dead letters with respect to g r and Γ. For x ∈ Σ * we use brackets [x] to denote a new letter. For words w ∈ {g r (a) | a ∈ Σ}, whenever γ D (w) = w 0 a 1 w 1 a 2 w 2 · · · a k−1 w k−1 a k w k with a 1 , . . . , a k Γ and w 0 , . . . , w k ∈ Γ * , we define
Here it is to be understood that
Let the alphabet ∆ consist of all letters [a] and all bracketed letters [w] occurring in words blocks(g r (a)) for a ∈ Σ. We define the morphism ξ : ∆ → ∆ * and the coding ρ : ∆ → Σ by
Remark. The requirement that g r respects Γ in Definition 18 guarantees for every a ∈ Σ that either g r (a) consists of dead letters only or g r (a) contains at least one near dead or resilient letter. In both cases, blocks(g r (a)) is well-defined. As a consequence ξ([w]) is well-defined for every [w] ∈ ∆.
Example 19. We let Σ = { a, b, c } and define a morphism g : Σ → Σ * by a → ab, b → ac and c → a. The word g ω (a) = abacabaabacababacabaabacabacabaabacababa · · · is known as the tribonacci word.
Let Γ = { a }, that is, we delete the letter a. The morphism g does not respect Γ since g(c) = a ∈ Γ * but g 2 (c) = ab Γ * . However, g 2 respects Γ: g 2 (a) = abac, g 2 (b) = aba and g 2 (c) = ab. The letter a is resurrecting and b, c are resilient with respect to g 2 and Γ. 
Then an application of the coding ρ yields ρ(
Example 20. We let Σ = { a,
where
Proposition 21. Let g : Σ * → Σ * be a morphism, a ∈ Σ such that g ω (a) ∈ Σ ω , and Γ ⊆ Σ a set of letters. Let ∆, ξ and ρ be the morphic system associated to the erasure of Γ from g ω in Definition 18. Then
We prove by induction on n that for all words w ∈ ∆ * , and for all n ∈ N, cat(ξ n (w)) = g nr (cat(w)). The base case is immediate. For the induction step, assume that we have n ∈ N such that for all words w ∈ ∆ * , cat(ξ
By the induction hypothesis, cat(ξ n+1 (w)) = g nr (cat(ξ(w))) = g nr (g r (cat(w))) = g (n+1)r (cat(w)). To complete the proof, note that by definition ρ([w a u]) = γ Γ (w a u) and thus ρ(w) = γ Γ (cat(w)) for every w ∈ ∆ * . Hence, for all n ≥ 1,
Definition 22. Let g, h : Σ * → Σ * be morphisms such that h is non-erasing. We construct an alphabet ∆, a morphism ξ : ∆ * → ∆ * and a coding ρ : ∆ → Σ as follows. We refer to ∆, ξ, ρ as the morphic system associated with the morphic image of g ω under h.
For nonempty words w = a 1 a 2 · · · a k ∈ Σ * we define head(w) = a 1 and tail(w) = a 2 · · · a k . We also define img(w)
We define the morphism ξ : ∆ * → ∆ * and the coding ρ : ∆ → Σ by
Notice here the ρ([a]) and u i , defined using head() and tail(), are well-defined since h is non-erasing and hence h(a i ) will be nonempty.
Here is an example illustrating Definition 22. Let g be the substitution from the Fibonacci word, g(a) = ab and g(b) = a. Further, let h be defined so that h(a) = bb and h(b) = a. As in Definition 22, let ξ and ρ be defined by
The point here is that applying ρ to the limit word ξ ω ([a]) is the same as h(g ω (a)):
Proposition 24. Let g, h : Σ * → Σ * be morphisms such that h is non-erasing, and a ∈ Σ such that g ω (a) ∈ Σ ω . Let ∆, ξ and ρ be as in Definition 18. Then
Proof. We define z : ∆ → Σ * by z(a) = ε and z([a]) = a for all a ∈ Σ. By induction on n > 0 we show
We start with the base case. Note that ρ(ξ([a])) = h(g(a)) = h(g(z([a]))) and ρ(ξ(a)) = ε = h(g(z(a))) for all a ∈ Σ, and thus ρ(ξ(w)) = h(g(z(w))) for all w ∈ ∆ * . Moreover, we have z(ξ([a])) = g(a) = g(z([a])) and z(ξ(a)) = ε = g(z(a)) for all a ∈ Σ, and hence z(ξ(w)) = g(z(w)) for all w ∈ ∆ * .
Let us consider the induction step. By the base case and induction hypothesis
Thus ρ(ξ n ([a])) = h(g n (a)) for all n ∈ N, and taking limits yields
Every morphic image of a word can be obtained by erasing letters, followed by the application of a non-erasing morphism. As a consequence we obtain:
Corollary 25. The morphic image of a pure morphic word is morphic or finite.
ω be a word and h : Σ → Σ * a morphism. Let Γ = { a | h(a) = ε } be the set of letters erased by h, and ∆ = Σ \ Γ. Then h(w) = g(γ Γ (w)) where g is the non-erasing morphism obtained by restricting h to ∆. Hence for purely morphic w, the result follows from Propositions 21 and 24.
Theorem 26 (Cobham [2] , Pansiot [7] ). The morphic image of a morphic word is morphic.
Proof. Follows from Corollary 25 since the coding can be absorbed into the morphic image.
The following lemma states that if a square matrix N is an extension of a square matrix M, and all added columns contain only zeros, then M and N have the same non-zero eigenvalues.
Lemma 27. Let Σ, ∆ be disjoint, finite alphabets. Let M = (m i, j ) i, j∈Σ and N = (n i, j ) i,j∈Σ∪∆ be matrices such that (i) n i, j = m i, j for all i, j ∈ Σ and (ii) n i, j = 0 for all i ∈ Σ ∪ ∆, j ∈ ∆. Then M and N have the same non-zero eigenvalues.
Proof. N is a block lower triangular matrix with M and 0 as the matrices on the diagonal. Hence the eigenvalues of N are the combined eigenvalues of M and 0. Therefore M and N have the same non-zero eigenvalues.
We now show that morphic images with respect to non-erasing morphisms preserve α-substitutivity. This strengthens a result obtained in [5] where it has been shown that the non-erasing morphic image of an α-substitutive sequence is α k -substitutive for some k ∈ N. We show that one can always take k = 1. Note that every α-substitutive sequence is also α k -substitutive for all k ∈ N, k > 0.
Theorem 28. Let Σ be a finite alphabet, w ∈ Σ ω be an α-substitutive sequence and h : Σ → Σ * a non-erasing morphism. Then the morphic image of w under h, that is h(w), is α-substitutive.
Proof. Let Σ = { a 1 , . . . , a k } be a finite alphabet, w ∈ Σ ω be an α-substitutive sequence and h : Σ → Σ * a non-erasing morphism. As the sequence w is α-substitutive, there exist a morphism g : Σ → Σ * with dominant eigenvalue α, a coding c : Σ → Σ and a letter a ∈ Σ such that w = c(g ω (a)) and all letters from Σ occur in g ω (a). Then h(w) = h(c(g ω (a))) = (h•c)(g ω (a))), and h • c is a non-erasing morphism. Without loss of generality, by absorbing c into h, we may assume that c is the identity. From h and g, we obtain an alphabet ∆, a morphism ξ, and a coding ρ as in Example 29. Let F be the Fibonacci word (generated by the morphism a → ab and b → a) and let T be the Thue-Morse sequence. We show that there exist no non-erasing morphisms g, h such that g(F) = h(T) and this image is not ultimately periodic. Let g and h be non-erasing morphisms. The Fibonacci word is ϕ-substitutive where ϕ = (1 + √ 5)/2 is the golden ratio, and the Thue-Morse sequence is 2-substitutive. By Theorem 28, g(F) is ϕ-substitutive and h(T) is 2-substitutive. Note that ϕ and 2 are multiplicatively independent: using induction on k ∈ N >0 it follows that every ϕ k is of the form a + b √ 5 for rational numbers a, b > 0. It follows by Theorem 9 that g(F) = h(T) implies that this word is ultimately periodic.
Remark. The restriction to non-erasing morphisms in Theorem 28 is important since every morphic sequence can be obtained by erasure of letters from a 2-substitutive sequence.
Nevertheless, we can use the above theorem to reason about morphic images with respect to erasing morphisms as follows. Let w ∈ Σ ω , and g : Σ → Σ * a morphism. Let Γ be the letters erased by g, and let h be the restriction of g to Σ \ Γ. Then h is non-erasing and g(w) = h(γ Γ (w)). Hence, if γ Γ (w) is α-substitutive, then so is g(w) by Theorem 28. As a consequence, it suffices to determine α-substitutivity of all sequences γ Γ (w) with Γ ⊆ Σ (using Definition 18 and Proposition 21).
In this section, we give a proof of the following theorem due to Dekking [4] .
Theorem 30 (Transducts of morphic sequences are morphic). If M = (Σ, ∆, Q, q 0 , δ, λ) is a transducer with input alphabet Σ and x ∈ Σ ω is a morphic sequence, then M(x) is morphic or finite.
This proof will proceed by annotating entries in the original sequence x with information about what state the transducer is in upon reaching that entry. This allows us to construct a new morphism which produces the transduced sequence M(x) as output. After proving this theorem, we will show that this process of annotation preserves α-substitutivity. Example 31. To illustrate several points in this section, we will consider the Fibonacci morphism (h(a) = ab, h(b) = a) and the transducer which doubles every other letter, shown in Figure 2. 
We show in Lemma 40 that transducts of morphic sequences are morphic. In order to prove this, we also need several lemmas about transducers which are of independent interest. The approach here is adapted from a result in Allouche and Shallit [1] ; it is attributed in that book to Dekking. We repeat it here partly for the convenience of the reader, but mostly because there are some details of the proof which are used in the analysis of the substitutivity property.
Definition 32 (τ w , Ξ(w)). Given a transducer M = (Σ, ∆, Q, q 0 , δ, λ) and a word w ∈ Σ * , we define τ w ∈ Q Q to be τ w (q) = δ(q, w).
Example 33. Recall the transducer M from Figure 2 . Let id : Q → Q be the identity, and let ν : Q → Q be the transposition ν(s) = t and ν(t) = s. For this transducer, τ w = id if |w| is even and τ w = ν if |w| is odd. We have Ξ(a) = (τ a , τ ab , τ aba , τ abaab , τ abaababa , . . .). In this notation,
Next, we show that { Ξ(w) : w ∈ Σ * } is finite.
Lemma 34. For any transducer M and any morphism h : Σ → ΣLemma 39. For all σ ∈ Σ, all w ∈ Σ * and all natural numbers n, if h
In particular, for 1 ≤ i ≤ , the first component of the i th term in h n (σ, Θ(w)) is s i .
Proof. By induction on n. For n = 0, the claim is trivial. Assume that it holds for n. Let h n (σ) = s 1 s 2 · · · s , and for 1
Concatenating the sequences h(s i , Θ((h n w)s 1 · · · s i−1 )) for i = 1, . . . , completes our induction step. 
This sequence z is morphic in the alphabet Σ × Q.
Proof. For (i), write h(x 1 ) as x 1 x 2 · · · x . Using the fact that h i ( ) = for all i, we see that
This verifies the prolongability. For (ii): if Θ(w) = Θ(u), then τ w and τ u are the first component of Θ(w) and are thus equal.
We turn to (iii). Taking w = in Lemma 39 shows that h
The image of this sequence under the coding c is
In view of the τ functions' definition (Def. 32), we obtain z in (2) . By definition, z is morphic.
Therefore, row k of N times v is
v b m a,b = αv a , since v is an eigenvector of M. Finally we note that the kth entry of v is v a by its definition. Hence multiplying v by N multiplies the kth entry of v by α for all k.
We have shown that v is a column eigenvector of N with eigenvalue α, so the (column) eigenvalues of M are all present in N. However, since a matrix and its transpose have the same eigenvalues, the (column) qualification on the eigenvalues is unnecessary.
If h is an annotation of h, then we have
Lemma 44. Let h, h be morphisms such that h :
Then every eigenvalue of h with a non-negative eigenvector is also an eigenvalue for h.
Proof. Let M = (m i, j ) i, j∈Σ be the incidence matrix of h and N = (n i, j ) i, j∈Σ×A be the incidence matrix of h. Let r be an eigenvalue of N with corresponding eigenvector v = (v (b, a) ) (b, a)∈Σ×A , that is, Nv = rv and v 0. We define a vector w = (w b ) b∈Σ as follows:
Hence Mw = rw. If w 0 it follows that r is an eigenvalue of M. Note that if v is non-negative, then w 0. This proves the claim.
* is an annotation of h : Σ → Σ * . Then the dominant eigenvalue for h coincides with the dominant eigenvalue for h.
Proof. By Lemma 43 every eigenvalue of h is an eigenvalue of h. Thus the dominant eigenvalue of h is greater or equal to that of h. By Theorem 7, the dominant eigenvalue of a non-negative matrix is a real number α > 1 and its corresponding eigenvector is nonnegative. By Lemma 43, every eigenvalue of h with a non-negative eigenvector is also an eigenvalue of h. Thus the dominant eigenvalue of h is also greater or equal to that of h.
Hence the dominant eigenvalues of h and h must be equal.
Theorem 46. Let α and β be multiplicatively independent real numbers. If v is a α-substitutive sequence and w is an β-substitutive sequence, then v and w have no common non-erasing transducts except for the ultimately periodic sequences.
Proof. Let h v and h w be morphisms whose fixed points are v and w, respectively. By the proof of Theorem 30, x is a morphic image of an annotation h v of h v , and also of an annotation h w of h w . The morphisms must be non-erasing, by the assumption in this theorem. By Corollary 45 and Theorem 28, x is both α-and β-substitutive. By Durand's Theorem 9, x is eventually periodic.
We conclude the section with an example of Theorem 30 and the lemmas in this section.
Example 47. We saw the Fibonacci sequence in Example 5:
We conclude our series of examples pertaining to this sequence and the transducer M which doubles every other letter (see Example 31 and Figure 2) . We want to exhibit h, following the recipe of Lemma 40. First, some examples of how h works: It turns out that only a few elements from this A end up appearing in the expressions for h(σ, Θ(w)): It is convenient to abbreviate some of the elements of Σ × A: Let us use x as an element of {a, b}, and also write (x, Θ( )) as x 0 , (x, Θ(a)) as x 1 , (x, Θ(b)) as x 2 and (x, Θ(ab)) as x 3 . It turns out that we do not need to exhibit h in full because only eight points are reachable from a 0 . We may take h to be
The fixpoint of this morphism starting with a 0 starts as Recall that λ : Σ × Q → ∆ * = Σ * in our transducer doubles whatever letter it sees while in state s and copies whatever letter it sees while in state t. That is, λ(x s ) = xx, and λ(x t ) = x. Thus when we apply the morphism λ to the sequence z, we get λ(z) = aa b aa a bb a bb a aa b aa a bb a bb a aa b aa b aa a bb a aa b aa b · · · As we saw in the proof of Theorem 30, this sequence aabaaabbabbaaabaaabbabbaaabaabaaabbaaabaab · · · is exactly M(x).
We have re-proven some of the central results in the area of morphic sequences, the closure of the morphic sequences under morphic images and transduction. However, the main results in this paper come from the eigenvalue analyses which followed our proofs in Sections 3 and 4. These are some of the only results known to us which enable one to prove negative results on the transducibility relation . One such result is in Theorem 46; this is perhaps the culmination of this paper.
The next step in this line of work is to weaken the hypothesis in some of results that the transducers be non-erasing. Although our results can be used to reason about erasing morphisms, see Remark 3, this does not help us with erasing transducers since annotating a morphism can yield an unbounded large alphabet. As a consequence, to reason about erasing transducers, we need to understand better what form of annotated morphisms arise from transducers, and how these interact with the erasure of letters (Proposition 21).
Stroke and seizures-induced neurodegeneration share a number of biological processes, including increased neuronal activity, neuronal plasticity, inflammation, and apoptosis [1, 2] . Separation of effects of these processes on gene expression, identification of participating transcription factors, and comparison of transcriptional regulation between the two pathological conditions remain a challenging task. Global gene expression following stroke and seizures were compared before at a single time-point [3] , but no comparison of time-series gene profiling datasets from the two conditions was reported to date.
Alter et al. first introduced a concept that orthogonal components (eigensystems) resulting from the singular value decomposition (SVD) of time-series gene expression dataset [4, 5] may help to separate concurrent effects of different processes and regulators on gene expression. These authors proposed that an eigen array may reflect a genome-wide input from a particular regulator, with the corresponding eigen gene reflecting this regulator's activity across the samples (arrays). For an illustration of the SVD nomenclature, when applied to gene expression -see Additional file 1.
A number of recent studies concentrated on usefulness of eigengenes [6] [7] [8] [9] [10] , whereas the properties and interpretation of eigenarrays remained relatively less explored. We previously suggested that conservation of eigenarrays between related biological systems may identify eigensystems of biological origin [11] . In the same work, utilizing a comparative SVD approach we identified an eigensystem conserved between hippocampal development and differentiation of hippocampal neurons in vitro. Analysis of cis-regulation of that eigensystem revealed that it reflected exit of neural precursors from the cell cycle and beginning of neuronal differentiation, regulated by transcription factors E2f1 and Nr2f1 [12] .
Bayesian Networks (BN) learning approach is a wellestablished method of modelling gene regulation and interactions between gene regulatory motifs, starting from gene expression data [13] or gene expression and genomic sequence data [14] [15] [16] [17] [18] [19] [20] . The use of linear regression in analysis of gene cis-regulation is grounded in the linear response model of gene regulation [21, 22] .
Here, we report a time-series dataset from gene expression profiling in the rat MCAO model of stroke, and compare these data to the published time-series dataset from the kainate-induced seizures model [23] . By comparative SVD approach, followed by Bayesian network analysis of cis-regulation, we identified two conserved eigensystems separating the effects of different well-defined biological processes on gene expression and regulated by distinct sets of transcription factor binding sites. The results obtained on either dataset were validated on the other.

We compared two time-series gene expression datasets from experimental rat models of stroke and epilepsy, which were the transient middle-cerebral artery occlusion (MCAO) and the kainate-induced seizures, respectively. The MCAO dataset was generated in our laboratory and probed gene expression in the cortex of the ischemic hemisphere at four time-points (6, 12, 24, 48 h ) following a 90 minutes occlusion of the right middle-cerebral artery in adult anesthetized rats, and included sham-operated animals as controls. The kainate dataset, published by Koh and co-workers [23] probed gene expression in the hippocampus of adult rats at five time-points (1, 6, 24, 72, 240 h) following the injection of kainate -a neurotransmitter analogue inducing seizures, which can last for several hours, followed by a seizure-free latent period.
As immobilization of a conscious animal and injection alters gene expression in the brain, this dataset included a control time-series following the injection of saline.
The overall design of our study is illustrated in Figure  1 . We transform each dataset (MCAO, kainate) separately by SVD ( Figure 1A ) and identify eigenarrays conserved between the two systems ( Figure 1B ). This is followed by analysis of biological function using Gene Ontology (GO), and gene cis-regulation using Bayesian networks (BN) and our TRAM database of putative regulatory regions and motifs. These analyses are performed separately for either dataset and then the results for the corresponding eigensystems are compared (GO terms) or statistically cross-validated (BN results) on the other dataset. The cross-validation between the stroke and seizures data is not contradictory with the goal of gaining information by comparison of the two, because the two experimental models can be assumed -on biological grounds -to share some, but not all, regulatory mechanisms. Note that features specific for one model can be identified, as for each model we separately account for the multiplicity of testing. The eigenarrays resulting from the SVD of either dataset were compared by correlation analysis performed for the genes common between the two datasets. (C) For the emerging conserved eigensystems 2 and 3, separately for all the genes in either dataset, we studied their functional Gene Ontology (GO) associations and employed Bayesian Networks (BN) to study their cis-regulation. The results obtained on one dataset were then compared (GO) or statistically tested (BN) on the other.
The global temporal changes in gene expression following MCAO in the stroke model are dominated by the top three eigensystems (Figure 2A ). The eigengene of the first eigensystem in the MCAO dataset (M1, "M" to indicate MCAO) is constant in time (data not shown) in the log-expression space and thus represents the average level of expression across all the conditions. The second eigengene (M2) represents an increased expression, as compared to control value, at 12-48 h following MCAO, with a peak at 12 h ( Figure 2B ). The third eigengene (M3) represents a complex pattern with an increase in gene expression at 12 h followed by down-regulation of expression at 24 h and further drop at 48 h ( Figure 2C ). Notably, the three top eigengenes indicate no changes in gene expression at 6 h after MCAO, which is in agreement with our earlier PCR results showing no changes in mRNA levels of a smaller number of genes [24] .
The global temporal pattern of gene expression following kainate-induced seizures in adult rats is dominated by the top three eigensystems ( Figure 2D ), of which the first again represents the magnitude (data not shown). The second eigengene (A2), represents an increased expression following the injection of kainate; starting at 1 h, largest at 6 and 24 h, returning to the baseline level at 72 and 240 h; and no change at any time-point after the injection of saline ( Figure 2E ). The third eigengene (A3) represents an increased expression at 1 and 6 h after the injection of kainite; followed by strong decrease in expression at 24 h, continuing, but less pronounced, also at the 72 and 240 h ( Figure 2F ).
Despite their overall similarity, the corresponding eigengenes are distinct between the two experimental models. In particular, eigengenes M2 and M3 show no change in expression at 6 h following the MCAO, in contrast to eigengenes A2 and A3, showing an increase at 6 h following the injection of kainate.
The kainate datasets comprised of expression profiles for 2786 genes (distinct Ensembl gene_stable_id) that significantly changed expression and the stroke dataset consisted of 2392 such genes, with 737 genes common between the two datasets. The correlation analysis revealed that the top three eigenarrays (compared for the common genes) were highly correlated ( Figure  2G ). The correlations between the respective first, second, and third eigenarrays were 0.87, 0.84, and 0.63, respectively. Note that the correspondence between the three conserved eigenarrays was one-to-one. Given the length (737) of the correlated vectors, these correlations are highly significant (p-values: 10 -229 , 10 -197 , 10 -83 , respectively, assuming independence of genes). This indicates that the top three eigenarrays are highly conserved between the two datasets. Figure 2H -I shows directly genes' loadings of the respective second (H) or third (I) eigensystem in the two datasets, sorted on their average loading in both datasets. This sorting of the genes aids visualization of the eigenarrays conservation, but is not in any way a reason for it, as the correlations shown in Figure 2G were computed before the sorting (and would not be affected by it, anyway). The tangent-like shape of the plots reflects the bell shape of the distributions of genes' loadings of eigensystems 2 and 3.
Further, we focus on eigensystems 2 and 3 characterized by conservation of their eigenarrays occurring despite differences between the corresponding eigengenes ( Figure 2B vs. E, C vs. F). This suggests that the two eigensystems reflect regulatory inputs operating in both systems, but with different kinetics and relative strengths.
A universal reason underlying co-regulation of genes is participation of their products in a common biological process. To assess if the contribution of the eigensystems 2 and 3 to the gene expression profiles is associated with biological functions, we analyzed the Gene Ontology "biological process" annotations of all genes in either dataset, ranked on the loadings of the respective eigensystems 2-3.
In both experimental models, the positive loading of the second eigensystem was significantly associated with overlapping GO terms describing the inflammatory response to the brain injury ( Figure 3A, B) . Additionally, in the MCAO system the positive loading of eigensystem M2 was also significantly associated with GO terms describing programmed cell death (apoptosis).
In the kainate system, the positive loading of eigensystem A3 was highly significantly associated with several overlapping GO terms describing neuronal activity, such as: synaptic transmission, transmission of the nerve impulse ( Figure 3C ). No such association was detected for third eigensystem (M3) from the SVD on the MCAO dataset, following its initial filtering (ANOVA p-value < 0.05). However, when the GO analysis was repeated for the third eigenarray in the SVD result on the MCAO dataset filtered at ANOVA p-value < 0.5 and thus containing more genes, there was a clear association between the loadings of the third eigensystem and GO terms describing neuronal activity ( Figure 3D ). Loosening of the p-value threshold was possible, because the top three eigensystems were extremely robust to the change of the p-value threshold, with eigenarrays correlations > 0.999 between vectors of length 2786 for the change of the threshold from 0.05 to 0.5 (data not shown). Comparison of the singular values ( Figure 2A vs. 2D) indicates that the relative contribution of the conserved third eigensystem (reflecting neuronal activity) was higher in the kainate system, while the relative contribution of the conserved second eigensystem (reflecting inflammation and/or apoptosis) was higher in the stroke.
Conservation of eigenarrays suggests that they reflect regulatory mechanisms, possibly operating at the level of transcription regulation. To identify such mechanisms, we employed Bayesian networks, previously successfully applied to modelling transcriptional regulation [14, 15, [17] [18] [19] [20] . We follow the above approaches in general, but several essentials are specific to our methodology:
• Regulation of gene expression is analysed separately for each conserved eigensystem. In the subspace of a given eigensystem gene expression is binarized into up-and down-regulation, according to the sign of its loading. (Figure 4B , D).
• Our combinatorial model of cis-regulation takes into account fragmentation of metazoan cis-regulatory regions into multiple conserved non-coding sequences (CNSs) [25, 26] , and distinguishes between co-occurrence of several TF-binding motifs in the Figure 3 Functional Gene Ontology annotations associated with the conserved eigensystems (A-D) Association of loadings of the conserved eigensystems with the functional annotations from the GO "biological process" ontology were analyzed by Wilcoxon sign rank test using RankGOstat [55] . Twenty GO terms most associated with a given eigensystem, and their association FDR q-values are shown as bar plots. For the plots the q-values were log10-transformed and multiplied by +1 or -1, to reflect association with the positive or negative loadings of a particular eigensystem. GO terms with overlapping meanings (identified by human inspection) are indicated by the same colour of the bars, with red marking terms related to "synaptic transmission", blue marking terms similar to "inflammatory response", and black marking terms describing cell death/apoptosis.
same CNS and their co-occurrence in the same gene ( Figure 4A , C). Following previous work [27, 28] , we term every possible subset of the motifs present in the same CNS a composite motif.
• Regulatory mechanism is predicted by learning Bayesian networks with an exact algorithm. Computations are performed by double application of the BNFinder program [29] . The first run selects the most promising composite motifs (possibly single motifs), while the second run selects the sets of such composite motifs that best predict the sign of the loading the chosen eigensystem ( Figure 4E , F).
Four BN analyses were performed, separately for each conserved eigensystem in either experimental model (M2, A2, M3, A3). BN scores were directly converted to q-values -the false discovery rate [30] analogue of p-values, by comparing each feature's score on the Figure 4 Bayesian network model of fragmented cis-regulatory regions (A, C) Sequence preprocessing consists of extracting instances of composite motifs i.e. sets of (up to three motifs) in the same conserved non-coding sequence (CNS), from the flanks of transcription start sites of all human-rat orthologous genes. (B, D) Expression data preprocessing consists of SVD, followed by discretization of expression into up-and down-regulation in the subspace of a particular conserved eigensystem -based on the sign of its loading. (C, D) Composite motifs and expression data are combined in one dataset, in which the data records correspond to genes. (E) This dataset becomes an input for our Bayesian networks (BN) learning algorithm, which identifies sets of composite motifs most associated with the sign of loadings of a given eigensystem. (F) The final output consists of a ranking of such sets, with conditional probability distributions representing their impact on a given eigensystem. BN learning was performed independently for each of the eigensystems: A2, A3, M2, M3; on the data for all the genes in the respective dataset. Eigensystem A3 is shown as an example.
original data to the distribution of scores from 1000 BN analyses on permuted data -each following an independent random permutation and assignment of expression values to the genes' putative cis-regulatory regions. The conservation of the two eigensystems between the stroke allowed for selection of best features on one dataset (we choose up to ten features with training q-values < 0.05) and then testing them on the other -containing the data for largely different genes. The training and testing were performed for the conserved second ( Figure 5A , B) and third eigensystem ( Figure 5C , D) in both directions. During the test we used the same q-values as during the training, i.e. they were corrected for all the hypotheses ever looked at on the test dataset. We note that this is a very stringent correction, as only up to 20 hypotheses Figure 5 BN analysis of cis-regulation for the conserved eigensystems. The four tables (A-D) present the results of BN analysis of cisregulation for the conserved second and third eigensystems from either dataset, followed by testing of highest-ranking features on the corresponding eigensystem from the other dataset. In each panel, the column Feature lists up to 10 nonempty sets of composite motifs with highest BN score and q-value < 0.05 on the indicated training dataset. Note that single motifs are included in the set of composite motifs. BN score of a composite motif set is the ratio of its posterior probability to the posterior probability of the empty set. The corresponding q-value derives from the permutational test. The shaded columns give the values of BN score and the corresponding q-value for the same feature computed on the other (test) dataset. Red color marks the cells with the test q-values < 0.05 for the features that also had training q-value < 0.05 and the descriptions of such features are given in bold. The q-values take into account the multiplicity of testing for each dataset separately, so it is possible to identify the features significant for one dataset only. (E, F) The conditional probability tables for the pairs of motifs: {AP1F, SATB} (E) and {EGRF, LHXF} (F).
were considered for each eigensystem during the test stage (up to ten for either direction of the comparison).
Antagonistic effects of motifs binding AP1 and SATB on gene expression following the stroke BN search identified just one feature, namely the motif AP1F -a family of binding sites for the transcription factor AP1 (Additional file 2) as the feature significantly (q-value < 0.05) associated with the positive sign of eigensystem A2 in the kainate model ( Figure 5A , columns: "Training: Kainate"). Notably, this feature was significantly associated with the corresponding eigensystem M2, when tested on the dataset from the MCAO model ( Figure 5A , column Test: MCAO). The choice of the MCAO data as the training dataset resulted in identification of 7 significant features, of which the second was again AP1F, and only this feature was significant also in the cross-system test on the kainate dataset ( Figure 5B , columns "Test: kainate"). All remaining features identified as significant (q-value < 0.05) on the training datasets included AP1F as one motif, and two of them were pairs of AP1F with another motif in the same gene. Of the features significant in the MCAO system, particularly interesting is the pair {AP1F, SATB} -a set of two motifs co-occurring in the same gene, which have antagonistic effects on expression in the subspace of eigensystem M2. The presence of motif AP1F in the absence of SATB in the same gene was associated with the positive sign of M2 loading, while the presence of SATB in the absence of AP1F was associated with the negative M2 loading ( Figure 5E ).
Identification of known and new regulators/targets for the eigensystem reflecting synaptic activity BN search identified a number of features as highly significantly (q-value < 0.001) associated with the sign of M3 loading during the training on the kainate dataset. The ten highest-ranking features, ranked on their BN score were tested on the MCAO dataset ( Figure 5C ). Of the top ten features significant on the kainate dataset, four were also significant on the MCAO dataset. All of these features, marked in bold in Figure 5C , were pairs of motifs co-occurring in the same gene. All these pairs contained LHXF as one motif, with EGRF, AHRR, ZF5F or ZBPF as the other motif. The highest-ranking featurethe pair EGRF and LHXF in the same gene, but neither motif of its own, was 79% specific for the positive sign of eigensystem 3 ( Figure 5F ). When the training was performed on the MCAO dataset, several features significantly (q-value < 0.001) associated with the sign of M3 were identified ( Figure 5D ). Importantly, out of the top ten features identified on the stroke dataset, nine were also significantly associated with the same sign of M3 on the kainate dataset. The features significant in the cross-system test were either single motifs (AP1R, PARF, CREB, AHRR) or pairs of motifs in the same gene. All these pairs contained AP1R as one motif, with PARF, AHRR, ZF5F, EGRF, E4FF as the other motif. Three motifs, namely EGRF, ZF5F, AHRR were common between the top ten features identified during training on the kainate and the MCAO datasets.
We wanted to check if a model taking into account motif multiplicity would allow a more precise prediction of the value of expression. Therefore, we applied a linear regression analysis to the motifs identified by BN analysis as significant in both systems, and additionally the motif SATB significant in the MCAO system only. For the reasons detailed in the Materials and methods, we always performed a weighted linear regression, with the average loadings in groups of genes with the same motif count as the response variable, and the weights set to the numbers of genes in each group, as suggested by Faraway [31] .
The regression analyses were performed separately for the MCAO and the kainate datasets. The linear regression confirmed that the AP1F and SATB motifs had antagonistic effects on expression in the subspace of eigensystem M2 (Figure 6A-C) . The count of motif SATB per gene had a clear linear (R 2 = 0.91) and highly significant (p = 1.6 × 10 -5 ) effect on the group-average expression in the subspace of eigensystem M2 ( Figure  6A ). In agreement with the earlier BN result, the count SATB had no effect on loading of eigensystem A2 (data not shown). The inhibitory effect of SATB on gene expression in the MCAO system was specific for eigensystem M2, with no inhibition of expression in the subspace of any other eigensystem (data not shown).
The count of motif AP1F had a significant, positive and possibly linear effect on the average expression in a subspace of the second eigensystem, both in the MCAO (p = 0.0019, R 2 = 0.64) and in the kainate dataset ( Figure 6B , p = 0.00059, R 2 = 0.71). Remarkably, when the effect of AP1F count on M2 loading was analyzed separately for the genes with and without motif SATB, the effect became more apparent for the genes without motif SATB ( Figure 6C , p = 0.00044, R 2 = 0.76), while the effect was nullified for the genes with the motif SATB ( Figure 6D) .
The linear regression revealed that the count of motif CREB had a highly significant and approximately linear effect on the average expression in a subspace of the third eigensystem in the kainate ( Figure 7A eigensystem, in particular in the MCAO model, where it had no effect on the loadings of the eigensystem M2 (data not shown).
The effects of motif multiplicity on gene expression prompted us to investigate by the linear regression if a related variable -the count of conserved non-coding sequences (CNSs) per gene had an effect on gene expression. That we found was true in both experimental models (Figure 7E, F) . Similarly to the effect of CREB count, the effect of CNS count was highly specific for the third eigensystem (data not shown). However, when the effect of CNS count was analyzed in a bivariate linear regression model, together with that of CREB, the effect of the CNS was completely (MCAO) or nearly completely (kainate) dependent on the CNSs' content of Creb-binding motifs ( Figure 7E, F) .
Here, we demonstrated that eigensystems conserved between stroke and seizures separate effects of inflammation/apoptosis and synaptic activity on gene expression. The contribution of the eigensystem 3 reflecting synaptic activity was relatively greater (compared to eigensystem 2) in the seizures model, in agreement with higher electrical activity of neurons. Remarkably, our analysis of cis-regulation revealed that the these two functionally well-interpretable eigensystems were regulated by distinct sets of transcription factors, with AP1 and SATB regulating the eigensystem reflecting inflammation/apoptosis, and numerous TFs including Creb and Egr regulating the eigensystem reflecting neuronal synaptic activity.
Activation of transcription factor AP1 following the kainate-induced seizures and cerebral ischemia is well established [32, 33] . In particular, Timp1 was shown to be the target of AP1 following kainate-induced seizures [34] . The mRNA profiles of Timp1 in both systems ( Figure 6E, F) are highly similar to the profiles of the respective second eigengenes, which is compatible with our identification of AP1 as the key regulator of this eigensystem. It is well established that activation of Mapk8-Jun/AP1 signalling pathway has a predominantly pro-apoptotic effect in neurons [35] , however, only few Mapk8-AP1 targets genes have been identified. Therefore, demonstrating the importance of the number of AP1-binding motifs per gene and the simultaneous absence of SATB motif for gene activation contributes to identification of AP1 target genes.
We report novel and exciting finding that presence of the motif binding Satb1 prevents -in a motif number dependent manner -transcriptional activation in the stroke system. Satb1, which is the best characterized MAR-binding protein, has recently emerged as a key factor integrating higher-order chromatin architecture and gene regulation -reviewed in [36] . Depending on cell type and locus, its effect on chromatin looping may either activate transcription, as described for Th2 interleukin gene cluster [37] , or inhibit transcription, as for the MHC class 1 locus [38] and tentatively for our eigensystem M2. A hypothetical mechanism, in which genes in longer chromatin loops, or at the peaks of the loops, are more accessible to binding or activation by AP1, is depicted in Figure 6G . Proteolytic degradation of Satb1 occurs during early phases of apoptosis [39] [40] [41] . In the current work, the effect of SATB motif on expression was limited to the MCAO eigensystem 2 associated with the apoptosis.
Our analysis of cis-regulation of conserved eigensystem 3 -reflecting neuronal (synaptic) activity correctly predicted the known role of Creb/Atf/E4f1 and Egr as key regulators of neuronal activity regulated genes, important for neuronal plasticity and memory -for review, see [42, 43] . CREB motif binds transcription factors of the Creb family [43] [44] [45] , while E4FF motif binds transcription factors from the Atf family. EGRF binds transcription factors of the Egr family [46, 47] . PARF binds PAR/bZIP family of TFs (Dbp, Hlf, Tef, and Vbp1). The motifs binding Creb, Atf and Vbp1 are similar (Additional file 2) and these transcription factors have been shown to bind to overlapping sites [48] . A loss of the PAR/bZIP transcription factors results in seizures [49] . Using classical experimental methods, about a hundred Creb target genes have been identified, of which about half encodes neuronspecific proteins -reviewed by Lonze & Ginty [44] . A genome-wide chromatin immunoprecipitation study by Impey et al. identified Creb binding genes in the neuron-like differentiating rat pheochromocytoma PC12 cells [50] . When this set of genes was analyzed in our datasets, we found a clear association between Crebbinding to the gene and the positive loading of the third eigensystem ( Figure 7D) . Thus, the experimental data of Impey and co-authors support our in silico results, demonstrating an importance of the presence of CREB motif for gene up-regulation in the subspace of eigensystem reflecting neuronal activity.
Much experimental evidence supports an important role of Egr transcription factors in brain function. Transcription factors from the Egr family are induced in the rat hippocampus following kainate-induced seizures with kinetics closely resembling eigengene A3 (data not shown) and regulate expression of Arc [51] , a gene important for neuronal plasticity and memory formation [52] . Transcriptional activation of Egrs was also demonstrated following brain ischemia -reviewed in [47] . In addition to Creb and Egr, our BN analysis identified several novel tentative transcriptional regulators of the eigensystem reflecting synaptic activity ( Figure 5 and Additional file 2).
We demonstrate linear effects of the counts of the motifs SATB and CREB on log-expression in subspaces of the respective regulated eigensystems following the MCAO. These findings are in agreement with the predictions of the linear response model of gene regulation [21] . Moreover -because this model is valid only for TFs operating within the same cell -the observed agreement is revealing of the underlying biology ( Figure 7F) . First, it suggests that Satb and Creb operate within the same cells, namely neurons. This prediction is in agreement with our previous experimental data that majority of the cells undergoing apoptosis in the MCAO system are neurons [24] . Second, our results suggest that neuronal apoptosis is triggered by inflammation occurring in other cell types, namely microglia and astrocytes. This could explain why effects of inflammation and apoptosis are reflected by the same eigensystem, uncorrelated to the one reflecting effects of synaptic transmission.
The observed linear effect of CNSs' count per gene on log-transformed gene expression, depending on their content of CREB, is very interesting in the context of high specificity of this effect (data not shown) for the conserved eigensystem reflecting neuronal synaptic activity. Lee et al. [53] reported relatively greater cumulative length of CNSs in the upstream regions of genes involved in development, cell communication, neural functions and signaling processes, and suggested that this may reflect their greater regulatory complexity. We suggest, as another possibility, that neuronal genes need more CNSs (putative enhancers) to accommodate CREB motifs needed for responsiveness to rapidly changing synaptic activity.
Our results, demonstrating conservation of eigenarrays of temporal log-expression profiles, between hippocampus following seizures and cortex following the stroke, corroborate and extend recent findings of Oldham et al. [54] . These authors applied SVD to clusters ('modules' in their terminology) of expression profiles identified separately for several brain regions, and demonstrated conservation of 'module membership' between the corresponding clusters from different regions. As the 'module membership' is closely related to the first eigenarray of each cluster, their findings imply conservation of the first eigenarrays between the corresponding clusters. Our results demonstrate conservation of eigenarrays that occurs genome-wide for three eigensystems, two of which reflect distinct well-defined biological processes and are regulated via different sets of transcription factor binding sites.
Eigensystems conserved between stroke and seizures separate effects of different biological processes on gene expression, exerted via distinct sets of transcription factor binding motifs. Motif recognized by the nuclear matrix attachment region-binding protein Satb1 blocks AP1-driven transcriptional activation. The effects of motifs binding Creb and Satb1 on gene expression conform to the assumptions of the linear response model of gene regulation.
Gene expression profiling in the MCAO system Animals and surgical procedures
The experimental protocol was approved by the Local Animal Care and Use Committee and conforms to the national guidelines for the care and use of animals in research. 3-months old male Wistar rats weighing 270-320 g were used.
The MCAO (a middle cerebral artery occlusion) surgeries were performed under general halothane anaesthesia. Transient MCAO was induced with the intraluminal filament method (3-0 nylon monofilament suture) as described before [24] . A filament was withdrawn after 90 min. of MCAO to allow reperfusion, the incision was closed and anaesthesia discontinued. Sham-operated animals were subjected to the similar surgery with exception of MCA occlusion.
At various times after reperfusion, sham-operated and MCAO subjected rats were anesthetized with an overdose of pentobarbital and decapitated. Brains were rapidly removed, bisected at the midline and dorsolateral fragments of cerebral cortex containing MCA territory was dissected from the ipsilateral to occlusion (right) and contralateral (left) hemisphere. Total RNA was extracted from the samples using a phenol-guanidine thiocyanatebased method (TRI REAGENT, Sigma, Germany) and cleaned using RNeasy Total RNA kit (Qiagen, Germany) according to the manufacturer's recommendations followed by DNAse treatment. The amount and quality of the RNA was determined by spectrophotometry and capillary electrophoresis. The microarray hybridizations were conducted in the microarray facility of the Institute of Oncology, Maria Sklodowska-Curie Memorial Cancer Center, Gliwice Branch, Gliwice, Poland. Each time-point (6, 12, 24, 48 h ) and sham-operated (sh) group consisted three animals per group; RNAs from each individual were separately labelled and analyzed by microarray hybridization, for a total of 15 microarray hybridizations. The experiment was loaded to ArrayExpress (accession E-MEXP-2222).
The published dataset of Wilson et al. [23] from expression profiling in the hippocampus of adult rats with Affymetrix RG-U34A chip was downloaded from the NIH Neuroscience Microarray Consortium http://arrayconsortium.tgen.org/, projects: Koh-7K08NS002068-05-3, Koh-2K08NS002068-04. These datasets probed gene expression in the hippocampus of adult (P30) and young (P15) rats at 5 time-points (1, 6, 24, 72, 240 h) following the intraperitoneal injection of kainate (treatment) or saline (control). Only animals with nearly continuous seizures for more than half an hour were included in that study. Age-specific doses of kainate (3 mg/kg at P15, and 10 mg/kg at P30) were used that had been determined previously to result in < 25% mortality while inducing seizures in >60% of the animals. At the time of RNA isolation the animal could be seizing or during the latent period. Each condition was probed by three microarray hybridizations. The kainate data from both projects were pre-processed together, and the MAS5 detection calls for both ages were used together for the P/A/M filtering described below. Subsequently, the Mas5 signal data only from the adult rats (10 conditions) were used in the current work.
The CEL files from the MCAO experiment and separately the CEL files from the kainate experiment (from the young and adult rats together) were pre-processed with the MAS 5.0 algorithm as implemented in the affy R Bioconductor package (Irizarry et al. 2002) . Only the profiles of the probesets detected (MAS 5 call: Present or Marginal) in all hybridizations for at least one condition in a given experiment were used. The profiles from either experiment identified by probe set identifiers were mapped to the Ensembl 39 gene_stable_ids. Separately for either dataset, we computed a single average MAS5 signal profile for each gene_stable_id, resulting in gene expression matrices: (11012 × 15) for the MCAO system, and (3908 × 30) for the Adult rats from the kainate system. These data matrices were log2 transformed and analyzed separately by ANOVA. For further analysis from either dataset we selected the genes with the respective ANOVA p-value < 0.05. The average log2 expression profiles of these genes over the three biological replicates were computed, resulting in matrices: M (2786 × 5) for the MCAO system, and A (2392 × 10) for the kainate system.
The SVD analysis and the comparison of eigenarrays between two datasets were performed essentially as previously described [11] . Briefly, SVD was performed separately on matrices M, A, resulting in matrices u M (2786 × 5), m genes common between these two datasets. This resulted in matrices u MA and u AM . We calculated the Pearson correlation coefficient r between each pair of columns of u MA and u AM . The two-sided p-values corresponding to these correlations were obtained from the Student t distribution, with the t statistics calculated with the formula t = r[d /(1-r
2 )] 1/2 , where d is the number of the degrees of freedom.
GO terms associated with loadings of conserved eigensystems were identified, separately for either dataset, using RankGOstat [55] , available at http://gostat.wehi. edu.au/. The lists of gene symbols (Ensembl display_id), together with loadings of a particular eigensystem for a given (ANOVA-filtered) dataset were used as the input files. Default options (Wilcoxon Signed Rank test, Benjamini False Discovery Rate correction for multiple testing) were used, with the RGD database chosen as the source of GO annotations and the analysis was restricted to the "biological process" ontology. The result files were saved, parsed and converted to graphics using local scripts.
We used conserved non-coding sequences (CNSs) between human and rat as putative regulatory regions. For each human-rat orthologous gene pair (ortholog_one2one and apparent_ortholog_one2one) in Ensembl release 39, a flank of 20 kilobase (kb) of the genomic sequence from -10 kb to + 10 kb from the transcription start site were aligned using the AVID global alignment algorithm [25] . Sequence windows at least 100 base-pairs (bp) long with ≥ 75% identity were selected as putative regulatory regions. This resulted in the identification of 49425 CNSs for 9099 orthologous gene pairs in the human and rat genomes. A large proportion of similarly identified human-rodent CNSs was shown experimentally to function as enhancers [26] . The input genomic sequence and annotation data, and the results of this analysis were stored in a relational database named TRAM (Transcription regulatory Regions And Motifs), built on the open MySQL platform. The average length of the CNSs was 190 +/-SD 136 bp.
Instances of transcription factor binding motifs were predicted for all the vertebrate nucleotide distribution matrices of the Matrix Family Library version 6.2 using the program MatInspector [56] (Genomatix). Default thresholds, optimized for each motif as described in [57] were used. Search was performed for all CNSs in the TRAM database, separately for the human and the rat sequence, resulting in identification of 1679998 vertebrate motif instances in the human and 1601216 in the rat. The motif library contained 464 vertebrate nucleotide distribution matrices grouped into 151 matrix families [57] . Motifs identified with matrices from the same family were treated as the same nonredundant (n-r) motif identified by the family name. An instance of a n-r motif X in a given CNS is defined as conserved, if both the human and the rat sequence of this CNS contain at least one instance of X (not necessarily in the same AVID-aligned position). According to this definition, TRAM contains 1061884 instances of conserved n-r motifs. Only the conserved n-r motifs, referred to as "motifs" in the main text, were used in further analysis.
A composite motif X_Y_... is defined to have an instance in a CNS if this CNS contains at least one instance of each of the conserved n-r motifs X, Y, ... . Note that every single motif is also a composite motif.
In our model of transcription regulation the set of Bayesian network vertices is split into two subsets: cisregulatory features (composite motifs) and expression patterns (sign of the loading of a particular eigensystem). Furthermore, all the edges lead from cis-regulatory features to a particular expression pattern. In order to identify these relationships, we learn Bayesian networks from a dataset joining cis-regulatory and expression data for each gene. The input dataset joins presence or absence of every composite motif with the sign of loading of a single conserved eigensystem ( Figure 4C, D) .
In Step 1 of our procedure (not illustrated) over 100 promising composite motifs (built of up to three motifs) associated with the sign of the chosen eigensystem are identified. Only these selected composite motifs are then used as the input for the Step 2 ( Figure 4E ) identifying the best sets of composite motifs and their conditional probability distributions ( Figure 4F ). Each set of composite motifs has a q-value derived from 1000 random permutations of gene labels. For each permutation we created a new cis-regulatory dataset (with gene labels permuted accordingly) and learned the optimal composite motif set. Both steps of a learning procedure were performed with the BNFinder software [29] -a Python package for learning Bayesian networks from data. BNFinder implements the polynomial time learning algorithm dedicated to dynamic Bayesian networks, as well as to static ones with constraints forcing the network acyclicity [58] , as is the case here.
We used the Bayesian-Dirichlet equivalence (BDe) [59, 60] criterion with priors on the conditional probability distributions according to [59] . A prior on the network structures is proportional to the product of penalty parameters over all the edges in the graph of the refined model. Furthermore, penalty parameters increase with composite motif size. This choice results in a preference for sparse graphs, and thus protects our procedure from overfitting. BN score of a composite motif set was computed as the ratio of its posterior probability to the posterior probability of the empty set. To permit the cross-system validation of BN scores, the sets of composite motifs selected during Step 1 for the corresponding eigensystems (e.g. A2-M2) from either dataset were combined to form their union, which was then used during Step 2.
The motif count per gene was defined as the number of instances of conserved non-redundant motifs in the rat sequences of all the CNSs assigned to this gene. Only the genes with at least one CNS were used in the univariate regression analysis when the count of a particular motif was used as the regressor variable. When the CNS count, or CNS count and the motif count, were used as the regressor variable(s), the genes with zero CNS count were also included during the analysis
The single gene loadings of eigensystems 2, 3 were not normally distributed, which precludes statistical interpretation of the results of the regression with singlegene loadings used as the response variable. Therefore, in linear regression analysis, we decided to use the average loadings of a particular eigensystem in groups of genes with the same motif count as the response variable. In the regression analysis on the average values we confirmed the approximate normality of the residua (Additional file 3). Since the average values for different motif counts were computed from different numbers of observations, generally decreasing with the motif count, which was accompanied by changing variance of the loadings, we employed the Goldfeld-Quandt (GQ) test to detect the existence and magnitude of heteroskedasticity. Results of this test indicate that (i) for MCAO data heretoscedastic errors were detected (p < 0.05) in all the regression models. Therefore, we used weighted least squares approach, with weights set to the number of genes in each group [31] , which is the well-known solution to the heteroskedasticity problem. The regression analysis was performed in Mathematica 7, and the GQ tests were performed in R.
In the scientific literature, many approaches to fault diagnosis have been proposed since 1980. The FDI approach, which focuses on fault detection in dynamical systems, was summarized in (Blanke, Kinnaert, Lunze and Staroswiecki, 2006) . Related papers in this journal deal with the design of redundancy relations (Shumsky, 2007) as well as with the use of fuzzy logic (Dalton, Klotzek and Frank, 1999; Koscielny, Syfert and Bartys, 1999; Lopez-Toribio, Patton and Uppal, 1999) and neural networks (Korbicz, Patan and Obuchowicz, 1999; Witczak, 2006) . The DX approach focuses on diagnosis reasoning. It is summarized in (Hamscher, Console and De Kleer, 1992) . Recently, a bridge approach between FDI and DX was proposed (Cordier, Dague, Lévy, Dumas, Montmain, Staroswiecki and Travé-Massuyès, 2000; Nyberg and Krysander, 2003; Ploix, Touaf and Flaus, 2003) . Thus, tools for solving diagnosis problems are now well established. However, designing an efficient diagnosis system does not start after the system design but it has to be done during the system design. Indeed, the performance of a diagnostic system highly depends on the number and location of actuators and sensors. Therefore, designing a system that has to be diagnosed requires not only relevant fault diagnosis procedures, but also efficient sensor placement algorithms. Madron and Veverka (1992) proposed a sensor placement method which deals with a linear system. This method makes use of the Gauss-Jordan elimination to find a minimum set of variables to be measured. This ensures the observability of variables while simultaneously minimizing the cost of sensors. In this approach, the observable variables include the measurable variables plus the unmeasured but deductible variables. Another method of sensor placement was proposed in (Maquin, Luong and Ragot, 1997) . This method aims at guaranteeing the detectability and isolability of sensor failures. It is based on the concept of the redundancy degree in variables and on the structural analysis of the system model. The sensor placement problem can be solved by an analysis of a cycle matrix or by using the technique of mixed linear programming. Commault, Dion and Yacoub Agha (2006) proposed an alternative method of sensor placement where a new set of separators (irreducible input separators), which generates sets of system variables in which additional sensors must be implemented to solve the considered problem, is defined.
However, all these methods are not suitable for the design of systems that include a diagnosis system because, in this context, the goal of sensor placement should be to make it possible to monitor hazardous components. The sensor placement algorithm should compute solutions that satisfy detectability and diagnosability properties where detectability is the possibility of detecting a fault on a component and diagnosability is the possibility of isolating a fault on a component without ambiguities with any other faulty components. Few methods have focused on this problem.
Travé-Massuyès, Escobet and Milne (2001) proposed a method based on consecutive additions of sensors, which takes into account diagnosability criteria. The principle of this method is to analyze the physical model of a system from a structural point of view. This structural approach is based on Analytical Redundancy Relations (ARRs) (Blanke et al., 2006) . However, this method requires an a priori design of all the ARRs for a given set of sensors. Recently, Frisk and Krysander (2007) proposed an efficient method based on a Dulmage-Mendelsohn decomposition (Dulmage and Mendelsohn, 1959; Pothen and Chin-Ju, 1990 ). Nevertheless, this method only applies to just-determined sets of constraints while most practical systems are under-determined when sensors are not taken into account and over-determined afterwards.
This paper presents a new sensor placement algorithm that takes into account detectability and diagnosability specifications. It applies to systems for which only the structure is known. Thanks to this algorithm, sensor placements satisfying diagnosability objectives can be computed without designing all the ARRs, which is still an open problem. It applies to any system described structurally and does not assume just-determination. Section 2 details the main concepts that are useful to model systems for sensor placement. Then, Section 3 presents how the sensor placement problem is formulated. Section 4 introduces tools for analyzing structural matrices. These tools are then used in Section 5 to determine diagnosability properties directly from the analysis of structural matrices. Section 6 proposes basic algorithms for extracting blocks with useful properties from structural matrices, and Section 7 shows how to use these algorithms to compute sensor placements that satisfy diagnosability specifications. Finally, Section 8 presents an application to an electronic circuit.
Let us introduce the concepts and the formalism used in the paper to formalize the sensor placement problem. Behavioural knowledge starts with phenomena. A phenomenon is a potentially observable element of information about the actual state of a system. It is modelled by an implicitly time-varying variable, which has to be distinguished from a parameter that is model-dependent. Generally speaking, even if a phenomenon is observable, it is not possible to merge it with data because in fault diagnosis data are only known provided that some actuators or sensors behave properly. Phenomena V (t) = {. . . , v i (t), . . . } are linked to a phenomenological space F(T, V ) = {V (t); t ∈ T }, where T stands for a continuous or discrete time set. At any given time t in T , these phenomena belong to a domain dom(t, V ) = dom(V (t)) representing all the possible values that the phenomena may have. Consequently, when considering all t ∈ T , {dom(V (t)); t ∈ T } represents a tube in the timed phenomenological space F(T, V ).
All the phenomena have thus to be considered as unknown because observable phenomena are not observations. Let us introduce the concept of a data flow to model actual data recorded on a system. A data flow models data provided by a source of information concerning a phenomenon. A data flow concerning a phenomenon v is denoted by val (t, v) with val(t, v) ∈ dom(t, v) . It corresponds to a trajectory belonging to the tube {dom (t, v) ; t ∈ T } (see Fig. 1 ). When information about v is coming from different sources, the different data flows can be denoted by val i (t, v) . Formally, a data flow provided by a component c can be linked to a phenomenon: ok(c) → ∀t ∈ T, val(t, v) = v, which means that if the component named c is in the mode ok, then the data val (t, v) correspond to the actual value of the phenomenon v at any time t ∈ T . In fault diagnosis, a system is not supposed to remain in a given mode. Indeed, diagnostic analysis aims at retrieving the actual behavioral modes of the components of a system. At minimum, two modes are defined: the ok mode, which corresponds to the expected normal behavior, and the cf mode, which is the complementary fault mode: it refers to all the behaviours that do not fit to the expected normal behavior. Sometimes, specific fault modes may be modelled (de Kleer and Williams, 1992; Struss, 1992) . They are denoted by a specific label, e.g., the leak mode. Consider, e.g., a pipe where ok and leak are modelled. It yields M odes(pipe) = {ok, leak, cf }, where cf (pipe) refers to the behaviours that do not correspond to ok(pipe) or to leak(pipe).
Except for the complementary fault mode, behavioural modes are modelled by cause-effect relationships between phenomena, which are represented by constraints. Each constraint refers to a set of mappings containing unknown variables and known data flows. Generally speaking, a mapping over dom(t, V ) is defined from one subspace dom(t, V 1 ) to another dom(t, V 2 ), where
is a mapping representing a constraint k that models, for example, a component c 1 in mode mode 1 and a component c 2 in mode mode 2 , we have
where the data flow val(V 3 ) is considered as being included in the mapping. But constraint is not strictly equivalent to mapping. A constraint corresponds to a set of equivalent mappings. Firstly, although mappings to multidimensional spaces could be used, they are difficult to manage. It is better to break them down into one-dimensional mappings. In the following, one-dimensional mappings modelling a constraint k are named realizations of k. Moreover, several realizations of a constraint may be equivalent. Let κ i be a realization from V \{v} to {v}. There may be equivalent realizations defined on V that also model the constraint. Therefore, the notion of constraint can be extended to represent all the equivalent realizations representing a given subset of dom(V ). In the following, a constraint k will be understood as a set of equivalent realizations. It is summarized by the set of variables occurring in the realizations: var(k). It is assumed that if k is a constraint, for all v ∈ var(k), there is an equivalent realization κ i : dom (t, var(k) 
To summarize, a system Σ is composed of a set of constraints K Σ and a set of behavioural modes M odes(Σ) related to components in Σ. var(K Σ ) is the set of variables, named port in (Chittaro and Ranon, 2004) , which models observable phenomena involved in Σ. Indeed, by extension, the set of variables appearing in a set of constraints K is denoted by var(K) = k∈K var(k). Each constraint κ ∈ K Σ is linked to a mode m ∈ M odes(Σ) by a first order relationship: m → κ. For the sake of simplicity, in this paper, it is assumed that:
• only ok modes are considered in the sensor placement,
• each constraint κ ∈ K Σ models one mode and, conversely, that a mode can be modelled by at most one constraint.
The sensor placement problem then consists in defining the variables of var(Σ) that have to be measured to facilitate the detection and identification of ok modes from M odes (Σ) . These modes are denoted by M odes ok (Σ). From a mathematical point a view, it is a kind of combinatorial problem. The next section proposes a precise problem formulation.
Let us present an intuitive formulation of the problem. Full definitions are given afterwards. The solving of a diagnostic problem is generally decomposed into two consecutive steps. The conflict or symptom generation, also called fault detection in the automatic control community, and the diagnostic analysis, also called fault isolation. The first step relies on consistency tests among minimal testable subsets of constraints 1 K ∈ K Σ that include data flows (often called OBS for observations). Let K be the set of minimal testable subsets of constraints. If K ∈ K is a set of constraints leading to a test which is inconsistent, this means that, at least, one of the modes corresponding to the constraints of K is not actual. It is therefore important to trace the constraints belonging to a minimal testable subset K because this makes it possible to solve the second sub-problem: the diagnostic analysis, which provides global conclusions in terms of modes about the actual system states. The performance of a diagnostic system is highly dependent on the set K and, consequently, dependent on the set K Σ , which highly depends on the dataflows, i.e., on the observations. Additional sensors lead to addtional constraints in K Σ and, therefore, to new sets in K. K can be obtained from combinations of constraints from K Σ using possible conflict generation (Pulido and Alonso, 2002) , a bipartite graph (Blanke et al., 2006) , the Dulmage-Mendelsohn decomposition (Krysander, Aslund and Nyberg, 2008) or elimination rules (Ploix, Désinde and Touaf, 2005) . Basically, once K has been generated, it is possible to compute the performance of the diagnostic system in terms of detectability, discriminability or discernability, and diagnosability. Irrespective of whether or not the performance satisfies the requested performance requirements, the set K Σ is modified and the process is conducted once again until the requested performance is reached. However, this process requires lots of computations because the generation of K is time consuming. Moreover, up to now, no one of these algorithms has been proved to be complete.
Another approach to sensor placement is proposed in this paper. It does not require the computation of K from K Σ . It directly solves the following problem by studying the structure of Σ: Let K Σ be a set of constraints modeling the ok modes of a system Σ. Let var(K Σ ) be the variables appearing in K Σ . The problem to be solved is as follows: What are the complementary constraints modelling sensors dedicated to variables from var(K Σ ) that have to be added to satisfy requested diagnosability performance requirements?
Let us precise the problem formulation by defining the concept of a testable subset or a subsystem (TSS) of constraints and its relationship with the concept of the ARR. Definition 1. Let K be a set of constraints and v a variable in var(K) characterized by its domain dom (v) . K is a solving constraint set for v if, using K, it is possible to instantiate v with a value set S such that S ⊂ dom (v) . A solving constraint set for v is minimal if there is no subset of K, which is also a solving constraint set for v. A minimal solving constraint set K for v is denoted by K v.
Definition 2. Let K be a set of constraints. K is testable if and only if there is a partition
If this property is satisfied, it is indeed possible to check if the value set S 1 deduced from K 1 is consistent with the value set S 2 deduced from K 2 :
Adding any constraint to a testable set also leads to a testable set of constraints. Only minimal testable sets are interesting.
Definition 3. A testable set of constraints is minimal if it is not possible to keep testability when removing a constraint.
A global testable constraint that can be deduced from a TSS is called an analytical relation (ARR). Let
. .} be the set of all the testable subsystems that can be deduced from K Σ according to (Blanke et al., 2006; Ploix et al., 2005) . Because of the assumed one-to-one relationships between constraints and components, the notions of detectability and discriminability can be extended to constraints.
Let K be a set of TSSs coming from (Struss, Rehfus, Brignolo, Cascio, Console, Dague, Dubois, Dressler and Millet, 2002) (Struss et al., 2002) 
Obviously, nondetectability implies nondiscriminability. (Struss et al., 2002; Console, Picardi and Ribando, 2000) 
In order to formulate the sensor placement problem, the notion of a terminal constraint has to be introduced.
Definition 7. A terminal constraint k is a constraint that satisfies card(var(k)) = 1, where var(k) is the set of variables appearing in the constraint k.
A terminal constraint usually models a sensor or an actuator. It is thus a major concept in sensor placement. Note that if a candidate sensor measures not only one variable v but a combination of several variables
where v * is a virtual measurable variable, has to be added into K Σ . Then, the solving is similar to the standard problem.
In fault diagnosis, sensor placement has to satisfy specifications dealing with detectability and diagnosability. Because a one-to-one relation between components and constraints is assumed, what is true for components is also true for constraints. In the following, only constraints will be considered: the analogy with components is implicit. In this paper, complete specifications are considered. Partial specifications can also be managed: they will be presented in a forthcoming paper. These complete specifications consist of a partition of the constraint set K Σ into the following subsets:
• the set of constraints K diag that must be diagnosable,
• the set of subsets of constraints K nondis = {. . . , K i , . . .} for which each set K i must be nondiscriminable but detectable,
• the set of constraints K nondet that must be nondetectable,
Complete specifications K diag , K nondis and K nondet for sensor placement problems are meaningful if the following two properties are satisfied:
1. Sets in specifications must not overlap one another to make sense. Constraint sets have to satisfy
2. The union of all the components appearing in K diag , K nondis and K nondet has to correspond to K Σ :
If these properties are satisfied, the complete specifications are qualified as consistent in K Σ . Satisfying the diagnosability specifications requires information delivered by sensors. Let K Σ represent the system Σ with additional sensors where K Σ contains the constraints K Σ of the system Σ plus the additional terminal constraints modelling the additional sensors. Therefore, solving a sensor placement problem consists in determining additional terminal constraints in K Σ that lead to the satisfaction of complete specifications.
In the next sections, diagnosability properties of structural matrices are established and used for the design of a sensor placement satisfying diagnosability specifications.
Before pointing out diagnosability properties, some basic properties of structural matrices have to be established.
. .} can be represented by a structural matrix M Σ , which is an incidence matrix representing the mapping
According to the definition, a TSS is a minimum set of constraints K such that there is at least one variable for which two different minimal solving sets can be found. A minimal solving set leading to a variable v corresponds to a value propagation (Apt, 2003) starting usually, but not necessarily, by terminal constraints and leading to v. Therefore, a TSS can also be seen as two distinct value propagations leading to a given variable. This point of view has been adopted as a theoretical tool to develop proofs.
Let k 1 and k 2 be two constraints. The propagation of a variable v between k 1 and k 2 is possible only if v ∈ var(k 1 ) ∩ var(k 2 ). The variable v is qualified as propagable between k 1 and k 2 : v is a link between k 1 and k 2 . In the corresponding structural matrix, this link is represented by a thick line:
Consider now a system defined by
Terminal constraints k 4 and k 5 model sensors or actuators. Each terminal constraint contains known data. Figure 2 represents examples of propagations that lead to a TSS with a bipartite graph. But in a bipartite graph, links do not appear clearly: they correspond to alternate paths (or chains) with the pattern 'constraintvariable-constraint'. Links appear more clearly in structural matrices as lines linking two constraints. In the fol- lowing structural matrices, the variables surrounded by a circle represent the variables that can be instantiated twice. The relevance of links remains obvious in Example 2, where a propagation does not start by a terminal constraint. The paths corresponding to propagations of solving sets were drawn. Variable v 2 was instantiated twice.
Once again, paths may be reduced to links (thick lines).
The following example points out another structural matrix with two propagations leading to variable v 3 :
The concept of linked constraints has to be formalized because discriminability depends on this concept. Before defining linked constraints, the concept of interconnected constraints has to be introduced. The constraints of a system Σ may be modelled by a non-directed bipartite graph (K Σ , var(K Σ ), E Σ ), where E Σ is the set of edges. Each edge e = (k, v) reflects v ∈ var(k).
with constraints at extremities (see, e.g., (Bollobás, 1998) 
To point out the link with bipartite graph theory, if K is interconnected by V in K Σ , V is necessarily a complete coupling for K with respect to variables. The notion of a linked set of constraints can now be introduced. 
The shape of a structural matrix dealing with linked constraints is drawn in Fig. 3 . The concept of linked constraints is strongly connected with discriminability.
Proof. Indeed, because variables in V only appear in the constraints belonging to K, the only way for propagating variables is to use the constraints in K and the variables in V . What is more, because there is a tree (K, V, E) ⊂ (K Σ , var(K Σ ), E Σ ) with constraints at extremities, instantiating all the variables in V involves at least the achievement of the propagations defined by the tree.
Therefore, all the constraints are invariably found together in the TSS. In order to improve the clarity of these explanations, let us introduce the notion of stump variables.
Definition 10. A set of variables var(K) appearing in a set of constraints K but not in the other constraints of K Σ (i.e., K Σ \K) are named stump variables in K Σ with respect to K. They are denoted by var stump (K, K Σ ).
For instance, the set of variables V that link a set of constraints K belong to the stump variables
A set of constraints cannot be used to generate a TSS if they are linked and if there are additional variables that cannot be propagated. These constraints are qualified as isolated. Detectability depends on this concept.
it is linked by V and if there is at least one variable in var(K)\V that does not belong to other constraints of K Σ (i.e., K Σ \K). If the set contains only one constraint, the link condition disappears.
The shape of a structural matrix dealing with isolated constraints is shown in Fig. 4 . The concept of isolated constraints is strongly linked with detectability.
Proof. The constraints K isolated in K Σ by V will always come together in the TSS because, by definition, they are linked by V . Because of the fact that, in isolated constraints, there is at least one additional variable in var(K) which does not appear in other constraints (i.e., K Σ \K), it is not possible to instantiate this variable and, therefore, this set of constraints cannot be involved into a TSS: constraints K are thus non-detectable.
This section aims at setting up a direct link from sets of constraints to detectability and diagnosability properties.
Firstly, it is obvious that adding additional constraints connected to all the variables var(k) appearing in a constraint k ensures the diagnosability of k. Lemma 3 can be directly applied to all the constraints of a constraint set.
In Lemma 2, a relationship between isolated constraints and the detectability property has been presented. The next lemma generalizes the previous results.
Lemma 4. A sufficient condition for a subset of constraints K ⊂ K Σ to be non-detectable is that there is a sequence
Proof. The case of K 1 has been discussed in Lemma 2: because the constraints in K 1 are isolated in K Σ , they are non-detectable and therefore cannot be included in the TSS. Then, the remaining candidate constraints for the TSS belong to K Σ \K 1 . Because K 2 is isolated in K Σ \K 1 , they are non-detectable. The reasoning can be extended to any K i . Consequently, the constraints in K = i K i are non-detectable. Figure 5 indicates the shape of a structural matrix of non-detectable constraints.
Consider, e.g., a system modelled by the following structural matrix:
Assume that the set K = {k 1 , k 2 , k 3 } is required to be non-detectable. In this example, there exists a pair ({k 1 } , {k 2 , k 3 }) such that each element K i satisfies Lemma 4. If there are no additional terminal constraints containing v 1 , v 2 and v 3 , the subset K is necessarily nondetectable.
Proof. This lemma is a direct application of Lemma 1 to several sets of constraints.
Consider, for example, a system modelled by the following structural matrix:
} is a constraint subset that should be non-discriminable. Because the constraints k 1 , k 2 , k 3 and k 4 are linked by V = {v 1 , v 2 , v 3 }, Lemma 5 is satisfied. Therefore, k 1 , k 2 , k 3 and k 4 are nondiscriminable provided that no additional terminal constraints contain a variable of V .
The following theorem collects the results of Lemmas 3, 4 and 5. 
K i belonging to K nondis = {K 1 , . . . , K m } such that ∀K i = K j , K i ∩ K j = ∅,
Proof. The proof relies on the resulting structure of the structural matrix, which directly stems from Corollary 5 as well as Lemmas 4 and 5. Note that Point 2 could also be stated for the whole set of constraints K Σ . However, it is not useful to include non-detectable constraints, which will not appear in the resulting TSS: it would be less conservative. Because of Lemmas 4 and 5, the variables of var(K diag ) cannot contain variables appearing in the variables involved in (1) and (2), that is to say, in var stump (K nondet , K Σ ) and in
Because the variables of V candidate can be instantiated with measured values, all the constraints of K diag are diagnosable following Corollary 5.
The point which has to be proved is that, in specifications, K nondis defines non-discriminable but detectable sets and not only non-discriminable sets as in Lemma 5: the detectability of sets in K nondis has to be proved. The variables var(K i ) of a constraint set K i ∈ K nondis can be decomposed into two sets: V constraint set K i is necessarily detectable. Because this result holds for any K i ∈ K nondis , it proves the theorem.
A block containing the set of isolated constraints subsets of K Δ in K Σ and the isolating variables, considering only the varcostiables V Δ Require:
Satisfying the assumptions of Theorem 1 guarantees that the specifications are satisfied. However, because the theorem provides only a sufficient condition for diagnosability, the number of additional terminal constraints is not necessarily minimal. It has to be checked afterwards.
In the next section, an algorithm for extracting blocks from a structural matrix is presented. This algorithm is required by methods for sensor placement based on complete specifications.
Before presenting an algorithm for extracting blocks from a structural matrix K Σ , let us introduce some notation. Firstly, the notion of a block is formalized: a block is a couple defined by block = (K, V ) where block.cons = K and block.var = V stand respectively for a set of con- 
A set of blocks is denoted by the symbol B. By extension, the block resulting from the merging of sets of blocks B is denoted by merge(B). Figure 7 represents the dependency scheme between the methods that are defined. The main algorithm is named findBlocks (Algorithm 1). It extracts the different blocks that appear in Theorem 1, considering only the variables V Δ .
In order to describe the methods findIsolatedBlocks() and findLinkedBlocks(), the notions of Knode and buffer of Knodes are introduced. A Knode is a couple of constraint sets:
A buffer is a special First-In First-Out buffer. The basic functionalities are buffer .push(Knode) and buffer .pop(). They respectively correspond to adding a Knode in the buffer and getting a Knode from the buffer.
Using these notions, the algorithm findIsolatedBlocks() (Algorithm 2) extracts the set of isolated blocks from a set of constraints K Δ ⊆ K Σ , considering only the variables V Δ . According to Lemma 4, the constraints belonging to the resulting blocks are not detectable.
This algorithm depends on the findIsolatingVariables() method. It is given by Algorithm 3.
The algorithm findLinkedBlocks() (Algorithm 4) extracts the set of linked constraints from a set K Δ ⊆ K Σ , considering only the variables V Δ . The structure of this algorithm is very closed to that of Algorithm 2. Accord-
A set of blocks, where each one corresponds to a linked but not isolated set of constraints, and its corresponding linking variables, considering only the variables V Δ Require:
ing to Lemma 5, the constraints belonging to the resulting blocks are not discriminable. This algorithm depends on the findLinkingStumpVariables() method, which is given by Algorithm 5.
Finally, according the Fig. 7 , the algorithms findIsolatingVariables() and findLinkedBlocks() depend on two methods findStumpVariables() (Algorithm 6) and isInter-
The top-level method findBlocks(K Σ ) leads to the blocks depicted in Fig. 6 . These results are very useful to support the sensor placement. Indeed, constraints belonging to B diag .cons are already diagnosable. Therefore, finding a sensor placement satisfying the specifications requires that the specified In much the same way, the constraints merge(B nondis ).cons ∪ B diag .cons are already detectable. Therefore, finding a sensor placement satisfying the specifications requires that the specified K spec nondet should be included in merge(B nondet ).cons:
A method for optimal sensor placements satisfying diagnosability specifications is proposed in this section. This method deals with complete specifications: K spec diag , K spec nondis and K spec nondet (see Section 4). There may be several sensor placements that satisfy diagnosability specifications. In order to select the most interesting one, a criterion based on the cost of the sensor placement is considered. Introduce the following notation: The cost of the measurement of a variable v is denoted cost (v) . By extension, the cost of the measurement of a set of variables V is defined as cost(V ) = v∈V cost (v) .
Adding sensors amounts to adding terminal constraints (see Definition 7). Indeed, as mentioned in Section 3, a sensor measuring a variable v is modelled by the constraint val (t, v) = v, where val(t, v) is a datum coming from the sensor. Therefore, structurally speaking, a sensor measuring v is modelled by a terminal constraint k satisfying var(k) = {v}. The constraint k will be denoted by k sensor (v) . By extension, the terminal constraints modelling sensors measuring variables V are denoted by K sensor (V ). 
The method to solve these complete specifications can be decomposed into two steps: the determination of candidate variables for sensor placements using Theorem 1, and the reduction of the candidate variables in order to find the minimal cost sensor placement that satisfies the complete diagnosability specifications using a branchand-bound algorithm. Figure 8 presents the dependency scheme of the method.
The findCandidates() (Algorithm 8) method is based on Theorem 1. It takes into account the specifications to determine a set of variables to be measured. If these variables are measured, the complete specifications will be satisfied. This algorithm depends on the findLinkingVariables() method, which is given by Algorithm 9. This algorithm uses the results issuing from Algorithm 5 to find a subset of variables linking a subset of constraints K Δ , considering only the variables V Δ .
In this algorithm, the cost of variables is considered. This algorithm depends on the sortVariables() method, which sorts a list of variables according to measurement costs in descending order.
A subset of the candidate variables may also lead to the satisfaction of the specifications. A branch-and-bound algorithm is used to select the most interesting candidate variables to be measured in order to find an optimal sensor placement. Before defining the optimisation algorithm, it is necessary to be able to check if the complete specifications are satisfied for a given subset of candidate variables.
True is the sensor placement satisfies the specifications Require: The optimality criterion for a feasible sensor placement defined by V measured is given by cost(V measured ). The branch-and-bound search algorithm is implemented in the placeSensor() method (Algorithm 11) using a simple First-In First-Out buffer of nodes of variables.
In this section, the special case of a dynamical system modelled by recurrent or differential equations is discussed. Then, an example is presented. 
Dynamical systems. The sensor placement method relies on structural modelling. Therefore it should be suitable for most systems. Let us examine the special case of dynamical systems. Generally speaking, a model is said to be dynamic if either:
• a variable appears several times in a system but at different time, stamps, or
• a variable and some of its derivatives or summations (whatever the order is) appear in the system.
The first case mainly concerns time-delays and discrete time recurrent systems. According to Section 3, each variable stands for a tube in a phenomenological space. Therefore, a time delay, modelled by y(t + Δ) = x(t), is a constraint that establishes a link between two tubes: {dom(y(t + Δ)); ∀t} and {dom(x(t)); ∀t}. Therefore, even if the two variables model the same phenomenon, in the structural model they cannot be merged. Consider now the following discrete-time recurrent model:
k ∈ N, where T e stands for the sampling period.

The phenomenon modelled by x appears twice. Therefore, the constraint must be implicitly completed by a time delay between variables x((k + 1)T e ) and x(kT e ). Structurally speaking, these constraints are modelled by the following structures:
Moreover, if the tube corresponding to x((k + 1)T e ) appears only once in these constraints (which is usually the case in practice), constraints k 1 and k 2 can be merged:
The second case mainly concerns integration and differential equations. Consider, e.g., the following model: and var(k 2 ) = {x, dx dt, x 0 }. In the same way as timedelays, the constraints k 1 and k 2 can be merged to obtain the following structure: var(k 12 ) = {u, x} or, if the initial condition is considered, var(k 12 ) = {u, x, x 0 }. This result remains true for summations and derivatives of any order.
Consequently, these kinds of dynamical systems can be handled just like other systems.
8.2. Example. The method presented in this paper has been applied to a sensor placement for an electronic circuit (Fig. 9) . It is modelled by the following constraints:
with
The corresponding structural matrix is given by Table 1. Suppose that the costs of the measurements are
Consider the following complete specifications:
In order to check if the specifications K nondet are satisfiable, Algorithm 2 is used with K Δ = {k 1 , k 4 , k 10 }, K Σ and V Δ = var(K Σ ). Algorithm 2 computes the following sets of isolated constraints: {{k 10 , k 1 } , {k 4 }}. The specifications K nondet are consequently satisfiable. Algorithm 2 also provides the isolated variables
In order to check if the specifications K nondis are satisfiable, Algorithm 9 is used with two subsets,
. Algorithm 9 computes the linking variable subsets V 1 = {i 1 } and V 2 = {v 3 }.
In order to find the candidate variables to be measured to satisfy the specifications, Algorithm 8 is used. It yields terminal constraints that correspond to the measurements of variables
In order to find the cheapest sensor placement that satisfies the specifications, Algorithm 11 is used. It yields
In order to validate the result, the method proposed in (Ploix et al., 2005) has been used to design all the ARRs. It has led to the fault signature given by Table 2 . According to these results, the constraint sets that cannot be discriminated are {k 2 , k 6 } and {k 7 , k 8 }. The constraint set that cannot be detected is {k 1 , k 4 , k 10 } and the diagnosable constraints are {k 3 , k 5 , k 9 , k 11 , k 12 }. Applying the function Φ : K Σ −→ C Σ , it is obvious that the components that cannot be discriminated are {c 2 , c 6 } and {c 7 , c 8 }, the components that cannot be detected are {c 1 , c 4 , c 10 }, and the diagnosable components are {c 3 , c 5 , c 9 , c 11 , c 12 }.
Suppose now that the specifications are given by K nondis = {{k 2 , k 3 } , {k 7 , k 8 }} ,
In order to check if the specifications K nondet are satisfiable, Algorithm 2 is used with K Δ = {k 1 , k 10 }, K Σ and V Δ = var(K Σ ). Algorithm 2 computes the following sets of isolated constraints: {{k 10 , k 1 }}. The specifications K nondet are consequently satisfiable. Algorithm 2 also provides the isolating variables V isolated = {i 4 , v 2 }.
In order to check if the specifications K nondis are satisfiable, Algorithm 9 is used with the two subsets K Δ1 = {k 2 , k 3 } and K Δ2 = {k 7 , k 8 }, considering V Δ = var(K Σ \ V isolated ). Because Algorithm 9 computes the linking variable subset V 1 = {∅} for the constraint subset K Δ1 = {k 2 , k 3 }, there is no solution that satisfies these specifications.
The results presented in this paper demonstrate that it is possible to design optimal sensor placements satisfying diagnosability criteria without designing the ARR a priori.
A new approach to sensor placement has been proposed that makes it possible to satisfy diagnosability specifications. It is thus possible to specify the performances that a diagnostic system has to meet and then to compute where the sensors should be placed.
The presented lemmas, theorems and algorithms are general and can be reused to develop other methods for sensor placement that deal with various kinds of specifications, e.g., a set of components that have to be at least detectable and another one of those that have to be diagnosable. The provided tools apply to any system including dynamical systems described by recurrent or differential equations because they are based on a structural approach: only the variables appearing in constraints are considered. However, the generality of the structural approach is paid by possible over-estimation depending on the nature of constraints: it is well known that it relies on the conditioning of constraints. But solutions taking into account the nature of constraints can only be specific.
An algorithm for sensor placement managing complete specifications has been presented. It deals with elements that have to be diagnosable, discriminable and nondetectable. Thanks to the proposed algorithm, cost optimal sensor placement satisfying complete diagnosability specifications is possible without designing the ARR a priori. This is a very important feature since it is no longer necessary to design all the possible ARRs assuming all the variables are measured.
This approach manages only specifications dealing with models of the normal behaviour. It does not take into account specific fault models such as a leak in a pipe. Therefore, if such models are considered, the sensor placement algorithm will lead to an over-estimation of the required sensors. Taking into account specific fault models may lead to a reduction of the required sensors. Nevertheless, fault models cannot be easily taken into account in sensor placement methods. It is still an open problem.
Wireless sensor networks (WSNs) have attracted a lot of attention. Recently, research efforts have been dedicated to power management [9] , routing [6] , deployment and coverage [2] , and localization [1] . Nowadays, many WSN systems have adopted ZigBee [10] as their communication protocol. ZigBee adopts the physical (PHY) and the medium access control (MAC) layers defined by IEEE 802.15.4 [5] and extends to network, application, and security services. ZigBee supports three network topologies, star, tree, and mesh. Regardless of network topology, there is a coordinator responsible for initializing, maintaining, and controlling the network. Star networks can only cover small areas, but tree and mesh networks can cover larger fields by allowing multi-hop communications. The backbone of a tree/mesh network is formed by one coordinator and multiple routers. An end device must associate with the coordinator or a router. In a tree network, routing can be done in a stateless manner based on nodes' 16-bit short addresses.
To form a ZigBee network, addressing is the first work to be done. ZigBee suggests a distributed address assignment mechanism (DAAM), which enforces some addressing rules. By this mechanism, the coordinator needs to decide three parameters: the maximum number of children of a router (C max ), the maximum number of child routers of a router (R max ), and the maximum network depth (L max ). While simple, the scheme may prohibit a node from accepting isolated child routers/devices. A node is an orphan node when it cannot associate with any parent router but there is still unused address space in the network. Some heuristics algorithms are proposed to reduce orphans [7] .
Reference [3] proposes an address borrowing scheme. When a new node sends an association request to a router of a ZigBee network and this router does not have any free address, it will ask other neighboring routers to lend an unassigned address to serve the new node. Park et al. [8] propose a distributed borrowing addressing scheme (DBAS) by allowing a parent node to borrow a maximum unused address space to alleviate the orphan problem.
The main goal of our work is to alleviate the orphan problem by a distributed address assignment scheme with address borrowing. We improve the borrowing scheme of [8] by allowing a parent router to borrow a flexible subtree of address space from a neighbor. We also define the detailed procedure for our address-borrowing scheme, which is compatible with the ZigBee standard. Our solution solves addressing and routing issues altogether. When a new node tries to associate with a parent router with no unassigned address, the new node will estimate the number of orphan nodes in its neighborhood and this parent router will ask its 2-hop neighbors to lend a suitable address space to serve the new node. Then this new node can use the ZigBee DAAM to serve its children. We also show that routing and maintaining the borrow and lend lists can be easily done. In ZigBee, address assignment is done in a distributed manner. To form a network, the coordinator determines C max , R max , and L max first. Note that the children of a router include both routers and end devices. So C max ≥ R max and up to C max − R max children must be end devices. Addresses are assigned in a top-down manner. The coordinator takes 0 as its address and divides the remaining address space into R max + 1 blocks. The first R max blocks are to be assigned to its child routers and the last block has C max −R max addresses, each to be assigned to one child end device. The similar process is adopted by each child router to partition its given address space in a recursive manner. From C max , R max , and
which is the size of one address block to be assigned to a child router [10] :
The value of d is 0 for the coordinator and is increased by one as we go down the tree.
A. Basic Idea Fig. 1 (a) shows a ZigBee address assignment example. Router y cannot accept router x as its child because it already has the maximum R max = 3 child routers, making x an orphan node. The orphan problem [7] refers to the scenario where some nodes in Fig. 1 (a) cannot connect to the network even though there are free addresses in the network.
We try to alleviate the orphan problem by allowing addressborrowing. We define a new Borrowing Information Base (BIB) attribute called MaxBorrowingNumber (B max ), which can be carried by any reserved field. B max is the maximum number of addresses that a router can borrow. A router can serve up to R max + B max child routers and up to C max − R max + B max child end devices. Here, these B max children can be routers or end devices. Fig. 1(b) shows an example where y has borrowed the address block rooted at 36 of depth 2 from router z. It also shows that the address block is assigned to router x. Then router x can assign them to its children.
Below, we propose a fully automated, distributed ZigBee address assignment scheme to facilitate such borrowing behaviors. Then we further show how to conduct address-based routing in the network.
During the formation process, each node x maintains the following variables:
• A x : the address of x in a ZigBee network.
• R(x) and E(x): the child routers and the child end devices currently associating with x, respectively.
• L r (x) and L e (x): the lend lists of child routers' addresses and child end devices' addresses, respectively, which x has lent out.
• B r (x) and B e (x): the borrow lists of child router's addresses and child end device's addresses, respectively, which has borrowed.
• p(x): the address of x's parent in a ZigBee network.
• o(x): the estimated number of orphans in x's neighborhood.
• d x : the depth of x in the ZigBee network. Note that there are two interpretations for the value of d x . Under the normal situation, d x is the actual depth value of x. However, d x is the "depth" of the address counting from its original location (not x's location). For example, the d z of z in Fig. 2 is 1 since originally address 107 is the child of address 0 (the coordinator t) even though its current physical depth in the ZigBee tree is 4. Similarly, the d y of y in Fig. 2 is 2 since it is the child of z, and its physical depth in the ZigBee tree is 5.
• s x : the state of A x 's location. We allow an address block to lent out if it is not a borrowed address black. Note that a child address block of a "borrowed" address block is also considered a "borrowed" address block. That is, a borrowed address block connot be further lent out to other nodes, and so is its sub-blocks. We set s x = "original" if A x is not a borrowed address; otherwise, we set s x = ICC'14 -W7: Workshop on M2M Communications for Next Generation IoT "borrowed". For example, in Fig. 2 , s z = "borrowed" since A z = 107 is borrowed from the coordinator. Since A y is a child of A z , s y = "borrowed", too. On the other hand, A w is not a borrowed address, so s w = "original". 2) Associating Scheme: To start a new network, a coordinator t first sets the following parameters: C max , R max , L max , and B max . Then t assigns A t = 0, d t = 0, and s t = "original", and sets R(t), E(t), B r (t), B e (t), L r (t), and L e (t) as empty sets. The detailed associating scheme is presented below.
(a) Router v receiving a MLME-ASSOCIATION-REQ request from a node u:
is free, and assigns the address A u to u by replying a MLME-ASSOCIATION response with A v , d v and s v to u. 
(ii) Otherwise, x ignores the request. (g) Router x receiving an ASK-ADD-Confirm with the lending address A y from router v: Router x updates L r (x) if this address would be assigned to a child router; otherwise, it updates L e (x). In the example of Fig. 1(b) , z lends address 36 to y, so its lend list L r (z) = {(36, y = 54)}. On the other hand, y should update its borrow list B r (y) = {(36, z = 1, 1)}. Fig. 3 shows the flow chart for an orphan node x to request a router node v to lend an address block to serve it. Next, we analyze the depth of the ZigBee tree after borrowing. Since we do not allow recursive borrowing and a node can only inquire its 2-hop neighbors for borrowing, the maximum depth of the ZigBee tree formed by ABS is L max + 2.
3) Disassociation Scheme: When a node wants to disassociate with its parent router, it should broadcast a MLME-DISASSOCIATION request to its parent and descendants to ensure that its address block can be re-used and its descendants can try to reassociate with other parents. The detailed disassociating scheme is presented below.
• Node v receiving a MLME-DISASSOCIATION request from node u:
, and B e (v). If this address is a borrowed one, then v sends a Return-ADD with the address A u to its original lender. This address A u then can be reused by other nodes. (b) If u is v's parent, then v sends a MLME-DISASSOCIATION request to its children. Since v is not an orphan, it should try to reassociate with the ZigBee network. (c) Otherwise, v ignores the request.
• Router x receiving a Return-ADD with A u from node v: Then x removes A u from L r (x) and L e (x). Fig. 4 shows the flow chart of the above procedure. 
With the above address borrowing scheme, ZigBee can still support very simple address-based routing as follows. When router v receives a packet with a destination address A dest , it accepts the packet if A v = A dest . Otherwise, v forwards this packet as follows.
1) If dest is a child router or grandchild of v, then v forwards this packet to a router as follows.
. Then v forwards this packet to its child router A r if A r / ∈ L r (v); otherwise, v forwards this packet to the borrower who borrows A r from v.
for some node x in B r (v), v forwards this packet to its child router x. 2) If dest is a child end device, then v forwards this packet to one of its child end device as follows.
; otherwise, v forwards this packet to the borrower who borrows A r from v. b) If A dest ∈ B e (v), v forwards the packet to this child end device. 3) Otherwise, v forwards the packet to its parent p(v) if v is not the coordinator, and v ignores this packet if v is the coordinator. Fig. 5 is an example that a packet is transmitted from node 108 to node 45. This packet is transmitted to node 0 in accordance with the step 3) of the routing scheme first. Then node 0 forwards it to node 1 according to the step 1.a) of the routing scheme. Node 1 finds that address 45 should be one of its grandchildren, and this address is lent to node 54. According to the step 1.a), node 1 then forwards this packet to node 54 by looking address 45 up in the lend list of node 1. When node 54 receives the packet, it looks this address up in the borrow list and then forwards it to the child router 36 according to the step 1.b). Finally node 36 forwards this packet to node 45 in accordance with the step 1.a).
To verify the benefit gained from our ABS, we evaluate the average number of orphans of our scheme as compared to other approaches. We assume the following simulation environment. The monitoring region is 500 × 500m 2 where 2000 routers are randomly deployed and the coordinator is set in the center. The communication range of each router is set to 50m.
For comparison, we design three modifications of our ABS by relaxing the borrowing strategy in the step (e) of associating scheme. A parent router can ask for borrowing from its 1-hop neighbors (ABS-1), 2-hop neighbors (ABS-2), and 3-hop neighbors (ABS-3). We also set DBAS-1 and DBAS-2 as the strategies of asking its 1-hop, and 2-hop neighbors, respectively.
We vary the maximum network depth (L max ) to see how this influences the average number of orphans. We set B max = 2. Fig. 6(a) shows the results. The average number of orphans is inversely proportional to the tree depth limit L max . Regardless of L max , our schemes always outperform other approaches. The average number of orphans by ABS-2 is fewer than that by ABS-1, since ABS-2 can serve further nodes. Fig. 6(b) shows the average number of orphans with different R max . Regardless of borrowing strategy, the higher R max = C max makes nodes join the network more easily, and thus fewer orphans.
ABS always outperform DBAS, because it allows a parent router to borrow a flexible address space through counting orphans. On the contrary, DBAS always lets a parent router borrow a maximum address space, exhausting address quickly.
Finally, we measure the impact of B max on the average number of orphans. B max affects the number of children that a router can serve in the ZigBee tree. In Fig. 7 , we observe that larger B max cause fewer orphans. 
To relieve the orphan problem in a ZigBee network, we have proposed the address-borrowing scheme (ABS) by allowing a node to borrow unused address spaces from neighbor nodes. We have shown that it effectively decreases the number of orphans in a ZigBee network when the network cannot expand due to the constraints of C max , R max , and/or L max . The unit of lending/borrowing is a subtree of address space under the original definition in ZigBee, thus reducing the cost of routing tables and the requirement of storage spaces in router nodes. We have also suggested a light-weight routing algorithm for ABS which follows the original ZigBee address-based routing strategy with a slight modification. In addition, the maximum depth of the ZigBee network is predictable. Simulation results also show that ABS can befit original ZigBee address assignment mechanism and thus effectively reduces orphans.
Current trends in object-oriented software construction, namely MDA (Model-Driven Architecture)-based approaches, promote designing high-level models that represent domain and application concepts ("Platform Independent Models"). These models, typically described in UML (Unified Modeling Language), are further on mapped to the target implementation platform ("Platform Specific Models"). Modeling has thus become a key activity within the software process whereas large efforts are currently spent in developing automated tools to assist it.
Formal Concept Analysis (FCA) has already been successfully applied to the analysis [1] and restructuring [2] [3] [4] [5] [6] [7] of conceptual class models: it helps reach optimal hierarchical organization of the initial classes by discovering relevant new abstractions. However, providing far-reaching abstraction mechanisms requires the whole feature set of UML to be covered, inclusive those encoding relational information (e.g., UML associations), whereas such features clearly outgrow the scope of standard FCA.
Making FCA work on UML models is the global aim of our study. Here, we propose a new relationally-aware abstraction technique, ICG (Iterative Cross Generalization), which works on several mutually related formal contexts that jointly encode a UML class diagram. It performs simultaneous analysis tasks on the set of contexts where inter-context links are used to propagate knowledge about the abstractions from a context into its related contexts (and thus broaden the discovery horizon on those contexts).
The paper recalls the basics of FCA (Section 2) before providing a motivating example (Section 3). Our recent FCA-based framework for processing relational data is presented in Section 4. In Section 5 we specify ICG while emphasizing the role UML meta-model plays in data description within ICG. Experiments done in the framework of industrial projects are then reported (Section 6) with a discussion of benefits and difficulties in applying ICG.
Formal concept analysis (FCA) [8] studies the way conceptual structures emerge out of observations. Basic FCA considers an incidence relation over a pair of sets (objects, further denoted by numbers) and (attributes, denoted by lower-case letters). Binary relations are introduced as formal contexts ! " ! $ #
. An example of a context, Foo, is provided in Figure 1 is a complete lattice with joins and meets based on intersection of concept intents and extents, respectively. The lattice of the Foo context is drawn in Figure 1 on the right (as a Hasse diagram).
Research on applications of FCA has yielded a set of meaningful substructures of the concept lattice. For instance, in object-oriented software engineering, the assignments of specifications/code to classes within a class hierarchy is easily modeled through a context, and applying FCA to a particular hierarchy may reveal crucial flaws in factorization [2] and therefore in maintainability. The dedicated substructure that specifies a maximally factorized class hierarchy of minimal size is called the Galois sub-hierarchy (GSH) of the corresponding context. Mathematically speaking, the GSH is made out of all the extremal concepts that contain an object/attribute in their extents/intents:
. Moreover, as practical applications of FCA may involve processing of non-binary data, many-valued contexts have been introduced in FCA. In a many-valued context 6 x V " A t #
, each object w is described by a set of attribute -value pairs S 7 9 G # , meaning that is a ternary relation that binds the objects from , the attributes from and the values from . The construction of a lattice on top of a many-valued context requires a pre-processing step, called scaling, which basically amounts to encoding each non-binary attribute by a set of binary ones.
3 Improving UML models: a motivating example A highly simplified example introduces the problem domain. Consider the UML model in Figure 2 . A class Diary is associated to a class Date through the association orderedBy. Class Date has three attributes (or variables) day, month and year and two methods including isLeapYear() and a comparison method (Date). Another class Clock is linked to Time class via the association shows. Class Time is described by the three attributes hour, min and sec, and by a method To infer a more elaborate UML model, we apply an approach that may be summarized as follows. On the one hand, we process various sorts of UML entities such as attributes, methods and associations, as first-class formal objects and assign a formal context to each entity sort. Moreover, we use relational attributes to express links between entities and model them as inter-context binary relations.
On the other hand, we use a repeated scaling along the relational attributes to propagate the knowledge about possible generalizations between related contexts. Thus, the concept construction process amounts to alternating scaling and proper construction until stability in concept structures is reached.
In Figure 4 , three many-valued formal contexts describe classes, associations and methods as first-level formal objects, respectively. Here, UML class attributes are not processed as objects for simplicity sake, but in the general case they are. Note that some formal attributes (e.g. originType) are relational ones while others are not (e.g. originMultiplicity or name). Figure 5 shows the main relational attributes of the example. The method lattice ( Figure 7 ) is now used as a scale for the formal attribute has owned by classes. Thus, if a class has a method © in the initial many-valued context, then it owns all the formal attributes has:m in the scaled class context where m stands for a method concept whose extent contains the formal object representing © . The resulting scaled class context and its lattice (with top and bottom dropped) are shown in Figure 8 . The lattice includes a new concept c1 which obviously represents comparable objects, hence it could be called Magnitude.
Our knowledge about the concept structure on classes has thus grown and the new abstractions can be used as descriptors that could, whenever shared, induce potential abstractions on related contexts. For example, the method context could be fed with the knowledge about Magnitude thus prompting a re-consideration of its con- (1):Time). The resulting concept lattice remains isomorphic to that of Figure 7 , however its concepts are explicitly related to existing concepts on classes, e.g., the top concept intent is bound to c1 via typeOfParam(1):c1. The same procedure can be applied for scaling the association context, revealing that the two formal objects can be generalized by a new association which ends into the c1 concept. The scaling of isOrigin and isDestination from the class context, using the augmented association lattice, introduces a new generalization of Diary and Clock (representing devices which manipulate magnitudes). The resulting set of abstractions, re-interpreted in UML, is shown in Figure 9 . The associations orderedBy and shows are linked to the new association manipulate by the constraint subset which indicates a specialization relationship. Because of this constraint, their names are now prefixed by the symbol "/", used in UML for highlighting elements that derive from others.
To sum up, we may claim that the apparent commonalties between the classes Time and Date have led to the constitution of common superclass, Magnitude. The discovery of this class has been propagated to both method and association contexts where new abstractions have been created to reflect the existence of Magnitude. In the following, we summarize the key elements of our relational FCA framework. A detailed description could be found in [10] . As in classical FCA, heterogeneous datasets, i.e., ones made out of several sorts of individuals, are introduced through a family of contexts, one per sort of formal objects. Here, a set of binary relations (or set-valued functions) is added to data description, which map objects from a context to sets of objects from another one.
A relational context family . By bringing those attributes to the objects from , conceptual scaling allows new concepts to occur in which the members of the extent share abstractions of the initial values rather than values themselves. Clearly, the choice of scale attributes has a direct impact on the structure of the target concept lattice: different attribute sets may lead to different lattices.
The same principle may be applied to the processing of relations which are basically object-valued attributes: given a relation q { | } 1 P and w s | the set q e S w # could be replaced by a collection of binary attributes that characterize it. As the entire process is ultimately aimed at detecting commonalties in the abstractions that conceptually describe the target objects, the scaling binds scale attributes to existing concepts on the co-domain context rather than to formal attributes of this context (see the attributes typeOfParam(1):cX from Section 3). Moreover, as we argued in [10] , the most natural choice for the scale lattice of q is the lattice of the context c since it embeds the most precise knowledge about the meaningful abstractions on the set "
. However, in specific situations smaller structures, such as the GSH, may be more appropriate.
Consider an object 
("wide"). The "narrow" scheme clearly fits lattice-shaped scales whereas the "wide" one suits also less extensive concept structures.
To sum up, the encoding by concepts rather than by formal attributes from the destination context eases the interpretation of the formal concepts discovered in the source context | . Moreover, such an encoding fits a step-wise discovery of the scale concepts as illustrated in Section 3: the formation of some new concepts within , one per context l , such that the concepts reflect both shared attributes and similarities in object relations, i.e., common concepts in the co-domain context. Obviously the relational scaling helps to reduce the lattice construction on relational data to the binary case so that the same algorithmic procedures could be applied. However, unlike conventional constructions, some RCF may require a step-wise construction process due to the mutual dependencies between contexts, as it was shown in the UML model analysis. Indeed, having aligned scales with actual concept hierarchies on the destination contexts, an apparent deadlock occurs whenever two contexts are connected both ways by a pair of relational attributes (or chains of such attributes). For instance, in Figure 5 , the class context is doubly connected to the method one by the initial attribute pair (typeOfParam(i), has).
To resolve the deadlocks resulting from circularity in the relational structure of a RCF, we apply a classical fixed-point computation mechanism that proceeds stepwise. Its grounding principle lies in the gradual incorporation of new knowledge gained through scaling: the computation starts with uniformly nominal scales for all relational attributes and at each subsequent step uses the previously discovered concept structures within the respective co-domain contexts as new and richer scales.
Technically speaking, the global lattice extraction process associated with a RCF alternates between relational scaling and lattice construction (see Algorithm 1) . At the initial step, relations are ignored (line 5), hence the lattices at this stage (line 6) are not impacted by the relational information and rather reflect common non-relational attributes. At the following step these lattices are used as new scales for a first-class relational scaling thus providing new possibilities for generalizations (lines 10-11). The scaling (line 10) / construction (line 11) steps go on until the global set of concepts stabilizes, i.e., for each context r . Stabilization of the process can be deduced from the fact that the formal objects do not change over the steps; the concept number of the lattice associated with ! " A t #
is bounded by
, which gives a bound to the scaling of relational attributes.
Out:
² array of lattices ) 3:
while not halt do 8:
Algorithm 1: Construction of the set of concept lattices corresponding to a RCF.
Once the lattices of all contexts are available, a post-processing step clarifies the links between concepts induced by relational scale attributes. In fact, many concept intents will present redundancies: a concept , only those corresponding to minimal Cl will be preserved. For instance, in Figure 8 , the attribute has:m3 is redundant in the intent of class concept c2 since c2 also owns has:m1, whereas m3 is a super-concept of m1 in the method lattice.

In class diagrams, classes are associated to structural features (attributes) and behavioral features (operations and methods). In Figure 10 (top) the main elements of attribute and method description are presented: visibility (+, -and #); attribute types e.g. String, Point or Color which can be classes; return type; parameter type list; multiplicity for many-valued attributes (like color), the multiplicity is a set of integer intervals restricting the value number (for color, multiplicity 1..* expresses the fact that color has one or more value); static status (underlined feature); derived status (introduced by /). Figure 10 (bottom) also illustrates the main aspects of UML associations. An association is composed of at least two association ends. When it has a name (for example place order), the name is followed by a triangle which establishes the direction for reading this name: a person places an order and not the other way round. An association end is typically characterized by: a type (the class involved in this end), for example Person and Order are the two end types of the association place order; a visibility; a multiplicity; a navigability (shown through an arrow next to the type end); a white or black diamond which indicates an aggregation or a composition. An association end is sometimes provided with a role name which gives more accurate semantics to objects when they are involved in the link, e.g.
for a person in association © 7 0 7 0 Ê G . When the association owns variables and methods it is considered as an association class, e.g. Access is an association class that supports the variable passwd. Using the UML meta-model to guide the context construction The definition of UML is established by the UML meta-model, that is a model that defines the language for models. The UML meta-model is described through a subset of UML, and is given with well-formedness rules in the formal language OCL (Object Constraint Language), as well as with semantics in natural language. Part of this meta-model [12] relevant to our problem, that considers the classes and their features, is shown in Figure 11 . The meta-class Class specializes Classifier, and as such, inherits from the possibility to own Features (Attribute or Method). An Attribute includes the meta-attributes initialValue, multiplicity, visibility, changeable; it has a type via the meta-association that links it to Classifier. A Method has the meta-attributes body, isQuery, visibility, and is composed of an ordered set of Parameters. An Association is composed of several AssociationEnds which have a type which is a classifier. AssociationEnds are described by a type (a classifier), and several meta-attributes including isNavigable, isOrdered, aggregation and multiplicity.
As a meta-description of UML, the meta-model naturally contains the good abstractions for determining the right formal contexts: meta-classes are straightly interpreted as formal objects, while meta-attributes and ends of meta-associations are their formal attributes. Nevertheless, such an approach can lead to the manipulation of many tables of data, and to the use of descriptors that generate too numerous uninteresting concepts. Parameter for example is preferably included in the description of methods. Associations should be described by an ordered set of association ends, but if we consider only binary and directed associations, as often suggested in modeling [13] , we can avoid having a specific formal context for association end description. Conversely, if we want to inspect all possible generalizations of associations in the general case, a formal context describing association ends would be relevant. Notice that the algorithm may propose to factorize role names depending on the way the designer has named the associations: association names, role names or both. This corresponds to the formal attributes name, nameOrigin and nameDestination of the formal context on associations. The multiplicity on the side of the class @Authentication context is also properly factorized into a 1..* multiplicity. On the other hand, one may question the factorization of 0..1 and 1 multiplicities into * (it could have been factorized into 0..1) but this is an internal choice of the algorithm that could be fine-tuned.
We presented a new FCA-based technique (ICG) which processes several mutually related formal contexts and sketched its application to UML class diagram restructuring. Experiments on industrial-scale projects established the feasibility of our approach (execution time and semantic relevance of the results) and highlighted the crucial role of parameter tuning and appropriate user interface. A key track of improvement is the separation of formal attributes that guide the construction of new abstractions (e.g. names, types of attributes, association ends, etc.) from secondary ones that only help to increase the precision (e.g., multiplicity or navigability). Another current concern is the integration of a domain ontology into the ICG framework that should enable the comparison of symbolic names used by the designer. This is crucial for any automated reconstruction technique such as our, because terms are not uniformly used over UML diagrams, many synonymy, homonymy or polysemy situations occur. Although Objecteering offers an operational user interface for ICG there is a large space for improvement. First, designers that are FCA neophytes would benefit from an automated assistance in tool fine-tuning. Second, navigation and edition tools should help make the entire ICG process more interactive and thence more purposeful, e.g., by supporting run-time filtering of the discovered abstractions.
1+o (1) )-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here n is the number of nodes in the network and D is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized (1 + o (1) In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the "hitting set argument" commonly used for shortest paths computation in var- * A full version of this paper is available at http://arxiv.org/ abs/1504.07056 † This work was done in part while the author was visiting the Simons Institute for the Theory of Computing. The research leading to this work has received funding from the European ious settings, and (ii) a simple, deterministic, construction of an (n o (1) , o (1) )-hop set of size O(n 1+o (1) ). We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g. ruling sets [26] , source detection [39] , and partial distance estimation [38] . Our hop set construction also leads to single-source shortest paths algorithms in two other settings: (i) a (1 + o (1) every node to know how far it is from s. The unweighted version -the breadth-first search tree computation -is one of the most basic tools in distributed computing, and is well known to require Θ(D) time (e.g. [44] ). In contrast, the only available solution for the weighted case is the distributed version of the Bellman-Ford algorithm [5, 23] , which requires O(n) time to compute an exact solution. In 2004, Elkin [16] raised the question whether distributed approximation algorithms can help improving this time complexity and showed that any α-approximation algorithm requires Ω((n/α) 1/2 / log n + D) time [17] . Das Sarma et al. [12] (building on [45, 35] ) later strengthened this lower bound by showing that any poly(n)-approximation (randomized) algorithm requires Ω(n 1/2 / log n + D) time. This lower bound was later shown to hold even for quantum algorithms [19] .
Since running times of the formÕ(n 1/2 + D) 1 show up in many distributed algorithms (e.g. MST [36, 45] , connectivity [53, 46] , and minimum cut [43, 25] ) it is natural to ask whether the lower bound of [12] can be matched. The first answer to this question is a randomized O(
-time algorithm by Lenzen and Patt-Shamir [37] 2 . The running time of this algorithm is nearly tight if we are satisfied with a large approximation ratio. For a small approximation ratio, Nanongkai [42] presented a randomized ( 
The running time of this algorithm is nearly tight when D is small, but can be close toΘ(n 2/3 ) even when D = o(n 2/3 ). This created a rather unsatisfying situation: First, one has to sacrifice a large approximation factor in order to achieve the near-optimal running time, and to achieve a (1 + o(1)) approximation factor, one must pay an additional running time of D 1/4 which could be as far from the lower bound as n 1/8 when D is large. Because of this, the question whether we can close the gap between upper and lower bounds for the running time of (1 + o(1))-approximation algorithms was left as the main open problem in [42, Problem 7.1] . Secondly, and more importantly, both these algorithms are randomized. Given that designing deterministic algorithms is an important issue in distributed computing. This leaves an important open problem whether there is a deterministic algorithm that is faster than BellmanFord's algorithm, i.e. that runs in sublinear-time.
Our Results.
In this paper, we resolve the two issues above. We present a deterministic (1+o(1))-approximation O(n 1/2+o(1) +D 1+o(1) )-time algorithm for this problem (the o(1) term in the approximation ratio hides a 1/ polylog n factor and the o(1) term in the running time hides an O( log log n/ log n) factor). Our algorithm almost settles the status of this problem as its running time matches the lower bound of Das Sarma et al. up to an O(n o(1) ) factor. Since an α-approximate solution to SSSP gives a 2α-approximate value of the network's weighted diameter (cf. Section 2), our algorithm can (2 + o(1))-approximate the weighted diameter within the same running time. Previously, Holzer et al. [31] showed that for any > 0, a (2 − )-approximation algorithm for this problem requiresΩ(n) time. Thus, the approximation ratio provided by our algorithm cannot be significantly improved without increasing the running time. The running time of our algorithm also cannot be significantly improved because of the lower bound of Ω(n 1/2 / log n + D) [12] for approximate SSSP which holds for any poly(n)-approximation algorithm.
Using the same techniques, we also obtain a deterministic (1 + o (1) (1))-approximation of the diameter requiresΩ(n) time in the worst case [30] in the congested clique.
Our techniques also lead to a (non-distributed) streaming algorithm for (1 + o(1))-approximate SSSP where the edges are presented in an arbitrary-order stream, and an algorithm with limited space (preferablyÕ(n log W ), when edge weights are in {1, 2, . . . W }) reads the stream in passes to determine the answer (see, e.g., [40] for a recent survey). It was known thatÕ(n log W ) space and one pass are enough to compute an O(log n/ log log n)-spanner and therefore approximate all distances up to a factor of O(log n/ log log n) [22] (see also [21, 3, 20, 18] ). This almost matches a lower bound which holds even for the s-t-shortest path problem (stSP), where we just want to compute the distance between two specific nodes s and t [22] . On unweighted graphs one can compute (1 + , β)-spanners in β passes and O(n 1+1/k ) space [20] , and get (1 + )-approximate SSSP in a total of O(β/ ) passes. In 2006, McGregor raised the question whether we can solve stSP better with a larger number of passes (see [1] ). Very recently Guruswami and Onak [27] showed that a p-pass algorithm on unweighted graphs requiresΩ(n 1+Ω(1/p) /O(p)) space. This does not rule out, for example, an O(log n)-passÕ(n)-space algorithm. Our algorithm, which solves the more general SSSP problem, gets close to this: it takes O(n o(1) log W ) passes and O(n 1+o(1) log W ) space.
Our crucial new technique is a deterministic process that can replace the following "path hitting" argument: For any c, if we pickΘ(c) nodes uniformly at random as centers (typically c = n 1/2 ), then a shortest path containing n/c edges will contain a center with high probability. This allows us to create shortcuts between centers -where we replace each path of length n/c between centers by an edge of the same length -and focus on computing shortest paths between centers. This argument has been repetitively used to solve shortest paths problems in various settings (e.g. [54, 28, 15, 4, 48, 49, 13, 14, 41, 7, 37, 42] ). In the sequential model a set of centers of sizeΘ(c) can be found deterministically with the greedy hitting set heuristic once the shortest paths containing n/c edges are known [55, 33] . We are not aware of any non-trivial deterministic process that can achieve the same effect in the distributed setting. The main challenge is that the greedy process is heavily sequential as the selection of the next node depends on all previous nodes, and is thus hard to implement efficiently in the distributed setting 4 . In this paper, we develop a new deterministic process to pickΘ(c) centers. The key new idea is to carefully divide nodes intoÕ(1) types. Roughly speaking, we associate each type t with a value wt and make sure that the following properties hold: (i) every path π with Ω(n/c) edges and weight Θ(wt) contains a node of type t, and (ii) there is a set of O(n/c) centers of type t such that every node of type t has at least one center at distance o(wt). We define the set of centers to be the collection of centers of all types. The two properties together guarantee that every long path will be almost hit by a center: for every path π containing at least n/c edges, there is a center whose distance to some node in π is o(w(π)) where, w(π) is the total weight of π. This is already sufficient for us to focus on computing shortest paths only between centers as we would have done after picking centers using the path hitting argument. To the best of our knowledge, such a deterministically constructed set of centers that almost hits every long path was not known to exist before. The process itself is not constrained to the distributed setting and thus might be useful for derandomizing other algorithms that use the path hitting argument.
To implement the above process in the distributed setting, we use the source detection algorithm of Lenzen and Peleg [39] to compute the type of each node. We then use the classic ruling set algorithm of Goldberg et al. [26] to compute the set of centers of each type that satisfies the second property above. (A technical note: we also need to compute a bounded-depth shortest-path tree from every center. In [42] , this was done using the random delay technique. We also derandomize this step by adapting the partial distance estimation algorithm of Lenzen and Patt-Shamir [38] .)
Another tool, which is the key to the improved running time, is a new hop set construction. An (h, )-hop set of a graph G = (V, E) is a set F of weighted edges such that the distance between any pair of nodes in G can be (1 + )-approximated by their h-hop distance (given by a path containing at most h edges) on G = (V, E ∪F ) (see Section 2 for details). The notion of hop set was defined by Cohen [10] in the context of parallel computing, although it has been used implicitly earlier, e.g. [54, 34] (see [10] for a detailed discussion). The previous SSSP algorithm [42] was able to construct an (n/k, 0)-hop set of size kn, for any integer k ≥ 1, as a subroutine (in [42] this was called shortest paths diameter reduction 5 ). In this paper, we show that this subroutine can be replaced by the construction of an ( (1) ). Our hop set construction is based on computing clusters which is the basic subroutine of Thorup and Zwick's distance oracles [51] and spanners [51, 52] . It builds on a line of work in dynamic graph algorithms. In [6] , Bernstein showed that clusters can be used to construct an (n o (1) , o(1))-hop set of size O(n 1+o (1) ). Later in [29] , we showed that the same kind of hop set can be constructed by using a structure similar to clusters while restricting the shortest-path trees involved to some small distance and use such a construction in the dynamic (more precisely, decremental) setting. The con-struction is, however, fairly complicated and heavily relies on randomization. In this paper, we build on the same idea, i.e., we construct a hop set using bounded-distance clusters. However, our construction is significantly simplified, to the point that we can treat the cluster computation as a black box. This makes it easy to apply on distributed networks and to derandomize. To this end, we derandomize the construction simply by invoking the deterministic clusters construction of Roditty, Thorup, and Zwick [47] and observe that it can be implemented efficiently on distributed networks 6 . We note that it might be possible to use Cohen's hop set construction instead. However, Cohen's construction heavily relies on randomness and derandomizing it seems significantly more difficult.
We start by introducing notation and the main definition in Section 2. Then in Section 3 we explain the deterministic hop set construction in, which is based on a variation of Thorup and Zwick's clusters [51] In Section 4, we give our main result, namely the (
In that section we explain the deterministic process for selecting centers mentioned above, as well as how to implement the hop set construction in the distributed setting.
In this paper we consider weighted undirected graphs. For a set of edges E, the weight of each edge (u, v) ∈ E is given by a function w (u, v, E) .
Whenever we define a set of edges E as the union of two sets of edges E1 ∪ E2, we set the weight of every edge (u, v) ∈ E to w(u, v, E) = min(w(u, v, E1), w(u, v, E2)). We denote the weight of a path π in a graph G by w(π, G) and the number of edges of π by |π|.
Given a graph G = (V, E) and a set of edges F ⊆ V 2 , we define G∪F as the graph that has V as its set of nodes and E∪ F as its set of edges. The weight of each edge (u, v) is given by
We denote the distance between two nodes u and v, i.e., the weight of the shortest path between u and v, by d (u, v, G) . We define the distance between a node u and a set of nodes 
We denote the hop-distance between two nodes u and v, i.e., the distance between u and v when we treat G as an unweighted graph, by hop (u, v, G) . (u, v, G) . When G is clear from the context, we use D instead of D(G). We note that this is different from the weighted diameter, which is defined as WD(G) = max u,v∈V (G) d (u, v, G) . Throughout this paper we use "diameter" to refer to the hop diameter (as it is typically done in the literature).
Given any graph G = (V, E), any integer h, and ≥ 0, we say that a set of weighted edges F is an (h, )-hop set of G if
where H = (V, E ∪ F ). In this paper we are only interested in (n o (1) , o(1))-hop sets of size O(n 1+o(1) ). We refer to them simply as "hop sets" (without specifying parameters).
In our algorithm we will repeatedly use the following established weight-rounding technique [9, 55, 6, 41, 7, 42 ] to scale down edge weights at the cost of approximation. 
An important subroutine in our algorithm is a procedure for solving the source detection problem [39] in which we want to find the σ nearest "sources" in a set S for every node u, given that they are of distance at most γ from u. Ties are broken lexicographically. The following definition if from [38] 
{(d(u, v, G), v)|v ∈ S ∧ d(u, v, G) ≤ γ}((d(u, v, G), v) < (d(u, v , G), v ) ⇐⇒ (d(u, v, G) < d(u, v , G))∨(d(u, v, G) = d(u, v , G)∧v < v ) ,
Lenzen and Peleg designed a source detection algorithm for unweighted networks [39] . One can also run the algorithm on weighted networks, following [38, proof of Theorem 3.3], by simulating each edge of some weight L with an unweighted path of length L. Note that nodes in the paths added in this way are never sources. We also use another source detection algorithm: Roditty, Thorup, and Zwick [47] also solve a variant of the source detection problem with γ = ∞ in their centralized algorithm for computing distances oracles and spanners deterministically. They reduce the source detection problem to a sequence of single-source shortest paths computations on graphs with some additional nodes and edges. Their algorithm can easily be generalized to arbitrary γ. Theorem 2.4 (implicit in [47] The classic result of Goldberg et al. [26] shows that in the distributed setting, for any c ≥ 1, we can compute a (c, cλ)-ruling set deterministically in O(c log n) rounds, where λ is the number of bits used to represent each ID in the network. Since it was not explicitly stated that this algorithm works in the CONGEST model, we sketch an implementation of this algorithm in the full version of this paper (see [ 
In this section we present a deterministic algorithm for constructing an (n o (1) , o(1))-hop set.
The basis of our hop set construction is a structure called cluster introduced by Thorup and Zwick [51] . Consider an integer p such that 2 ≤ p ≤ log n and a hierarchy A of sets of nodes (Ai) 0≤i≤p such that A0 = V , Ap = ∅, and A0 ⊇ A1 ⊇ . . . ⊇ Ap. We say that a node v has priority i if v ∈ Ai \ Ai+1 (for 0 ≤ i ≤ p − 1). For every node v ∈ V we define the restricted cluster up to distance R as
where i is the priority of v. For every node v ∈ V , every 0 ≤ i ≤ p − 1, and every R ≥ 1, we define the restricted i-bunch up to distance R as
Clusters and bunches are dual concepts, i.e., u ∈ C (v, A, R, G) if and only if v ∈ 0≤i≤p−1 Bi(u, A, R, G). In our hop set construction we will use sets of edges obtained from the clusters in the straightforward way: every node has an edge to each node in its cluster. Note that the size of such a set of edges (which in turn influences the size of our hop set) is at most v∈V |C (v, A, R, G)|, the size of all clusters, which is equal to
A second motivation for keeping cluster sizes small is that in the models of computation considered in this paper also the time needed for constructing all clusters will depend on both R and the size of the clusters. Given a hierarchy of sets A, the clusters can be computed as follows. First, for every 1 ≤ i ≤ p − 1, compute the distance of every node to its closest node in Ai by constructing a shortest-path tree in a modification of the graph where all nodes of Ai are contracted to a single source node. Second, compute the cluster of every node v by constructing a shortest-path tree up to distance R from v under the following restriction: only let a node u to join the tree if d(u, v, G) < d(u, Ai+1, G) . This additionally computes the distances between v and every node in its cluster.
If randomization is allowed, bunches of small size can be obtained as follows [51] : If we set A0 = V and Ap = ∅, and for each 0 ≤ i ≤ p − 2 we obtain Ai+1 by picking each node from Ai with probability (ln n/n) 1/p , then the expected size of each i-bunch is at most n 1/p and thus the expected size of all bunches (and hence all clusters) is O(pn 1+1/p ). Roditty, Thorup and Zwick [47] also give a deterministic algorithm for setting A such that the resulting clusters will have the same asymptotic size. Following their algorithm we iteratively compute Ai's such that the restricted i-bunch of every node will have size q =Õ(n 1/p ). We set A0 = V and, given Ai, we construct Ai+1 as follows. Using a source detection algorithm, we first determine for each node v the set L(v, Ai, R, q, G), which among all nodes of Ai at distance at most R from v contains the q closest ones. We now find a set Ai+1 ⊆ Ai of size |Ai+1| ≤ |Ai|/n 1/p such that, for every node v, L(v, Ai, R, q, G) contains at least one node of Ai+1. This restricts the size of each i-bunch to q. Finding such a set Ai+1 of minimum size is exactly the hitting set problem. By probabilistic arguments there exists a hitting set of size |Ai|/n 1/p . Once all sets L(v, Ai, R, q, G) are known, we can compute an approximation of the minimum hitting set using a greedy heuristic. The guarantees of our algorithm for computing clusters can be summarized as follows.
A = (Ai) 0≤i≤p , where V = A0 ⊆ A1 ⊆ · · · ⊆ Ap = ∅, such that v∈V |C (v, A, R, G)| = O(pn 1+1/p ).

We now explain how to construct the hop set. We omit many details, which can be found in the full version of the paper.
Assume we are given a hierarchy of sets A and the corresponding clusters and consider the set of edges F containing for every node u edges to all nodes v in its cluster with weight equal to the distance from u to v. Using an analysis similar to [52] and [29] we can show that if the number of priorities p is small enough the following holds after adding the edges of F to the graph: for every ∆ sufficiently smaller than the distance range R we can, for all pairs of nodes u and v find a path from u to v with O ((p + 1) d(u, v, G) /∆ ) edges that overestimates the distance from u to v by a multiplicative error of (1 + ) and an additive error of n o (1) .
and parameters ∆ ≥ 1 and 0 < ≤ 1. Then F has sizẽ O(pn 1+1/p ), where p = (log n)/(log (4/ )) , and in the graph H = G ∪ F , for every pair of nodes u and v, we have
Procedure 1: HopReductionAdditiveError(G, ∆, ) Input: Graph G = (V, E) with non-negative integer edge weights, ∆ ≥ 1, 0 < ≤ 1 Output: Hop-reducing set of edges F ⊆ V 2 as specified in Lemma 3.2
Consider a shortest path π from u to v with h edges and weight R ≥ ∆. With the hop reduction of Procedure 1 we can compute a set of edges that reduces the number of hops from u to v toÕ(R/∆) (at the cost of approximating the distance). This is not yet sufficient for computing the desired hop set because R might be as large as nW . Instead we would like to reduce the number of hops toÕ(h/∆) as h can be at most n. We can achieve this by using the weight-rounding technique of Lemma 2.1: for every distance range of the form 2 j . . . 2 j+1 , we scale down the edge weights by a certain factor ρj (depending on j, h, and ) and run Procedure 1 on this scaled-down version of G to obtain a set of edges Fj. The set j Fj will then provide a reduction toÕ(h/∆) hops. Additionally, if h is sufficiently larger than ∆, then the additive error inherent in the hop reduction of Procedure 1 can be counted as an additional multiplicative error of . 
We now use the following iterative approach in which we repeatedly apply the hop reduction of Procedure 2 with ∆ = n o (1) . We first compute a set of edges F1 that reduces the number of hops in G from h0 = n to h1 = h0/∆ = n/∆. We then add all these edges to G and consider the graph HopReduction(G, ∆, h, , W ) Input: Weighted graph G = (V, E) with integer edge weights from 1 to W , ∆ ≥ 1, 0 < ≤ 1 Output: Hop-reducing set of edges F ⊆ V 2 as specified in Lemma 3.3
We apply Procedure 2 again on H1 to compute a set of edges F2 that reduces the number of hops in H1 from h1 to h1/∆ = n/∆ 2 . Now observe that the set of edges F1 ∪ F2 reduces the number of hops in G from n to n/∆ 2 and in general, after i iterations, the number of hops is reduced to n/∆ i . This process stops when the number of hops reaches the bound h ≥ n 1/p ∆/(p + 2) of Lemma 3.3. We show that by repeating this process p = Θ( log n/ log (1/ )) times we can compute a set F that reduces the number of hops to n/∆ p = n o (1) .
Input: Weighted graph G = (V, E) with integer edge weights from 1 to
2 be the set of edges computed by Procedure 3 for a weighted graph G = (V, E) and a parameter 0 < ≤ 1. Then, for p = (log n)/(log (108
The main computational cost for constructing the hop set comes from computing the clusters in Procedure 1, which is used as a subroutine repeatedly. If 1/ ≤ polylog n, then (1) and Procedure 3 will compute an (n o (1) , o(1))-hop set of size O(n 1+o(1) log W ); it will performÕ(log W )
cluster computations each with p = Θ( log n/ log (1/ )) priorities up to distance range O(n o(1) ) on a graphs of size O(m 1+o(1) log W ). The concrete time complexity depends on the model of computation we want to consider (see the implementation in Section 4.2). As for each cluster computation the priorities are set deterministically, our whole algorithm is deterministic.
Our algorithm consists of two parts, presented in two sections: In Section 4.1 we give a deterministic algorithm for constructing an overlay network such that it is sufficient to compute SSSP on this network. A randomized version of this result was given in [42] . In Section 4.2 we present a more efficient algorithm for computing SSSP on this overlay network using Procedures 1 to 3 from before. We finish the computation in the same way as in [42] .
An overlay network (aka landmark or skeleton) [42, 50, 37] is a virtual network G of nodes and "virtual edges" that is built on top of an underlying real network G; i.e.,
such that the weight of an edge in G is an approximation of the distance of its endpoints in G. The nodes in V (G ) are called centers. Computing G means that after the computation every node in G knows whether it is a center and knows all virtual edges to its neighbors in G with their weights. We show in this subsection that there is aÕ(n 1/2 )-time algorithm that constructs an overlay network G ofÕ(n 1/2 ) nodes such that a (1+o(1))-approximation to SSSP in G , can be converted to a (1 + o(1) )-approximation to SSSP in G, as stated formally below.
Theorem 4.1. Given any weighted undirected network G and source node s, there is anÕ(n 1/2 )-time deterministic distributed algorithm that computes an overlay network G and some additional information for every node with the following properties.
• Property 1: |V (G )| =Õ(n 1/2 ) and s ∈ V (G ).
• Property 2: For every node u ∈ V (G), as soon as
In Theorem 4.2 of the full version of [42] , the following randomized algorithm that achieves the result above was given 7 . In the first step of [42] , the algorithm selects each node to be a center with probabilityΘ(1/n 1/2 ) and also makes s a center. By a standard "hitting set" argument (e.g. [54, 13] ), any shortest path containing n 1/2 edges will contain a center with high probability. Also, the number of centers isΘ(n 1/2 ) with high probability. In the second step, the algorithm makes sure that every node v knows theΘ(n 1/2 )-hop distances between v and all centers using a light-weight bounded-hop single-source shortest paths algorithm from all centers in parallel combined with the random delay technique to avoid congestion.
We derandomize the first step as follows: In Section 4.1.1 we assign to each node u a type t(u) such that every path π containing n 1/2 edges contains a special node u with 2 t(u) =  O( w(π, G) ). This is comparable to the property obtained from the hitting set argument, which would be achieved if we made the special node of every path a center. However, this might create too many centers. Instead we select some nodes to be centers using the ruling set algorithm, as described in Section 4.1.2, which outputs a small set of centers and we can show that every special node u is at distanceÕ (2 t(u) ) to one of the centers. Thus, while we cannot guarantee that the path π contains a center, we can guarantee that it contains a node that is not far from a center. To derandomize the second step, we use the recent algorithm of Lenzen and Patt-Shamir [38] for the Partial Distance Estimation (PDE) problem together with the above Procedures 1 to 3, as explained in Section 4.1.2. The parameters used by our algorithm in the following are = 1/ log
, and k = (1 + 2/ )k. Recall that λ is the number of bits used to represent each ID in the network.
For any integer i, we let ρi = Proof. Let l = |π|/h ≥ 1/ and let x and y denote the endpoints of π. Partition π into the path πx consisting of the (l − 1)h edges closest to x and the path πy consisting of the |π| − (l − 1)h edges closest to y. Further partition πu into l − 1 non-overlapping subpaths of exactly h edges, and expand the path πy by adding edges of πx to it until it has h edges. Thus, there are now l paths of exactly h edges each and total weight at most 2w(π, G). It follows that there exists a subpath π of π consisting of exactly h edges and weight at most 2w(π, G)/l ≤ 2 w(π, G). Let u and v be the two endpoints of π and let i be the index such that
Computing Types of Nodes.
To compute t(u) for all nodes u, it is sufficient for every node u to know, for each i, whether |B(u, Gi, h )| ≥ h. We do this by solving the (S, γ, σ)-detection problem on Gi with S = V (G), γ = h and σ = h, i.e., we compute the list L(u, S, γ, σ, G) for all nodes u, which contains the σ nodes from S that are closest to u, provided their distance is at most γ. By Theorem 2. 
Having computed the types of the nodes, we compute ruling sets for the nodes of each type to select a small subset of nodes of each type as centers. Remember the two properties of an (α, β)-ruling set T of a base set U : (1) all nodes of T are at least distance α apart and (2) each node in U \ T has at least one "ruling" node of T in distance β. We use the algorithm of Theorem 2.6 to compute a (2h + 1, (2h + 1)λ))-ruling set Ti for each graph Gi where the input set Ui consists of all nodes of type i. The number of rounds for this computation is O(h log n) =Õ(n 1/2 ). We define the set of centers as V = ( i Ti) ∪ {s}. Property (1) allows us to bound the number of centers and by property (2) the centers "almost" hit all paths with n 1/2 edges. We prove the following lemma in the full version of our paper. 
Next, we compute for every node u and every center v a valued(u, v) that is a (1 + o(1) The goal is that each node u knowsd(u, v) for all centers v. In particular we also computed(u, v) for all pairs of centers u and v. As in Section 4.1.1, we do this by solving the source detection problem on a graph with rounded weights. 
Equations (1) and (3) it then follows that
We define our final overlay network to be the graph G where the weight between any two centers u, v ∈ V (G ) iŝ d(u, v) (as computed in Section 4.1.2). Additionally, for every node u ∈ V (G) we store the value ofd(u, v) to all centers v ∈ V (G ). All steps for computing G above takeÕ(n 1/2 / ) rounds and |V (G )| =Õ(n 1/2 ). It is thus left to prove Property 2 in Theorem 4.1. This is similar to the standard path-hitting argument (see the full version for details): The shortest path from node u to s can be partitioned into subpaths of at most n 1/2 hops with nodes u1, u2, . . . , ut hitting these paths (where t ≤ n 1/2 and ut is nearest to u) such that each ui has a center vi nearby (by Lemma 4.3). The distance from vi to vi+1 (1 + o (1))-approximates the distance between ui and ui+1.
We now show how to simulate the hop set algorithm presented in Section 3 on an overlay network G , whose set of nodes V (G ) are the centers, to compute a hop set of G (not of G) and how to compute approximate shortest paths from s in G using the hop set. Throughout the algorithm we will work on overlay networks with the same nodes as G , but which might have different edge weights as, e.g., Procedure 2 calls Procedure 1 and Procedure 1 calls Clusters on overlay networks with modified edge weights. Thus, we will use G to refer to an overlay network on which the subroutines run.
Computing Bounded-Distance Single-Source Shortest Paths.
We will repeatedly use an algorithm for computing a shortest-paths tree up to distance R rooted at s on an overlay network G , where R = O(n o (1) ). At the end of the algorithm every center knows this tree. We do this in a breadth-first search manner, in R + 1 iterations. Like in Dijkstra's algorithm, every center keeps a tentative distance δ(s, u) from s and a tentative parent in the shortest-paths tree, i. to all other centers a message (u, δ(s, u), v) where v is the parent of u. Using this information, every center u will update ("relax") its tentative distance δ(s, u) and its tentative parent.
Clearly, after the L th iteration, centers that have distance L+1 from s will already know their correct distance. Thus, at the end of the last iteration every center knows the shortest- 9 More precisely, there is a designated center (e.g. the center with lowest ID) that aggregates and distributes the messages (via upcasting and downcasting on the breadth-first search tree of the network G), and tells other centers when the iteration starts and ends. Computing Priorities, Clusters, and Hop Sets.
To compute the hop set of G we simulate the algorithm of Section 3 on the overlay network. We sketch the main idea here and refer to the full version of our paper for details. The main idea is that the main computational cost for computing the hop set comes from repeatedly calling its subroutine for computing bounded-distance clusters. We observe that computing these clusters on the overlay network basically needs the same analysis as the bounded-distance shortestpath trees discussed above. In particular, for computing the priorities of centers deterministically, we use the source detection algorithm of Theorem 2.4 as a subroutine, which reduces the problem to computing a series of bounded-depth shortest-path trees. Technically, this reduction would require us to add some nodes to the overlay network, which we can avoid by simulating the behaviour of the additional nodes by centers that are already present in the network. We can argue that a single cluster computation takesÕ(RD + N 1+o (1) ) rounds and thus the hop set can be computed deterministically in O(n o(1) D + n 1/2+o(1) ) rounds.
Final Steps. (1) ) rounds by the same method as in Lemma 4.6 in the full version of [42] . The details are given in the full version of our paper. 
[5], [6] , [7] , [8] ). Terrestrial SLAM algorithms often assume within the AQUA robot. We combine range information exa predictable vehicle odometry model in order to assist tracted from stereo irnagery with 3DOF orientation from an in the probabilistic association of sensor information with inertial measurement unit (IMU) within a SLAM algorithm salient visual features. The underwater domain necessitates to accurately estimate dense 3D models of the environment solutions to the SLAM problem which are more dependent and the trajectory of the sensor. It is important to note that upon sensor information than is traditional in the terrestrial although the AQUASENSOR does not process the collected domain. This has prompted recent research in robotic vehicle data in realtime, the algorithms used to analyze the data are design, sensing, localization and mapping for underwater being developed with realtime performance in mind. vehicles (see [9] , [10] , [11] , [12] ).
The AQUASENSOR is sealed within a custom underwater housing that permits operation of the device to a depth II. AQUA AND THE AQUASENSOR of 30 metres. The sensor is operated either via an IEEE One vehicle that requires robust and versatile sensing 802.1 Ig wireless link for surface operation, or via waterproof strategies to extract 3D models of the environment is the switches mounted on the exterior of the sensor. The wireless AQUA robot (see Figure 1 and [13] ). The AQUA robot is interface onboard the sensor provides status information a visually guided autonomous robot developed through a and sarnple imagery to offboard devices. LEDs mounted on collaboration between researchers at Dalhousie University, the exterior of the sensor provide status information to the McGill University, and York University. The vehicle is operator. The LEDs and waterproof switches are the only a hexapod capable of amphibious operation. On land, its communication possible with the device when submerged.
legs provide foot-ground contact that propels the vehicle. Although designed primarily to be integrated within the Underwater these same legs act as flippers or fins to drive AQUA housing, the AQUASENSOR was also designed to the vehicle both along the surface of the water and at depths be deployed independently of the AQUA vehicle. Although of up to 30 metres. In order to meet the sensing needs of bulky when handled in the terrestrial domain, it is easily the AQUA robot a series of visual-inertial sensors [14] have operated by a single diver underwater (Figure 2(b) ). been constructed. The primary goal of these sensors is to III. OBTAINING LOCAL SURFACE MODELS collect high framerate multi-camera video imagery coupled Upon return to the surface, data from the AQUASENSOR with synchronized time-stamped inertial information. This is offloaded to higher performance computers and a larger data is later processed off-line to obtain 6DOF ego-motion disk array for processing. Recovering accurate depth from and 3D models of the environment. Although we describe stereo imagery is performed by leveraging the optimized the standalone sensor package here, the long-term goal is sum-of-squared differences algorithm implemented in the to incorporate the sensing hardware and algorithms into the Point Grey Triclops library'. This provides a dense set of 3D vehicle itself.
points per acquired image frame. To estimate the motion of
In order to permit the sensor package to be operated the camera, point sets from different times are combined into independently of the robot itself, an independent sensor a common reference frame. The change in the 6DOF position package known as AQUASENSOR (Figure 2 ) has been and orientation of the sensor between frames is estimated by developed. The hardware design goal for the AQUASEN-tracking interesting features temporally, using only features SOR is to provide a compact, fully-contained unit that is that correspond to estimated 3D locations in both framnes. (1) First, "good" features are extracted from the reference i=1 camera at time t using the Kanade-Lucas-Tomasi feature The rotation, R(.), and scale, s, are estimated using a linear tracking algorithm (see [15] , [16] ) and are tracked into least-squares approach (detailed in [18] ). After estimating the subsequent image at time t + 1. Using the disparity the rotation, the translation is estimated by transfomning the map previously extracted for both time steps, tracked points centroids into a common frame and subtracting. that do not have a corresponding disparity at both time t The final step is to refine the rotation and translation and t + 1 are eliminated. Surviving points are subsequently using a noninear Levenberg-Marquardt minimization [19] triangulated to determtne the metric 3D points associated over six parameters. For this stage we parameterize the with each disparity.
In underwater scenes, many objects and points are visually and translation parameters by minimizing the transformation similar and thus many of the feature tracks will be incorrect. error Dynamic illumination effects, aquatic snow, and moving n objects (e.g. fish) increase the number of spurious points
(2) that may be tracked from frame-to-frame. To overcome these problems, we employ robust statistical estimation techniques to label the feature tracks as belonging to either a static or In practice, we find that the minimization takes few iterations non-static world model. This is achieved by estimating a to minimize the error to acceptable levels and as such rotation and translation model under the assumption that the does not preclude realtime operation. This approach to pose scene is stationary. The resulting 3D temporal correspon-estimation differs from the traditional Bundle-Adjustment dences are associated with stable scene points for the basis approach [21] in the structure-from-motion literature in that of later processing.
it does not refine the 3D locations of the features as well
We represent the camera orientation using a quatemion as the trajectory. We chose not to refine the 3D structure to and compute the least-squares best-fit rotation and trans-limit the number of unknowns in our minimization and thus lation for the sequence in a two stage process. First, provide a solution to our system more quickly. 
The reconstruction algorithm is summarized below: two sources of error in the estimate of 6DOF pose. First, the 1) Perform stereo algorithm and extract 3D point cloud point-cloud registration computed from feature tracks can at time t never be perfect. As such, there is always a small residual 2) Track salient features from time t to t -1 error in the registration per frame which accumulates over 3) Prune 2D features to only the ones with a correspondtime. Second, the intrinsic camera parameters are not known ing 3D point perfectly and any error in these parameters introduces an 4) Estimate 3D vision-only pose change using RANSAC, error in each 3D point. In particular, the radial distortion CAM qb, Tt estimate of the lens is prone to error and as a result the 5) Compute IMUq6 and qt as above per-point error is non-uniform over the visual field. This 6) Refine qt, Tt using Levenberg-Marquardt minimization can introduce an artificial surface curvature in the 3D point for a few iterations (2-3) clouds which is subtle but noticeable when many frames are 7) Apply the pose to the point cloud and add the point registered. This effect can be seen in Figure 3(a) . Here, the cloud to the octree data structure registration error is small and the points line up very well 8) Extract the surface mesh using a constrained elastic creating a visually "correct" model when viewed closely, surface-net algorithm [22] or the Marching Cubes however after registering many frames it can be seen that algorithm [23] there is an introduced curvature to the recovered surface. D. Local surface models To help counteract this effect, we utilize the 3DOF IMU to provide more information about the orientation of the device.
The IMU provides us with a quatemion representing absolute accuracy of the reconstruction system and to create 3D orientation in 3D space. We enforce the fact that the change models of real-world objects in the field. Results from in orientation as computed from the visual system must be field experiments near Holetown, Barbados show the reconconsistent with the change in the absolute orientation of struction of a coral bed and sections of a sunken barge the device. This is accomplished by transformning the IMU lying in the Folkstone Marine Reserve. Sample qualitative orientation change, reconstructions from underwater sequences are shown in Figure 4 . As in the 2D case, the problem of segmenting a 3D range of applications where the raw data must be segmented dataset can be expressed as a graph cut problem, with the cut into semantically important structures for later processing. dividing the foreground voxels from the background voxels.
Consider the recovered surface model shown in Figure 4 . The 3D segmentation process begins by converting the input For some applications -such as investigating the cause of 3D polygon or point cloud representation into a discrete 3D the sinking -the surface that belongs to the wreck may be voxel grid. A graph is created, where a node in the graph of interest while for other applications -such as monitoring is used to represent each non-empty voxel. Adjacent nonecosystem health -the surface that belongs to the coral may empty voxels are considered to be connected nodes. A sink be more salient. For these and similar applications automatic node and a source node are added to the graph with edges and semi-automatic tools are required to segment the dataset to each voxel node. Weighting heuristics are applied to the into appropriate components. It is interesting to observe that voxel graph in a similar fashion to that performed on the this problem is not unique to the 3D environment. through the 3D voxel scene. An arbitrary number of voxels EDGE WEIGHT CALCULATIONS FOR VOXEL GRAPH. may be marked by the user and the view of the 3D scene can be translated and rotated to allow the user to mark voxels that may be hidden in some views. In addition to each voxel possessing a color value, a voxel has a state attribute. This being more likely to be marked as foreground, and similarly state attribute can be one of the following four values: empty, for background. colored, foreground, or background.
A penalty term is assigned to adjacent nodes along the
The task of marking the voxels is simply an instance of segmentation boundary proportional to the difference tfl the common problem of 3D ray-picking. When a user wishes color The edge weight between these adjacent nodes thus to mark a voxel, the 2D window-coordinate of the mouse increases in proportion with the color similarity of the two position is converted into a 3D ray directed into the 3D voxel nodes, so that similar color nodes are less likely to be on grid. Each voxel intersected along the ray into the grid is different sides of the segmentation boundary. A small color tested. The first non-empty voxel encountered is tagged. difference between adjacent voxel nodes results in a large A graph is then constructed in which a node exists in the weight for their shared edge, and a large color difference graph for each non-empty voxel and edges are constructed results in a small weight.
between nodes that represent adjacent cells in the voxel Table I nodes that are connected to the source in the resulting cut are labelled as foreground nodes, and all nodes that are a) Create a graph node.
connected to the sink are labelled as background nodes. The b) Create an edge from this node to each node that dataset is now fully labelled and can be redisplayed using corresponds to an adjacent voxel. this semantic information. c) Create an edge from the source terminal node to Once the nodes in the graph have been marked the surface this node.
reconstruction can be performed. Points contained within d) Add an edge from this node to the sink terminal voxels with corresponding graph nodes marked as foreground node. are identified as foreground points. The remaining points If the node is tagged as foreground, then the edge from the are background points. A variation of the Marching Cubes source terminal is assigned a weight of oo. If the node is algorithm [23] is applied to the foreground points to generate tagged as background, then the edge from this node to the a 3D texture-mapped triangle mesh representation of the sink terminal is given a weight of oc.
foreground object. Graph edge weights are established so that edges connecting similar voxels have a high weight, while edges B Example segmentation connecting dissimilar voxels have a low weight. Similar to Figure 6 shows the results of segmenting a coral growth the method used by [24] the colors in the foreground and from the 3D mesh recovered from a wreck in the Folkstone background seeds are clustered using the k-means method Marine Reserve, Barbados. The growth in the lower left [25] .
hand view of Figure 6 (a) was segmented by having the For each node in the graph, the minimum distance from its user identify sample regions of the mesh as corresponding color to the foreground and background colors is computed. to either foreground or background (Figure 6(b) ). The 3D This information regarding the likelihood of a node being Lazy Snapping algorithm was then used to automatically foreground or background is represented as the weight be-identify regions as being foreground (here the coral growth) tween the node and the source and sink nodes. This has the or the background (everything else). Elements of the raw effect of making nodes with similar colors to the foreground point cloud that correspond to the foreground volume were then used to construct a textured mesh surface using the be used to overcome many of these difficulties. Results from Marching Cubes algorithm. The resulting segmented coral land-based experiments demonstrate the accuracy of our growth is shown in Figure 6(c)-(d 
With the advent of Intelligent Transportation System (ITS) [1] , automobile companies have started deploying more Electronic Control Units (ECUs) in vehicles for engine management, air bag controlling, automatic brake systems and air condition control. Along with these enhancements the increase in vehicles on roads and streets has also made it compulsory to implement safety applications for vehicles. Vehicular Ad Hoc Networks (VANETs) [2] are gaining scope because they are providing numerous applications like traffic safety, driver assistance, entertainment, internet access, automatic toll payment and many more for roadside environments.
Vehicular Ad Hoc Networks (VANETs) have similarities to Mobile Ad Hoc Networks (MANETs) like random topology and short communication range. These two characteristics depict that messages could not be directly delivered to destinations rather a message shall be routed by intermediate nodes to given destination. So routing protocol is very important in Copyright ⓒ 2014 SERSC both VANET and MANET environments and almost having same requirements with some changes. But the second side of coin, there are some major differences which are also available for these two networks. In Mobile Ad Hoc Networks (MANETs) environments mobile nodes can move in any direction without any geographical and spatial constraints. But in Vehicular Ad Hoc Networks (VANETs) mobile nodes have to follow some patterns because of roads, streets and buildings in their way of movement. Second difference in VANETs is that the connectivity among vehicles, size of network and speed of node are highly dynamic as compared to MANETs.
Thus by doing some little changes to MANET routing protocols, these protocols can be used for better performance in VANETs. This paper has presented new routing protocol for VANETs by modifying the existing MANET routing protocol AODV for achieving better performance in terms of routing overheads, no. of advertisements and packet delivery ratio. Second section of this paper highlights literature survey which explains the performance comparison of various MANET routing protocols. Third section of this paper give details regarding MANET routing protocols, section four elaborates new routing protocol AODV-FN (Forget Neighbors), design and implementation of new routing protocol is given in next section. Simulation results for various parameters of network performance are given in the following section and final section contains conclusion and future scope of this research.
Asma et al. [3] , compared DSR, AODV and DSDV. DSDV has higher routing load than AODV and DSR but has lower throughput than both of these. AODV and DSR have performed well in all comparisons but in some cases AODV outperforms DSR. But DSR was better when evaluated in terms of average end to end delay. Changing the packet size has affected the performance of AODV and DSR but had no effect on DSDV. Authors of Monarch project [4] have evaluated DSR, DSDV, AODV and TORA in terms of periodic advertisements, source routing, on demand route discovery and hop by hop routing. They have observed that DSR performs well for all parameters.
In [5] , authors compared AODV and DYNMO protocols for path optimally, routing overheads, packet delivery ratio, throughput and end to end delay. In these comparisons DYNMO shows better performance than AODV because it can handle different traffic patterns and mobility conditions. Paul et al., [6] , have done analysis of AODV and DSR in VANET environment in terms of node speed and node density. Higher packet delivery ratio (PDR) has been recorded in lower density and lower speed. Packet loss has been also decreased. Author compared AODV and DSR with respect to CBR and TCP. Packet delivery ratio of AODV is average under high speed conditions.
Three main classes of routing protocols which are available for MANETs: first, proactive routing protocols; second is reactive routing protocol and last one is hybrid routing protocols.
In this category of routing protocols a table is maintained for every reachable node in the network. These protocols update this table time to time. Advantage of this kind of routing protocol is that whenever a node wants to send data it can find path to destination node very easily. The examples of these kinds of protocols are Fisheye State Protocol (FSR) [7] , Optimized Link State Routing (OLSR) [8] and Topology Dissemination Based on Reverse Path Forwarding (TBRPF) [9] .
Optimized Link State Routing (OLSR) is routing protocol in which node routing table maintains paths to all nodes of network and at the time of change in topology it changes the tables according to change occurred. So due to this characteristic of this protocol it is not appropriate for highly dynamic VANET environments. On the other hand Fisheye State Protocol (FSR) is good for VANET environment because it does not send message on link failure. But major drawback of FSR is its processing time and storage needs for routing table. In Topology Dissemination Based on Reverse Path Forwarding (TBRPF) each node periodically updates the paths to other nodes by sharing control messages to each other. Problem in this protocol is flooding of control message whenever a change occurs.
