# Should we be Pre-Training ? Exploring End-Task Aware Training In Lieu of Continued Pre-training

This repository contains the source code for the paper [Should we be Pre-Training ? Exploring End-Task Aware Training In Lieu of Continued Pre-training](https://openreview.net/forum?id=2bO2x8NAIMB), by Lucio M Dery, Paul Michel, Ameet Talwalkar and Graham Neubig (ICLR 2022).

---

<p align="center"> 
    <img src="https://github.com/ldery/ATTITTUD/blob/main/eatmt" width="800">
</p>

## Links

1. [Paper](https://openreview.net/forum?id=2bO2x8NAIMB)
2. Bibtext :
```
@inproceedings{
dery2022should,
title={Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative},
author={Lucio M. Dery and Paul Michel and Ameet Talwalkar and Graham Neubig},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=2bO2x8NAIMB}
}
```

## Installation Instructions


## Running

### To obtain results on sample datasets

#### Baseline 


##### Ours 
